---
title: "Hvad er AI mat?"
image: "../images/logo.png"
description: "En kort beskrivelse af indholdet på aimat.dk -- henvendt til lærerne."
---

Hvad er **AI mat**? Først og fremmest er **AI mat** skrevet til brug i matematikundervisningen i gymnasiet. Fokus har været på STX, men andre gymnasiale uddannelser vil også kunne gøre brug af indholdet her på siden.


Kort fortalt handler **AI mat** om al den spændende gymnasiematematik, som ligger bag kunstig intelligens! Til gengæld handler **AI mat** *ikke* om at lære eleverne, hvordan de får en kunstig intelligens til at løse deres matematikopgaver (det kan de vist fint selv finde ud af)! Det har helt overordnet været et mål at åbne små vinduer ind til den kæmpestore \"black box\", som kunstig intelligens er for mange.

Alt indhold på **AI mat** henvender sig til eleverne og er således forsøgt skrevet i et sprog, som de kan forstå. Den eneste undtagelse herfra er sider eller områder markeret med \"Til lærerne\".


Siden består af følgende fire dele:

* [Undervisningsforløb](../undervisningsforlob/index.qmd){target="_blank"}: Disse er til brug i den daglige undervisning. Hvert forløb henvender sig direkte til eleverne med videoer, tekst og opgaver, som de arbejder sig igennem.
* [Noter](../noter/index.qmd){target="_blank"}: Disse er længere noter (i mange tilfælde med videogennemgang af teksten), som danner baggrund for forløbene, og som er velegnede til selvstudie i forbindelse med SRP.
* [SRP](../srp/index.qmd){target="_blank"}: Konkrete forslag til, hvordan vores materiale kan bruges i forbindelse med SRP – herunder konkrete forslag til, hvad der kan arbejdes med i forskellige fag udover matematik.
* [AI apps](../apps/index.qmd){target="_blank"}: Her har vi samlet en række AI apps, som blandt andet kan bruges til at træne en kunstig neuron og til at lave logistisk regression.

Nedenfor skriver vi lidt mere om opbygningen af undervisningsforløbene og noterne.

Leder man efter noget specifikt, kan vi anbefale vores \"Søg\"-funktion øverst til højre på alle sider.


## Undervisningsforløb

Under [undervisningsforløb](../undervisningsforlob/index.qmd){target="_blank"} har vi samlet en lang række eksempler på undervisningsforløb, som på den ene eller den anden måde introducerer centrale begreber og problemstillinger i forbindelse med kunstig intelligens. Alle forløb er kategoriseret efter niveau (A, B eller C), og nogle forløb har også et andet fag som kategori, hvis vi tænker, at et tværfagligt samarbejde vil kunne give mening. De fleste af forløbene er kategoriseret som **korte** forløb, hvilket betyder, at vi tænker, at forløbet kan afvikles på 1-3 undervisningstimer. Derudover er nogle få af forløbene kategoriseret som **lange** forløb. Disse forløb er tænkt som et bud på supplerende stof, og alle lange forløb inkluderer derfor også et eller flere bud på tilhørende eksamensspørgsmål til den mundtlige prøve.

   I alle forløb vil der i starten være angivet, hvilke **forudsætninger** forløbet kræver, samt et estimeret bud på et **tidsforbrug**. Derudover er et overordnet **formål** for forløbet også anført. 
   
   Alle forløb henvender sig direkte til eleverne. Og med mindre andet er angivet under \"Forudsætninger og tidsforbrug\", så kan forløbene bruges uafhængigt af hinanden. Dette er tilfældet for langt størstedelen af forløbene.
   
   I nogle forløb skal eleverne læse et afsnit i en af vores noter eller se en video, men det vil tydeligt fremgå.
   
   I flere af forløbene er der også givet idéer til, hvad man eventuelt kan springe over eller kun lade de dygtigste af eleverne arbejde med. Der vil således i mange af forløbene være rig mulighed for undervisningsdifferentiering.
   
   De steder, hvor det giver mening, har vi også til sidst i forløbet indsat et link til en delvis facitliste.


## Noter

Vores [noter](../noter/index.qmd){target="_blank"} består af en lang række noter, hvor flere af dem er suppleret med tilhørende videoer. Vi henviser til mange af noterne i vores undervisningsforløb -- og omvendt vil der også i mange af noterne være henvisninger til relaterede forløb. Alle noter kan bruges som materiale til SRP.

Noterne er opdelt i følgende fire grupper:


#### [1 -- Noter om kunstige neurale netværk](../noter/neurale_net.qmd){.morkblaa target="_blank"} 

Her findes fire noter i forskellig sværhedsgrad og med fokus på forskellige aspekter af kunstige neurale netværk: 

* I noten [Kunstige neuroner](../noter/kunstige_neuroner/kunstige_neuroner.qmd){target="_blank"} går vi i dybden med den helt basale byggesten, som kunstige neurale netværk består af -- nemlig de kunstige neuroner. Her introduceres træningsdata, sigmoid-funktionen som aktiveringsfunktion, squared error tabsfunktionen og gradientnedstigning. 

* I noten [Simple neurale netværk](../noter/simple_neurale_net/simple_neurale_net.qmd){target="_blank"} forklarer vi, hvad feedforward og backpropagation går ud på i den simplest tænkelige setting: nemlig et kunstigt neuralt netværk med to skjulte lag, hvor hvert skjult lag kun består af én neuron. 

* Til forskel fra de to foregående noter handler noten [Simple kunstige neurale netværk til multipel klassifikation](../noter/softmax/softmax.qmd){target="_blank"} om multipel klassifikation (i stedet for binær klassifikation). Igen er noten holdt så simpel som mulig ved kun at behandle multipel klassifikation i et netværk uden skjulte lag. Her introduceres softmax som aktiveringsfunktion og cross-entropy tabsfunktionen. 

* Den sidste note [Kunstige neurale netværk](../noter/neurale_net/neurale_net.qmd){target="_blank"} behandler kunstige neuale netværk til binær klassifikation mere generelt -- nemlig i det tilfælde hvor netværket består af to skjulte lag med to neuroner i hvert skjult lag.
  
Alle fire noter kan læses uafhængigt af hinanden, men stiger i sværhedsgrad.
  
#### [2 -- Noter om sprogmodeller](../noter/sprogmodeller.qmd){.morkblaa target="_blank"}

Her findes fem noter om sprogmodeller, som forklarer idéen bag at få en computer til at generere ny tekst.

* I noten [Introduktion til sprogmodeller](../noter/sprogmodeller/intro.qmd){target="_blank"} giver vi en kort og letlæselig introduktion til sprogmodeller, som handler om, hvordan sprogmodeller i virkeligheden bare prøver at forudsige det næste ord i en sætning. Vi anbefaler, at man starter med at læse denne note.

* Noten [Simple sprogmodeller](../noter/sprogmodeller/simple.qmd){target="_blank"} giver en \"blød\" introduktion til idéen bag sprogmodeller, men denne note er ikke en forudsætning for de efterfølgende noter, så man kan vælge at springe den over.

* Noten om algoritmen [Word2Vec](../noter/sprogmodeller/word2vec.qmd){target="_blank"} handler om, hvordan ord kan repræsenteres som vektorer, hvor den semantiske betydning af ordet indgår i vektorrepræsentationen. Denne note er central for at forstå de store sprogmodeller. 

* Noten [Neurale netværk til tekstgenerering](../noter/sprogmodeller/tekstgenerering.qmd){target="_blank"}  handler om, hvordan de vektorer, som repræsenterer hvert enkelt ord i vores ordforråd, kan bruges som input i et kunstigt neuralt netværk, der kan trænes til at prædiktere det næste ord i en sætning. 

* Noten om [Transformeren](../noter/sprogmodeller/transformeren.qmd){target="_blank"} handler om de kraftige sprogmodeller, der indgår i de nyeste former for sproglig kunstig intelligens. Disse sprogmodeller benytter ikke helt den tilgang, vi hidtil har beskrevet, hvor man først laver Word2Vec og derefter træner et neuralt netværk til at prædiktere ord. I stedet bruges en videreudvikling, kaldet *transformeren*, der kombinerer de to trin i én algoritme. 
I denne note beskriver vi nogle af de centrale dele af transformeren. Bemærk, at selv om vi prøver at give en intuition for, hvad transformeren gør, så er der ingen, der helt forstår i detaljer, hvorfor den virker.

Noterne her bør læses fortløbende (eventuelt kan noten om \"Simple sprogmodeller\" springes over), men man behøver ikke at læse alle noter for at få en overordnet forståelse for sprogmodellerne. Ønsker man blot en overordnet forståelse, er det fint at læse \"Introduktion til sprogmodeller\" og dernæst \"Word2Vec\".
  
#### [3 -- Øvrige AI-noter](../noter/diverse_ai.qmd){.morkblaa target="_blank"}

Her findes noter om andre former for kunstig intelligens, som ikke hører under kategorien \"Kunstige neurale netværk\" eller \"Sprogmodeller\". Det drejer sig om:

* [Anbefalingssystemer](../noter/anbefalinger/anbefalinger.qmd){target="_blank"}
* [Logistisk regression](../noter/logistisk/log-reg.qmd){target="_blank"}
* [Tabsfunktioner](../noter/tabsfunktioner/tabsfunktioner.qmd){target="_blank"}
* [Overfitting, modeludvælgelse og krydsvalidering](../noter/krydsvalidering/krydsvalidering.qmd){target="_blank"}
* [Sensitivitet, specificitet, ROC-kurver og AUC](../noter/ROC/ROC.qmd){target="_blank"}
* [Naiv Bayes klassifier](../noter/naivbayes/NaivBayes.qmd){target="_blank"}
* [Clustering med K-means](../noter/kmeans/kmeans.qmd){target="_blank"}

Alle noter kan læses uafhængigt af hinanden.
  
#### [4 -- Baggrundsmateriale](../noter/baggrund.qmd){.morkblaa target="_blank"}

Noterne under \"Baggrundsmateriale\" behandler noget af den teori, som ligger nede bag ved mange af AI-algoritmerne. Man behøver ikke at have læst nogle af noterne her for at læse og forstå det øvrige materiale. Vi har blot haft et ønske om at uddybe og være i stand til at henvise til \"Videre læsning...\" i nogle af de øvrige noter.

* [Funktioner af flere variable](../noter/funktioner_af_flere_variable/funktioner_af_flere_variable.qmd){target="_blank"}
* [Gradientnedstigning](../noter/gradientnedstigning/gradientnedstigning.qmd){target="_blank"}
* [Sandsynlighedsregning og mængdeteori](../noter/sandsynlighed/index_sandsynlighed.qmd){target="_blank"}: I noten om sandsynlighedsregning og mængdeteori er der også indsat nogle få opgaver. Derudover findes et egentlig forløb om test for sygdomme (hvor vi blandt andet introducerer sensitivitet, specificitet, positiv og negativ prædiktiv værdi).
* [Afstande og feature-skalering](../noter/afstande/index_afstande.qmd){target="_blank"}: Her findes forskellige noter om feature-skalering og afstande. 

Man bør læse noten om \"Funktioner af flere variable\", inden man læser noten om \"Gradientnedstigning\". Bortset fra det kan noterne læses uafhængigt af hinanden. 


