---
title: "Tabsfunktioner"
image: "images/tabsfunktion.jpg"
description-meta: "I langt de fleste tilfælde sker træning af AI modeller ved at minimere en tabsfunktion. Lidt løst sagt kan man sige, at en tabsfunktion måler, hvor god en AI model er til at forudsige det, vi gerne vil have den til at sige noget om. I denne note lærer du lidt om, hvordan tabsfunktioner kan se ud, og hvad træningsdata er for en størrelse."
tbl-cap-location: top 
listing:
  id: main-listing-content
  field-display-names:
    description-meta: "Kort beskrivelse"
    title: "Forløb"
  filter-ui: false 
  sort-ui: false 
  sort: false
  type: table
  contents: 
    - path: ../../undervisningsforloeb/aktiveringsfunktioner.qmd 
    - path: ../../undervisningsforloeb/Opdatering_af_vægte_i_en_kunstig_neuron.qmd
    - path: ../../undervisningsforloeb/Opdatering_af_vægte_i_NN_med_2_skjulte_lag.qmd
    - path: ../../undervisningsforloeb/Opdatering_af_vægte_i_NN_med_1_skjult_lag.qmd
  fields: [title, description-meta] 
---


I langt de fleste tilfælde sker træning af AI modeller ved at minimere en tabsfunktion. Lidt løst sagt kan man sige, at en tabsfunktion måler, hvor god en AI model er til at forudsige det, vi gerne vil have den til at sige noget om. I denne note lærer du lidt om, hvordan tabsfunktioner kan se ud, og hvad træningsdata er for en størrelse.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```




## Træningsdata og targetværdier

Du har sikkert hørt vendingen, at man \"træner en AI\". Du kan nok også godt regne ud, at det ikke er en klassisk fodboldtræner, som står for den træning. Men gad vide hvordan det så foregår?

Når man træner en AI model, har man brug for noget, som kaldes for **træningsdata**. I træningsdata har man givet en række **inputværdier** 

$$
x_1, x_2, \dots, x_n
$$ 

på baggrund af hvilke, man ønsker at forudsige en såkaldt **targetværdi** $t$, som også er en del af træningsdata. Det betyder, at et enkelt træningsdatasæt kan skrives sådan her:

$$
(x_1, x_2, \dots, x_n, t).
$$
Altså inputværdierne sammen med den tilhørende targetværdi.

Lad os tage et eksempel fra den virkelige verden, så bliver det lidt nemmere at forstå: Vi forestiller os, at vi på baggrund af en blodprøve gerne vil kunne prædiktere, om en patient har kræft eller ej. Her kan inputværdierne $x_1, x_2, \dots, x_n$ være forskellige biomarkører, man måler i blodet (spørg en biologilærer om, hvad det kunne være). Targetværdien $t$ kan antage to værdier:

$$
t=
\begin{cases}
1 & \textrm{hvis patienten har kræft} \\
0 & \textrm{hvis patienten ikke har kræft} \\
\end{cases}
$$
Man kunne også have valgt:
$$
t=
\begin{cases}
1 & \textrm{hvis patienten har kræft} \\
-1 & \textrm{hvis patienten ikke har kræft} \\
\end{cases}
$$
Eller noget helt tredje! Lad os for nu sige, at vi vælger den første mulighed, hvor $t \in \{0,1\}$. 

At bestemme om patienten har kræft eller ej -- det vil sige, om targetværdien er $0$ eller $1$ --  beror på faglig ekspertise indenfor det genstandsfelt, hvor AI modellen skal anvendes. I eksemplet med kræft vil det for eksempel være baseret på forskellige diagnostiske tests, som en læge kan bruge til at vurdere, om patienten har kræft[^4].

[^4]: Måske er disse tests først taget et stykke tid efter blodprøven, fordi det ikke er muligt at stille diagnose på tidspunktet for blodprøven. I så fald kan man måske være heldig at få udviklet en AI model, som kan prædiktere kræft *tidligere* end med gængse metoder. 

Targetværdien $t$ er altså en slags facitliste for, hvad det rigtige svar er. AI-modellen skal så trænes til at forudsige (eller **prædiktere**), hvad det korrekte svar er. 

I @fig-data ses et eksempel på et fiktivt [datasæt](data/data.xlsx), hvor en biomarkør er målt på en skala fra 0 til 50 ($x$-aksen) samtidig med, at det ved hjælp af targetværdien er angivet, om patienten har kræft eller ej ($y$-aksen). Det ser her ud som om, at en lav værdi af biomarkøren (cirka under 25) indikerer, at patienten ikke har kræft, men en bestemt skæringsværdi findes ikke. 


```{r echo=FALSE}
#| fig-cap: Her ses et plot af data med biomarkør på $x$-aksen og sygdomsstatus på $y$-aksen.
#| label: fig-data
set.seed(1)
xp <- runif(50,0,1)
mu <- ifelse(xp<0.9, 20, 40)
x <- rnorm(50, mu, sd = 7.5)
u<-runif(50,0,1)
b=-9.2
a=0.35
p = function(x,a,b){exp(a*x + b)/(1+exp(a*x+ b))}
px <- p(x,a,b)
y <- as.numeric(u<px)
# writexl::write_xlsx(
# data.frame(biomarker = x, target = y),
# path = here::here("materialer", "tabsfunktioner", "data",  "data.xlsx")
# )
fit <- glm(y~x, family = binomial())
a_mle <- coef(fit)[2]
b_mle <- coef(fit)[1]
a_mle_pretty <- unname(signif(a_mle, 2))
b_mle_pretty <- unname(signif(b_mle, 2))
OR_mle <- unname(signif(exp(a_mle),4))
par(mar=c(4,4,0.5,1))
plot(x,y,pch=1,xlab="Biomarkør",ylab="Targetværdi")
```

Idéen er nu, at man \"fodrer\" sin AI algoritme med en hel masse inputværdier (her værdien af biomarkøreren) med tilhørende targetværdier og finder den AI model, som på en eller anden måde er god til at forudsige targetværdien baseret på inputværdierne. Det med om en model er god eller ej, måler man typisk ved at se på, hvor stor en fejl AI modellen begår, og man ønsker så at finde den model, som samlet set laver så små fejl som muligt. Funktioner, som kan måle sådanne fejl, kaldes på engelsk for *error functions*, mens de på dansk typisk kaldes for **tabsfunktioner**. 

Overordnet set skal en tabsfunktion $E$ have følgende egenskaber:

::: {.callout-caution collapse="false" appearance="minimal"}
### Egenskaber ved en tabsfunktion $E$

* Tabsfunktionen skal være positiv eller $0$: $E \geq 0$ (vi vil ikke operere med negative fejl). 

* Hvis AI modellen er god til at prædiktere, skal $E$ være tæt på $0$, mens hvis AI modellen er dårlig til at prædiktere, skal $E$ være langt væk fra $0$.

:::

Overordnet set vil vi altså gerne have en AI model med så lille et samlet tab eller så lille en samlet fejl som muligt. **Derfor skal tabsfunktionen minimeres.** 

I AI modellen vil det typisk være sådan, at de forskellige inputværdier vægtes med en række vægte $w_0, w_1, \dots, w_n$ på denne måde:

$$
w_0 + w_1 \cdot x_1 + \cdots +w_n \cdot x_n.
$$
Tænk på det på den måde, at hvis inputværdien $x_i$ er vigtig, så er vægten $w_i$ stor (som i langt væk fra $0$), mens hvis $x_i$ ikke er vigtig, så er $w_i$ tæt på $0$.

Når AI modellen trænes, er det dybest set bare disse vægte, man \"skruer\" på, sådan at modellen bliver så god som mulig til at prædiktere det, den er trænet på. For store kunstige neurale netværk -- for eksempel de store sprogmodeller -- taler vi om milliarder af vægte!


## Prædiktion

Man kan tænke på en AI model som en funktion $f$, der afhænger af vægtene $w_0, w_1, \dots, w_n \!:$

$$
f(w_0, w_1, \dots, w_n).
$$
Funktionen afhænger selvfølgelig også af hele træningsdatasættet, men eftersom det er vægtene, man skal justere, alt imens træningsdata er fastlagt, vil vi blot tænke på $f$ som en funktion af vægtene.

Funktionen $f$ kaldes for øvrigt i mange AI modeller for en [**aktiveringsfunktion**](../../undervisningsforloeb/aktiveringsfunktioner.qmd). Ofte vil værdimængden for $f$ være $(0,1)$. Det gælder for eksempel, hvis $f$ er [sigmoid-funktionen ](../../undervisningsforloeb/aktiveringsfunktioner.qmd#sigmoid) $\sigma$ med forskrift:

$$
\sigma (x) = \frac{1}{1+e^{-x}}.
$$
Med inputværdier $(x_1, \dots, x_n)$ og vægte $w_0, w_1, \dots, w_n$ bruges sigmoid-funktionen, som aktiveringsfunktion sådan her:

$$
f(w_0, w_1, \dots, w_n) = \frac{1}{1+e^{-(w_0 + w_1 \cdot x_1 + \cdots + w_n \cdot x_n)}}.
$$

Hvis der bare er tale om en enkelt inputværdi $x_1$, kan det simpliciferes til:

$$
f(w_0, w_1) = \frac{1}{1+e^{-(w_0 + w_1 \cdot x_1)}}.
$$

Så er det ikke helt så slemt at se på!

Grafen for sigmoid-funktionen kan ses i @fig-sigmoid.

![Grafen for sigmoid-funktionen.](images/sigmoid.png){width=75% #fig-sigmoid}

Da funktionsværdien ligger mellem $0$ og $1$, betyder det, at vi kan tolke værdien af $f$ som en sandsynlighed. Vi forestiller os, at vi får et nyt sæt af inputværdier -- for eksempel målingerne fra en ny blodprøve, og vi vil gerne finde ud af, om patienten har kræft eller ej. Disse værdier \"sendes\" nu ind i funktionen $f$ og ud kommer en outputværdi, som vi vil kalde for $o$. Værdien af $o$ betragtes nu som sandsynligheden for, at den rigtige targetværdi er $1$. Det kunne for eksempel være sådan her:

$$
\textrm{prædiktion}=
\begin{cases}
\textrm{patienten har kræft} & \textrm{hvis } o \geq 0.5 \\
\textrm{patienten har ikke kræft} & \textrm{hvis } o < 0.5 \\
\end{cases}
$$

I vores eksempel med kræft, vil det nok typisk være sådan, at det er meget vigtigt, at alle patienter, hvor der er den mindste mistanke om kræft, sendes til videre udredning. Og derfor kunne man for eksempel også have valgt følgende prædiktion:

$$
\textrm{prædiktion}=
\begin{cases}
\textrm{patienten har kræft} & \textrm{hvis } o \geq 0.05 \\
\textrm{patienten har ikke kræft} & \textrm{hvis } o < 0.05 \\
\end{cases}
$$

Det afhænger altså af den konkrete anvendelse, hvordan man vil forholde sig til den beregnede outputværdi.

Men uanset hvad, så vil det kun give mening, hvis vi har fundet de værdier af vægtene, som gør, at denne prædiktion rent faktisk bliver god. 

Vi har hele tiden sagt, at det gør vi ved at minimere tabsfunktionen, men det kræver jo, at vi har en tabsfunktion. Inden vi dykker længere ned i forskellige typer af tabsfunktioner, skal vi lige have skrevet vores træningsdata lidt mere formelt op.

Vi antager, at vi har $M$ forskellige træningseksempler bestående af inputværdier med tilhørende targetværdi. Det kan opskrives sådan her:

$$
\begin{aligned}
&\text{Træningseksempel 1:} \quad (x_1^{(1)}, x_2^{(1)}, \dots, x_n^{(1)}, t^{(1)}) \\
&  \quad \quad \quad \quad \vdots \\
&\text{Træningseksempel m:} \quad (x_1^{(m)}, x_2^{(m)}, \dots, x_n^{(m)}, t^{(m)}) \\
&  \quad \quad \quad \quad \vdots \\
&\text{Træningseksempel M:} \quad (x_1^{(M)}, x_2^{(M)}, \dots, x_n^{(M)}, t^{(M)}) \\
\end{aligned}
$$

For det $m$'te træningseksempel beregnes outputværdien $o^{(m)}$ ofte som

$$
o^{(m)} = f(w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_n \cdot x_n^{(m)})
$$

og med sigmoid-funktionen som aktiveringsfunktion bliver det

$$
\begin{aligned}
o^{(m)} &= \sigma(w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_n \cdot x_n^{(m)}) \\
&= \frac{1}{1+e^{-(w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_n \cdot x_n^{(m)})}}.
\end{aligned}
$${#eq-o_m}

For perceptroner og kunstige neuroner svarer $x_1, x_2, \dots, x_n$ direkte til inputværdierne, som AI modellen i sidste ende skal virke på. I et kunstig neuralt netværk er det mere kompliceret, men ovenstående er korrekt, hvis man tænker på $x_1, x_2, \dots, x_n$, som de værdier neuronerne i det sidste skjulte lag sender videre til outputlaget.

Hele idéen omkring træningsdata, outputværdi og tabsfunktion er illustreret i @fig-kort_fortalt herunder.

![Idéen med træningsdata, outputværdi og tabsfunktion kort fortalt.](images/kort_fortalt.png){width=100% #fig-kort_fortalt}


## Gradientnedstigning

En AI model trænes som sagt ved at finde minimum for tabsfunktionen. Et sådant minimum bestemmes ofte ved hjælp af en numerisk metode, som kaldes for [gradientnedstigning](../gradientnedstigning/gradientnedstigning.qmd). Her kommer gradientnedstigning kort fortalt. Tabsfunktionen 

$$
E(w_0,w_1,\dots, w_n)
$$
er en [funktion af flere variable](../funktioner_af_flere_variable/funktioner_af_flere_variable.qmd). Her viser det sig, at gradienten (bestående af alle de partielle afledede)
$$
\nabla E(w_0,w_1,\dots, w_n) = 
\begin{pmatrix}
\frac{\partial E}{ \partial w_0} \\
\frac{\partial E}{ \partial w_1} \\
\vdots \\
\frac{\partial E}{ \partial w_n} \\
\end{pmatrix}
$$

vil pege i den retning, hvor funktionværdien for $E$ i et givent punkt vokser *mest*. Derfor vil den negative gradient $-\nabla E(w_0,w_1,\dots, w_n)$ pege i den retning, hvor funktionværdien for $E$ i et givent punkt aftager *mest*. 

Denne viden udnyttes i gradientnedstigning: Stil dig i et tilfældig punkt $(w_0, w_1, \dots, w_n)$ og gå et lille stykke i den negative gradients retning. Så kommer du -- hvis ikke du tager et alt for stort skridt -- en lille smule tættere på minimum. Det betyder, at alle vægte bliver opdateret på denne måde:

::: {.callout-note collapse="false" appearance="minimal"} 
### Opdateringsregler baseret på gradientnedstigning
$$
w_i^{(\textrm{ny})} \leftarrow w_i - \eta\cdot \frac{\partial E}{\partial w_i}
$${#eq-opdatering}

:::

I ovenstående opdateringsregel svarer $\eta$ til skridtlængden (den kaldes også i AI verdenen for en *learning rate*), mens pilen til venstre betyder, at vægten $w_i$ skal opdateres ved at tage den nuværende værdi af $w_i$ og trække $\eta\cdot \frac{\partial E}{\partial w_i}$ fra.

Og så er vi endelig klar til at kaste os over forskellige tabsfunktioner!

## Squared error tabsfunktionen

En ofte anvendt tabsfunktion er **squared error**, som måler de kvadrerede fejl:

$$
E = \frac{1}{2} \sum \left (t-o \right)^2,
$$ {#eq-tabsfunktion}

hvor der summeres over alle træningsdata. Det vil sige, at hvis vi skal skrive det lidt mere korrekt op, bliver det 
$$
\begin{aligned}
E(w_0, w_1, &\dots, w_n) = \frac{1}{2} \sum_{m=1}^{M} \left (t^{(m)}-
o^{(m)} \right)^2,
\end{aligned}
$$ {#eq-squared_error}

hvor $o^{(m)}$ er den beregnede outputværdi hørende til det $m$'te træningseksempel. Faktisk vil man måske i virkeligheden oftere støde på **mean squared error** tabsfunktionen:
$$
\begin{aligned}
E(w_0, w_1, &\dots, w_n) = \frac{1}{M} \sum_{m=1}^{M} \left (t^{(m)}-
o^{(m)} \right)^2,
\end{aligned}
$$
hvor man ser på den gennemsnitlige kvadrede fejl. Men eftersom vi kun er interesseret i at finde minimum for tabsfunktionen, så gør det ingen forskel, om vi ser på *squared error* eller *mean squared error*. Vi vil derfor her på siden nøjes med at se på *squared error*.

Lad os starte med at indse, at $E$ opfylder de to betingelser for en tabsfunktion. For det første kan vi se, at $E \geq 0$, fordi der er tale om en kvadreret sum. For det andet kan vi se, at hvis AI modellen er god, så vil den beregnede sandsynlighed $o$, for at patienten har kræft være tæt på $1$, når $t=1$, og $o$ vil være tæt på $0$, når $t=0$. Det betyder, at de kvadrerede forskelle $(t-o)^2$ i det tilfælde vil være små, og dermed vil tabsfunktionen også være lille. Altså lever $E$ op til de krav, vi stiller til en tabsfunktion.

Differentierer vi tabsfunktionen i (@eq-tabsfunktion) med hensyn til den $i$'te vægt $w_i$ får vi:
$$
\begin{aligned}
\frac{\partial E}{\partial w_i} &= \frac{\partial}{\partial w_i}\left ( \frac{1}{2} \sum_{m=1}^M \left (t^{(m)}-o^{(m)} \right)^2 \right ) \\
&=\frac{1}{2} \sum_{m=1}^M 2 \cdot \left (t^{(m)}-o^{(m)} \right) \frac{\partial}{\partial w_i} (t^{(m)}-o^{(m)}) \\
&= \sum_{m=1}^M \left (t^{(m)}-o^{(m)} \right) (-1) \frac{\partial o^{(m)}}{\partial w_i} \\
&= - \sum_{m=1}^M \left (t^{(m)}-o^{(m)} \right) \frac{\partial o^{(m)}}{\partial w_i} 
\end{aligned}
$${#eq-partialE_partialwi}

Bruger vi sigmoid-funktionen til at beregne outputværdien $o^{(m)}$ så får vi
$$
\begin{aligned}
\frac{\partial o^{(m)}}{\partial w_i} &= \frac{\partial }{\partial w_i} \left ( \sigma (w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_i \cdot x_i^{(m)} + \cdots + w_n \cdot x_n^{(m)}) \right ) \\
& = \sigma'(w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_n \cdot x_n^{(m)}) \cdot x_i^{(m)}
\end{aligned}
$$

Bemærk, at vi her ganger med $x_i^{(m)}$, fordi vi skal differentiere summen:

$$
w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_i \cdot x_i^{(m)} + \cdots + w_n \cdot x_n^{(m)}
$$

med hensyn til $w_i$ og det eneste led, der afhænger af $w_i$ er $w_i \cdot x_i^{(m)}$.

[Sigmoid-funktionen](../../undervisningsforloeb/aktiveringsfunktioner.qmd#sigmoid) har den særlige egenskab, at 

$$
\sigma'(x) = \sigma(x) \cdot (1-\sigma(x))
$$
Og da aktiveringsfunktionen netop beregner outputværdien $o^{(m)}$, så vil 

$$
\frac{\partial o^{(m)}}{\partial w_i}  = o^{(m)} \cdot (1-o^{(m)}) \cdot x_i^{(m)}
$$ {#eq-sigmoid_aktiveringsfkt}

Indsættes dette i (@eq-partialE_partialwi), kan den partielle afledede af tabfunktionen med hensyn til $w_i$ skrives som

$$
\begin{aligned}
\frac{\partial E}{\partial w_i} &= -\sum_{m=1}^M \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)} \cdot (1-o^{(m)}) \cdot x_i^{(m)}.
\end{aligned}
$$
Bruger vi den generelle opdateringsregel i (@eq-opdatering), får vi

$$
w_i^{(\textrm{ny})} \leftarrow w_i - \eta \cdot \left ( -\sum_{m=1}^M \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)} \cdot (1-o^{(m)}) \cdot x_i^{(m)}\right).
$$

Som kan omskrives til:

::: {.callout-note collapse="false" appearance="minimal"} 

## Opdateringsregler: Squared error tabsfunktion (med sigmoid som aktiveringsfunktion)

$$
w_i^{(\textrm{ny})} \leftarrow w_i + \eta \cdot \sum_{m=1}^M \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)} \cdot (1-o^{(m)}) \cdot x_i^{(m)}.
$${#eq-opdatering_kvadreredefejl}

:::

Lad os se på et eksempel.

:::{#exm-squared_error}

Vi vil i vores fiktive eksempel med biomarkør og prædiktion af kræft bruge *squared error* tabsfunktionen sammen med sigmoid som aktiveringsfunktion.

En delmængde af træningsdatasættet er (ud af i alt 50):

|Biomarkør| Target $t$ |
|:------:|:------:|
| $19.58$ | $0$ |
| $18.83$ | $0$ |
| $8.97$  | $0$ |
| $36.41$ | $1$ |
| $\vdots$ | $\vdots$ |
| $10.60$ | $0$ |


Vi kan på den baggrund opskrive *squared error* tabsfunktionen:

$$
\begin{aligned}
E(w_0,w_1) = \frac{1}{2} (
& (0-\sigma(w_0+w_1 \cdot 19.58))^2 + \\
& (0-\sigma(w_0+w_1 \cdot 18.83))^2 + \\
& (0-\sigma(w_0+w_1 \cdot 8.97))^2 + \\
& (1-\sigma(w_0+w_1 \cdot 36.41))^2 + \\
& \qquad \qquad \qquad\vdots \\
& (0-\sigma(w_0+w_1 \cdot 10.60))^2)
\end{aligned}
$$

Vi søger nu de værdier af $w_0$ og $w_1$, som minimerer tabsfunktionen. Bruges gradientnedstigning (du kan selv prøve ved at bruge [app'en](https://apps01.math.aau.dk/adaline/)) her fås[^1]:

$$
w_0 = -7.455 \quad \quad \textrm{og} \quad \quad w_1= 0.2539
$$

[^1]: For at være sikker på, at man finder et lokalt minimum for tabsfunktionen, er \"Maksimalt antal iterationer før algoritmen stopper\" her sat op til 10000. Husk også at vælge \"squared error\" som tabsfunktion og \"sigmoid\" som aktiveringsfunktion.


Grafen for 
$$
\sigma(-7.455+0.2539 \cdot x) = \frac{1}{1+e^{-(-7.455+0.2539 \cdot x)}}
$$
ses indtegnet sammen med datapunkterne i @fig-eksempel_squared_error:

```{r echo=FALSE}
#| fig-cap: Her ses et plot af data med biomarkør på $x$-aksen og sygdomsstatus på $y$-aksen sammen med grafen for den fundne sigmoid-funktion baseret på at minimere *squared error* tabsfunktionen. De to mørkeblå punkter illustrerer to nye, fiktive patienter.
#| label: fig-eksempel_squared_error
fit <- glm(y~x, family = binomial())
a_mle <- coef(fit)[2]
b_mle <- coef(fit)[1]
a_mle_pretty <- unname(signif(a_mle, 2))
b_mle_pretty <- unname(signif(b_mle, 2))
OR_mle <- unname(signif(exp(a_mle),4))
par(mar=c(4,4,0.5,1))
plot(x,y,pch=1,xlab="Biomarkør",ylab="Targetværdi")
w0 = -7.455
w1 = 0.2539
xx <- seq(5,45,by=0.1)
lines(xx,p(xx,w1,w0))
points(20,p(20,w1,w0),pch=19, col="#020873", cex=1.5)
points(35,p(35,w1,w0),pch=19, col="#020873", cex=1.5) 
text(20,p(20,w1,w0), "Patient 1", pos=4, col="#020873")
text(35,p(35,w1,w0), "Patient 2", pos=4, col="#020873")
```

Vi kan nu bruge de fundne vægte til at prædiktere om en fremtidig patient har kræft eller ej. Lad os se på to forskellige værdier af biomarkøren og udregne outputværdi:

$$
\begin{aligned}
\textrm{Patient 1} \quad \quad & x = 20: \quad \quad o = \sigma(-7.455+0.2539 \cdot 20)= 0.085 \\
\textrm{Patient 2} \quad \quad & x = 35: \quad \quad o = \sigma(-7.455+0.2539 \cdot 35)= 0.807\\
\end{aligned}
$$

Det vil sige, at hvis patient 1 har en målt biomarkør på 20, så vil vi prædiktere, at der er 8.5% risiko for, at patienten har kræft. Og tilsvarende hvis patient 2 har en målt biomarkør på 35, så vil vi prædiktere, at risikoen for kræft er steget til 80.7%. Disse to patienter ses også indtegnet på @fig-eksempel_squared_error.

:::

## Cross-entropy tabsfunktionen

En anden tabsfunktion, som meget ofte anvendes, men som nok er lidt sværere umiddelbart at forstå, er **cross-entropy** tabsfunktionen. Den er defineret sådan her:

$$
\begin{aligned}
E(w_0, w_1, \dots, & w_n) = \\ &- \sum_{m=1}^{M} \left (t^{(m)} \cdot \ln(o^{(m)}) + (1-t^{(m)}) \cdot \ln(1-o^{(m)})  \right)
\end{aligned}
$$ {#eq-crossentropy}

her skal outputværdien $o^{(m)}$ ligger mellem $0$ og $1$. Vi bruger derfor igen sigmoid-funktionen[^2]: 

$$
\begin{aligned}
o^{(m)} &= \sigma (w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_n \cdot x_n^{(m)}) \\
&=\frac{1}{1+e^{-(w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_n \cdot x_n^{(m)})}}
\end{aligned}
$$

[^2]: Hvis du har læst om logistisk regression, så kan du i [afsnittet om likelihood funktionen](../logistisk/log-reg.qmd#yderligere-omskrivning-af-likelihoodfunktionen){target=”blank”} se, at *cross-entropy* tabsfunktionen faktisk svarer til minus log-likelihood funktionen. Det vil sige, at finde minimum for *cross-entropy* svarer til at finde maksimum for log-likelihood funktionen, når man laver logistisk regression. 

Det er ikke oplagt, at *cross-entropy* overhovedet er en tabsfunktion. Lad os starte med at argumentere for, at $E \geq 0$.

Hvis vi ser på hvert led i summen i (@eq-crossentropy), så er der to muligheder alt efter, om $t^{(m)}=0$ eller $t^{(m)}=1$:

$$
\begin{aligned}
t^{(m)}=0: & \quad  t^{(m)} \cdot \ln(o^{(m)}) + (1-t^{(m)}) \cdot \ln(1-o^{(m)}) = \ln(1-o^{(m)}) \\
t^{(m)}=1: & \quad t^{(m)} \cdot \ln(o^{(m)}) + (1-t^{(m)}) \cdot \ln(1-o^{(m)}) = \ln(o^{(m)})
\end{aligned}
$$ {#eq-to_muligheder}

Nu ved vi, at $0<o^{(m)}<1$, og derfor er også $0<1-o^{(m)}<1$. På @fig-ln_graf ses grafen for den naturlige logaritme. Her ses det tydeligt, at hvis $x$ ligger mellem $0$ og $1$, så er $\ln(x)$ negativ.

![Grafen for den naturlige logaritmefunktion.](images/ln_graf.png){width=75% fig-align='center' #fig-ln_graf}

Det vil sige, at alle led i summen i (@eq-crossentropy) er negative, og da der står et minustegn foran hele summen, bliver *cross-entropy* tabsfunktionen altså positiv.

Vi mangler nu at se, at hvis AI modellen er god, så vil tabsfunktionen være tæt på 0. Her er der igen to muligheder. En god model vil have det sådan, at hvis targetværdien $t^{(m)}=1$, så vil den tilhørende outputværdi $o^{(m)}$ også være tæt på $1$ og omvendt, hvis $t^{(m)}=0$. Vi ser igen på hvert led i summen i (@eq-crossentropy), som vi gjorde det ovenfor:

$$
t^{(m)}=0: \quad  t^{(m)} \cdot \ln(o^{(m)}) + (1-t^{(m)}) \cdot \ln(1-o^{(m)}) = \ln(1-o^{(m)}) \\
$$
og hvis også $o^{(m)}$ er tæt på $0$, så vil 
$$
\ln(1-o^{(m)}) \approx \ln(1)=0
$$

Tilsvarende hvis:
$$
t^{(m)}=1: \quad t^{(m)} \cdot \ln(o^{(m)}) + (1-t^{(m)}) \cdot \ln(1-o^{(m)}) = \ln(o^{(m)})
$$
og hvis $o^{(m)}$ er tæt på $1$, så får vi også i det tilfælde, at 
$$
\ln(o^{(m)}) \approx \ln(1)=0.
$$
Det betyder altså samlet set, at hvis modellen er god, så vil alle led i tabsfunktionen i (@eq-crossentropy) være tæt på $0$, og den samlede tabsfunktion vil dermed også være tæt på $0$. 

Vi vil nu differentiere *cross-entropy* tabsfunktionen i (@eq-crossentropy) med hensyn til $w_i$. Husk på at selvom det ikke umiddelbart ser ud som om, at tabsfunktionen afhænger af nogle vægte, så gør den det alligevel via outputværdien $o^{(m)}$:

$$
o^{(m)} = f(w_0 + w_1 \cdot x_1^{(m)} + \cdots + w_n \cdot x_n^{(m)})
$$
Differentierer vi tabsfunktionen ledvist får vi:

$$
\begin{aligned}
\frac{\partial E}{\partial w_i} &= - \sum_{m=1}^{M} \left ( \frac{\partial }{\partial w_i} \left (t^{(m)} \cdot \ln(o^{(m)}) \right) + \frac{\partial }{\partial w_i}\left ((1-t^{(m)}) \cdot \ln(1-o^{(m)}) \right ) \right) \\
&= - \sum_{m=1}^{M} \left ( t^{(m)} \cdot \frac{1}{o^{(m)}} \cdot \frac{\partial o^{(m)}}{\partial w_i} + (1-t^{(m)}) \cdot \frac{-1}{1-o^{(m)}} \cdot \frac{\partial o^{(m)}}{\partial w_i} \right) \\
\end{aligned}
$$ 
Vi ser nu, at $\frac{\partial o^{(m)}}{\partial w_i}$ indgår som en fællesfaktor i hvert led i summen, og vi kan derfor sætte denne størrelse uden for parentesen:

$$
\begin{aligned}
\frac{\partial E}{\partial w_i} &= - \sum_{m=1}^{M} \left ( \frac{t^{(m)}}{o^{(m)}}  - \frac{(1-t^{(m)})}{1-o^{(m)}}  \right)\cdot \frac{\partial o^{(m)}}{\partial w_i} \\
\end{aligned}
$$ 

De to brøker har $o^{(m)} \cdot (1-o^{(m)})$ som fællesnævner og vi kan derfor sætte på fælles brøkstreg:

$$
\begin{aligned}
\frac{\partial E}{\partial w_i} &= - \sum_{m=1}^{M} \frac{t^{(m)} \cdot (1-o^{(m)}) - o^{(m)} \cdot (1-t^{(m)})}{o^{(m)} \cdot (1-o^{(m)})} \cdot \frac{\partial o^{(m)}}{\partial w_i} \\
&= - \sum_{m=1}^{M}  \frac{t^{(m)}-t^{(m)} \cdot o^{(m)} - o^{(m)} +o^{(m)} \cdot t^{(m)})}{o^{(m)} \cdot (1-o^{(m)})} \cdot \frac{\partial o^{(m)}}{\partial w_i} \\
&= - \sum_{m=1}^{M} \frac{t^{(m)}- o^{(m)}}{o^{(m)} \cdot (1-o^{(m)})}\cdot \frac{\partial o^{(m)}}{\partial w_i} \\
\end{aligned}
$$

Da vi bruger sigmoid som aktiveringsfunktion, ved vi fra (@eq-sigmoid_aktiveringsfkt), at

$$
\frac{\partial o^{(m)}}{\partial w_i} = o^{(m)} \cdot (1-o^{(m)}) \cdot x_i^{(m)}
$$
og derfor får vi
$$
\begin{aligned}
\frac{\partial E}{\partial w_i} &= - \sum_{m=1}^{M} \frac{t^{(m)}- o^{(m)}}{o^{(m)} \cdot (1-o^{(m)})}\cdot o^{(m)} \cdot (1-o^{(m)}) \cdot x_i^{(m)} \\
&= - \sum_{m=1}^{M} (t^{(m)}- o^{(m)})\cdot x_i^{(m)}
\end{aligned}
$$
Bemærk her, at $o^{(m)} \cdot (1-o^{(m)})$ forkorter ud.

Opdateringsreglen i (@eq-opdatering) bliver derfor med *cross-entropy* tabsfunktionen:
$$
w_i^{(\textrm{ny})} \leftarrow w_i - \eta \cdot \left (  - \sum_{m=1}^{M} (t^{(m)}- o^{(m)})\cdot x_i^{(m)} \right)
$$
som kan omskrives til

::: {.callout-note collapse="false" appearance="minimal"} 

## Opdateringsregler: Cross-entropy tabsfunktion (med sigmoid som aktiveringsfunktion)

$$
w_i^{(\textrm{ny})} \leftarrow w_i + \eta \cdot \sum_{m=1}^{M} (t^{(m)}- o^{(m)})\cdot x_i^{(m)}.
$$ {#eq-opdatering_crossentropy}

:::

Der er en interessant sammenhæng mellem denne opdateringsregel og [Perceptron Learning algoritmen](../perceptron/perceptron.qmd#sec-perceptron_learning_algortimen), som du kan læse mere om i boksen herunder.

::: {.callout-tip collapse="true" appearance="minimal"}

## *Cross-entropy* versus Perceptron Learning algoritmen

I perceptron learning algoritmen er både target- og outputværdien enten $-1$ eller $1$ og her opdateres vægtene ét træningseksempel ad gangen på denne måde:

$$
w_i^{(\textrm{ny})} \leftarrow w_i + \eta \cdot (t-o) \cdot x_i
$$
hvor $t-o$ enten kan være $-2, 0$ eller $2$. I opdateringsreglen i (@eq-opdatering_crossentropy) er $t-o$ enten $-1, 0$ eller $1$, men eftersom vi bare kan vælge en skridtlængde $\eta$, som er dobbelt så stor, så gør det ingen reel forskel. 

Det betyder, at den eneste forskel på perceptron learning algoritmen og *cross-entropy* tabsfunktionen sammen med sigmoid som aktiveringsfunktion er, at *alle* træningsdata bruges i opdateringsreglen i sidstnævnte. 

:::

Lad os igen se på et eksempel.

:::{#exm-cross_entropy}

Vi vil i vores fiktive eksempel med biomarkør og prædiktion af kræft nu bruge *cross-entropy* tabsfunktionen sammen med sigmoid som aktiveringsfunktion. Vi skal igen datasættet fra @exm-squared_error og indsætte i *cross-entropy* tabsfunktionen i (@eq-crossentropy). Her bruger vi de to udtryk vi fandt i (@eq-to_muligheder) alt efter om $t^{(m)}=0$ eller $t^{(m)}=1$:


$$
\begin{aligned}
E(w_0,w_1) = - (
&\ln(1-\sigma(w_0+w_1 \cdot 19.58)) + \\
& \ln(1-\sigma(w_0+w_1 \cdot 18.83)+ \\
& \ln(1-\sigma(w_0+w_1 \cdot 8.97) + \\
& \ln(\sigma(w_0+w_1 \cdot 36.41) + \\
& \qquad \qquad \qquad\vdots \\
& \ln(1-\sigma(w_0+w_1 \cdot 10.60))
\end{aligned}
$$

Vi søger nu igen de værdier af $w_0$ og $w_1$, som minimerer tabsfunktionen. Bruges gradientnedstigning (du kan selv prøve ved at bruge [app'en](https://apps01.math.aau.dk/adaline/)) her fås[^3]:

$$
w_0 = -9.413  \quad \quad \textrm{og} \quad \quad w_1=0.3218 
$$

[^3]: For at være sikker på, at man finder et lokalt minimum for tabsfunktionen, er \"Maksimalt antal iterationer før algoritmen stopper\" igen sat op til 10000. Husk også at vælge \"cross-entropy\" som tabsfunktion og \"sigmoid\" som aktiveringsfunktion.


Grafen for 
$$
\sigma(-9.413+0.3218 \cdot x) = \frac{1}{1+e^{-(-9.413+0.3218 \cdot x)}}
$$
ses indtegnet sammen med datapunkterne i @fig-eksempel_cross-entropy:

```{r echo=FALSE}
#| fig-cap: Her ses et plot af data med biomarkør på $x$-aksen og sygdomsstatus på $y$-aksen sammen med grafen for den fundne sigmoid-funktion baseret på at minimere *cross-entropy* tabsfunktionen. De to lyseblå punkter illustrerer to nye, fiktive patienter.
#| label: fig-eksempel_cross-entropy
fit <- glm(y~x, family = binomial())
a_mle <- coef(fit)[2]
b_mle <- coef(fit)[1]
a_mle_pretty <- unname(signif(a_mle, 2))
b_mle_pretty <- unname(signif(b_mle, 2))
OR_mle <- unname(signif(exp(a_mle),4))
par(mar=c(4,4,0.5,1))
plot(x,y,pch=1,xlab="Biomarkør",ylab="Targetværdi")
w0 = -9.413
w1 = 0.3218
xx <- seq(5,45,by=0.1)
lines(xx,p(xx,w1,w0))
points(20,p(20,w1,w0),pch=19, col="#8086F2", cex=1.5)
points(35,p(35,w1,w0),pch=19, col="#8086F2", cex=1.5) 
text(20,p(20,w1,w0), "Patient 1", pos=4, col="#8086F2")
text(35,p(35,w1,w0), "Patient 2", pos=4, col="#8086F2")
# + geom_label(
#     label="Patient 1", 
#     x=25,
#     y=0.1,
#     label.padding = unit(0.55, "lines"), # Rectangle size around label
#     label.size = 0.35,
#     color = "black",
#     fill="lightgray"
#   )
```

Vi kan nu igen bruge de fundne vægte til at prædiktere om en fremtidig patient har kræft eller ej. Lad os se på to forskellige værdier af biomarkøren og udregne outputværdi:

$$
\begin{aligned}
\textrm{Patient 1} \quad \quad & x = 20: \quad \quad o = \sigma(-9.413+0.3218 \cdot 20)= 0.048 \\
\textrm{Patient 2} \quad \quad & x = 35: \quad \quad o = \sigma(-9.413+0.3218 \cdot 35)= 0.864\\
\end{aligned}
$$

Det vil sige, at hvis patient 1 har en målt biomarkør på 20, så vil vi nu prædiktere, at der er 4.8% risiko for, at patienten har kræft. Og tilsvarende hvis patient 2 har en målt biomarkør på 35, så er vores bud på risikoen for kræft nu helt oppe på 86.4%. Disse to patienter ses også indtegnet på @fig-eksempel_cross-entropy.

:::

Umiddelbart kan man jo tænke, at det er hip som hap, om vi vælger den ene eller den anden tabsfunktion. I det næste afsnit skal vi se, at *cross-entropy* tabsfunktionen faktisk passer bedre sammen med sigmoid-funktionen som aktiveringsfunktion end *squared error* tabsfunktionen gør. Men lad os for nu sammenligne de to sigmoid-funktioner, som vi fandt i @exm-squared_error og @exm-cross_entropy.

```{r echo=FALSE}
#| fig-cap: Her ses et plot af data med biomarkør på $x$-aksen og sygdomsstatus på $y$-aksen sammen med graferne for de to fundne sigmoid-funktioner baseret på at minimere henholdsvis *squared error* (mørkeblå) og *cross-entropy* (lyseblå) tabsfunktionen. De to punkter på hver graf illustrerer to nye, fiktive patienter.
#| label: fig-eksempel_squared-error-versus-cross-entropy
fit <- glm(y~x, family = binomial())
a_mle <- coef(fit)[2]
b_mle <- coef(fit)[1]
a_mle_pretty <- unname(signif(a_mle, 2))
b_mle_pretty <- unname(signif(b_mle, 2))
OR_mle <- unname(signif(exp(a_mle),4))
par(mar=c(4,4,0.5,1))
plot(x,y,pch=1,xlab="Biomarkør",ylab="Targetværdi")
# Squared error
w0_s = -7.455
w1_s = 0.2539
xx <- seq(5,45,by=0.1)
lines(xx,p(xx,w1_s,w0_s), col="#020873")
points(20,p(20,w1_s,w0_s),pch=19, col="#020873", cex=1.5)
points(35,p(35,w1_s,w0_s),pch=19, col="#020873", cex=1.5) 
# Cross-entropy
w0_c = -9.413
w1_c = 0.3218
lines(xx,p(xx,w1_c,w0_c),col="#8086F2")
points(20,p(20,w1_c,w0_c),pch=19, col="#8086F2", cex=1.5)
points(35,p(35,w1_c,w0_c),pch=19, col="#8086F2", cex=1.5) 
legend(8,0.9,          # Position
        legend = c("Squared error", "Cross-entropy"), 
        lty = c(1, 1),           
        col = c("#020873", "#8086F2"),        
        lwd = 2) 
```


I @fig-eksempel_squared-error-versus-cross-entropy har vi tegnet de to grafer for de to forskellige sigmoid-funktioner ind i samme koordinatsystem. Her kan man se, at de to grafer, som er baseret på at minimere henholdsvis *cross-entropy* tabsfunktionen (lyseblå) og *squared error* tabsfunktionen (mørkeblå), ikke er sammenfaldende. Det fremgår også af de sandsynligheder, som vi beregnede i @exm-squared_error og @exm-cross_entropy, og som er opsummeret herunder. 


| | Patient 1 ($x=20$) | Patient 2 ($x=35$)  |
|:------:|:------:|:------:|
| *Squared error* | $0.085$ | $0.807$ |
| *Cross-entropy* | $0.048$ | $0.864$ |
: Prædikteret sandsynlighed for at patienten har kræft 


Om den ene model til forudsigelse af kræft er bedre end den anden, kan man ikke udtalelse sig om alene på baggrund af graferne i @fig-eksempel_squared-error-versus-cross-entropy. Det vil i stedet kræve, at man for eksempel bruger nogle af de metoder til modeludvælgelse, som er beskrevet i [noten om krydsvalidering](../krydsvalidering/krydsvalidering.qmd).

## Slow learning

Som vi lige har set, er resultatet forskelligt, alt efter hvilken tabsfunktion man bruger. Vi skal i dette afsnit se, at *cross-entropy* sammen med aktiveringsfunktionen sigmoid løser et problem, som *squared error* (også sammen med sigmoid) har. Dermed kan der være algoritmiske fordele ved at bruge *cross-entropy* tabsfunktionen. 

Vi starter med at huske, at de to tabsfunktioner giver følgende to opdateringsregler:

$$
\begin{aligned}
&\textrm{Squared error:} \quad \quad  w_i^{(\textrm{ny})} \leftarrow w_i + \eta \cdot \sum_{m=1}^M \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)} \cdot (1-o^{(m)}) \cdot x_i^{(m)} \\
&\textrm{Cross-entropy:} \quad \quad w_i^{(\textrm{ny})} \leftarrow w_i + \eta \cdot \sum_{m=1}^{M} (t^{(m)}- o^{(m)})\cdot x_i^{(m)}.
\end{aligned}
$$

Problemet opstår, fordi man i forbindelse med gradientnedstigning starter i et tilfældigt punkt $(w_0, w_1, \dots, w_n)$. Hvis dette punkt svarer til vægte, som resulterer i forkerte prædiktioner, så for eksempel $o^{(m)} \approx 0$, men $t^{(m)} = 1$, så vil faktoren $o^{(m)} \cdot (1-o^{(m)})$ være tæt på $0$. Det betyder, at vægtene næsten ikke bliver opdateret, når vi bruger *squared error*, fordi opdateringsleddene alle er tæt på $0$. Noget tilsvarende gør sig gældende, hvis $o^{(m)} \approx 1$ og $t^{(m)} = 0$. Dette fænomen kalder man for **slow learning**. 

I @fig-squared ses grafen for *squared error* tabsfunktionen fra eksemplet om biomarkører og prædiktion af kræft. Her er det ret tydeligt, at der findes værdier af vægtene $w_0$ og $w_1$, hvor grafen flader helt ud. Hvis man får startet sin gradientnedstigning her, så kan der gå lang tid inden, man ender i minimum.


```{r message=FALSE, warning=FALSE}
#| fig-cap: Graf for *squared error* tabsfunktionen fra eksemplet om biomarkører og prædiktion af kræft.
#| label: fig-squared
library(plotly)
hovertemplate <- paste0(
                'w0: %{y:.2f}<br>',
                'w1: %{x:.2f}<br>',
                'E: %{z:.2f}<br>',
                '<extra></extra>')
par(mar=c(4,4,0,1))
tabs_data <- readRDS(file.path("data", "tabsfunktioner.rds"))
# contour(llik$a, llik$b, llik$vals, xlab = "a", ylab = "b", levels = c(-1028, -1030, -1034, -1038, -1046, -1052, -1084), labels = "")
ax_x <- list(title = "w1")
ax_y <- list(title = "w0")
ax_z <- list(title = "Squared error")
plot_ly(showscale = FALSE) |>
  add_surface(x = tabs_data$w1, y = tabs_data$w0, z = tabs_data$squared_vals, hovertemplate = hovertemplate) |>
  add_markers(x = tabs_data$squared_w1, y = tabs_data$squared_w0, z = tabs_data$squared_opt,
              marker = list(size = 5, color = "red"), name = NULL, hovertemplate = hovertemplate) |>
  layout(scene = list(xaxis = ax_x, yaxis = ax_y, zaxis = ax_z), showlegend = FALSE)
```



Noget tilsvarende opstår ikke med *cross-entropy*, da faktoren $o^{(m)} \cdot (1-o^{(m)})$ ikke indgår i opdateringsleddet i for *cross-entropy*:
$$
w_i^{(\textrm{ny})} \leftarrow w_i + \eta \cdot \sum_{m=1}^{M} (t^{(m)}- o^{(m)})\cdot x_i^{(m)}
$$
Det ses også på grafen for *cross-entropy* tabsfunktionen i @fig-cross. Her er der ingen steder, hvor grafen flader ud. Det vil sige, at man ikke på samme måde kan komme til at starte et uheldigt sted på grafen.  Derimod vil man ret hurtigt \"lande\" i minimum uanset, hvor man starter. 


```{r message=FALSE, warning=FALSE}
#| fig-cap: Graf for *cross-entropy* tabsfunktionen fra eksemplet om biomarkører og prædiktion af kræft.
#| label: fig-cross
par(mar=c(4,4,0,1))
# mle_data <- data.frame(a = a_mle, b = b_mle, ll = as.numeric(logLik(fit)))
ax_z <- list(title = "Cross-entropy")
plot_ly(showscale = FALSE) |>
  add_surface(x = tabs_data$w1, y = tabs_data$w0, z = tabs_data$cross_vals,
              hovertemplate = hovertemplate) |>
  add_markers(x = tabs_data$cross_w1, y = tabs_data$cross_w0, z = tabs_data$cross_opt,
              marker = list(size = 5, color = "red"), name = NULL, hovertemplate = hovertemplate) |>
  layout(scene = list(xaxis = ax_x, yaxis = ax_y, zaxis = ax_z), showlegend = FALSE)
```

## Relaterede forløb

::: {#main-listing-content}
:::


