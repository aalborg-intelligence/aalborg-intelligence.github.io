<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Denne note giver en grundig gennemgang af matematikken bag kunstige neurale netværk.">

<title>Kunstige neurale netværk helt generelt – AI MAT - matematikken bag magien</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo/SVG/Bomaerke_05_AIMAT_2024.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-7fb3a01a0b0e2f41f582b839b3357eaa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y219BCPS45"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
 
  gtag('consent', 'default', {
    'ad_storage': 'denied',
    'analytics_storage': 'denied'
  });
gtag('config', 'G-Y219BCPS45', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo/PNG/Logo_multi_AIMAT_RGB_2024.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../undervisningsforlob.html"> 
<span class="menu-text">Undervisningsforløb</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../materialer.html"> 
<span class="menu-text">Materialer</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sro.html"> 
<span class="menu-text">SRO</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../srp.html"> 
<span class="menu-text">SRP</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../apps.html"> 
<span class="menu-text">Apps</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../referencer.html"> 
<span class="menu-text">Referencer</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Om os</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.youtube.com/@ai-mat"> 
<span class="menu-text"><img src="../../logo/YouTube/youtube-color-darkblue-icon.svg" style="height:2em"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indhold</h2>
   
  <ul>
  <li><a href="#sec-bogstaver_til_indekser" id="toc-sec-bogstaver_til_indekser" class="nav-link active" data-scroll-target="#sec-bogstaver_til_indekser">Fra bogstaver til indekser</a>
  <ul class="collapse">
  <li><a href="#sec-feedforward_indekser" id="toc-sec-feedforward_indekser" class="nav-link" data-scroll-target="#sec-feedforward_indekser">Feedforward med indekser</a></li>
  <li><a href="#sec-backpropagation_indekser" id="toc-sec-backpropagation_indekser" class="nav-link" data-scroll-target="#sec-backpropagation_indekser">Backpropagation med indekser</a>
  <ul class="collapse">
  <li><a href="#sec-opdatering_lag4" id="toc-sec-opdatering_lag4" class="nav-link" data-scroll-target="#sec-opdatering_lag4">Opdateringsregler for lag <span class="math inline">\(4\)</span></a></li>
  <li><a href="#sec-opdatering_lag3" id="toc-sec-opdatering_lag3" class="nav-link" data-scroll-target="#sec-opdatering_lag3">Opdateringsregler for lag <span class="math inline">\(3\)</span></a></li>
  <li><a href="#sec-opdatering_lag2" id="toc-sec-opdatering_lag2" class="nav-link" data-scroll-target="#sec-opdatering_lag2">Opdateringsregler for lag <span class="math inline">\(2\)</span></a></li>
  </ul></li>
  <li><a href="#var-det-så-egentlig-smart-med-alle-de-indekser" id="toc-var-det-så-egentlig-smart-med-alle-de-indekser" class="nav-link" data-scroll-target="#var-det-så-egentlig-smart-med-alle-de-indekser">Var det så egentlig smart med alle de indekser?</a></li>
  </ul></li>
  <li><a href="#sec-NN_generelt" id="toc-sec-NN_generelt" class="nav-link" data-scroll-target="#sec-NN_generelt">Kunstige neurale netværk helt generelt</a>
  <ul class="collapse">
  <li><a href="#backpropagation---generelt" id="toc-backpropagation---generelt" class="nav-link" data-scroll-target="#backpropagation---generelt">Backpropagation - generelt</a>
  <ul class="collapse">
  <li><a href="#opdateringsregler-i-outputlaget" id="toc-opdateringsregler-i-outputlaget" class="nav-link" data-scroll-target="#opdateringsregler-i-outputlaget">Opdateringsregler i outputlaget</a></li>
  <li><a href="#opdateringsregler-i-et-vilkårligt-skjult-lag" id="toc-opdateringsregler-i-et-vilkårligt-skjult-lag" class="nav-link" data-scroll-target="#opdateringsregler-i-et-vilkårligt-skjult-lag">Opdateringsregler i et vilkårligt skjult lag</a></li>
  </ul></li>
  <li><a href="#stokastisk-gradientnedstigning" id="toc-stokastisk-gradientnedstigning" class="nav-link" data-scroll-target="#stokastisk-gradientnedstigning">Stokastisk gradientnedstigning</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Kunstige neurale netværk helt generelt</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>I denne note vil vi se på, hvordan man kan udvide det neurale netværk, vi så på i <a href="../../materialer/neurale_net/neurale_net.html">noten om neurale netværk</a> til et netværk med et vilkårligt antal skjulte lag.</p>
<section id="sec-bogstaver_til_indekser" class="level1 page-columns page-full">
<h1>Fra bogstaver til indekser</h1>
<p>Det står på nuværende tidspunkt nok klart for de fleste, at den notation, som vi har anvendt i <a href="../../materialer/neurale_net/neurale_net.html">noten om neurale netværk</a>, er en lille smule tung. Der er bare rigtig mange bogstaver, og det kan være svært at huske om et givet bogstav betegner en outputværdi fra en neuron eller, om det er en vægt. Desuden skalerer notationen ufattelig dårligt! Forestil dig, at vi tilføjer <span class="math inline">\(2-3\)</span> ekstra lag til vores netværk - det bliver svært at blive ved med at finde nye bogstaver!</p>
<p>Derfor griber man traditionelt set notationen i forbindelse med kunstige neurale netværk lidt anderledes an, så det skalerer bedre, og så det bliver nemmere at læse (i hvert tilfælde når man lige har vænnet sig til de ekstra indekser, som vi bliver nødt til at indføre).</p>
<p>Lad os se på netværket i <a href="#fig-netvaerk3" class="quarto-xref">figur&nbsp;1</a>. Dette netværk har <span class="math inline">\(3\)</span> outputværdier i stedet for én, og der er ændret på antallet af neuroner i det ene af de skjulte lag i forhold til tidligere.</p>
<div id="fig-netvaerk3" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-netvaerk3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/netvaerk3.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-netvaerk3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figur&nbsp;1: Kunstigt neuralt netværk med flere outputneuroner.
</figcaption>
</figure>
</div>
<p>Hvert lag i netværket er nu nummeret fortløbende fra <span class="math inline">\(1\)</span> til <span class="math inline">\(4\)</span>. Desuden giver vi nu en samlet betegnelse for den værdi, som hver neuron "spytter ud". F.eks. vil den <span class="math inline">\(3.\)</span> neuron i det <span class="math inline">\(2.\)</span> lag "outputte" eller "fyre" værdien <span class="math display">\[
a_3^{(2)}
\]</span> Det vil altså sige, at det tal, som står hævet i parentesen<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, refererer til laget og det tal, som er sænket, refererer til nummeret på rækken i det givne lag.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Bemærk, at vi i de andre noter har brugt en hævet parentes til at angive nummeret på et træningseksempel. Betydningen her er altså anderledes.</p></div></div><p>Bemærk også at inputværdierne i det første lag nu har to forskellige betegnelser for det samme: <span class="math display">\[
x_i  =a_i^{(1)}
\]</span> og tilsvarende har outputværdierne i det sidste og fjerde lag også to forskellige betegnelser: <span class="math display">\[
y_i  =a_i^{(4)}
\]</span> Lad os nu se på hvordan <strong>feedforward</strong> virker med vores nye notation. For det første er vi nødt til at være lidt smartere end tidligere i forhold til, hvad vi kalder vores vægte og bias. Der er tradition for, at man navngiver vægtene, som det ses på <a href="#fig-w_jik" class="quarto-xref">figur&nbsp;2</a>.</p>
<div id="fig-w_jik" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-w_jik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/w_jik.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-w_jik-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figur&nbsp;2: Navngivning af vægte.
</figcaption>
</figure>
</div>
<p>Her er tanken, at vi gerne vil beregne outputværdien fra den <span class="math inline">\(j\)</span>’te neuron i det <span class="math inline">\(k\)</span>’te lag. Det gør vi ved at vægte alle outputværdierne fra det foregående lag: (<span class="math inline">\(k-1\)</span>). Den vægt, som vi ganger outputværdien fra den <span class="math inline">\(i\)</span>’te neuron i det <span class="math inline">\((k-1)'te\)</span> lag (<span class="math inline">\(a_i^{(k-1)}\)</span>) med, og som skal bruges for at beregne <span class="math inline">\(a_j^{(k)}\)</span>, vælger vi at kalde for <span class="math display">\[
w_{ji}^{(k)}
\]</span> Hvis man tænker på, at vi skal <em>til</em> række <span class="math inline">\(j\)</span> i <em>lag</em> <span class="math inline">\(k\)</span> <em>fra</em> række <span class="math inline">\(i\)</span>, så kan man måske huske på notationen sådan her: <span class="math display">\[
w_{\textrm{til fra}}^{(\textrm{lag})}
\]</span> Se også hvordan det passer med <a href="#fig-w_jik" class="quarto-xref">figur&nbsp;2</a>.</p>
<section id="sec-feedforward_indekser" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-feedforward_indekser">Feedforward med indekser</h2>
<p>Vi vil nu se på, hvordan de forskellige <span class="math inline">\(a_j^{(k)}\)</span>-værdier beregnes. Det vil sige, at vi altså skal se på, hvordan de forskellige feedforward-ligninger ser ud med vores nye notation.</p>
<div id="fig-udregn_a_1_2" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-udregn_a_1_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/udregn_a_1-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-udregn_a_1_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figur&nbsp;3: Udregning af <span class="math inline">\(a_1^{(2)}\)</span>.
</figcaption>
</figure>
</div>
<p>Lad os se på et konkret eksempel, så bliver det lidt nemmere at forholde sig til. Vi starter med at udregne outputværdien <span class="math inline">\(a_1^{(2)}\)</span> for den første neuron i det andet lag. Denne neuron får input fra alle neuroner i det foregående lag (som her er inputlaget). Bruger vi den notation for vægtene, som vi lige har indført, så starter vi med at beregne: <span class="math display">\[
z_1^{(2)} = w_{11}^{(2)} \cdot x_1 + w_{12}^{(2)} \cdot x_2 + w_{13}^{(2)} \cdot x_3 + w_{14}^{(2)} \cdot x_4 + b_1^{(2)}
\]</span> Der er to ting at bemærke her: 1) Vi vælger, at kalde udtrykket på højreside for <span class="math inline">\(z_1^{(2)}\)</span> og, 2) vi har kaldt biasen for <span class="math inline">\(b_1^{(2)}\)</span>.</p>
<p>Bruger vi nu de mere generelle udtryk for inputværdierne <span class="math inline">\(a_1^{(1)}, a_2^{(1)}, \dots, a_4^{(1)}\)</span> kan vi skrive: <span class="math display">\[\begin{align}
z_1^{(2)} &amp;= w_{11}^{(2)} \cdot a_1^{(1)} + w_{12}^{(2)} \cdot a_2^{(1)} + w_{13}^{(2)} \cdot a_3^{(1)} + w_{14}^{(2)} \cdot a_4^{(1)} + b_1^{(2)} \\
&amp;= \sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)}
\end{align}\]</span> Og endelig finder vi outputværdien <span class="math inline">\(a_1^{(2)}\)</span> for den første neuron i det andet lag ved som tidligere at anvende sigmoid-funktionen på ovenstående udtryk: <span class="math display">\[\begin{align}
a_1^{(2)} &amp;= \sigma(z_1^{(2)}) \\
&amp;= \sigma \left( \sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)} \right)
\end{align}\]</span> Det her er faktisk notationsmæssigt selve idéen. Folder vi det ud til hele det andet lag får vi derfor:</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feedforwardligninger til lag 2
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Beregn først: <span class="math display">\[\begin{align}
z_1^{(2)} &amp;= \sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)} \\
&amp; \\
z_2^{(2)} &amp;=\sum_{i=1}^{4} w_{2i}^{(2)} a_i^{(1)} +  b_2^{(2)} \\
&amp; \\
z_3^{(2)} &amp;= \sum_{i=1}^{4} w_{3i}^{(2)} a_i^{(1)} +  b_3^{(2)} \\
\end{align}\]</span> Outputværdierne for neuroner i det andet lag udregnes dernæst på denne måde: <span class="math display">\[\begin{align}
a_1^{(2)} &amp;= \sigma(z_1^{(2)}) \\
&amp; \\
a_2^{(2)} &amp;= \sigma(z_2^{(2)}) \\
&amp;\\
a_3^{(2)} &amp;= \sigma(z_3^{(2)}) \\
\end{align}\]</span></p>
</div>
</div>
</div>
<p>Og vover vi pelsen, kan vi helt generelt skrive:</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feedforward-ligninger til lag 2
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Beregn først: <span class="math display">\[\begin{align}
z_j^{(2)} = \sum_{i=1}^{4} w_{ji}^{(2)} a_i^{(1)} +  b_j^{(2)}
\end{align}\]</span> Outputværdierne for neuroner i det andet lag udregnes dernæst på denne måde: <span class="math display">\[\begin{align}
a_j^{(2)} &amp;= \sigma(z_j^{(2)})
\end{align}\]</span> for <span class="math inline">\(j \in \{1, 2, 3 \}\)</span>.</p>
</div>
</div>
</div>
<p>Fordelen ved denne notation er, at det nu er utrolig nemt at opskrive feedforward-ligningerne for lag 3 og 4 - det er blot nogle indekser, som skal ændres lidt. I det tredje lag er der to neuroner, hvis outputværdier beregnes på denne måde:</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feedforward-ligninger til lag 3
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Beregn først: <span id="eq-lag3"><span class="math display">\[
z_j^{(3)} = \sum_{i=1}^{3} w_{ji}^{(3)} a_i^{(2)} +  b_j^{(3)}
\tag{1}\]</span></span> Outputværdierne for neuroner i det tredje lag udregnes dernæst på denne måde: <span class="math display">\[\begin{align}
a_j^{(3)} &amp;= \sigma(z_j^{(3)})
\end{align}\]</span> for <span class="math inline">\(j \in \{1, 2 \}\)</span>.</p>
</div>
</div>
</div>
<p>Og endelig beregnes outputtet fra hele netværket i det fjerde lag:</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feedforward-ligninger til lag 4
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Udregn først: <span id="eq-lag4"><span class="math display">\[
z_j^{(4)} = \sum_{i=1}^{2} w_{ji}^{(4)} a_i^{(3)} +  b_j^{(4)}
\tag{2}\]</span></span> Outputværdierne for neuroner i det tredje lag udregnes dernæst på denne måde: <span class="math display">\[\begin{align}
y_j = a_j^{(4)} &amp;= \sigma(z_j^{(4)})
\end{align}\]</span> for <span class="math inline">\(j \in \{1, 2, 3 \}\)</span>.</p>
</div>
</div>
</div>
<p>Det fremgår nu tydeligt, at feedforward-ligningerne er på fuldstændig samme form, og vi vil derfor helt generelt kunne skrive:</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feedforward-ligninger generelt
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Beregn først: <span class="math display">\[\begin{align}
z_j^{(k)} = \sum_{i} w_{ji}^{(k)} a_i^{(k-1)} +  b_j^{(k)}
\end{align}\]</span> Outputværdierne for neuroner i det <span class="math inline">\(k\)</span>’te lag udregnes dernæst på denne måde: <span class="math display">\[\begin{align}
a_j^{(k)} &amp;= \sigma(z_j^{(k)})
\end{align}\]</span> for <span class="math inline">\(k \in \{2, 3, 4 \}\)</span>.</p>
</div>
</div>
</div>
<p>Når man bruger feedforward, starter man altså med at udregne outputværdierne for det første skjulte lag, dernæst for det andet skjulte lag og så videre, indtil man når til outputværdierne for selve netværket (deraf navnet: <em>feedforward</em> ). Bemærk her, at det ikke giver mening at udregne <span class="math inline">\(a_j^{(1)}\)</span>, fordi det svarer til inputværdierne til netværket.</p>
</section>
<section id="sec-backpropagation_indekser" class="level2">
<h2 class="anchored" data-anchor-id="sec-backpropagation_indekser">Backpropagation med indekser</h2>
<p>Lad os nu se på hvordan backpropagation fungerer. Vi skal altså have opskrevet vores opdateringsregler med den nye notation, og vi vil gribe det an, ligesom vi gjorde det i afsnittet <a href="../../materialer/neurale_net/neurale_net.html#sec-backpropagation_bogstaver">Hvordan træner man et kunstigt neuralt netværk?</a>. Nemlig ved at starte i det sidste lag (her lag <span class="math inline">\(4\)</span>) og finde opdateringsreglerne for de vægte og bias, som har direkte indflydelse på outputværdierne fra lag <span class="math inline">\(4\)</span>.</p>
<section id="sec-opdatering_lag4" class="level3">
<h3 class="anchored" data-anchor-id="sec-opdatering_lag4">Opdateringsregler for lag <span class="math inline">\(4\)</span></h3>
<p>Vi er altså i første omgang på jagt efter <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(4)}} \quad \text{og} \quad \frac{\partial E}{\partial b_j^{(4)}},
\]</span> for <span class="math inline">\(j \in \{1, 2, 3\}\)</span>, <span class="math inline">\(i \in \{1, 2\}\)</span>. Tabsfunktionen <span class="math inline">\(E\)</span>, som hører til netværket i <a href="#fig-netvaerk3" class="quarto-xref">figur&nbsp;1</a>, bliver her: <span id="eq-E_indekser"><span class="math display">\[
E=\frac{1}{2} \sum_{j=1}^3 \left( t_j-y_j \right)^2 = \frac{1}{2} \sum_{j=1}^3 \left( t_j-a_j^{(4)} \right)^2
\tag{3}\]</span></span> hvor igen <span class="math inline">\(t_j\)</span> er den ønskede target-værdi for den <span class="math inline">\(j\)</span>’te outputneuron.</p>
<p>Lad os starte med at bestemme <span class="math inline">\(\frac{\partial E}{\partial w_{ji}^{(4)}}\)</span>. Vi må derfor først se på, hvordan <span class="math inline">\(w_{ji}^{(4)}\)</span> påvirker tabsfunktionen <span class="math inline">\(E\)</span>. Da <span class="math inline">\(w_{ji}^{(4)}\)</span> kun indgår i udtrykket for beregningen af <span class="math inline">\(z_j^{(4)}\)</span>, som igen bruges til beregningen af <span class="math inline">\(a_j^{(4)}\)</span>, som dernæst direkte påvirker tabsfunktionen, kan vi skrive: <span class="math display">\[
w_{ji}^{(4)} \rightarrow z_j^{(4)} \rightarrow a_j^{(4)} \rightarrow E
\]</span> Bruger vi først kædereglen én gang, får vi derfor <span id="eq-partialE_w_ji_4"><span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(4)}} = \frac{\partial E}{\partial z_j^{(4)}} \cdot \frac{\partial z_j^{(4)}}{\partial w_{ji}^{(4)}}
\tag{4}\]</span></span> og bruges kædereglen igen, kan første faktor udfoldes yderligere <span class="math display">\[
\frac{\partial E}{\partial z_j^{(4)}} = \frac{\partial E}{\partial a_j^{(4)}} \cdot  \frac{\partial a_j^{(4)}}{\partial z_j^{(4)}}
\]</span> Lad os starte med at udregne <span class="math inline">\(\frac{\partial E}{\partial z_j^{(4)}}\)</span> ved at udregne hver faktor på højresiden i ovenstående udtryk for sig. Fra (<a href="#eq-E_indekser" class="quarto-xref">3</a>) får vi, at <span class="math display">\[
E=\frac{1}{2} \sum_{j=1}^3 \left( t_j-a_j^{(4)} \right)^2 = \frac{1}{2} \left( \left( t_1-a_1^{(4)} \right)^2 + \left( t_2-a_2^{(4)} \right)^2+ \left( t_3-a_3^{(4)}\right)^2 \right)
\]</span> Hvis vi f.eks. skal differentiere ovenstående med hensyn til <span class="math inline">\(a_2^{(4)}\)</span>, kan vi se at alle de led, som ikke indeholder <span class="math inline">\(a_2^{(4)}\)</span>, vil være at betragte som konstanter, når vi differentierer - og når vi differentierer konstanter, får vi som bekendt <span class="math inline">\(0\)</span>. Derfor får vi, at <span class="math display">\[
\frac{\partial E}{\partial a_2^{(4)}} = \frac{1}{2}\cdot 2 \cdot (t_2-a_2^{(4)})\cdot (-1) = -(t_2-a_2^{(4)})
\]</span> På tilsvarende vis har vi derfor generelt, at <span id="eq-partialE_a_j_4"><span class="math display">\[
\frac{\partial E}{\partial a_j^{(4)}} = -(t_j-a_j^{(4)})
\tag{5}\]</span></span> Vi ved også, at <span class="math display">\[
a_j^{(4)} = \sigma(z_j^{(4)})
\]</span> Og bruger vi endnu en gang, at sigmoid-funktionen differentieret er <span class="math inline">\(\sigma'(x)=\sigma(x)(1-\sigma(x))\)</span>, får vi, at <span id="eq-partial_a_j^(z)_z_j^(4)"><span class="math display">\[
\frac{\partial a_j^{(4)}}{\partial z_j^{(4)}}  = \sigma'(z_j^{(4)})= \sigma(z_j^{(4)}) \cdot (1-\sigma(z_j^{(4)}))= a_j^{(4)}\cdot (1-a_j^{(4)})
\tag{6}\]</span></span> Indtil videre har vi altså, at <span class="math display">\[\begin{align}
\frac{\partial E}{\partial z_j^{(4)}} &amp;= \frac{\partial E}{\partial a_j^{(4)}} \cdot  \frac{\partial a_j^{(4)}}{\partial z_j^{(4)}} \\
&amp;=-(t_j-a_j^{(4)}) \cdot  a_j^{(4)}\cdot (1-a_j^{(4)})
\end{align}\]</span> I forhold til det videre arbejde viser det sig hensigtsmæssigt, at lave en samlet betegnelse for <span class="math display">\[\begin{align}
\frac{\partial E}{\partial z_j^{(4)}}  
&amp;=-(t_j-a_j^{(4)}) \cdot  a_j^{(4)}\cdot (1-a_j^{(4)})
\end{align}\]</span> Vi sætter derfor <span class="math display">\[
\delta_j^{(4)} = \frac{\partial E}{\partial z_j^{(4)}}  = -(t_j-a_j^{(4)}) \cdot a_j^{(4)} \cdot (1-a_j^{(4)})
\]</span> Udtrykket <span class="math inline">\(\delta_j^{(4)}\)</span> kalder man også for fejlleddet for det fjerde lag, men det kommer vi tilbage til senere.</p>
<p>Vi har nu fundet den første faktor i (<a href="#eq-partialE_w_ji_4" class="quarto-xref">4</a>), og mangler derfor kun at bestemme <span class="math inline">\(\frac{\partial z_j^{(4)}}{\partial w_{ji}^{(4)}}\)</span>. Bruger vi (<a href="#eq-lag4" class="quarto-xref">2</a>) ser vi, at <span class="math display">\[
z_j^{(4)} =  \sum_{i=1}^{2} w_{ji}^{(4)} a_i^{(3)} +  b_j^{(4)}  =
w_{j1}^{(4)} a_1^{(3)}+ w_{j2}^{(4)} a_2^{(3)} +  b_j^{(4)}
\]</span> Skal vi f.eks. differentiere dette udtryk med hensyn til <span class="math inline">\(w_{j1}^{(4)}\)</span>, får vi (fordi de fleste led i ovenstående, vil være at betragte som konstanter) <span class="math display">\[
\frac{\partial z_j^{(4)}}{\partial w_{j1}^{(4)}} =  a_1^{(3)}
\]</span> Og helt tilsvarende hvis vi differentierer med hensyn til <span class="math inline">\(w_{j2}^{(4)}\)</span>, får vi <span class="math display">\[
\frac{\partial z_j^{(4)}}{\partial w_{j2}^{(4)}} =  a_2^{(3)}
\]</span> Generelt har vi derfor, at <span id="eq-partial_z_j_4_w_ji_4"><span class="math display">\[
\frac{\partial z_j^{(4)}}{\partial w_{ji}^{(4)}} = a_i^{(3)}
\tag{7}\]</span></span></p>
<p>Samler vi nu de tre udtryk, som vi netop har udledt og indsætter i (<a href="#eq-partialE_w_ji_4" class="quarto-xref">4</a>) får vi <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(4)}} =  -(t_j-a_j^{(4)}) \cdot a_j^{(4)} \cdot (1-a_j^{(4)}) \cdot a_i^{(3)}
\]</span> og med den lidt kortere notation, som vi indførte ovenfor, kan vi nu skrive <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(4)}} =  \delta_j^{(4)} \cdot a_i^{(3)}
\]</span> For at finde opdateringsreglerne for biasene, må vi først bestemme de partielle afledede af <span class="math inline">\(E\)</span> med hensyn til <span class="math inline">\(b_j^{(4)}\)</span>. På helt tilsvarende vis får vi, at <span class="math display">\[
\frac{\partial E}{\partial b_j^{(4)}} = \frac{\partial E}{\partial z_j^{(4)}} \cdot \frac{\partial z_j^{(4)}}{\partial b_j^{(4)}}
\]</span> Vi ved allerede, at <span class="math display">\[\begin{align}
\frac{\partial E}{\partial z_j^{(4)}}  = \delta_j^{(4)}
\end{align}\]</span> og ser man på ligningen i (<a href="#eq-lag4" class="quarto-xref">2</a>), ses det nemt, at <span class="math display">\[
\frac{\partial z_j^{(4)}}{\partial b_j^{(4)}} = 1
\]</span> og derfor har vi, at <span class="math display">\[
\frac{\partial E}{\partial b_j^{(4)}}  =\delta_j^{(4)}
\]</span> Opdateringsreglerne for de vægte og bias, som hører til outputlaget (lag <span class="math inline">\(4\)</span>) er derfor <span class="math display">\[
w_{ji}^{(4)} \leftarrow w_{ji}^{(4)} - \eta \cdot \frac{\partial E}{\partial w_{ji}^{(4)}} = w_{ji}^{(4)} - \eta \cdot \delta_j^{(4)} \cdot a_i^{(3)}
\]</span> og <span class="math display">\[
b_j^{(4)} \leftarrow b_j^{(4)} - \eta \cdot \frac{\partial E}{\partial b_j^{(4)}}  = b_j^{(4)} - \eta \cdot \delta_j^{(4)}
\]</span> Vi kan altså opsummere:</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Opdateringsregler til vægte og bias i outputlaget (lag 4)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Vægtene i outputlaget opdateres på denne måde: <span class="math display">\[
w_{ji}^{(4)} \leftarrow w_{ji}^{(4)} - \eta \cdot \delta_j^{(4)} \cdot a_i^{(3)}
\]</span> Biasene i outputlaget opdateres på denne måde: <span class="math display">\[
b_j^{(4)} \leftarrow b_j^{(4)} - \eta \cdot \delta_j^{(4)}
\]</span> hvor <span id="eq-error_term_j_4"><span class="math display">\[
\delta_j^{(4)} = \frac{\partial E}{\partial z_j^{(4)}}= -(t_j-a_j^{(4)}) \cdot a_j^{(4)} \cdot (1-a_j^{(4)})
\tag{8}\]</span></span></p>
</div>
</div>
</div>
<p>Udtrykket <span class="math inline">\(\delta_j^{(4)}\)</span> kalder man, som nævnt tidligere, også for fejlleddet i den <span class="math inline">\(j\)</span>’te række i det fjerde lag, og man kan se på ovenstående opdateringsregler, at dette fejlled netop indgår i opdateringen af både vægtene og biasene. Faktisk kan vi, præcis som vi gjorde det tidligere, tillægge dette fejlled en intuitiv god mening. Det kommer vi tilbage til igen senere!</p>
<p>Bemærk, at hvis vi i vores netværk starter med at vælge mere eller mindre tilfældige vægte, så kan vi på baggrund af dem bruge feedforwardligningerne til at udregne, <span class="math inline">\(a_j^{(4)}\)</span>- og <span class="math inline">\(a_i^{(3)}\)</span>- værdierne. Samtidig kender vi target-værdierne <span class="math inline">\(t_j\)</span>, og vi kan derfor også udregne fejlleddene <span class="math inline">\(\delta_j^{(4)}\)</span>. Vi har altså alt, hvad vi skal bruge for at benytte ovenstående opdateringsregler.</p>
</section>
<section id="sec-opdatering_lag3" class="level3">
<h3 class="anchored" data-anchor-id="sec-opdatering_lag3">Opdateringsregler for lag <span class="math inline">\(3\)</span></h3>
<p>Vi bevæger os nu et trin længere bagud i netværket og udleder opdateringsreglerne for det næstsidste lag - lag <span class="math inline">\(3\)</span>. Altså skal vi have bestemt <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(3)}} \quad \text{og} \quad \frac{\partial E}{\partial b_j^{(3)}},
\]</span> for <span class="math inline">\(j \in \{1, 2\}\)</span>, <span class="math inline">\(i \in \{1, 2, 3\}\)</span>. Vi må igen se på, hvordan <span class="math inline">\(w_{ji}^{(3)}\)</span> påvirker tabsfunktionen <span class="math inline">\(E\)</span>. Ser vi på figur <a href="#fig-netvaerk3" class="quarto-xref">figur&nbsp;1</a>, kan vi se, at <span class="math inline">\(w_{ji}^{(3)}\)</span> direkte påvirker <span class="math inline">\(z_j^{(3)}\)</span>, som igen direkte påvirker <span class="math inline">\(a_j^{(3)}\)</span>. Nu vil den <span class="math inline">\(j\)</span>’te neuron i det tredje lag fyre værdien <span class="math inline">\(a_j^{(3)}\)</span> til alle neuroner i det fjerde lag. Altså vil <span class="math inline">\(a_j^{(3)}\)</span> påvirke <span class="math inline">\(z_1^{(4)}, z_2^{(4)}\)</span> og <span class="math inline">\(z_3^{(4)}\)</span>, som bruges til beregning af <span class="math inline">\(a_1^{(4)}, a_2^{(4)}\)</span> og <span class="math inline">\(a_3^{(4)}\)</span>, som så igen vil påvirke tabsfunktionen <span class="math inline">\(E\)</span>. Det kan illustreres på denne måde <span class="math display">\[
\begin{matrix}
&amp; &amp; &amp; &amp; z_1^{(4)} \rightarrow a_1^{(4)} &amp; &amp; \\
&amp; &amp; &amp; \nearrow  &amp; &amp;  \searrow &amp; \\
w_{ji}^{(3)} &amp; \rightarrow &amp; z_j^{(3)} \rightarrow a_j^{(3)} &amp; \rightarrow &amp;
z_2^{(4)} \rightarrow a_2^{(4)} &amp; \rightarrow &amp; E \\
&amp; &amp; &amp; \searrow &amp; &amp;  \nearrow &amp;  \\
&amp; &amp; &amp; &amp; z_3^{(4)} \rightarrow a_3^{(4)} &amp; &amp; \\
\end{matrix}
\]</span> I første omgang kan vi skrive <span id="eq-partialE_w_ji_3"><span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(3)}} = \frac{\partial E}{\partial z_j^{(3)}} \cdot \frac{\partial z_j^{(3)}}{\partial w_{ji}^{(3)}}
\tag{9}\]</span></span> og så gentagne gange bruge kædereglen til at udfolde dette udtryk.</p>
<p>Lad os starte med det nemmeste, nemlig <span class="math inline">\(\frac{\partial z_j^{(3)}}{\partial w_{ji}^{(3)}}\)</span>. Ser vi på definitionen af <span class="math inline">\(z_j^{(3)}\)</span> i (<a href="#eq-lag3" class="quarto-xref">1</a>), kan vi argumentere helt tilsvarende, som da vi ovenfor udledte udtrykket i (<a href="#eq-partial_z_j_4_w_ji_4" class="quarto-xref">7</a>) og får <span class="math display">\[
\frac{\partial z_j^{(3)}}{\partial w_{ji}^{(3)}} = a_i^{(2)}
\]</span></p>
<p>Lad os nu kaste os over den første faktor i (<a href="#eq-partialE_w_ji_3" class="quarto-xref">9</a>). Vi kan starte med at udnytte denne lidt overordnede måde, som <span class="math inline">\(z_j^{(3)}\)</span> påvirker <span class="math inline">\(E\)</span> på <span class="math display">\[
z_j^{(3)} \rightarrow a_j^{(3)} \rightarrow E
\]</span> Kædereglen giver os derfor i første omgang <span id="eq-partial_E_z_j_3"><span class="math display">\[
\frac{\partial E}{\partial z_j^{(3)}} = \frac{\partial E}{\partial a_j^{(3)}} \cdot
\frac{\partial a_j^{(3)}}{\partial z_j^{(3)}}
\tag{10}\]</span></span> Igen er sidste faktor nem nok, idet <span class="math display">\[
a_j^{(3)} = \sigma (z_j^{(3)})
\]</span> og derfor er <span class="math display">\[
\frac{\partial a_j^{(3)}}{\partial z_j^{(3)}} = \sigma' (z_j^{(3)})=\sigma(z_j^{(3)})\cdot (1-\sigma(z_j^{(3)}))= a_j^{(3)} \cdot (1-a_j^{(3)}),
\]</span> hvor vi endnu en gang har benyttet, at <span class="math inline">\(\sigma'(x)=\sigma(x)(1-\sigma(x))\)</span>.</p>
<p>Når vi skal bestemme <span class="math inline">\(\frac{\partial E}{\partial a_j^{(3)}}\)</span> kommer vi ikke udenom kædereglen for funktioner af flere variable. Det bliver tydeligt, når vi zoomer ind på hvordan <span class="math inline">\(a_j^{(3)}\)</span> påvirker <span class="math inline">\(E\)</span>: <span class="math display">\[
\begin{matrix}
  &amp; &amp; z_1^{(4)} \rightarrow a_1^{(4)} &amp; &amp; \\
  &amp; \nearrow  &amp; &amp;  \searrow &amp; \\
  a_j^{(3)} &amp; \rightarrow &amp;
z_2^{(4)} \rightarrow a_2^{(4)} &amp; \rightarrow &amp; E \\
&amp; \searrow &amp; &amp;  \nearrow &amp;  \\
&amp; &amp; z_3^{(4)} \rightarrow a_3^{(4)} &amp; &amp; \\
\end{matrix}
\]</span> For at vi senere kan udnytte nogle af de ligninger, som vi udledte i lag <span class="math inline">\(4\)</span>, vil vi faktisk bare nøjes med at se på det, på denne måde: <span class="math display">\[
\begin{matrix}
  &amp; &amp; z_1^{(4)} &amp; &amp; \\
  &amp; \nearrow  &amp; &amp;  \searrow &amp; \\
  a_j^{(3)} &amp; \rightarrow &amp;
z_2^{(4)} &amp; \rightarrow &amp; E \\
&amp; \searrow &amp; &amp;  \nearrow &amp;  \\
&amp; &amp; z_3^{(4)}  &amp; &amp; \\
\end{matrix}
\]</span> Nu er vi endelig klar til at bruge kædereglen for funktioner af flere variable: <span class="math display">\[\begin{align}
\frac{\partial E}{\partial a_j^{(3)}} &amp;=  \frac{\partial E}{\partial z_1^{(4)}} \cdot \frac{\partial z_1^{(4)}}{\partial a_j^{(3)}}  + \frac{\partial E}{\partial z_2^{(4)}} \cdot \frac{\partial z_2^{(4)}}{\partial a_j^{(3)}} + \frac{\partial E}{\partial z_3^{(4)}} \cdot \frac{\partial z_3^{(4)}}{\partial a_j^{(3)}} \\
&amp;= \sum_{k=1}^{3}\frac{\partial E}{\partial z_k^{(4)}} \cdot \frac{\partial z_k^{(4)}}{\partial a_j^{(3)}}  
\end{align}\]</span> Se nu dukker der noget op, som vi har set før! Nemlig det fejlled, som vi definerede i (<a href="#eq-error_term_j_4" class="quarto-xref">8</a>), og som vi allerede har regnet ud, da vi opdaterede vægtene og biasene i lag <span class="math inline">\(4\)</span>. Tænk lige over det - det er faktisk ret fedt! Dvs. at vi kan skrive: <span id="eq-partial_E_a_j_3"><span class="math display">\[
\begin{aligned}
\frac{\partial E}{\partial a_j^{(3)}} &amp;= \sum_{k=1}^{3} \delta_k^{(4)} \cdot \frac{\partial z_k^{(4)}}{\partial a_j^{(3)}}  
\end{aligned}
\tag{11}\]</span></span> Så mangler vi kun lige at finde <span class="math inline">\(\frac{\partial z_k^{(4)}}{\partial a_j^{(3)}}\)</span>! Fra (<a href="#eq-lag4" class="quarto-xref">2</a>) har vi, at <span class="math display">\[
z_k^{(4)} = \sum_i w_{ki}^{(4)} a_i^{(3)}+b_k^{(4)}
\]</span> så <span class="math display">\[
\frac{\partial z_k^{(4)}}{\partial a_j^{(3)}} = \frac{\partial}{\partial a_j^{(3)}} \left(\sum_i w_{ki}^{(4)} a_i^{(3)}+b_k^{(4)} \right)
\]</span> Når vi skal differentierer summen i ovenstående udtryk, får vi kun et led med, når <span class="math inline">\(i=j\)</span>, fordi i alle andre tilfælde, vil vi med hensyn til <span class="math inline">\(a_j^{(3)}\)</span> skulle differentiere en konstant. Og da <span class="math display">\[
\frac{\partial}{\partial a_j^{(3)}}\left ( w_{kj}^{(4)} a_j^{(3)} \right) = w_{kj}^{(4)}
\]</span> har vi altså, at <span class="math display">\[
\frac{\partial z_k^{(4)}}{\partial a_j^{(3)}} = w_{kj}^{(4)}
\]</span> Indsætter vi dette i (<a href="#eq-partial_E_a_j_3" class="quarto-xref">11</a>), har vi nu <span class="math display">\[\begin{align}
\frac{\partial E}{\partial a_j^{(3)}} = \sum_{k=1}^{3} \delta_k^{(4)} \cdot \frac{\partial z_k^{(4)}}{\partial a_j^{(3)}}  = \sum_{k=1}^{3} \delta_k^{(4)} \cdot w_{kj}^{(4)}
\end{align}\]</span></p>
<p>Nu skal vi i første omgang tilbage til (<a href="#eq-partial_E_z_j_3" class="quarto-xref">10</a>) og indsætte det vi netop er kommet frem til: <span class="math display">\[\begin{align}
\frac{\partial E}{\partial z_j^{(3)}}=\underbrace{\left ( \sum_{k=1}^{3} \delta_k^{(4)} \cdot w_{kj}^{(4)}\right )}_{\frac{\partial E}{\partial a_j^{(3)}}}
\cdot \underbrace{a_j^{(3)} \cdot (1-a_j^{(3)})}_{\frac{\partial a_j^{(3)}}{\partial z_j^{(3)}}}
\end{align}\]</span> Som vi gjorde i <a href="#sec-opdatering_lag4" class="quarto-xref">afsnit&nbsp;1.2.1</a>, vil vi også give dette lidt lange udtryk en særlig betegnelse, nemlig <span class="math display">\[\begin{align}
\delta_j^{(3)} = \frac{\partial E}{\partial z_j^{(3)}}=\left ( \sum_{k=1}^{3} \delta_k^{(4)} \cdot w_{kj}^{(4)}\right )
\cdot a_j^{(3)} \cdot (1-a_j^{(3)})
\end{align}\]</span> Det kan vist godt være lidt svært at bevare overblikket her, men nu er vi faktisk i mål! Vi indsætter i (<a href="#eq-partialE_w_ji_3" class="quarto-xref">9</a>) <span class="math display">\[\begin{align}
\frac{\partial E}{\partial w_{ji}^{(3)}} &amp;= \frac{\partial E}{\partial z_j^{(3)}} \cdot \frac{\partial z_j^{(3)}}{\partial w_{ji}^{(3)}} \\
&amp;= \delta_j^{(3)} \cdot a_i^{(2)}
\end{align}\]</span></p>
<p>Det er nu en smal sag at bestemme <span class="math inline">\(\frac{\partial E}{\partial b_j^{(3)}}\)</span>, da <span class="math display">\[\begin{align}
\frac{\partial E}{\partial b_j^{(3)}} &amp;= \frac{\partial E}{\partial z_j^{(3)}} \cdot \frac{\partial z_j^{(3)}}{\partial b_j^{(3)}} \\ &amp;= \delta_j^{(3)}\cdot \frac{\partial z_j^{(3)}}{\partial b_j^{(3)}}
\end{align}\]</span> Fra (<a href="#eq-lag3" class="quarto-xref">1</a>) har vi, at <span class="math display">\[
z_j^{(3)} = \sum_{i=1}^3 w_{ji}^{(3)} a_i^{(2)} + b_j^{(3)}
\]</span> og derfor er <span class="math display">\[
\frac{\partial z_j^{(3)}}{\partial b_j^{(3)}} = 1
\]</span> Altså får vi <span class="math display">\[\begin{align}
\frac{\partial E}{\partial b_j^{(3)}} = \delta_j^{(3)}
\end{align}\]</span> Glæden er stor, da vi nu har alle ingredienser til at opskrive opdateringsreglerne for det tredje lag!</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Opdateringsregler til vægte og bias i lag 3
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Vægtene i outputlaget opdateres på denne måde: <span class="math display">\[
w_{ji}^{(3)} \leftarrow w_{ji}^{(3)} - \eta \cdot \delta_j^{(3)} \cdot a_i^{(2)}
\]</span> Biasene i outputlaget opdateres på denne måde: <span class="math display">\[
b_j^{(3)} \leftarrow b_j^{(3)} - \eta \cdot \delta_j^{(3)}
\]</span> hvor <span id="eq-error_term_j_3"><span class="math display">\[
\delta_j^{(3)} = \frac{\partial E}{\partial z_j^{(3)}}=\left( \sum_{k=1}^{3}\delta_k^{(4)} \cdot w_{kj}^{(4)}   \right) \cdot a_j^{(3)} \cdot (1-a_j^{(3)})
\tag{12}\]</span></span></p>
</div>
</div>
</div>
<p>Bemærk, at udgangspunktet for ovenstående er, at vi først har lavet et feedforward i netværket, så vi har alle <span class="math inline">\(a_i^{(2)}\)</span>- og <span class="math inline">\(a_j^{(3)}\)</span>-værdier. Derudover har vi allerede opdateret vægtene og biasene i lag <span class="math inline">\(4\)</span>. Derfor kender vi også fejleddene <span class="math inline">\(\delta_k^{(4)}\)</span> fra lag <span class="math inline">\(4\)</span>, som indgår i beregningen af <span class="math inline">\(\delta_j^{(3)}\)</span> i (<a href="#eq-error_term_j_3" class="quarto-xref">12</a>). Altså er det muligt at foretage de beregninger, som opdateringsreglerne i lag <span class="math inline">\(3\)</span> kræver.</p>
</section>
<section id="sec-opdatering_lag2" class="level3">
<h3 class="anchored" data-anchor-id="sec-opdatering_lag2">Opdateringsregler for lag <span class="math inline">\(2\)</span></h3>
<p>Vi er nu fremme ved det sidste lag, hvor vi skal have opdateret vægte og bias (husk på at <span class="math inline">\(a_i^{(1)}\)</span>-værdierne jo ikke skal beregnes, men er inputværdierne til netværket). Den gode nyhed her er, at der absolut intet nyt er under solen! Vi vil derfor heller ikke gå i drabelige deltaljer med alle udregninger her, men blot skitsere idéen.</p>
<p>Vi er nu på jagt efter <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(2)}} \quad \text{og} \quad \frac{\partial E}{\partial b_j^{(2)}}
\]</span> og kigger vi på vores netværk i <a href="#fig-netvaerk3" class="quarto-xref">figur&nbsp;1</a> kan vi se følgende afhængigheder: <span class="math display">\[
\begin{matrix}
&amp; &amp; &amp; &amp; z_1^{(3)}  &amp; &amp; \\
&amp; &amp; &amp; \nearrow  &amp; &amp;  \searrow &amp; \\
w_{ji}^{(2)} &amp; \rightarrow &amp; z_j^{(2)} \rightarrow a_j^{(2)} &amp;  &amp;  &amp;  &amp; E \\
&amp; &amp; &amp; \searrow &amp; &amp;  \nearrow &amp;  \\
&amp; &amp; &amp; &amp; z_2^{(3)} &amp; &amp; \\
\end{matrix}
\]</span> Vi kan nu igen skrive <span id="eq-partialE_w_ji_2"><span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(2)}} = \frac{\partial E}{\partial z_j^{(2)}} \cdot \frac{\partial z_j^{(2)}}{\partial w_{ji}^{(2)}}
\tag{13}\]</span></span> Helt analogt til tidligere ses det nemt, at <span class="math display">\[
\frac{\partial z_j^{(2)}}{\partial w_{ji}^{(2)}} = a_i^{(1)}
\]</span> og kædreglen giver igen, at <span id="eq-partial_E_z_j_2"><span class="math display">\[
\frac{\partial E}{\partial z_j^{(2)}}= \frac{\partial E}{\partial a_j^{(2)}} \cdot
\frac{\partial a_j^{(2)}}{\partial z_j^{(2)}}
\tag{14}\]</span></span> Her fås også uden problemer, at den sidste faktor kan skrives som <span class="math display">\[
\frac{\partial a_j^{(2)}}{\partial z_j^{(2)}} = a_j^{(2)} \cdot (1-a_j^{(2)})
\]</span> og bruger man kædereglen for funktioner af flere variable, kommer man frem til, at <span class="math display">\[\begin{align}
\frac{\partial E}{\partial a_j^{(2)}} &amp;= \sum_{k=1}^2 \frac{\partial E}{\partial z_k^{(3)}}
\cdot \frac{\partial  z_k^{(3)}}{\partial a_j^{(2)}} \\
&amp;= \sum_{k=1}^2  \delta_k^{(3)} w_{kj}^{(3)},
\end{align}\]</span> hvor vi allerede har udregnet <span class="math inline">\(\delta_k^{(3)}\)</span>, da vi opdaterede vægtene og biasene i lag <span class="math inline">\(3\)</span>.</p>
<p>Indsætter vi i (<a href="#eq-partial_E_z_j_2" class="quarto-xref">14</a>) og samtidig definerer fejlleddet <span class="math inline">\(\delta_j^{(2)}\)</span> for det andet lag, får vi <span class="math display">\[
\delta_j^{(2)} = \frac{\partial E}{\partial z_j^{(2)}} = \left ( \sum_{k=1}^2  \delta_k^{(3)} w_{kj}^{(3)} \right ) \cdot a_j^{(2)} \cdot (1-a_j^{(2)})
\]</span> Alt i alt ender vi med <span class="math display">\[\begin{align}
\frac{\partial E}{\partial w_{ji}^{(2)}} &amp;= \delta_j^{(2)} \cdot a_i^{(1)} \\
&amp;= \delta_j^{(2)} \cdot x_i
\end{align}\]</span> fordi alle <span class="math inline">\(a_i^{(1)}\)</span>-værdierne svarer til selve inputværdierne <span class="math inline">\(x_i\)</span> til netværket.</p>
<p>Det er nu ikke svært at se, at <span class="math display">\[
\frac{\partial E}{\partial b_j^{(2)}} = \delta_j^{(2)}
\]</span> og vi får derfor følgende opdateringsregler for lag 2:</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Opdateringsregler til vægte og bias i lag 2
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Vægtene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
w_{ji}^{(2)} \leftarrow w_{ji}^{(2)} - \eta \cdot \delta_j^{(2)} \cdot a_i^{(1)}
\end{align}\]</span> Biasene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
b_j^{(2)} \leftarrow b_j^{(2)} - \eta \cdot \delta_j^{(2)}
\end{align}\]</span> hvor <span id="eq-error_term_j_2"><span class="math display">\[
\delta_j^{(2)} = \frac{\partial E}{\partial z_j^{(2)}}= \left ( \sum_{k=1}^2  \delta_k^{(3)} w_{kj}^{(3)} \right ) \cdot a_j^{(2)} \cdot (1-a_j^{(2)})
\tag{15}\]</span></span></p>
</div>
</div>
</div>
</section>
</section>
<section id="var-det-så-egentlig-smart-med-alle-de-indekser" class="level2">
<h2 class="anchored" data-anchor-id="var-det-så-egentlig-smart-med-alle-de-indekser">Var det så egentlig smart med alle de indekser?</h2>
<p>Hvis man er nået hertil, kan man godt følge sig en lille smule forpustet. Der har godt nok været mange indekser at holde styr på! Både nogle der var sænkede, og nogle der var hævede og sat i parenteser! Alligevel kan man måske godt se fidusen nu.</p>
<p>Hvis vi ser på de opdateringsregler, som vi lige har udledt, så kan man se, at selve opdateringsreglerne af vægte og bias følger <em>præcis</em> samme form. Faktisk kan man, hvis man sammenligner opdateringsreglerne for de tre lag se, at opdateringsreglerne er på denne form:</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Generelle opdateringsregler til vægte og bias
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Vægtene opdateres generelt på denne måde: <span class="math display">\[\begin{align}
w_{ji}^{(\text{lag})} \leftarrow w_{ji}^{(\text{lag})} - \eta \cdot \delta_j^{(\text{lag})} \cdot a_i^{(\text{lag-1})}
\end{align}\]</span> Biasene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
b_j^{(\text{lag})} \leftarrow b_j^{(\text{lag})} - \eta \cdot \delta_j^{(\text{lag})}
\end{align}\]</span></p>
</div>
</div>
</div>
<p>Bemærk her, at da vi allerede har lavet en feedforward i netværket, så kender vi outputværdierne <span class="math inline">\(a_i^{(\text{lag})}\)</span> i alle lag. Det vil sige, at vi kan opdatere vægtene og biasene, når blot vi kan beregne fejlleddene.</p>
<p>Den eneste reelle forskel på opdateringsreglerne er, at fejlleddene udregnes lidt forskelligt, alt efter om der er tale om outputlaget eller et skjult lag:</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Beregning af fejlleddene
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Fejlleddene i outputlaget beregnes på denne måde: <span class="math display">\[\begin{align}
\delta_j^{(\text{outputlag})} = -(t_j-y_j) \cdot y_j \cdot (1-y_j),
\end{align}\]</span> idet outputværdierne fra netværket netop er <span class="math inline">\(y_1, y_2, \dots\)</span>.</p>
<p>Fejlleddene i et skjult lag beregnes på denne måde: <span class="math display">\[\begin{align}
\delta_j^{(\text{lag})} = \left ( \sum_{k}  \delta_k^{(\text{lag+1})} w_{kj}^{(\text{lag+1})} \right ) \cdot a_j^{(\text{lag})} \cdot (1-a_j^{(\text{lag})})
\end{align}\]</span></p>
</div>
</div>
</div>
<p>Bemærk her, at fejlleddene fra outputlaget uden videre kan beregnes, da vi kender target-værdierne <span class="math inline">\(t_j\)</span> og outputværdierne <span class="math inline">\(y_j\)</span> fra netværket (fordi vi allerede har lavet en feedforward). Vi kan også beregne fejlleddene i alle skjulte lag, idet vi hele tiden arbejder bagud i netværket (<em>backpropagation</em>). Det vil sige, at vi hele tiden har adgang til fejlleddene i laget længere fremme (lag+1), hvor (lag+1) første gang vil svare til outputlaget. Desuden kender vi pga. feedforward alle outputværdier <span class="math inline">\(a_j^{(\text{(lag)})}\)</span> og alle vægte <span class="math inline">\(w_{kj}^{(\text{(lag+1)})}\)</span>. Derfor kan vi også beregne fejlleddene i alle de skjulte lag.</p>
<p>Denne indsigt og den generelle overordnede struktur på opdateringsreglerne, var meget svær at indse med fremgangsmåde i afsnittet <a href="../../materialer/neurale_net/neurale_net.html#sec-backpropagation_bogstaver">Hvordan træner man et kunstigt neuralt netværk?</a>. Her druknede alt bare i et sandt bogstavshelvede!</p>
<p>Der er et par andre interessante ting at sige om beregningen af fejlleddene. Lad os først se på outputlaget: <span class="math display">\[
\delta_j^{(\text{outputlag})} = -(t_j-y_j) \cdot y_j \cdot (1-y_j)
\]</span> Hvis der er stor forskel på target-værdien <span class="math inline">\(t_j\)</span> og outputværdien <span class="math inline">\(y_j\)</span>, så bliver forskellen <span class="math inline">\(t_j-y_j\)</span> numerisk stor. Altså vil en stor forskel på det, vi ønsker, og det vi får ud af netværket betyde, at fejlleddet bliver større og i sidste ende, at de vægte, som direkte påvirker outputtet, også vil blive opdateret meget. Endelig ser vi igen, at hvis outputneuronen er mættet (dvs. at <span class="math inline">\(y_j\)</span> enten er tæt på <span class="math inline">\(0\)</span> eller <span class="math inline">\(1\)</span>), så vil fejlleddet ikke blive opdateret i samme grad, som hvis outputneuronen ikke havde været mættet (fordi hvis <span class="math inline">\(y_j\)</span> enten er tæt på <span class="math inline">\(0\)</span> eller <span class="math inline">\(1\)</span>, så vil <span class="math inline">\(y_j \cdot (1-y_j)\)</span> være tæt på <span class="math inline">\(0\)</span>).</p>
<p>Vi ser altså, at fejlleddet fra det sidste lag direkte afhænger af hvor stor forskellen er på target-værdi og outputværdi.</p>
<p>Ser vi så på fejlleddene fra de skjulte lag: <span class="math display">\[
\delta_j^{(\text{lag})} = \left ( \sum_{k}  \delta_k^{(\text{lag+1})} w_{kj}^{(\text{lag+1})} \right ) \cdot a_j^{(\text{lag})} \cdot (1-a_j^{(\text{lag})})
\]</span> Så kan vi igen se, at hvis den tilhørende outputneuron, som fyrer værdien <span class="math inline">\(a_j^{(\text{lag})}\)</span>, er mættet, så vil fejlleddet være tættere på <span class="math inline">\(0\)</span>, end hvis neuronen ikke havde været mættet. Samtidig kan vi også se, at der i fejlleddet indgår en vægtet sum af alle fejlleddene fra laget længere fremme: <span class="math display">\[
\sum_{k}  \delta_k^{(\text{lag+1})} w_{kj}^{(\text{lag+1})}
\]</span> På den måde vil store fejl i laget længere fremme også få indflydelse på fejlleddet i det nuværende lag.</p>
</section>
</section>
<section id="sec-NN_generelt" class="level1 page-columns page-full">
<h1>Kunstige neurale netværk helt generelt</h1>
<p>Med det benarbejde vi lige har været igennem, ligger det faktisk lige til højrebenet at generalisere overstående til et vilkårligt kunstigt neuralt netværk.</p>
<p>Vi forestiller os nu, at vi har <span class="math inline">\(K\)</span> lag i netværket, som det er illustreret på <a href="#fig-generelt_netvaerk" class="quarto-xref">figur&nbsp;4</a>.</p>
<div id="fig-generelt_netvaerk" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-generelt_netvaerk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/generelt_netvaerk.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-generelt_netvaerk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figur&nbsp;4: Generelt kunstigt neuralt netværk med <span class="math inline">\(K\)</span> lag.
</figcaption>
</figure>
</div>
<p>Det vil sige ét inputlag, ét outputlag og <span class="math inline">\(K-2\)</span> skjulte lag. Antallet af neuroner i hvert lag kan variere og behøver ikke at være det samme. Så lad os navngive antallet af neuroner i de <span class="math inline">\(K\)</span> lag på denne måde: <span class="math display">\[
n_1, n_2, \dots, n_k, \dots, n_K
\]</span> Inputværdierne til netværket betegner vi: <span class="math display">\[
a_1^{(1)}, a_2^{(1)}, \dots, a_{n_1}^{(1)}
\]</span> Feedforward ligningerne er nu helt identiske med dem vi opstillede i <a href="#sec-feedforward_indekser" class="quarto-xref">afsnit&nbsp;1.1</a>:</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feedforwardligninger - helt generelt
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Beregn først: <span id="eq-z_j_k_generelt"><span class="math display">\[
z_j^{(k)} = \sum_{i} w_{ji}^{(k)} a_i^{(k-1)} +  b_j^{(k)}
\tag{16}\]</span></span> Outputværdierne for neuroner i det <span class="math inline">\(k\)</span>’te lag udregnes dernæst på denne måde: <span id="eq-a_j_k_generelt"><span class="math display">\[
a_j^{(k)} = \sigma(z_j^{(k)})
\tag{17}\]</span></span> for <span class="math inline">\(k \in \{2, 3, \dots, K \}\)</span>.</p>
</div>
</div>
</div>
<p>Hvis vi også kalder outputværdierne fra netværket for <span class="math display">\[
y_1, y_2, \dots, y_{n_K}
\]</span> så beregnes disse værdier altså ved <span class="math display">\[\begin{align}
y_j = a_j^{(K)} &amp;= \sigma(z_j^{(K)})
\end{align}\]</span> for <span class="math inline">\(j \in \{1, 2, \dots, n_K \}\)</span>.</p>
<section id="backpropagation---generelt" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation---generelt">Backpropagation - generelt</h2>
<p>Lad os så se på backpropagation. Vi ved fra det foregående, at der reelt set kun er to ting, vi skal gøre:</p>
<ol type="1">
<li><p>Finde opdateringsreglerne for vægte og bias i outputlaget.</p></li>
<li><p>Finde opdateringsreglerne for vægte og bias i et vilkårligt skjult lag.</p></li>
</ol>
<section id="opdateringsregler-i-outputlaget" class="level3">
<h3 class="anchored" data-anchor-id="opdateringsregler-i-outputlaget">Opdateringsregler i outputlaget</h3>
<p>Vores tabsfunktion er stadig <span class="math display">\[
E= \frac{1}{2} \sum_{i=1}^{n_K} \left ( t_i - a_i^{(K)} \right )^2,
\]</span> hvor <span class="math inline">\(t_i\)</span> igen er targetværdierne. Vi skal bestemme <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(K)}} \quad \text{og} \quad \frac{\partial E}{\partial b_j^{(K)}}
\]</span> Vi gør præcis som i <a href="#sec-opdatering_lag4" class="quarto-xref">afsnit&nbsp;1.2.1</a>. Vi indser først, at vi har denne direkte afhængighed fra <span class="math inline">\(w_{ji}^{(K)}\)</span> til <span class="math inline">\(E\)</span>: <span class="math display">\[
w_{ji}^{(K)} \rightarrow z_j^{(K)} \rightarrow a_j^{(K)} \rightarrow E
\]</span> Derfor får vi <span id="eq-partialE_w_ji_K"><span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(K)}} = \frac{\partial E}{\partial z_j^{(K)}} \cdot \frac{\partial z_j^{(K)}}{\partial w_{ji}^{(K)}}
\tag{18}\]</span></span> På grund af feedforwardligningen i (<a href="#eq-z_j_k_generelt" class="quarto-xref">16</a>) får vi for det første, at <span id="eq-partial_z_j_K_w_ji_K"><span class="math display">\[
\frac{\partial z_j^{(K)}}{\partial w_{ji}^{(K)}} = a_i^{(K-1)}
\tag{19}\]</span></span> Nu bruger vi kædereglen til at bestemme <span id="eq-partial_E_z_j_K_generelt"><span class="math display">\[
\frac{\partial E}{\partial z_j^{(K)}} = \frac{\partial E}{\partial a_j^{(K)}} \cdot  \frac{\partial a_j^{(K)}}{\partial z_j^{(K)}}
\tag{20}\]</span></span> På grund af feedforwardligningen i (<a href="#eq-a_j_k_generelt" class="quarto-xref">17</a>) og egenskaben ved differentialkvotienten af sigmoid-funktionen får vi sidste faktor til <span class="math display">\[
\frac{\partial a_j^{(K)}}{\partial z_j^{(K)}}  =  a_j^{(K)}\cdot (1-a_j^{(K)})
\]</span> Endelig får vi, ved at differentiere tabsfunktionen med hensyn til <span class="math inline">\(a_j^{(K)}\)</span> <span class="math display">\[
\frac{\partial E}{\partial a_j^{(K)}} = -(t_j-a_j^{(K)})
\]</span> Vi definerer nu igen fejlleddet for outputlaget <span class="math inline">\(\delta_j^{(K)}\)</span>, som tidligere <span class="math display">\[
\delta_j^{(K)} = \frac{\partial E}{\partial z_j^{(K)}}
\]</span> og indsætter vi det, vi netop har udledt, i (<a href="#eq-partial_E_z_j_K_generelt" class="quarto-xref">20</a>) får vi <span class="math display">\[
\delta_j^{(K)} =  -(t_j-a_j^{(K)}) \cdot a_j^{(K)} \cdot (1-a_j^{(K)})
\]</span> Indsætter vi nu det hele i (<a href="#eq-partialE_w_ji_K" class="quarto-xref">18</a>), har vi altså: <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(K)}} = \delta_j^{(K)}  \cdot a_i^{(K-1)}
\]</span> Det er ikke svært at overbevise sig selv om, at <span class="math display">\[
\frac{\partial E}{\partial b_j^{(K)}} = \delta_j^{(K)}
\]</span> og derfor har vi:</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Generelle opdateringsregler til vægte og bias i outputlaget (lag <span class="math inline">\(K\)</span>)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Vægtene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
w_{ji}^{(K)} \leftarrow w_{ji}^{(K)} - \eta \cdot \delta_j^{(K)} \cdot a_i^{(K-1)}
\end{align}\]</span> Biasene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
b_j^{(K)} \leftarrow b_j^{(K)} - \eta \cdot \delta_j^{(K)}
\end{align}\]</span> hvor <span class="math display">\[\begin{align}
\delta_j^{(K)} = \frac{\partial E}{\partial z_j^{(K)}}= -(t_j-a_j^{(K)}) \cdot a_j^{(K)} \cdot (1-a_j^{(K)})
\end{align}\]</span></p>
</div>
</div>
</div>
</section>
<section id="opdateringsregler-i-et-vilkårligt-skjult-lag" class="level3">
<h3 class="anchored" data-anchor-id="opdateringsregler-i-et-vilkårligt-skjult-lag">Opdateringsregler i et vilkårligt skjult lag</h3>
<p>Vi ser nu på et vilkårligt skjult lag <span class="math inline">\(k\)</span>, som hverken er inputlaget eller outputlaget. Det vil sige, at <span class="math inline">\(k \in \{2, 3, \dots, K-1 \}\)</span>. Vi antager, at vi har kørt backpropagation på alle lag, der ligger længere fremme i netværket, og specielt har vi altså beregnet fejlleddene i lag <span class="math inline">\(k+1:\)</span> <span class="math display">\[
\delta_j^{(k+1)} = \frac{\partial E}{\partial z_j^{(k+1)}}
\]</span> Vi indser først, at vi har denne afhængighed fra <span class="math inline">\(w_{ji}^{(k)}\)</span> til tabsfunktionen <span class="math inline">\(E\)</span>: <span id="eq-dep_E_w_ji_k"><span class="math display">\[
\begin{matrix}
&amp; &amp; &amp; &amp; z_1^{(k+1)}  &amp; &amp; \\
&amp; &amp; &amp; \nearrow  &amp; \vdots &amp;  \searrow &amp; \\
w_{ji}^{(k)} &amp; \rightarrow &amp; z_j^{(k)} \rightarrow a_j^{(k)} &amp; \rightarrow  &amp;  z_j^{(k+1)} &amp; \rightarrow  &amp; E \\
&amp; &amp; &amp; \searrow &amp; \vdots &amp;  \nearrow &amp;  \\
&amp; &amp; &amp; &amp; z_{n_{k+1}}^{(k+1)} &amp; &amp; \\
\end{matrix}
\tag{21}\]</span></span> Vi starter som tidligere med at bruge kædereglen én gang: <span id="eq-partialE_w_ji_k"><span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(k)}} = \frac{\partial E}{\partial z_j^{(k)}} \cdot \frac{\partial z_j^{(k)}}{\partial w_{ji}^{(k)}}
\tag{22}\]</span></span> Fra feedforwardligningen i (<a href="#eq-z_j_k_generelt" class="quarto-xref">16</a>) får vi for det første, at <span class="math display">\[
\frac{\partial z_j^{(k)}}{\partial w_{ji}^{(k)}} = a_i^{(k-1)}
\]</span> Endnu en anvendelse af kædereglen, og hvor vi også i samme hug definerer fejlleddet <span class="math inline">\(\delta_j^{(k)}\)</span> for det <span class="math inline">\(k\)</span>’te skjulte lag, giver: <span id="eq-partial_E_z_j_k"><span class="math display">\[
\delta_j^{(k)} = \frac{\partial E}{\partial z_j^{(k)}}= \frac{\partial E}{\partial a_j^{(k)}} \cdot
\frac{\partial a_j^{(k)}}{\partial z_j^{(k)}}
\tag{23}\]</span></span> Den sidste partielle afledede kan vi udlede fra feedforwardligningen (<a href="#eq-a_j_k_generelt" class="quarto-xref">17</a>) og egenskaben ved differentialkvotienten af sigmoid-funktionen: <span class="math display">\[
\frac{\partial a_j^{(k)}}{\partial z_j^{(k)}} = a_j^{(k)} \cdot (1-a_j^{(k)})
\]</span> For at beregne <span class="math inline">\(\frac{\partial E}{\partial a_j^{(k)}}\)</span> må vi have fat i kædereglen for funktioner af flere variable (se illustrationen i (<a href="#eq-dep_E_w_ji_k" class="quarto-xref">21</a>): <span class="math display">\[\begin{align}
\frac{\partial E}{\partial a_j^{(k)}} = \sum_{i=1}^{n_{k+1}} \frac{\partial E}{\partial z_i^{(k+1)}}
\cdot \frac{\partial  z_i^{(k+1)}}{\partial a_j^{(k)}}
\end{align}\]</span> Vi udnytter nu, at vi allerede kender fejlleddene fra lag <span class="math inline">\(k+1\)</span> og kan derfor omskrive til</p>
<p><span id="eq-partialE_aj_k"><span class="math display">\[
\frac{\partial E}{\partial a_j^{(k)}} = \sum_{i=1}^{n_{k+1}} \delta_i^{(k+1)}
\cdot \frac{\partial  z_i^{(k+1)}}{\partial a_j^{(k)}}
\tag{24}\]</span></span></p>
<p>Fra feedforwardligningen i (<a href="#eq-z_j_k_generelt" class="quarto-xref">16</a>) får vi, at <span class="math inline">\(z_i^{(k+1)}\)</span> kan skrives som <span class="math display">\[
z_i^{(k+1)} = \sum_{j} w_{ij}^{(k+1)} a_j^{(k)} +  b_i^{(k+1)}
\]</span> og derfor er <span class="math display">\[\begin{align}
\frac{\partial  z_i^{(k+1)}}{\partial a_j^{(k)}} = w_{ij}^{(k+1)}
\end{align}\]</span> Indsætter vi i (<a href="#eq-partialE_aj_k" class="quarto-xref">24</a>) fås <span class="math display">\[\begin{align}
\frac{\partial E}{\partial a_j^{(k)}} = \sum_{i=1}^{n_{k+1}} \delta_i^{(k+1)}
\cdot w_{ij}^{(k+1)}
\end{align}\]</span> og ved indsættelse i (<a href="#eq-partial_E_z_j_k" class="quarto-xref">23</a>) får vi nu fejlleddet i det <span class="math inline">\(k\)</span>’te lag <span class="math display">\[
\delta_j^{(k)} = \frac{\partial E}{\partial z_j^{(k)}}=  \left ( \sum_{i=1}^{n_{k+1}} \delta_i^{(k+1)} \cdot w_{ij}^{(k+1)} \right) \cdot a_j^{(k)} \cdot (1-a_j^{(k)})
\]</span> Vi bruger nu udtrykket for <span class="math inline">\(\frac{\partial E}{\partial w_{ji}^{(k)}}\)</span> i (<a href="#eq-partialE_w_ji_k" class="quarto-xref">22</a>) og ender med <span class="math display">\[\begin{align}
\frac{\partial E}{\partial w_{ji}^{(k)}} = \delta_j^{(k)} \cdot a_i^{(k-1)}
\end{align}\]</span> og tilsvarende får vi også, at <span class="math display">\[
\frac{\partial E}{\partial b_j^{(k)}} = \delta_j^{(k)}
\]</span> Opdateringsreglerne for et vilkårligt skjult lag bliver så:</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Opdateringsregler til vægte og bias i et vilkårligt skjult lag <span class="math inline">\(k\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Vægtene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
w_{ji}^{(k)} \leftarrow w_{ji}^{(k)} - \eta \cdot \delta_j^{(k)} \cdot a_i^{(k-1)}
\end{align}\]</span> Biasene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
b_j^{(k)} \leftarrow b_j^{(k)} - \eta \cdot \delta_j^{(k)}
\end{align}\]</span> hvor <span class="math display">\[\begin{align}
\delta_j^{(k)} = \frac{\partial E}{\partial z_j^{(k)}}=  \left ( \sum_{i=1}^{n_{k+1}} \delta_i^{(k+1)} \cdot w_{ij}^{(k+1)} \right) \cdot a_j^{(k)} \cdot (1-a_j^{(k)})
\end{align}\]</span></p>
</div>
</div>
</div>
</section>
</section>
<section id="stokastisk-gradientnedstigning" class="level2">
<h2 class="anchored" data-anchor-id="stokastisk-gradientnedstigning">Stokastisk gradientnedstigning</h2>
<p>Vi har faktisk snydt lidt… Okay – indrømmet – det er lidt træls at komme at sige nu! Men i alt hvad vi har lavet indtil nu, har vi kun kigget på ét træningseksempel. Vi har ladet inputværdierne for det ene træningseksempel "kører igennem" netværket (feedforward), beregnet tabsfunktionen og brugt resultatet herfra til at opdatere alle vægtene (backpropagation). Men vi har jo ikke kun ét træningseksempel. Vi har faktisk rigtig mange! Måske ligefrem tusindvis af træningsdata. Men hvad gør man så?</p>
<p>Lad os lige genopfriske den tabsfunktion, som vi endte med i det helt generelle tilfælde: <span id="eq-E_singletraining"><span class="math display">\[
E= \frac{1}{2} \sum_{i=1}^{n_K} \left ( t_i - a_i^{(K)} \right )^2.
\tag{25}\]</span></span> Her er <span class="math inline">\(t_i\)</span> target-værdien for den <span class="math inline">\(i\)</span>’te outputneuron for lige præcis det træningseksempel vi står med. Husk på at et givet træningseksempel består af inputværdierne <span class="math display">\[
x_1, x_2, \dots, x_{n_1}
\]</span> <em>og</em> de ønskede target-værdier <span class="math display">\[
t_1, t_2, \dots, t_{n_K}.
\]</span> Når vi kører disse inputværdier igennem netværket, får de selvfølgelig i sidste ende direkte betydning for outputværdierne i det sidste lag (<span class="math inline">\(K\)</span>): <span class="math display">\[
a_1^{(K)}, a_2^{(K)}, \cdots, a_{n_K}^{(K)}.
\]</span> Det vil sige, at i vores tabsfunktion i (<a href="#eq-E_singletraining" class="quarto-xref">25</a>), så afhænger både <span class="math inline">\(t_i\)</span>’erne og <span class="math inline">\(a_i^{(K)}\)</span>’erne af træningseksemplet. Hvis vi sådan lidt generelt benævner vores træningseksempel med <span class="math inline">\(x\)</span>, så vil det kunne udtrykkes sådan her: <span class="math display">\[
E_x= \frac{1}{2} \sum_{i=1}^{n_K} \left ( t_{x,i} - a_{x,i}^{(K)} \right )^2,
\]</span> hvor så <span class="math inline">\(t_{x,i}\)</span> er target-værdien for den <span class="math inline">\(i\)</span>’te outputneuron fra træningsdata <span class="math inline">\(x\)</span> og <span class="math inline">\(a_{x,i}^{(K)}\)</span> er outputværdien for den <span class="math inline">\(i\)</span>’te outputneuron, som er beregnet på baggrund af inputværdierne fra træningsdata <span class="math inline">\(x\)</span>.</p>
<p>Den samlede tabsfunktion, som er den, vi i virkeligheden ønsker at minimere, bliver så gennemsnittet af tabsfunktionerne hørende til de enkelte træningsdata: <span id="eq-E_generel"><span class="math display">\[
E = \frac{1}{n} \sum_x E_x= \frac{1}{n} \sum_x \left ( \frac{1}{2} \sum_{i=1}^{n_K} \left ( t_{x,i} - a_{x,i}^{(K)} \right )^2 \right ).
\tag{26}\]</span></span> Husk på at vi er ude efter <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(k)}} \quad \text{og} \quad \frac{\partial E}{\partial b_j^{(k)}}
\]</span> for <span class="math inline">\(k \in \{2, 3, \dots, K\}\)</span> og hvor <span class="math inline">\(E\)</span> nu er summen i (<a href="#eq-E_generel" class="quarto-xref">26</a>). Heldigvis kan vi differentiere ledvis, og der gælder derfor <span class="math display">\[
\frac{\partial E}{\partial w_{ji}^{(k)}} = \frac{1}{n} \sum_x \frac{\partial E_x}{\partial w_{ji}^{(k)}}
\]</span> og tilsvarende <span class="math display">\[
\frac{\partial E}{\partial b_j^{(k)}} = \frac{1}{n} \sum_x \frac{\partial E_x}{\partial b_j^{(k)}}
\]</span> Det kommer så til at betyde, at opdateringsreglerne nu generelt bliver på formen <span id="eq-opdatering_w_ji_alledata"><span class="math display">\[
w_{ji}^{(k)} \leftarrow w_{ji}^{(k)}-\eta \cdot \frac{\partial E}{\partial w_{ji}^{(k)}}  =
w_{ji}^{(k)}-\eta \cdot \frac{1}{n} \sum_x \frac{\partial E_x}{\partial w_{ji}^{(k)}}
\tag{27}\]</span></span> og tilsvarende for biasene <span id="eq-opdatering_b_j_alledata"><span class="math display">\[
b_j^{(k)} \leftarrow b_j^{(k)} -\eta \cdot \frac{\partial E}{\partial b_j^{(k)}}  =
b_j^{(k)}-\eta \cdot \frac{1}{n} \sum_x \frac{\partial E_x}{\partial b_j^{(k)}}
\tag{28}\]</span></span> Alle leddene <span class="math inline">\(\frac{\partial E_x}{\partial w_{ji}^{(k)}}\)</span> og <span class="math inline">\(\frac{\partial E_x}{\partial b_j^{(k)}}\)</span>, som indgår i opdateringsreglerne, svarer netop til hvad vi har udledt i de foregående afsnit, fordi vi jo netop her kun så på ét træningseksempel ad gangen. Hvis vi overfører dette til opdateringsreglerne i outputlaget, så vil vi f.eks. få</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Generelle opdateringsregler til vægte og bias i outputlaget (lag <span class="math inline">\(K\)</span>) med brug af alle træningsdata
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Vægtene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
w_{ji}^{(K)} \leftarrow w_{ji}^{(K)} - \eta \cdot \frac{1}{n} \sum_x \left ( \delta_{x,j}^{(K)} \cdot a_{x,i}^{(K-1)} \right )
\end{align}\]</span> Biasene i outputlaget opdateres på denne måde: <span class="math display">\[\begin{align}
b_j^{(K)} \leftarrow b_j^{(K)} - \eta \cdot \frac{1}{n} \sum_x \left ( \delta_{x,j}^{(K)} \right )
\end{align}\]</span> hvor <span class="math display">\[\begin{align}
\delta_{x,j}^{(K)} = \frac{\partial E_x}{\partial z_j^{(K)}}= -(t_{x,j}-a_{x,j}^{(K)}) \cdot a_{x,j}^{(K)} \cdot (1-a_{x,j}^{(K)})
\end{align}\]</span></p>
</div>
</div>
</div>
<p>Og helt tilsvarende vil det se ud for opdateringsreglerne i de skjulte lag.</p>
<p>Lad os lige dvæle lidt ved, hvad det her, det egentlig betyder. Lad os sige at vi har <span class="math inline">\(1000\)</span> træningsdata. Så skal vi lade de <span class="math inline">\(1000\)</span> træningsdata kører igennem netværket, så vi kan beregne de <span class="math inline">\(1000\)</span> led, som indgår i de ovenstående summer. Herefter kan vi opdatere alle vægte og bias én gang. Det vil blot være ét lille skridt på vej ned i dalen mod det lokale minimum, som vi er på jagt efter. Dette lille skridt skal gentages rigtig mange gange, indtil værdien af tabsfunktionen ser ud til at begynde at konvergere – svarende til at vi har ramt det lokale minimum.</p>
<p>Så selvom gradientnedstigning kan bruges til at finde et lokalt minimum for tabsfunktionen <span class="math inline">\(E\)</span>, så er det faktisk også en beregningsmæssig stor og tung opgave! Derfor er der forsket meget videre i at gøre det endnu bedre og endnu hurtigere. I algoritmer som disse er der ofte et trade-off: Man kan gøre noget hurtigere ved at bruge mere hukommelse – eller bruge mindre hukommelse ved at gøre det en smule langsommere. En af de teknikker, der er kommet ud af den forskning, er, at man kan bruge mindre hukommelse ved i hvert opdateringsskridt kun at bruge en tilfældigt udvalgt del af træningsdata – det kunne f.eks. være <span class="math inline">\(10\%\)</span> af alle træningsdata. Så vil man i hvert skridt stadig bruge opdateringsreglerne i (<a href="#eq-opdatering_w_ji_alledata" class="quarto-xref">27</a>) og (<a href="#eq-opdatering_b_j_alledata" class="quarto-xref">28</a>), men hvor der nu kun summeres over de <span class="math inline">\(10 \%\)</span> af træningsdatene. Hver gang man laver et nyt opdateringsskridt, vil man tage en ny tilfældigt udvalgt del af træningsdata. Denne teknik kalder man <em>stokastisk gradientnedstigning</em> (stochastic gradient descent). Og der er endnu flere af sådanne små ændringer, der enten gør algoritmen hurtigere eller, at den bruger mindre hukommelse. Det vil komme an på den enkelte anvendelse, hvad der er vigtigst her.</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-baktoft" class="csl-entry" role="listitem">
Baktoft, Allan. 2014. <em>Matematik i Virkeligheden. Bind 2</em>. Forlaget Natskyggen.
</div>
<div id="ref-bishop" class="csl-entry" role="listitem">
Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.
</div>
<div id="ref-james" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. <em>An Introduction to Statistical Learning with Applications in r. Second Edition</em>. Springer.
</div>
<div id="ref-mitchell" class="csl-entry" role="listitem">
Mitchell, Tom M. 1997. <em>Machine Learning</em>. The McGraw-Hill Companies, Inc.
</div>
<div id="ref-nielsen" class="csl-entry" role="listitem">
Nielsen, Michael A. 2015. <em>Neural Networks and Deep Learning</em>. Determination Press. <a href="http://neuralnetworksanddeeplearning.com/index.html">http://neuralnetworksanddeeplearning.com/index.html</a>.
</div>
<div id="ref-videnskabsteori" class="csl-entry" role="listitem">
Sørensen, Henrik Kragh, and Mikkel Willum Johansen. 2020. <em>"Invitation Til de Datalogiske Fags Videnskabsteori". Lærebog Til Brug for Undervisning Ved Institut for Naturfagenes Didaktik, Københavns Universitet</em>. Under udarbejdelse.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/aimat\.dk");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>