Tekstgenerering med neurale netværk {#tekstgenerering-med-neurale-netværk .unnumbered}
===================================

Vi har set, hvordan man kan få repræsenteret alle ord i ordforrådet som
vektorer med Word2Vec. Næste skridt er at bruge disse vektorer til at
generere tekst. I denne note ser vi på, hvordan det for eksempel kan
gøres ved hjælp af *neurale netværk*. I praksis foretages
tekstgenerering som regel ved hjælp af en særlig smart algoritme kaldet
*transformeren*, der i en vis forstand laver Word2Vec og neurale netværk
på én gang. Når du har læst denne note, kan du læse videre om
tranformeren her LINK.

Vi vil gerne kunne generere en tekst ét ord ad gangen. Lad os som
eksempel sige, at vi har fået lavet sætningen
$$
\text{"En hund og en kat ---"}
$$
og skal generere næste ord. Lige som
med $N$-grams LINK gør vi det lidt simplere ved kun at kigge på de
sidste $N-1$ ord i sætningen. Vi vil sætte $N=4$ i denne note, så vi
gætter næste ord på baggrund af de 3 foregående. I praksis ville man
bruge et større $N$. I eksemplet skal vi så gætte næste ord efter \"og
en kat\". Til det bruger vi et neuralt netværk, som er en slags
funktion, der tager de tre seneste ord som input. Som output giver
netværket for hvert ord i ordforrådet en sandsynlighed for, at det er
det næste ord.

I eksemplet er vores input til det neurale netværk altså \"og en kat\".
For hvert af de tre ord, har vi lavet en vektorrepræsentation med
Word2Vec. Hvis vektorerne fra Word2Vec har $m$ koordinater, samler vi de
tre vektorer i én vektor $\overrightarrow{x}$ med $3m$ koordinater, hvor
de første $m$ koordinater er vektoren for \"og\", de næste $m$
koordinater er vektoren for \"en\", og de sidste $m$ koordinater er
vektoren for \"kat\". Hvis fx vores vektorer for \"og\", \"en\" og
\"kat\" er
$$
\overrightarrow{v}_{\text{og}}=\begin{pmatrix} 2\\1 \end{pmatrix}, \quad \overrightarrow{v}_{\text{en}}=\begin{pmatrix} -1\\3 \end{pmatrix}, \quad \overrightarrow{v}_{\text{kat}}=\begin{pmatrix} 7\\-4 \end{pmatrix}
$$
så er vores input til det neurale netværk vektoren
$$
\overrightarrow{x} =\begin{pmatrix} 2\\1 \\-1\\ 3\\ 7\\ -4 \end{pmatrix}
$$
Denne vektor indeholder både informationen fra Word2Vec og information
om rækkefølgen af de tre ord, som svarer til rækkefølgen, de tre
vektorer er sat ind i $\overrightarrow{x}$.

Output fra det neurale netværk skal være en vektor
$$
\overrightarrow{z}=\begin{pmatrix} z_1\\ \vdots \\ z_V\end{pmatrix}
$$
med $V$ koordinater, hvor $V$ er antallet af ord i vores ordforråd. Vi
forestiller os, at vi har nummereret alle ord i ordforrådet. Den $i$te
koordinat i $\overrightarrow{z}$ hører sammen med det $i$te ord, som vi
vil kalde \"ord$_i$\". Den $i$te koordinat i $\overrightarrow{z}$ skal
give sandsynligheden for, at \"ord$_i$\" er det næste ord efter \"og en
kat\". Med andre ord, 
$$
z_i=P(\text{ord}_i|\text{ "og en kat" })
$$

En god outputvektor $\overrightarrow{z}$ skal opfylde følgende.

-   Alle koordinater skal være sandsynligheder, så $0\leq z_i\leq 1$ for
    alle $i$.

-   Summen af alle sandsynlighederne skal give 1, altså
    $z_1+\dotsm + z_V=1$, da det er den samlede sandsynlighed for at få
    et af de mulige ord.

-   Hvis \"ord$_i$\" er et godt bud på et ord, der følger efter \"og en
    kat\", skal $z_i$ være tæt på $1$. FODNOTE: Der er i praksis mange
    ord, som er gode bud og summen af sandsynlighederne skal give $1$,
    så $z_i$ bliver ikke $1$.

-   Hvis ord$_i$ er et dårligt bud på næste ord, skal $z_i$ være tæt på
    $0$.

Vores neurale netværk skal altså være en funktion, der tager vektoren
$\overrightarrow{x}$ som input og giver vektoren $\overrightarrow{z}$
som output. Funktionen dannes ved at sammensætte en masse simplere
funktioner. For at holde overblik kan man skitsere det neurale netværk
som i Figur ?.

FLOT FIGUR MED NETVÆRKET

Vores input er vektoren $\overrightarrow{x}$, der har $3m$ koordinater.
Disse koordinater svarer til cirklerne i venstre side af figuren kaldet
*inputlaget*. Cirklerne i midten angiver *det skjulte lag*. (FODNOTE:
Man kan gøre sit neurale netværk mere fleksibelt ved at have flere
skjulte lag. For at holde forklaringen simpel vil vi dog kun gennemgå
tilfældet med et skjult lag her.) Her omregnes $\overrightarrow{x}$ til
en ny vektor $\overrightarrow{h}$ med $d$ koordinater, hvor $d$ er et
tal, vi har valgt. Hver koordinat i $\overrightarrow{h}$ svarer til en
af cirklerne i midten, og pilene i venstre side angiver, at hver
koordinat i $\overrightarrow{h}$ er en funktion af koordinaterne i
$\overrightarrow{x}$. Mere præcist beregnes koordinaterne i
$\overrightarrow{h}$ ved: 
$$
\begin{aligned}
h_1&=f(w_{1,0}+w_{1,1}x_1+w_{1,2}x_2+\ldots +w_{1,3m}x_{3m})\\
&\vdots\\
h_j&=f(w_{j,0}+w_{j,1}x_1+w_{j,2}x_2+\ldots +w_{j,3m}x_{3m})\\
&\vdots\\
h_{d}&=f(w_{d,0}+w_{d,1}x_1+w_{d,2}x_2+\ldots +w_{d,3m}x_{3m})\\\end{aligned}
$$
Funktionen $f$, der indgår, er en *aktiveringsfunktion*. Man kan fx
bruge sigmoid-funktionen 
$$
f(x)=\frac{1}{1+e^{-x}}
$$
Du kan læse mere om
aktiveringsfunktioner her LINK. Desuden indgår der $d \cdot (3m+1)$
*vægte* på formen $w_{j,k}$. Vægtene er reelle konstanter. Når vi om
lidt træner det neurale netværk, forsøger vi at bestemme værdien af
disse vægte.

Cirklerne til højre i Figur ? udgør *outputlaget*. I outputlaget laves
vektoren $\overrightarrow{h}$ om til outputvektoren $\overrightarrow{z}$
med $V$ koordinater svarende til cirklerne til højre. Igen viser pilene,
at hver koordinat i $\overrightarrow{z}$ er en funktion af koordinaterne
i $\overrightarrow{h}$. Funktionen udregnes i to trin:

-   Først udregner vi en vektor $\overrightarrow{y}$ med $V$
    koordinater, hvor $i$te koordinat er
    $$
    y_i=u_{i,0}+u_{i,1}h_1+u_{i,2}h_2+\ldots + u_{i,d}h_{d}
    $$ 
    Igen indgår der nogle vægte $u_{i,j}$. Dem er der i alt $(d+1)\cdot V$ af.

-   Derefter laver vi $\overrightarrow{y}$ om til sandsynligheder. Det
    gør vi ved at bruge softmax-funktionen, som blev introduceret i
    noten Word2Vec LINK. Softmax-funktionen tager en $V$-dimensional
    vektor $\overrightarrow{y}$ som input og giver en ny $V$-dimensional
    vektor $\overrightarrow{z}=\text{Softmax}(\overrightarrow{y})$ som
    output. Den $i$te koordinat i $\overrightarrow{z}$ udregnes som
    $$
    z_i=\frac{e^{y_i}}{e^{y_1} + \dotsm + e^{y_V}}
    $$
    Vi så i
    Word2Vec-noten, at $\overrightarrow{z}$ opfylder 1.--2. i LINK
    TILBAGE I DENNE TEKST, således at det giver mening at tænke på
    $\overrightarrow{z}$ som en vektor af sandsynligheder. Dette
    $\overrightarrow{z}$ er vores outputvektor.

Samlet set er der ret mange vægte i modellen. Der er $d\cdot (3m+1)$
vægte i det skjulte lag og $(d+1)\cdot V$ vægte i ouputlaget. I alt
bliver det $d\cdot(1+3m +V) +V$ vægte. Læg mærke til, at valget af $d$
er det, der bestemmer antallet af vægte: $m$ var fastlagt da vi lavede
Word2Vec, $V$ er antallet af ord i vores ordforråd, og 3-tallet er antal
ord, vi prædikterer udfra. Valget af $d$ er i praksis et kompromis. Jo
større $d$ er, des mere præcis en model kan vi lave. Omvendt bliver der
også flere vægte, der skal bestemmes. Det kræver stor regnekraft.
Desuden kræver det meget træningsdata, hvis man vil undgå overfitting -
et problem, som du kan læse mere om her LINK.

Træning af netværket {#træning-af-netværket .unnumbered}
--------------------

Som sagt indgår der ret mange vægte i modellen. Indtil nu har vi ikke
sagt, hvilken værdi disse vægte skal have. Husk på, at vores neurale
netværk skal give os sandsynligheden for, at et ord er næste ord i en
sætning, når vi kender de 3 foregående ord. For at lære, hvilket ord der
typisk kommer efter tre givne ord i virkelige tekster, får vi endnu
engang brug for noget træningsdata i form af vores store tekstkorpus. Ud
fra dette tekstkorpus laver vi et datasæt bestående af alle 4-gram, det
vil sige alle sekvenser på 4 ord, der forekommer i teksten. De tre
første ord kalder vi input, og det sidste kalder vi *target*.

Hvis fx vores træningsdata består af sætningen
$$
\text{"Solen skinner, og en kat løber på græsplænen."}
$$
så laver vi
en datatabel som i Tabel [\[fig:data\]](#fig:data){reference-type="ref"
reference="fig:data"}. (FODNOTE: Vi ignorerer tegnsætning.)

   Input 1   Input 2   Input 3   Target
  --------- --------- --------- --------
      ⋮         ⋮         ⋮        ⋮
              Solen    skinner     og
   skinner     og        en       kat
     og        en        kat     løber
     en        kat      løber      på
      ⋮         ⋮         ⋮        ⋮

  : Træningsdata[]{label="fig:data"}

Hvis vores neurale netværk er valgt godt, skal det gerne give en høj
sandsynlighed for targetordet, når vi giver de tre inputord som input.
Vi forsøger derfor at vælge vægtene i det neurale netværk, så netværket
giver en høj sandsynlighed for targetordet. Når vi bestemmer vægtene, så
de passer til træningsdata, siger vi, at vi *træner* det neurale
netværk.

Lad os se på en enkelt række i datasættet, fx den der svarer til
sekvensen \"og en kat løber\". Vores inputord er \"og\", \"en\" og
\"kat\". Dem oversætter vi til vektoren $\overrightarrow{x}$ ved at
bruge ordenes Word2Vec-vektorer som beskrevet ovenfor. Output fra det
neurale netværk er en vektor $\overrightarrow{z}$, hvis $i$te koordinat
giver sandsynligheden for, at det $i$te ord i ordforrådet er det næste
ord. Hvis vi udelukkende ser på sekvensen \"og en kat løber\", og
ignorerer alle de andre sekvenser i træningsdata, så skal \"løber\" have
sandsynligheden 1, og alle andre ord skal have sandsynligheden 0.
Vektoren med $1$ i den koordinat, der svarer til det korrekte ord, og
$0$ i alle andre koordinater kaldes *targetvektoren*
$\overrightarrow{t}$.

I praksis rammer vores sandsynlighedsvektor $\overrightarrow{z}$ aldrig
target $\overrightarrow{t}$ præcist, fordi der også skal tages højde for
de andre sekvenser i datasættet. Det kunne fx være, at sekvensen \"og en
kat spiser\" også forekommer et sted i træningsdata. I så fald skal
\"spiser\" også have høj sandsynlighed.

I det mindste vil vi gerne have, at $\overrightarrow{z}$ kommer tæt på
targetvektoren $\overrightarrow{t}$. Vi måler, hvor langt vi er fra
target med en *tabsfunktion* LINK. Den tabsfunktion, vi vil bruge her,
kaldes *cross-entropy*. Med output $\overrightarrow{z}$ og targetvektor
$\overrightarrow{t}$, er cross-entropy givet ved
$$
CE(\overrightarrow{z},\overrightarrow{t})=-t_1\ln(z_1)-t_2\ln(z_2)- \cdots  -t_V\ln(z_V)
$$
Targetvektoren er $0$ på alle koordinater undtagen den, der svarer til
det korrekte ord, så alle andre led i summen er 0. Lad os sige, det
korrekte ord har nummeret $c$ i vores ordforråd, så $t_c=1$ og $t_j=0$
for alle $j\neq c$. Så er
$CE(\overrightarrow{z},\overrightarrow{t})=-\ln(z_c)$. Da $z_c$ er
sandsynligheden for, at vores targetord er det næste, vil vi gerne have,
at $z_c$ er så stor som muligt. Da den naturlige logaritme er en
voksende funktion, svarer det til, at
$CE(\overrightarrow{z},\overrightarrow{t})=-\ln(z_c)$ skal være så lille
som muligt.GRAF FOR LN

Dette gentager vi nu for hver eneste række i vores træningsdata. Vi
beregner en cross-entropy for hver. Til sidst lægger vi alle disse
cross-entropy sammen til en samlet tabsfunktion $L$, som helst skal være
så lille som muligt. Vi har udregnet $L$ udfra vores træningsdata og
vægtene. Træningsdata er det, vi går ud fra, vi ved, så det kan vi ikke
lave om på for at minimere $L$. Derfor betragter vi nu $L$ som en
funktion af vægtene $w_{j,k}$ og $u_{i,j}$. Vi ønsker at bestemme
vægtene således, at $L$ bliver mindst mulig svarende til, at vores
sandsynligheder kommer så tæt på target som muligt. Vi skal altså finde
minimum for en funktion af mange variable. Det kan man fx gøre ved hjælp
af *gradientnedstigning*, som du kan læse mere om HER. For at lave
gradientnedstigning er det vigtigt at kunne finde de partielt afledte af
$L$. En smart måde at lave gradientnedstigning på kaldes
*backpropagation*. Se mere om backpropagation HER.

Tekstgenerering {#tekstgenerering .unnumbered}
---------------

Las os sige, at vi har fået trænet vores neurale netværk. Det vil sige,
at vi har bestemt de vægte, der skal indgå. Så er vores neurale netværk
en fastlagt funktion. Når vi giver netværket en inputvektor
$\overrightarrow{x}$, beregner det en outputvektor $\overrightarrow{z}$
af sandsynligheder ved brug af de valgte vægte.

Vi kan nu gå i gang med at generere tekst. Lad os sige, at vi har dannet
de første ord i en sætning. Det kunne være
$$
\text{"En hund og en kat ---"}
$$
Vi tager de $3$ sidste ord \"og\",
\"en\" og \"kat\" og oversætter dem til en vektor $\overrightarrow{x}$.
Denne vektor giver vi som input til det neurale netværk. For hvert ord i
ordforrådet beregner det neurale netværk sandsynligheden for, at det er
det næste ord. Det kan være at \"løber\" får sandsynligheden 1/2,
\"spiser\" får sandsynligheden 1/3, mens alle andre ord får meget små
sandsynligheder. En mulighed er så at vælge det mest sandsynlige ord som
det næste. Det ville være \"løber\" i vores eksempel. Det viser sig dog,
at det giver for lidt variation i de sætninger, der dannes. I stedet kan
man vælge et tilfældigt næste ord ud fra deres sandsynligheder. I vores
eksempel ville vi vælge \"løber\" med sandsynlighed 1/2, \"spiser\" med
sandsynlighed 1/3, osv.
