---
title: "Word2vec"
image: "images/COLOURBOX35006551.jpg"
description-meta: "Word2vec."
---

# Word2vec

Lad os igen se på en situation, hvor vi gerne vil kunne gætte næste ord
i en sætning. Lad os sige, at vi har sætningen
$$
\text{"Min hund har en blød ---"}
$$
og vil gætte næste ord. Hvis vi
har en stor mængde tekst til rådighed, et tekstkorpus, kan vi
selvfølgelig lede efter ordsekvensen \"Min hund har en blød\" og se
hvilket ord, der oftest kommer efter som beskrevet i link. Men hvis ikke
sekvensen forekommer i vores korpus, så har vi et problem. I stedet
kunne vi lede efter en sætning med en betydning, der minder om \"Min
hund har en blød\" og se, hvad der kommer efter den. Men hvordan får vi
en computer til at forstå betydningen af ord?

Den simpleste måde at repræsentere et ord på i en computer ville være
ved at nummerere alle ordene i det danske sprog fra 1 til $V$, hvor $V$
er det samlede antal ord. Nummeret på et ord giver dog ikke megen
information om ordets betydning.

En anden nærliggende idé kunne være at repræsentere et ord ved
bogstaverne i ordet. For at en computer skal kunne forstå det, kunne man
give hvert bogstav et tal ud fra bogstavets nummer i alfabetet. Så ville
\"kat\" blive til $(11,1,20)$ og \"hund\" til $(8,21,14,4)$. Stavemåden
fortæller dog heller ikke meget om betydningen af et ord. Ordet \"mund\"
staves næsten lige som \"hund\", men har en helt anden betydning.
Omvendt betyder ordet \"vovse\" næsten det samme som \"hund\", men
staves helt anderledes.

I stedet vil vi gerne repræsentere hvert ord med en vektor. Idéen er, at
ord, hvis betydning ligner hinanden, skal repræsenteres med vektorer,
der peger i nogenlunde samme retning og har nogenlunde samme længde, som
illustreret på Figur ?.

FIGUR

I kender vektorer i 2 eller 3 dimensioner og ved, at de kan skrives på
formen
$$
\begin{pmatrix}
a_1\\a_2\end{pmatrix}\text{ og } \begin{pmatrix} a_1\\a_2 \\a_3
\end{pmatrix}
$$
hvor $a_1, a_2$ og (eventuelt) $a_3$ er reelle tal, der kaldes vektorens
koordinater. Tre koordinater er dog ikke nok til at indfange betydningen
af alle ord i sproget. Derfor bruger man i stedet en *$m$-dimensional
vektor*, som man kan tænke på som en liste af $m$ koordinater
$$
\begin{pmatrix}
a_1\\a_2\\a_3\\ \vdots \\ a_m
\end{pmatrix}
$$
hvor
$a_1,a_2,a_3,\ldots,a_m$ er reelle tal. I praksis vælger man $m$ stort,
fx $m=100$. Man kan regne med $m$-dimensionale vektorer, lige som man
gør i to eller tre dimensioner. Vi kommer for eksempel til at se,
hvordan man kan finde skalarprodukter. Til gengæld har man ikke mulighed
for at visualisere en $m$-dimensional vektor som en pil i et
koordinatsystem, men det har vi heldigvis heller ikke brug for.

I denne note ser vi på algoritmen Word2Vec som et eksempel på, hvordan
man kan oversætte ord til vektorer, der repræsenterer ordenes betydning.
Algoritmen blev opfundet af en gruppe medarbejdere hos Google i 2013. I
dag bruges diverse forfininger af algoritmen i mange store
sprogmodeller.

Betydning og kontekst {#betydning-og-kontekst .unnumbered}
---------------------

Hvilke egenskaber skal de vektorer, der repræsenterer ord, så have? Jo,
idéen er, at vektorerne skal indfange betydningen af et ord i den
forstand, at ord, hvis betydning minder om hinanden, svarer til
vektorer, der ligner hinanden. Og hvordan ved vi så, om to ords
betydning minder om hinanden? Betydningen af et ord har noget at gøre
med, hvilke sammenhænge det optræder i, når man kigger i rigtig mange
dokumenter. En sproglig sammenhæng kaldes også en *kontekst*. Ord, der
betyder næsten det samme vil altså ofte optræde i samme kontekst. Ordene
\"hund\" og \"kat\" er for eksempel forskellige, men de vil ofte optræde
i sammenhænge, der ligner hinanden. Se for eksempel på sætningerne
$$
\text{"Min --- har spist af sin madskål"}}
$$
og
$$
\text{"Sikken en blød pels, din --- har"}
$$
Her ville der kunne stå
\"hund\" eller \"kat\", men nok ikke \"kælk\" eller \"badedragt\".
Betydningen af ordene \"hund\" og \"kat\" er tættere på hinanden end
betydningen af \"hund\" og \"kælk\". På den anden side kunne der ikke
stå \"hund\" i sætningen
$$
\text{"Den lille --- har hvide knurhår"}
$$
mens både \"kat\" og \"mis\" ville passe ind. Ordene \"kat\" og \"hund\"
er altså tætte på hinanden, men ikke så tætte som \"kat\" og \"mis\". Vi
vil derfor gerne have, at vektorerne for \"hund\" og \"kat\" minder mere
om hinanden end vektorerne for \"hund\" og \"kælk\", men ikke så meget
som \"kat\" og \"mis\". Hvad vi forstår ved, at vektorer minder om
hinanden, kommer vi tilbage til.

Konteksten er særlig vigtig i forbindelse med ord med flere betydninger.
Vi forstår fx betydningen af ordet \"marsvin\" forskelligt alt efter om
ordet \"hav\" eller ordet \"mælkebøtte\" optræder i nærheden af det.

Træningsdata {#træningsdata .unnumbered}
------------

Vi har altså brug for at vide, hvilken kontekst ordene indgår i. For at
lære, hvilken kontekst et ord forekommer i, tager vi udgangspunkt i et
stort tilgængeligt tekstkorpus. Vi kalder dette korpus for vores
*træningsdata*.

Ved konteksten til et ord vil vi her forstå de ord, der står umiddelbart
før og efter ordet. Mere præcist vælger vi et vindue, lad os sige på fem
ord, hvor ordet i midten er det, vi gerne vil kende betydningen af. Vi
vil kalde dette for *inputordet*. De to første ord og de to sidste ord i
vinduet er inputordets *kontekstord*. Se fx på sætningen
$$
\text{\fbox{Den sorte \textbf{hund} logrer med} halen}
$$
hvor boksen
angiver vores 5-ords vindue. Det midterste ord \"hund\" er vores
inputord, ordene \"Den\", \"sorte\", \"logrer\" og \"med\" er
kontekstord til \"hund\".

Vi starter med at placere vinduet omkring det første ord i vores datasæt
og noterer dets fire kontekstord (hvoraf de to vil være blanke). Vi
flytter nu vinduet mod højre et ord ad gangen, og hver gang noterer vi
inputordet og dets fire kontekstord. Vi gør det for al teksten i vores
træningsdata og samler informationen i et datasæt som vist i Tabel
[\[tab:positiv\]](#tab:positiv){reference-type="ref"
reference="tab:positiv"}.

   Input   Kontekst
  ------- ----------
     ⋮        ⋮
   sorte  
   sorte     Den
   sorte     hund
   sorte    logrer
   hund      Den
   hund     sorte
   hund     logrer
   hund      med
     ⋮        ⋮

  : Positive eksempler på input- og kontekstord[]{label="tab:positiv"}

Hver række i tabellen består af et inputord og et af dets kontekstord.
Sådan et par kalder vi *positive eksempler*. (Fodnote: Når man vil gætte
det næste ord i en sætning har man selvfølgelig kun lov til at bruge de
ord, der kommer før ordet. Det er dog ikke det, vi er ude på, når vi
laver Word2Vec. Vi er ude på at forstå, hvordan et ord forholder sig til
dets kontekst, altså de omkringstående ord. Derfor er der ikke noget
problem i, at vinduet både indeholder ord før og efter inputordet. )

Vi vil gerne kunne modellere sandsynligheden for, at et ord $w$ har
ordet $c$ som kontekst. For at kunne gøre det, er vi nødt til at have
noget at sammenligne med i form af eksempler på ord, der ikke er
kontekstord. Og hvor får vi så dem fra? Ja, dem laver vi da bare selv.

For hvert positivt eksempel i datasættet vælger vi $n$ *negative
eksempler*, som er $n$ tilfældigt valgte ord blandt alle ordene i vores
ordforråd undtagen kontekstordet i eksemplet. For eksempel kunne man
sætte $n=3$. For parret bestående af inputordet \"hund\" og
kontekstordet \"sorte\" vælger man så tre tilfældige kontekstord, som
ikke er \"sorte\". Det kunne være \"og\", \"palme\" og \"synger\". Det
giver os et datasæt på formen vist i Tabel
[\[tab:negativ\]](#tab:negativ){reference-type="ref"
reference="tab:negativ"}.

   Input   Positiv   Negativ   Negativ   Negativ
  ------- --------- --------- --------- ---------
     ⋮        ⋮         ⋮         ⋮         ⋮
   sorte    hund       og       palme    synger
   sorte   logrer     bage       fra     tredive
   hund      Den     mellem    jordbær    emne
     ⋮        ⋮         ⋮         ⋮         ⋮

  : Positive og negative eksempler

[\[tab:negativ\]]{#tab:negativ label="tab:negativ"}

I praksis vælges alle de negative eksempler ikke med samme
sandsynlighed. Hyppigt forekommende ord har en større sandsynlighed for
at blive valgt end sjældnere ord. Hvis $h_i$ er hyppigheden af $i$te
ord, så vælges det $i$te ord med sandsynligheden
$$
\frac{h_i^\alpha}{\sum_{j=1}^V h_j^\alpha}
$$
hvor $V$ er antallet af
ord i sproget. I praksis benyttes ofte $\alpha=3/4$. På den måde bliver
sandsynlighederne ikke helt proportionale med hyppighederne. Vi giver en
lille fordel til sjældne ord, men ikke så meget som hvis vi valgte alle
ord med samme sandsynlighed.

Nu har vi fået lavet et datasæt bestående af både positive og negative
eksempler på input- og kontekstord. I de næste to afsnit ser vi på,
hvordan man kan modellere sandsynligheden for at et ord $c$ optræder som
kontekst til et inputord $w$ ved hjælp af vektorer. Det vil vi sidenhen
bruge til at vælge vektorerne, således at vores positive eksempler i
datasættet bliver meget sandsynlige og de negative eksempler bliver
meget usandsynlige.

Input- og kontekstvektorer {#input--og-kontekstvektorer .unnumbered}
--------------------------

I første omgang vil vi lade hvert ord $w$ være repræsenteret af to
vektorer, $\overrightarrow{v}_{w}$ og $\overrightarrow{k}_{w}$, hvor
$\overrightarrow{v}_{w}$ repræsenterer ordet, når det optræder som
input, mens $\overrightarrow{k}_{w}$ repræsenterer ordet, når det
optræder som kontekst. Betydningen af $w$ afgøres som nævnt af hvordan
ordet forholder sig til konteksten. Det vil vi oversætte matematisk til,
hvordan $\overrightarrow{v}_{w}$ forholder sig til kontekstvektorerne
$\overrightarrow{k}_{c}$ for diverse kontekstordord $c$. Vi vil derfor
tænke på $\overrightarrow{v}_{w}$ som den vektor, der repræsenterer
betydningen af ordet, og altså den vi er ude på at bestemme.

For at måle hvordan inputvektoren $\overrightarrow{v}_{w}$ for ordet $w$
forholder sig til kontekstvektoren $\overrightarrow{k}_{c}$ for ordet
$c$, vil vi bruge *skalarproduktet*
$\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$. Husk på, at man
finder skalarproduktet mellem to vektorer i to dimensioner ved formlen
$$
\begin{pmatrix}
a_1\\a_2
\end{pmatrix}
\cdot
\begin{pmatrix}
b_1\\b_2
\end{pmatrix}
=a_1b_1+a_2b_2
$$
og i tre dimensioner ved formlen
$$
\begin{pmatrix}
a_1\\a_2\\a_3
\end{pmatrix}
\cdot 
\begin{pmatrix} 
b_1\\b_2\\b_3
\end{pmatrix}
=a_1b_1+a_2b_2+a_3b_3
$$
Tilsvarende kan man definere skalarproduktet mellem to $m$-dimensionale
vektorer ved
$$
\begin{pmatrix}
a_1\\a_2\\a_3\\ \vdots\\a_m
\end{pmatrix}
\cdot 
\begin{pmatrix} 
b_1\\b_2\\b_3\\ \vdots \\b_m
\end{pmatrix}
=a_1b_1+a_2b_2+a_3b_3+\dotsm + a_mb_m
$$

Vi vil gerne have, at vores input- og kontekstvektorer skal opfylde, at
hvis $w$ ofte har $c$ som kontekst, så er skalarproduktet
$\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ stort, mens en
meget negativ værdi af
$\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ indikerer, at $w$
sjældent har $c$ som kontekst.

Hvad fortæller skalarproduktet om, hvordan to vektorer forholder sig til
hinanden? I 2 og 3 dimensioner kan vi give en geometrisk fortolkning af
skalarproduktet ved hjælp af formlen 
$$
\label{eq:skalar}
\overrightarrow{a}\cdot\overrightarrow{b} = |\overrightarrow{a}||\overrightarrow{b}| \cos(\angle (\overrightarrow{a},\overrightarrow{b}))
$$
hvor $\angle (\overrightarrow{a},\overrightarrow{b})$ er vinklen mellem
vektorerne $\overrightarrow{a}$ og $\overrightarrow{b}$, og
$|\overrightarrow{a}|$ betegner længden af $\overrightarrow{a}$, som
findes med formlen
$$
|\overrightarrow{a}|=\sqrt{\overrightarrow{a}\cdot\overrightarrow{a}}
$$
Cosinus er en aftagende funktion på intervallet $[0,\pi]$, så jo større
vinklen $\angle (\overrightarrow{a},\overrightarrow{b})$ er, desto
mindre vil $\cos(\angle (\overrightarrow{a},\overrightarrow{b}))$ være.
figur Det betyder, at
$\cos(\angle (\overrightarrow{a},\overrightarrow{b}))$ er størst når
vinklen mellem $\overrightarrow{a}$ og $\overrightarrow{b}$ er 0,
svarende til at vektorerne peger samme vej. Her er
$\cos(\angle (\overrightarrow{a},\overrightarrow{b}))=1$. Den mindste
værdi af $\cos(\angle (\overrightarrow{a},\overrightarrow{b}))$ er $-1$,
som antages ved en vinkel på $\pi$, hvor vektorerne peger i modsat
retning.

Jo mindre vinklen mellem $\overrightarrow{v}_{w}$ og
$\overrightarrow{k}_{c}$ er, desto større er deres skalarprodukt
$\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ altså, og jo oftere
har ordet $w$ dermed $c$ som kontekst. Desuden viser
[\[eq:skalar\]](#eq:skalar){reference-type="eqref"
reference="eq:skalar"}, at lange vektorer tæller mere, både positivt og
negativt, end korte vektorer. Ord, der er gode til at forudsige
konteksten udfra, fx \"logre\", vil derfor blive repræsenteret med
længere inputvektorer end ord som \"og\" eller \"er\", der ikke
indeholder megen information om konteksten. Altså vil
$|\overrightarrow{v}_{logre}|$ være større end
$|\overrightarrow{v}_{og}|$ og $|\overrightarrow{v}_{er}|$.

Bemærk også, at hvis to ord ofte optræder i samme kontekst, skal deres
inputvektorer gerne have nogenlunde samme skalarprodukt med alle
kontekstvektorer. Formlen
[\[eq:skalar\]](#eq:skalar){reference-type="eqref"
reference="eq:skalar"} viser, at det betyder, at de dels skal have
nogenlunde samme længde og dels skal have nogenlunde samme vinkel med
alle kontekstvektorerne. Sidstnævnte kræver, at de har nogenlunde samme
retning. Ord, hvis betydning ligner hinanden, kommer derfor til at svare
til vektorer, hvis længde og retning ligner hinanden.

I praksis bruger vi vektorer af højere dimension end 3. Det er måske
ikke helt oplagt, hvad man skal forstå ved længden af en vektor eller
vinklen mellem to vektorer i højere dimensioner, men det viser sig, at
man stadigvæk godt kan give mening til formlen
[\[eq:skalar\]](#eq:skalar){reference-type="eqref"
reference="eq:skalar"}. Intuitionen fra to eller tre dimensioner er
derfor god at have, selv om vi arbejder med højere dimensioner.

### Eksempel 1 {#eksempel-1 .unnumbered}

Forslag til eksempel. Forslag til bedre ord modtages gerne, der kan nok
også laves en flottere figur. Lad os sige, at vi har fundet en
vektorrepræsentation, der opfylder det ønskede. Det gav vektorerne
$$
\begin{aligned}
&\overrightarrow{v}_{hund} =  \begin{pmatrix} 1.5\\ 1\end{pmatrix}, \overrightarrow{v}_{kat} =  \begin{pmatrix} 1\\ 1.5\end{pmatrix} \\
&\overrightarrow{k}_{madsk\aa l}  =  \begin{pmatrix} 1.5\\1.5\end{pmatrix}, \overrightarrow{k}_{badedragt} =  \begin{pmatrix} -1\\-0.75 \end{pmatrix}, \overrightarrow{k}_{lufte} =  \begin{pmatrix} 1\\0.25 \end{pmatrix}
\end{aligned}
$$
Vektorerne er vist herunder, hvor inputvektorer er røde, og
kontekstvektorer er blå:

Vi kan udregne skalarprodukterne
$$
\begin{aligned}
&\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l} = \begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} 1.5\\ 1.5\end{pmatrix} = 1\cdot 1.5 +  1.5\cdot 1.5 = 3.75 \\ 
&\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{badedragt} = \begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} -1\\ -0.75\end{pmatrix} = 1\cdot (-1) +  1.5\cdot (-0.75) = -2.125 
\end{aligned}
$$
Vi ser, at
$\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l}$ er
større end
$\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{badedragt}$. Det
svarer til, at ordet \"kat\" oftere har \"madskål\" som kontekst, end
det har \"badedragt\". Det ses også ved, at
$\overrightarrow{k}_{madsk\aa l}$ peger i nogenlunde samme retning som
$\overrightarrow{v}_{kat}$, mens $\overrightarrow{k}_{badedragt}$ peger
i en helt anden retning.

Vi kan også udregne
$$
\begin{aligned}
&\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{madsk\aa l} = \begin{pmatrix} 1.5\\ 1\end{pmatrix}  \cdot  \begin{pmatrix} 1.5\\ 1.5\end{pmatrix} = 1.5\cdot 1.5 +  1\cdot 1.5 = 3.75
\end{aligned}
$$
Vi ser, at
$\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{madsk\aa l} =\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l} =3.75$,
svarende til at både \"hund\" og \"kat\" ofte har \"madskål\" som
kontekst. Vi ser da også, at vektorerne $\overrightarrow{v}_{hund}$ og
$\overrightarrow{v}_{kat}$ peger i nogenlunde samme retning og har
nogenlunde samme længde, fordi de tit har samme kontekst. De to vektorer
er dog ikke helt ens, da der også vil være nogle kontekstord, der er
ikke er lige hyppige for \"hund\" og \"kat\". Vi kan fx udregne
$$
\begin{aligned}
&\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{lufte} = \begin{pmatrix} 1.5\\ 1\end{pmatrix}  \cdot  \begin{pmatrix} 1\\ 0.25\end{pmatrix} = 1.5\cdot 1 + 1\cdot 0.25 =1.75\\
&\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{lufte} =\begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} 1\\ 0.25\end{pmatrix} = 1\cdot 1 + 1.5\cdot 0.25 = 1.375 
\end{aligned}
$$
Her er $\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{lufte}$
større end $\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{lufte}$
svarende til, at \"hund\" oftere har \"lufte\" som kontekst, end \"kat\"
har. bedre ord end \"lufte\"?

begynd box

Måske har du undret dig over, hvorfor vi har brug for to forskellige
vektorer for hvert ord. Hvert positivt eksempel forekommer jo to gange i
vores datasæt, hvor de to ord skiftes til at spille rollen som input og
kontekst. Alligevel er der ikke symmetri mellem de to ord. Hvis fx
inputordet er \"logre\", er det ret sandsynligt, at \"hund\" er et af
kontekstordene. Inputordet \"hund\" forekommer derimod i mange
kontekster, der ikke involverer ordet \"logre\". Sandsynligheden for
kontekstordet afhænger derfor af, om det er \"hund\" eller \"logre\",
der er input. Hvis vi kun brugte én vektor for hvert ord,
$\overrightarrow{u}_{hund}$ og $\overrightarrow{u}_{logre}$, ville vi få
samme skalarprodukt
$\overrightarrow{u}_{hund}\cdot \overrightarrow{u}_{logre} =  \overrightarrow{u}_{logre}\cdot \overrightarrow{u}_{hund}$,
og dermed samme sandsynlighed, uanset hvilket ord der var input.

slut box

### Opgaver {#opgaver .unnumbered}

-   Antag at vi har lavet 4-dimensionale input- og kontekstvektorer
    således, at jo større skalarproduktet
    $\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ er, desto mere
    sandsynligt er det, at ordet $w$ har $c$ som kontekst. Inputvektoren
    for \"hund\" og kontekstvektorerne for \"pels\" og \"fjer\" er
    $$
    \begin{aligned}
    \overrightarrow{v}_{hund}=\begin{pmatrix} 0.5\\2\\1\\-1\end{pmatrix} ,\quad
    \overrightarrow{k}_{pels}=\begin{pmatrix} 0\\3\\2\\-2\end{pmatrix},\quad
    \overrightarrow{k}_{fjer}=\begin{pmatrix} 1\\-2\\1.5\\0.5\end{pmatrix}
    \end{aligned}
    $$
    Udregn skalarprodukterne
    $\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{pels}$ og
    $\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{fjer}$. Passer
    det med, hvilket af ordene \"pels\" og \"fjer\" der er mest
    sandsynligt som kontekst til \"hund\"?

-   Antag at vi har lavet 3-dimensionale input- og kontekstvektorer som
    beskrevet i afsnittet ovenfor. Så skulle ord, der ofte har samme
    kontekst gerne have nogenlunde samme længde og retning, mens ord,
    der betyder noget helt forskelligt, kan have meget forskellig længde
    og retning. Antag at inputvektorerne for \"kat\", \"hund\", \"mis\"
    og \"kælk\" er 
    $$
    \begin{aligned}
    \overrightarrow{v}_{kat}=\begin{pmatrix}0\\2\\1 \end{pmatrix},\quad
    \overrightarrow{v}_{hund}=\begin{pmatrix}0\\1\\1.7\end{pmatrix}, \quad
    \overrightarrow{v}_{mis}=\begin{pmatrix}0.4\\2\\0.9\end{pmatrix},\quad
    \overrightarrow{v}_{kælk}=\begin{pmatrix} 2\\-1\\0 \end{pmatrix}
    \end{aligned}
    $$
    Find længden af de fire vektorer. Find vinklen mellem
    $\overrightarrow{v}_{kat}$ og de tre øvrige vektorer. Stemmer
    resultatet overens med hvilke ord der er tættest på \"kat\" i
    betydning? Tegn vektorerne ind i GeoGebra.

Model for sandsynligheder {#model-for-sandsynligheder .unnumbered}
-------------------------

Som nævnt, vil vi gerne have, at jo større skalarproduktet
$\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ er, desto mere
sandsynligt er det, at ordet $w$ har ordet $c$ som kontekst.
Skalarprodukter kan imidlertid antage alle reelle værdier, så de egner
sig ikke til at repræsentere en sandsynlighed, der jo skal være et tal
mellem 0 og 1. Derfor anvender vi en funktion $f$ på skalarproduktet med
definitionsmængde $\mathbb{R}$ og værdimængde $]0,1[$ for at få
skalarprodukterne lavet om til tal mellem 0 og 1. Den funktion, vi vil
bruge, er *sigmoid-funktionen* (også nogle gange kaldet den logistiske
funktion). Forskriften er
$$
\label{eq:sigmoid}
f(x)=\frac{1}{1+e^{-x}}
$$
og grafen ses herunder indsæt graf Det ses af
grafen, at $f$ er strengt voksende, og at værdimængden for $f$ er
$]0,1[$.

Word2Vec-algoritmen modellerer sandsynligheden $P(c\mid w)$ for at ordet
$c$ er kontekst til inputordet $w$ som (Fodnote: Bemærk at vi bruger
notationen $P(c\mid w)$ lidt anderleds end i \"Simple sprogmodeller\".
Der betød det sandsynligheden for at $c$ er næste ord efter $w$. Her
betyder det sandsynligheden for, at $c$ er kontekst til $w$.)
$$
\label{eq:pcw}
P(c\mid w) = f(\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c})=\frac{1}{1+e^{-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}}}
$$
Denne sandsynlighed vil ligge i værdimængden for $f$, som var $]0,1[$.
Da $f$ er strengt voksende, vil sandsynligheden $P(c \mid w)$ være
større, jo større $\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}$
er. Bemærk desuden, at sandsynligheden for, at $c$ ikke er kontekst til
$w$, er givet ved
$$
\begin{aligned}
\label{eq:pneg}
1-P(c\mid w) &= 1-f(\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}) = f(-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c})
= \frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}}}
\end{aligned}
$$
hvor vi brugte, at $1-f(x)=f(-x)$. Dette vises i boksen nedenfor.

### Eksempel 2 {#eksempel-2 .unnumbered}

I Eksempel 1 fandt vi, at
$\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte} = 1.75$.
Sandsynligheden for, at \"hund\" har \"lufte\" som kontekst, er derfor
$$
P(\text{ lufte } \mid \text{ hund }) = \frac{1}{1+e^{-\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte}}}=\frac{1}{1+e^{-1.75}} \approx  0.852.
$$
Sandsynligheden for, at \"hund\" ikke har \"lufte\" som kontekst, er
$$
1- P(\text{ lufte } \mid \text{ hund }) = \frac{1}{1+e^{\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte}}}=\frac{1}{1+e^{1.75}} \approx 0.148.
$$

begynd box1

Man kan også finde værdimængden for $f$ ved at kigge på forskriften for
$f$. Da både tæller og nævner i
[\[eq:sigmoid\]](#eq:sigmoid){reference-type="eqref"
reference="eq:sigmoid"} er positive, får vi at
$$
f(x)=\frac{1}{1+e^{-x}}>0
$$
Da eksponentialfunktionen kun kan antage
positive værdier, er $1<1+e^{-x}$, så
$$
f(x)=\frac{1}{1+e^{-x}}< \frac{1+e^{-x}}{1+e^{-x}} =1
$$
Vi har derfor,
at værdimængden for $f$ er indeholdt i $]0,1[$, hvilket kan skrives som
$$
\text{Vm}(f) \subseteq ]0,1[
$$
For at vise, at hele intervallet
$]0,1[$ er med i værdimængden, viser vi, at funktionsværdierne kan komme
så tæt på 0 og 1, som vi ønsker. Vi ser derfor på, hvad der sker med
$f(x)$ når $x$ nærmer sig $-\infty$ og $\infty$.

Figur ? viser grafen for $e^{-x}$:

indsæt graf

Da $e^{-x}$ er en aftagende eksponentialfunktion, har vi at
$$
e^{-x} \to \infty \text{ når } x\to -\infty
$$
og derfor
$$
f(x)=\frac{1}{1+e^{-x}}\to 0 \text{ når } x\to -\infty
$$
Desuden vil
$$
e^{-x} \to 0 \text{ når } x\to \infty
$$
og derfor
$$
f(x)=\frac{1}{1+e^{-x}}\to 1 \text{ når } x\to \infty
$$
Alt i alt har
vi altså vist, at $f$ har værdimængde $]0,1[$, nærmer sig 0, når
$x\to -\infty$, og nærmer sig 1, når $x\to \infty$.

Hvis man differentierer $f$, får man
$$
f'(x)=\frac{e^{-x}}{(1+e^{-x})^2}
$$
Vi ser, at både tæller og nævner
er positive, så $f'(x)>0$ for alle $x$. Da den afledte er positiv, er
$f$ en strengt voksende funktion.

slut box1

begynd box2

Vi viser nu, at $1-f(x) = f(-x)$.
$$
\label{eq:1-f}
1-f(x) = 1-\frac{1}{1+e^{-x}} = \frac{1+e^{-x}}{1+e^{-x}}-\frac{1}{1+e^{-x}}=\frac{e^{-x}}{1+e^{-x}}=\frac{1}{1+e^{x}} =f(-x)
$$
hvor vi forlængede brøken med $e^x$ i det sidste lighedstegn. det kunne
man evt også vise på en figur?

slut box2

### Opgaver {#opgaver-1 .unnumbered}

-   Lad $f$ være sigmoidfunktionen. Vis at
    $$
    f'(x)=\frac{e^{-x}}{(1+e^{-x})^2}
    $$

-   Antag at vi har fundet vektorrepræsentationer af ord, og at
    inputvektoren for \"hund\" og kontekstvektorerne for \"pels\" og
    \"fjer\" er givet ved 
    $$
    \begin{aligned}
    \overrightarrow{v}_{hund}=\begin{pmatrix} 0.5\\2\\1\\-1\end{pmatrix} ,\quad
    \overrightarrow{k}_{pels}=\begin{pmatrix} 0\\3\\2\\-2\end{pmatrix},\quad
    \overrightarrow{k}_{fjer}=\begin{pmatrix} 1\\-2\\1.5\\0.5\end{pmatrix}
    \end{aligned}
    $$
    Hvad er sandsynligheden for at henholdsvis \"pels\" og \"fjer\" er
    kontekst til \"hund\"? Hvad er sandsynligheden for at \"fjer\" ikke
    er kontekst til \"hund\"?

Estimation af vektorer {#estimation-af-vektorer .unnumbered}
----------------------

Hvordan bestemmer vi så vektorrepræsentationerne
$\overrightarrow{v}_{w}$ og $\overrightarrow{k}_{c}$ i praksis? Her får
vi brug for det datasæt, som vi lavede ud fra vores tekstkorpus. Hver
række i datasættet bestod af et inputord $w$, et positivt kontekstord
$c^{+}$, som forekom som kontekst til $w$ i datasættet, og tre negative
eksempler på kontekstord, som vi vil betegne $c_{1}^-,c_{2}^-,c_{3}^-$.
Vores datasæt har altså formen

   Input   Positiv    Negativ     Negativ     Negativ
  ------- --------- ----------- ----------- -----------
    $w$    $c^{+}$   $c^-_{1}$   $c_{2}^-$   $c_{3}^-$
   sorte    hund        og         palme      synger
   sorte   logrer      bage         fra       tredive
     ⋮        ⋮          ⋮           ⋮           ⋮

  : Eksempel på datatabel[]{label="tab:data"}

Lad os sige, at der er $M$ rækker i vores træningsdata. Vi betegner data
i den $j$te række med $(w_j,c_j^+,c_{j1}^-,c_{j2}^-,c_{j3}^-)$ for
$j=1,\ldots,M$.

Vi ser nu på $j$te række. Hvis vores model
[\[eq:pcw\]](#eq:pcw){reference-type="eqref" reference="eq:pcw"} for
sandsynligheden for kontekstord er god, skulle det positive eksempel, vi
har observeret, gerne have høj sandsynlighed. Vi vil altså gerne vælge
vores vektorer således, at $P(c_j^+\mid w_j)$ er høj. Disse
sandsynligheder vil være meget små i praksis, da der kan være mange
meningsfulde kontekstord til $w_j$. For at undgå numeriske problemer,
tager vi derfor den naturlige logaritme og får $\ln (P(c_j^+\mid w_j))$.
Fodnote: Husk på, at hvis vi har et tal meget tæt på nul, fx $10^{-a}$,
og vi tager den naturlige logaritme, så får vi
$\ln(10^{-a}) = -a\cdot \ln(10)$, som er et tal på en mere normal skala,
som en computer bedre kan finde ud af at regne på. Da den naturlige
logaritme er en strengt voksende funktion, svarer en høj sandsynlighed
$P(c_j^+\mid w_j)$ til, at $\ln(P(c_j^+\mid w_j))$ er høj. Vi vil derfor
gerne have, at $-\ln(P(c_j^+\mid w_j))$ er lav. Malene indsætter graf.
Fodnote: Vores sandsynligheder vil være tal mellem 0 og 1, så når vi
tager den naturlige logaritme, får vi et negativt tal, se Figur ?. Minus
den naturlige logaritme bliver derfor positiv.

Samtidig vil vi gerne have, at sandsynligheden for, at vores negative
eksempler ikke er kontekstord, er høj. Altså skal
$1-P(c_{ji}^-\mid w_j)$ være høj for $i=1,2,3$. Igen svarer det til, at
$-\ln(1-P(c_{ji}^-\mid w_j))$ skal være lav.

For at sikre, at både $-\ln(P(c_j^+\mid w_j))$ og
$-\ln(1-P(c_{ji}^-\mid w_j))$ for $i=1,2,3$ er lave, vil vi kræve at
deres sum er lav. Vi kræver altså at
$$
L_j = -\ln(P(c_j^+\mid w_j)) -\ln(1-P(c_{j1}^-\mid w_j)) -\ln(1-P(c_{j2}^-\mid w_j)) -\ln(1-P(c_{j3}^-\mid w_j))
$$
er lav. Formlerne [\[eq:pcw\]](#eq:pcw){reference-type="eqref"
reference="eq:pcw"} og [\[eq:pneg\]](#eq:pneg){reference-type="eqref"
reference="eq:pneg"} giver 
$$
\begin{aligned}
L_j =& -\ln(P(c_j^+\mid w_j)) -\ln(1-P(c_{j1}^-\mid w_j))\\
&-\ln(1-P(c_{j2}^-\mid w_j)) -\ln(1-P(c_{j3}^-\mid w_j))
\\=& -\ln \bigg(\frac{1}{1+e^{-\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c^+_j}}}\bigg) - \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j1}^-}}}\bigg)\\
&- \ln \bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j2}^-}}}\bigg) - \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j3}^-}}}\bigg)
\end{aligned}
$$
Her kan vi bruge logaritmeregnereglen $\ln(\tfrac{1}{a}) = -\ln(a)$ til
at skrive $L_j$ lidt pænere.
$$
\begin{aligned}
L_j =&\ln \Big({1+e^{-\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c^+_j}}}\Big) + \ln\Big({1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j1}^-}}}\Big)\\
&+ \ln \Big({1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j2}^-}}}\Big) + \ln\Big({1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j3}^-}}}\Big)
\end{aligned}
$$
Vores $L_j$ afhænger altså af, hvordan vores input- og kontekstvektorer
er valgt. Vi ønsker at vælge vektorerne således, at $L_j$ er lav.

Dette kan vi gentage for hver række i vores træningsdata. Vi får et
$L_j$ for hver række $j=1,\ldots,M$. Da vi gerne vil have, at alle $L_j$
er lave, kræver vi, at deres sum
$$L=L_1+ L_2 +\dotsm + L_M$$
er lav. Vi
kalder $L$ for vores *tabsfunktion*. Da hvert $L_j$ afhænger af input-
og kontekstvektorerne, kommer $L$ også til at afhænge af disse vektorer.
Vi ønsker derfor at vælge input- og kontekstvektorerne således, at de
minimerer $L$. Hvordan finder man minimum for $L$ i praksis? Det kan man
fx gøre ved hjælp af *gradientnedstigning*, som du kan læse mere om her.

Der er rigtig mange ord i det danske sprog. For hvert af dem skal vi
finde både en input- og en kontekstvektor, der hver har $m$ koordinater.
Alt i alt giver det rigtig mange koordinater, der skal bestemmes.
Antallet af ord på dansk afhænger lidt af, hvad man forstår ved et ord,
men 200.000 er et fornuftigt bud. Hvis vi vil repræsentere dem ved
vektorer af dimension $m=100$, får man brug for at bestemme 40.000.000
koordinater. For at kunne gøre det, er man også nødt til at have enormt
store mængder træningsdata til rådighed, dvs. antallet $M$ af positive
eksempler i træningsdataet skal være gigantisk stort. Bemærk dog, at
40.000.000 koordinater stadig er væsentlig færre, end hvis vi skulle
bestemme en sandsynlighed for hvert eneste af de $200.000^2$ mulige
kombinationer af input- og kontekstord. Dette er en anden fordel ved
Word2Vec.

Nu har vi set, hvordan man kan repræsentere et ord $w$ ved en
inputvektor $\overrightarrow{v}_{w}$ og en kontekstvektor
$\overrightarrow{k}_{w}$. Vektoren $\overrightarrow{v}_{w}$ er den, der
viser, hvordan $w$ forholder sig til sin kontekst, så det er den, der
repræsenterer betydningen af $w$. Normalt vil man derfor arbejde videre
med $\overrightarrow{v}_{w}$, mens $\overrightarrow{k}_{w}$ smides væk.
Vi mangler dog stadig at se, hvordan vektorerne
$\overrightarrow{v}_{w}$, som vi har fundet med Word2Vec, kan bruges i
forbindelse med kunstig intelligens.

### Eksempel 3 {#eksempel-3 .unnumbered}

Eges børnebog

Fra vektorer til tekstgenerering {#fra-vektorer-til-tekstgenerering .unnumbered}
--------------------------------

I det følgende ser vi på, hvordan vektorrepræsentationerne, som vi fandt
med Word2Vec, kan bruges til at lave en algoritme, der kan generere ny
tekst. De fleste tekstgenereringsalgoritmer fungerer ved, at de danner
teksten et ord ad gangen. Givet den tekst der allerede er dannet, prøver
algoritmen hele tiden at gætte, hvad det næste ord skal være. Det kan
gøres i to trin:

-   Først benyttes Word2Vec til at oversætte alle ordene i sproget til
    vektorer.

-   Dernæst genereres teksten. Givet den tekst, der allerede er dannet,
    bruger vi en (kompliceret) funktion, der tager
    vektorrepræsentationerne af de hidtil genererede ord som input. Som
    output giver funktionen det mest sandsynlige næste ord (eller et af
    de mest sandsynlige).

Hvis vi fx har genereret teksten
$$
\text{"Hunden spiser sit ---"}
$$
så
skal vi prøve at gætte, hvilket ord der kommer efter \"sit\". Vi
oversætter derfor ordene \"Hunden\", \"spiser\" og \"sit\" til
vektorerne $\overrightarrow{v}_{Hunden}$, $\overrightarrow{v}_{spiser}$
og $\overrightarrow{v}_{sit}$. Disse tre vektorer giver vi funktionen
som input, og som output får vi et nyt ord, måske \"kødben\". Den
funktion, der bruges i punkt 2., ville typisk være et *neuralt netværk*.
Du kan læse mere om, hvordan det fungerer i \"Tekstgenerering med
neurale netværk\".

Det smarte ved at bruge vektorrepræsentationerne er, hvis vi for
eksempel vil generere næste ord i
$$
\text{"Jeg skal huske, at katten skal have ---"}
$$
Det skulle gerne
give \"mad\" som muligt næste ord. Men måske har sprogmodellen aldrig
set sætningen \"katten skal have mad\". Hvis den til gengæld har set
\"hunden skal have mad\", og modellen ved at \"hund\" og \"kat\" tit har
samme kontekst, og dermed har næsten samme vektorrepræsentation, så vil
man alligevel få \"mad\" som muligt næste ord.

Kan man også få det til at virke for Eges børnebog?
