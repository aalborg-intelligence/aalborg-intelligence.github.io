<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Word2vec.">

<title>Word2vec – AI MAT - matematikken bag magien</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo/SVG/Bomaerke_05_AIMAT_2024.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a9a1858958e16a084745df87b4be869b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y219BCPS45"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
 
  gtag('consent', 'default', {
    'ad_storage': 'denied',
    'analytics_storage': 'denied'
  });
gtag('config', 'G-Y219BCPS45', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo/PNG/Logo_multi_AIMAT_RGB_2024.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../undervisningsforlob.html"> 
<span class="menu-text">Undervisningsforløb</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../materialer.html"> 
<span class="menu-text">Materialer</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../sro.html"> 
<span class="menu-text">SRO</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../srp.html"> 
<span class="menu-text">SRP</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../apps.html"> 
<span class="menu-text">Apps</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../referencer.html"> 
<span class="menu-text">Referencer</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Om os</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.youtube.com/@ai-mat"> 
<span class="menu-text"><img src="../../logo/YouTube/youtube-color-darkblue-icon.svg" style="height:2em"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indhold</h2>
   
  <ul>
  <li><a href="#word2vec" id="toc-word2vec" class="nav-link active" data-scroll-target="#word2vec">Word2vec</a>
  <ul class="collapse">
  <li><a href="#betydning-og-kontekst" id="toc-betydning-og-kontekst" class="nav-link" data-scroll-target="#betydning-og-kontekst">Betydning og kontekst</a></li>
  <li><a href="#træningsdata" id="toc-træningsdata" class="nav-link" data-scroll-target="#træningsdata">Træningsdata</a></li>
  <li><a href="#input--og-kontekstvektorer" id="toc-input--og-kontekstvektorer" class="nav-link" data-scroll-target="#input--og-kontekstvektorer">Input- og kontekstvektorer</a>
  <ul class="collapse">
  <li><a href="#eksempel-1" id="toc-eksempel-1" class="nav-link" data-scroll-target="#eksempel-1">Eksempel 1</a></li>
  <li><a href="#opgaver" id="toc-opgaver" class="nav-link" data-scroll-target="#opgaver">Opgaver</a></li>
  </ul></li>
  <li><a href="#model-for-sandsynligheder" id="toc-model-for-sandsynligheder" class="nav-link" data-scroll-target="#model-for-sandsynligheder">Model for sandsynligheder</a>
  <ul class="collapse">
  <li><a href="#eksempel-2" id="toc-eksempel-2" class="nav-link" data-scroll-target="#eksempel-2">Eksempel 2</a></li>
  <li><a href="#opgaver-1" id="toc-opgaver-1" class="nav-link" data-scroll-target="#opgaver-1">Opgaver</a></li>
  </ul></li>
  <li><a href="#estimation-af-vektorer" id="toc-estimation-af-vektorer" class="nav-link" data-scroll-target="#estimation-af-vektorer">Estimation af vektorer</a>
  <ul class="collapse">
  <li><a href="#eksempel-3" id="toc-eksempel-3" class="nav-link" data-scroll-target="#eksempel-3">Eksempel 3</a></li>
  </ul></li>
  <li><a href="#fra-vektorer-til-tekstgenerering" id="toc-fra-vektorer-til-tekstgenerering" class="nav-link" data-scroll-target="#fra-vektorer-til-tekstgenerering">Fra vektorer til tekstgenerering</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Word2vec</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="word2vec" class="level1">
<h1>Word2vec</h1>
<p>Lad os igen se på en situation, hvor vi gerne vil kunne gætte næste ord i en sætning. Lad os sige, at vi har sætningen <span class="math display">\[
\text{"Min hund har en blød ---"}
\]</span> og vil gætte næste ord. Hvis vi har en stor mængde tekst til rådighed, et tekstkorpus, kan vi selvfølgelig lede efter ordsekvensen "Min hund har en blød" og se hvilket ord, der oftest kommer efter som beskrevet i link. Men hvis ikke sekvensen forekommer i vores korpus, så har vi et problem. I stedet kunne vi lede efter en sætning med en betydning, der minder om "Min hund har en blød" og se, hvad der kommer efter den. Men hvordan får vi en computer til at forstå betydningen af ord?</p>
<p>Den simpleste måde at repræsentere et ord på i en computer ville være ved at nummerere alle ordene i det danske sprog fra 1 til <span class="math inline">\(V\)</span>, hvor <span class="math inline">\(V\)</span> er det samlede antal ord. Nummeret på et ord giver dog ikke megen information om ordets betydning.</p>
<p>En anden nærliggende idé kunne være at repræsentere et ord ved bogstaverne i ordet. For at en computer skal kunne forstå det, kunne man give hvert bogstav et tal ud fra bogstavets nummer i alfabetet. Så ville "kat" blive til <span class="math inline">\((11,1,20)\)</span> og "hund" til <span class="math inline">\((8,21,14,4)\)</span>. Stavemåden fortæller dog heller ikke meget om betydningen af et ord. Ordet "mund" staves næsten lige som "hund", men har en helt anden betydning. Omvendt betyder ordet "vovse" næsten det samme som "hund", men staves helt anderledes.</p>
<p>I stedet vil vi gerne repræsentere hvert ord med en vektor. Idéen er, at ord, hvis betydning ligner hinanden, skal repræsenteres med vektorer, der peger i nogenlunde samme retning og har nogenlunde samme længde, som illustreret på Figur ?.</p>
<p>FIGUR</p>
<p>I kender vektorer i 2 eller 3 dimensioner og ved, at de kan skrives på formen <span class="math display">\[
\begin{pmatrix}
a_1\\a_2\end{pmatrix}\text{ og } \begin{pmatrix} a_1\\a_2 \\a_3
\end{pmatrix}
\]</span> hvor <span class="math inline">\(a_1, a_2\)</span> og (eventuelt) <span class="math inline">\(a_3\)</span> er reelle tal, der kaldes vektorens koordinater. Tre koordinater er dog ikke nok til at indfange betydningen af alle ord i sproget. Derfor bruger man i stedet en <em><span class="math inline">\(m\)</span>-dimensional vektor</em>, som man kan tænke på som en liste af <span class="math inline">\(m\)</span> koordinater <span class="math display">\[
\begin{pmatrix}
a_1\\a_2\\a_3\\ \vdots \\ a_m
\end{pmatrix}
\]</span> hvor <span class="math inline">\(a_1,a_2,a_3,\ldots,a_m\)</span> er reelle tal. I praksis vælger man <span class="math inline">\(m\)</span> stort, fx <span class="math inline">\(m=100\)</span>. Man kan regne med <span class="math inline">\(m\)</span>-dimensionale vektorer, lige som man gør i to eller tre dimensioner. Vi kommer for eksempel til at se, hvordan man kan finde skalarprodukter. Til gengæld har man ikke mulighed for at visualisere en <span class="math inline">\(m\)</span>-dimensional vektor som en pil i et koordinatsystem, men det har vi heldigvis heller ikke brug for.</p>
<p>I denne note ser vi på algoritmen Word2Vec som et eksempel på, hvordan man kan oversætte ord til vektorer, der repræsenterer ordenes betydning. Algoritmen blev opfundet af en gruppe medarbejdere hos Google i 2013. I dag bruges diverse forfininger af algoritmen i mange store sprogmodeller.</p>
<section id="betydning-og-kontekst" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="betydning-og-kontekst">Betydning og kontekst</h2>
<p>Hvilke egenskaber skal de vektorer, der repræsenterer ord, så have? Jo, idéen er, at vektorerne skal indfange betydningen af et ord i den forstand, at ord, hvis betydning minder om hinanden, svarer til vektorer, der ligner hinanden. Og hvordan ved vi så, om to ords betydning minder om hinanden? Betydningen af et ord har noget at gøre med, hvilke sammenhænge det optræder i, når man kigger i rigtig mange dokumenter. En sproglig sammenhæng kaldes også en <em>kontekst</em>. Ord, der betyder næsten det samme vil altså ofte optræde i samme kontekst. Ordene "hund" og "kat" er for eksempel forskellige, men de vil ofte optræde i sammenhænge, der ligner hinanden. Se for eksempel på sætningerne <span class="math display">\[
\text{"Min --- har spist af sin madskål"}}
\]</span> og <span class="math display">\[
\text{"Sikken en blød pels, din --- har"}
\]</span> Her ville der kunne stå "hund" eller "kat", men nok ikke "kælk" eller "badedragt". Betydningen af ordene "hund" og "kat" er tættere på hinanden end betydningen af "hund" og "kælk". På den anden side kunne der ikke stå "hund" i sætningen <span class="math display">\[
\text{"Den lille --- har hvide knurhår"}
\]</span> mens både "kat" og "mis" ville passe ind. Ordene "kat" og "hund" er altså tætte på hinanden, men ikke så tætte som "kat" og "mis". Vi vil derfor gerne have, at vektorerne for "hund" og "kat" minder mere om hinanden end vektorerne for "hund" og "kælk", men ikke så meget som "kat" og "mis". Hvad vi forstår ved, at vektorer minder om hinanden, kommer vi tilbage til.</p>
<p>Konteksten er særlig vigtig i forbindelse med ord med flere betydninger. Vi forstår fx betydningen af ordet "marsvin" forskelligt alt efter om ordet "hav" eller ordet "mælkebøtte" optræder i nærheden af det.</p>
</section>
<section id="træningsdata" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="træningsdata">Træningsdata</h2>
<p>Vi har altså brug for at vide, hvilken kontekst ordene indgår i. For at lære, hvilken kontekst et ord forekommer i, tager vi udgangspunkt i et stort tilgængeligt tekstkorpus. Vi kalder dette korpus for vores <em>træningsdata</em>.</p>
<p>Ved konteksten til et ord vil vi her forstå de ord, der står umiddelbart før og efter ordet. Mere præcist vælger vi et vindue, lad os sige på fem ord, hvor ordet i midten er det, vi gerne vil kende betydningen af. Vi vil kalde dette for <em>inputordet</em>. De to første ord og de to sidste ord i vinduet er inputordets <em>kontekstord</em>. Se fx på sætningen <span class="math display">\[
\text{\fbox{Den sorte \textbf{hund} logrer med} halen}
\]</span> hvor boksen angiver vores 5-ords vindue. Det midterste ord "hund" er vores inputord, ordene "Den", "sorte", "logrer" og "med" er kontekstord til "hund".</p>
<p>Vi starter med at placere vinduet omkring det første ord i vores datasæt og noterer dets fire kontekstord (hvoraf de to vil være blanke). Vi flytter nu vinduet mod højre et ord ad gangen, og hver gang noterer vi inputordet og dets fire kontekstord. Vi gør det for al teksten i vores træningsdata og samler informationen i et datasæt som vist i Tabel <a href="#tab:positiv" data-reference-type="ref" data-reference="tab:positiv">[tab:positiv]</a>.</p>
<table class="table">
<caption>Positive eksempler på input- og kontekstord<span data-label="tab:positiv"></span></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Kontekst</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
</tr>
<tr class="even">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;">Den</td>
</tr>
<tr class="even">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;">hund</td>
</tr>
<tr class="odd">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;">logrer</td>
</tr>
<tr class="even">
<td style="text-align: center;">hund</td>
<td style="text-align: center;">Den</td>
</tr>
<tr class="odd">
<td style="text-align: center;">hund</td>
<td style="text-align: center;">sorte</td>
</tr>
<tr class="even">
<td style="text-align: center;">hund</td>
<td style="text-align: center;">logrer</td>
</tr>
<tr class="odd">
<td style="text-align: center;">hund</td>
<td style="text-align: center;">med</td>
</tr>
<tr class="even">
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
</tr>
</tbody>
</table>
<p>Hver række i tabellen består af et inputord og et af dets kontekstord. Sådan et par kalder vi <em>positive eksempler</em>. (Fodnote: Når man vil gætte det næste ord i en sætning har man selvfølgelig kun lov til at bruge de ord, der kommer før ordet. Det er dog ikke det, vi er ude på, når vi laver Word2Vec. Vi er ude på at forstå, hvordan et ord forholder sig til dets kontekst, altså de omkringstående ord. Derfor er der ikke noget problem i, at vinduet både indeholder ord før og efter inputordet. )</p>
<p>Vi vil gerne kunne modellere sandsynligheden for, at et ord <span class="math inline">\(w\)</span> har ordet <span class="math inline">\(c\)</span> som kontekst. For at kunne gøre det, er vi nødt til at have noget at sammenligne med i form af eksempler på ord, der ikke er kontekstord. Og hvor får vi så dem fra? Ja, dem laver vi da bare selv.</p>
<p>For hvert positivt eksempel i datasættet vælger vi <span class="math inline">\(n\)</span> <em>negative eksempler</em>, som er <span class="math inline">\(n\)</span> tilfældigt valgte ord blandt alle ordene i vores ordforråd undtagen kontekstordet i eksemplet. For eksempel kunne man sætte <span class="math inline">\(n=3\)</span>. For parret bestående af inputordet "hund" og kontekstordet "sorte" vælger man så tre tilfældige kontekstord, som ikke er "sorte". Det kunne være "og", "palme" og "synger". Det giver os et datasæt på formen vist i Tabel <a href="#tab:negativ" data-reference-type="ref" data-reference="tab:negativ">[tab:negativ]</a>.</p>
<table class="table">
<caption>Positive og negative eksempler</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Positiv</th>
<th style="text-align: center;">Negativ</th>
<th style="text-align: center;">Negativ</th>
<th style="text-align: center;">Negativ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
</tr>
<tr class="even">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;">hund</td>
<td style="text-align: center;">og</td>
<td style="text-align: center;">palme</td>
<td style="text-align: center;">synger</td>
</tr>
<tr class="odd">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;">logrer</td>
<td style="text-align: center;">bage</td>
<td style="text-align: center;">fra</td>
<td style="text-align: center;">tredive</td>
</tr>
<tr class="even">
<td style="text-align: center;">hund</td>
<td style="text-align: center;">Den</td>
<td style="text-align: center;">mellem</td>
<td style="text-align: center;">jordbær</td>
<td style="text-align: center;">emne</td>
</tr>
<tr class="odd">
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
</tr>
</tbody>
</table>
<p><span id="tab:negativ" data-label="tab:negativ">[tab:negativ]</span></p>
<p>I praksis vælges alle de negative eksempler ikke med samme sandsynlighed. Hyppigt forekommende ord har en større sandsynlighed for at blive valgt end sjældnere ord. Hvis <span class="math inline">\(h_i\)</span> er hyppigheden af <span class="math inline">\(i\)</span>te ord, så vælges det <span class="math inline">\(i\)</span>te ord med sandsynligheden <span class="math display">\[
\frac{h_i^\alpha}{\sum_{j=1}^V h_j^\alpha}
\]</span> hvor <span class="math inline">\(V\)</span> er antallet af ord i sproget. I praksis benyttes ofte <span class="math inline">\(\alpha=3/4\)</span>. På den måde bliver sandsynlighederne ikke helt proportionale med hyppighederne. Vi giver en lille fordel til sjældne ord, men ikke så meget som hvis vi valgte alle ord med samme sandsynlighed.</p>
<p>Nu har vi fået lavet et datasæt bestående af både positive og negative eksempler på input- og kontekstord. I de næste to afsnit ser vi på, hvordan man kan modellere sandsynligheden for at et ord <span class="math inline">\(c\)</span> optræder som kontekst til et inputord <span class="math inline">\(w\)</span> ved hjælp af vektorer. Det vil vi sidenhen bruge til at vælge vektorerne, således at vores positive eksempler i datasættet bliver meget sandsynlige og de negative eksempler bliver meget usandsynlige.</p>
</section>
<section id="input--og-kontekstvektorer" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="input--og-kontekstvektorer">Input- og kontekstvektorer</h2>
<p>I første omgang vil vi lade hvert ord <span class="math inline">\(w\)</span> være repræsenteret af to vektorer, <span class="math inline">\(\overrightarrow{v}_{w}\)</span> og <span class="math inline">\(\overrightarrow{k}_{w}\)</span>, hvor <span class="math inline">\(\overrightarrow{v}_{w}\)</span> repræsenterer ordet, når det optræder som input, mens <span class="math inline">\(\overrightarrow{k}_{w}\)</span> repræsenterer ordet, når det optræder som kontekst. Betydningen af <span class="math inline">\(w\)</span> afgøres som nævnt af hvordan ordet forholder sig til konteksten. Det vil vi oversætte matematisk til, hvordan <span class="math inline">\(\overrightarrow{v}_{w}\)</span> forholder sig til kontekstvektorerne <span class="math inline">\(\overrightarrow{k}_{c}\)</span> for diverse kontekstordord <span class="math inline">\(c\)</span>. Vi vil derfor tænke på <span class="math inline">\(\overrightarrow{v}_{w}\)</span> som den vektor, der repræsenterer betydningen af ordet, og altså den vi er ude på at bestemme.</p>
<p>For at måle hvordan inputvektoren <span class="math inline">\(\overrightarrow{v}_{w}\)</span> for ordet <span class="math inline">\(w\)</span> forholder sig til kontekstvektoren <span class="math inline">\(\overrightarrow{k}_{c}\)</span> for ordet <span class="math inline">\(c\)</span>, vil vi bruge <em>skalarproduktet</em> <span class="math inline">\(\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}\)</span>. Husk på, at man finder skalarproduktet mellem to vektorer i to dimensioner ved formlen <span class="math display">\[
\begin{pmatrix}
a_1\\a_2
\end{pmatrix}
\cdot
\begin{pmatrix}
b_1\\b_2
\end{pmatrix}
=a_1b_1+a_2b_2
\]</span> og i tre dimensioner ved formlen <span class="math display">\[
\begin{pmatrix}
a_1\\a_2\\a_3
\end{pmatrix}
\cdot
\begin{pmatrix}
b_1\\b_2\\b_3
\end{pmatrix}
=a_1b_1+a_2b_2+a_3b_3
\]</span> Tilsvarende kan man definere skalarproduktet mellem to <span class="math inline">\(m\)</span>-dimensionale vektorer ved <span class="math display">\[
\begin{pmatrix}
a_1\\a_2\\a_3\\ \vdots\\a_m
\end{pmatrix}
\cdot
\begin{pmatrix}
b_1\\b_2\\b_3\\ \vdots \\b_m
\end{pmatrix}
=a_1b_1+a_2b_2+a_3b_3+\dotsm + a_mb_m
\]</span></p>
<p>Vi vil gerne have, at vores input- og kontekstvektorer skal opfylde, at hvis <span class="math inline">\(w\)</span> ofte har <span class="math inline">\(c\)</span> som kontekst, så er skalarproduktet <span class="math inline">\(\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}\)</span> stort, mens en meget negativ værdi af <span class="math inline">\(\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}\)</span> indikerer, at <span class="math inline">\(w\)</span> sjældent har <span class="math inline">\(c\)</span> som kontekst.</p>
<p>Hvad fortæller skalarproduktet om, hvordan to vektorer forholder sig til hinanden? I 2 og 3 dimensioner kan vi give en geometrisk fortolkning af skalarproduktet ved hjælp af formlen <span class="math display">\[
\label{eq:skalar}
\overrightarrow{a}\cdot\overrightarrow{b} = |\overrightarrow{a}||\overrightarrow{b}| \cos(\angle (\overrightarrow{a},\overrightarrow{b}))
\]</span> hvor <span class="math inline">\(\angle (\overrightarrow{a},\overrightarrow{b})\)</span> er vinklen mellem vektorerne <span class="math inline">\(\overrightarrow{a}\)</span> og <span class="math inline">\(\overrightarrow{b}\)</span>, og <span class="math inline">\(|\overrightarrow{a}|\)</span> betegner længden af <span class="math inline">\(\overrightarrow{a}\)</span>, som findes med formlen <span class="math display">\[
|\overrightarrow{a}|=\sqrt{\overrightarrow{a}\cdot\overrightarrow{a}}
\]</span> Cosinus er en aftagende funktion på intervallet <span class="math inline">\([0,\pi]\)</span>, så jo større vinklen <span class="math inline">\(\angle (\overrightarrow{a},\overrightarrow{b})\)</span> er, desto mindre vil <span class="math inline">\(\cos(\angle (\overrightarrow{a},\overrightarrow{b}))\)</span> være. figur Det betyder, at <span class="math inline">\(\cos(\angle (\overrightarrow{a},\overrightarrow{b}))\)</span> er størst når vinklen mellem <span class="math inline">\(\overrightarrow{a}\)</span> og <span class="math inline">\(\overrightarrow{b}\)</span> er 0, svarende til at vektorerne peger samme vej. Her er <span class="math inline">\(\cos(\angle (\overrightarrow{a},\overrightarrow{b}))=1\)</span>. Den mindste værdi af <span class="math inline">\(\cos(\angle (\overrightarrow{a},\overrightarrow{b}))\)</span> er <span class="math inline">\(-1\)</span>, som antages ved en vinkel på <span class="math inline">\(\pi\)</span>, hvor vektorerne peger i modsat retning.</p>
<p>Jo mindre vinklen mellem <span class="math inline">\(\overrightarrow{v}_{w}\)</span> og <span class="math inline">\(\overrightarrow{k}_{c}\)</span> er, desto større er deres skalarprodukt <span class="math inline">\(\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}\)</span> altså, og jo oftere har ordet <span class="math inline">\(w\)</span> dermed <span class="math inline">\(c\)</span> som kontekst. Desuden viser <a href="#eq:skalar" data-reference-type="eqref" data-reference="eq:skalar">[eq:skalar]</a>, at lange vektorer tæller mere, både positivt og negativt, end korte vektorer. Ord, der er gode til at forudsige konteksten udfra, fx "logre", vil derfor blive repræsenteret med længere inputvektorer end ord som "og" eller "er", der ikke indeholder megen information om konteksten. Altså vil <span class="math inline">\(|\overrightarrow{v}_{logre}|\)</span> være større end <span class="math inline">\(|\overrightarrow{v}_{og}|\)</span> og <span class="math inline">\(|\overrightarrow{v}_{er}|\)</span>.</p>
<p>Bemærk også, at hvis to ord ofte optræder i samme kontekst, skal deres inputvektorer gerne have nogenlunde samme skalarprodukt med alle kontekstvektorer. Formlen <a href="#eq:skalar" data-reference-type="eqref" data-reference="eq:skalar">[eq:skalar]</a> viser, at det betyder, at de dels skal have nogenlunde samme længde og dels skal have nogenlunde samme vinkel med alle kontekstvektorerne. Sidstnævnte kræver, at de har nogenlunde samme retning. Ord, hvis betydning ligner hinanden, kommer derfor til at svare til vektorer, hvis længde og retning ligner hinanden.</p>
<p>I praksis bruger vi vektorer af højere dimension end 3. Det er måske ikke helt oplagt, hvad man skal forstå ved længden af en vektor eller vinklen mellem to vektorer i højere dimensioner, men det viser sig, at man stadigvæk godt kan give mening til formlen <a href="#eq:skalar" data-reference-type="eqref" data-reference="eq:skalar">[eq:skalar]</a>. Intuitionen fra to eller tre dimensioner er derfor god at have, selv om vi arbejder med højere dimensioner.</p>
<section id="eksempel-1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="eksempel-1">Eksempel 1</h3>
<p>Forslag til eksempel. Forslag til bedre ord modtages gerne, der kan nok også laves en flottere figur. Lad os sige, at vi har fundet en vektorrepræsentation, der opfylder det ønskede. Det gav vektorerne <span class="math display">\[
\begin{aligned}
&amp;\overrightarrow{v}_{hund} =  \begin{pmatrix} 1.5\\ 1\end{pmatrix}, \overrightarrow{v}_{kat} =  \begin{pmatrix} 1\\ 1.5\end{pmatrix} \\
&amp;\overrightarrow{k}_{madsk\aa l}  =  \begin{pmatrix} 1.5\\1.5\end{pmatrix}, \overrightarrow{k}_{badedragt} =  \begin{pmatrix} -1\\-0.75 \end{pmatrix}, \overrightarrow{k}_{lufte} =  \begin{pmatrix} 1\\0.25 \end{pmatrix}
\end{aligned}
\]</span> Vektorerne er vist herunder, hvor inputvektorer er røde, og kontekstvektorer er blå:</p>
<p>Vi kan udregne skalarprodukterne <span class="math display">\[
\begin{aligned}
&amp;\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l} = \begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} 1.5\\ 1.5\end{pmatrix} = 1\cdot 1.5 +  1.5\cdot 1.5 = 3.75 \\
&amp;\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{badedragt} = \begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} -1\\ -0.75\end{pmatrix} = 1\cdot (-1) +  1.5\cdot (-0.75) = -2.125
\end{aligned}
\]</span> Vi ser, at <span class="math inline">\(\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l}\)</span> er større end <span class="math inline">\(\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{badedragt}\)</span>. Det svarer til, at ordet "kat" oftere har "madskål" som kontekst, end det har "badedragt". Det ses også ved, at <span class="math inline">\(\overrightarrow{k}_{madsk\aa l}\)</span> peger i nogenlunde samme retning som <span class="math inline">\(\overrightarrow{v}_{kat}\)</span>, mens <span class="math inline">\(\overrightarrow{k}_{badedragt}\)</span> peger i en helt anden retning.</p>
<p>Vi kan også udregne <span class="math display">\[
\begin{aligned}
&amp;\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{madsk\aa l} = \begin{pmatrix} 1.5\\ 1\end{pmatrix}  \cdot  \begin{pmatrix} 1.5\\ 1.5\end{pmatrix} = 1.5\cdot 1.5 +  1\cdot 1.5 = 3.75
\end{aligned}
\]</span> Vi ser, at <span class="math inline">\(\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{madsk\aa l} =\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l} =3.75\)</span>, svarende til at både "hund" og "kat" ofte har "madskål" som kontekst. Vi ser da også, at vektorerne <span class="math inline">\(\overrightarrow{v}_{hund}\)</span> og <span class="math inline">\(\overrightarrow{v}_{kat}\)</span> peger i nogenlunde samme retning og har nogenlunde samme længde, fordi de tit har samme kontekst. De to vektorer er dog ikke helt ens, da der også vil være nogle kontekstord, der er ikke er lige hyppige for "hund" og "kat". Vi kan fx udregne <span class="math display">\[
\begin{aligned}
&amp;\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{lufte} = \begin{pmatrix} 1.5\\ 1\end{pmatrix}  \cdot  \begin{pmatrix} 1\\ 0.25\end{pmatrix} = 1.5\cdot 1 + 1\cdot 0.25 =1.75\\
&amp;\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{lufte} =\begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} 1\\ 0.25\end{pmatrix} = 1\cdot 1 + 1.5\cdot 0.25 = 1.375
\end{aligned}
\]</span> Her er <span class="math inline">\(\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{lufte}\)</span> større end <span class="math inline">\(\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{lufte}\)</span> svarende til, at "hund" oftere har "lufte" som kontekst, end "kat" har. bedre ord end "lufte"?</p>
<p>begynd box</p>
<p>Måske har du undret dig over, hvorfor vi har brug for to forskellige vektorer for hvert ord. Hvert positivt eksempel forekommer jo to gange i vores datasæt, hvor de to ord skiftes til at spille rollen som input og kontekst. Alligevel er der ikke symmetri mellem de to ord. Hvis fx inputordet er "logre", er det ret sandsynligt, at "hund" er et af kontekstordene. Inputordet "hund" forekommer derimod i mange kontekster, der ikke involverer ordet "logre". Sandsynligheden for kontekstordet afhænger derfor af, om det er "hund" eller "logre", der er input. Hvis vi kun brugte én vektor for hvert ord, <span class="math inline">\(\overrightarrow{u}_{hund}\)</span> og <span class="math inline">\(\overrightarrow{u}_{logre}\)</span>, ville vi få samme skalarprodukt <span class="math inline">\(\overrightarrow{u}_{hund}\cdot \overrightarrow{u}_{logre} =  \overrightarrow{u}_{logre}\cdot \overrightarrow{u}_{hund}\)</span>, og dermed samme sandsynlighed, uanset hvilket ord der var input.</p>
<p>slut box</p>
</section>
<section id="opgaver" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="opgaver">Opgaver</h3>
<ul>
<li><p>Antag at vi har lavet 4-dimensionale input- og kontekstvektorer således, at jo større skalarproduktet <span class="math inline">\(\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}\)</span> er, desto mere sandsynligt er det, at ordet <span class="math inline">\(w\)</span> har <span class="math inline">\(c\)</span> som kontekst. Inputvektoren for "hund" og kontekstvektorerne for "pels" og "fjer" er <span class="math display">\[
\begin{aligned}
\overrightarrow{v}_{hund}=\begin{pmatrix} 0.5\\2\\1\\-1\end{pmatrix} ,\quad
\overrightarrow{k}_{pels}=\begin{pmatrix} 0\\3\\2\\-2\end{pmatrix},\quad
\overrightarrow{k}_{fjer}=\begin{pmatrix} 1\\-2\\1.5\\0.5\end{pmatrix}
\end{aligned}
\]</span> Udregn skalarprodukterne <span class="math inline">\(\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{pels}\)</span> og <span class="math inline">\(\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{fjer}\)</span>. Passer det med, hvilket af ordene "pels" og "fjer" der er mest sandsynligt som kontekst til "hund"?</p></li>
<li><p>Antag at vi har lavet 3-dimensionale input- og kontekstvektorer som beskrevet i afsnittet ovenfor. Så skulle ord, der ofte har samme kontekst gerne have nogenlunde samme længde og retning, mens ord, der betyder noget helt forskelligt, kan have meget forskellig længde og retning. Antag at inputvektorerne for "kat", "hund", "mis" og "kælk" er <span class="math display">\[
\begin{aligned}
\overrightarrow{v}_{kat}=\begin{pmatrix}0\\2\\1 \end{pmatrix},\quad
\overrightarrow{v}_{hund}=\begin{pmatrix}0\\1\\1.7\end{pmatrix}, \quad
\overrightarrow{v}_{mis}=\begin{pmatrix}0.4\\2\\0.9\end{pmatrix},\quad
\overrightarrow{v}_{kælk}=\begin{pmatrix} 2\\-1\\0 \end{pmatrix}
\end{aligned}
\]</span> Find længden af de fire vektorer. Find vinklen mellem <span class="math inline">\(\overrightarrow{v}_{kat}\)</span> og de tre øvrige vektorer. Stemmer resultatet overens med hvilke ord der er tættest på "kat" i betydning? Tegn vektorerne ind i GeoGebra.</p></li>
</ul>
</section>
</section>
<section id="model-for-sandsynligheder" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="model-for-sandsynligheder">Model for sandsynligheder</h2>
<p>Som nævnt, vil vi gerne have, at jo større skalarproduktet <span class="math inline">\(\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}\)</span> er, desto mere sandsynligt er det, at ordet <span class="math inline">\(w\)</span> har ordet <span class="math inline">\(c\)</span> som kontekst. Skalarprodukter kan imidlertid antage alle reelle værdier, så de egner sig ikke til at repræsentere en sandsynlighed, der jo skal være et tal mellem 0 og 1. Derfor anvender vi en funktion <span class="math inline">\(f\)</span> på skalarproduktet med definitionsmængde <span class="math inline">\(\mathbb{R}\)</span> og værdimængde <span class="math inline">\(]0,1[\)</span> for at få skalarprodukterne lavet om til tal mellem 0 og 1. Den funktion, vi vil bruge, er <em>sigmoid-funktionen</em> (også nogle gange kaldet den logistiske funktion). Forskriften er <span class="math display">\[
\label{eq:sigmoid}
f(x)=\frac{1}{1+e^{-x}}
\]</span> og grafen ses herunder indsæt graf Det ses af grafen, at <span class="math inline">\(f\)</span> er strengt voksende, og at værdimængden for <span class="math inline">\(f\)</span> er <span class="math inline">\(]0,1[\)</span>.</p>
<p>Word2Vec-algoritmen modellerer sandsynligheden <span class="math inline">\(P(c\mid w)\)</span> for at ordet <span class="math inline">\(c\)</span> er kontekst til inputordet <span class="math inline">\(w\)</span> som (Fodnote: Bemærk at vi bruger notationen <span class="math inline">\(P(c\mid w)\)</span> lidt anderleds end i "Simple sprogmodeller". Der betød det sandsynligheden for at <span class="math inline">\(c\)</span> er næste ord efter <span class="math inline">\(w\)</span>. Her betyder det sandsynligheden for, at <span class="math inline">\(c\)</span> er kontekst til <span class="math inline">\(w\)</span>.) <span class="math display">\[
\label{eq:pcw}
P(c\mid w) = f(\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c})=\frac{1}{1+e^{-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}}}
\]</span> Denne sandsynlighed vil ligge i værdimængden for <span class="math inline">\(f\)</span>, som var <span class="math inline">\(]0,1[\)</span>. Da <span class="math inline">\(f\)</span> er strengt voksende, vil sandsynligheden <span class="math inline">\(P(c \mid w)\)</span> være større, jo større <span class="math inline">\(\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}\)</span> er. Bemærk desuden, at sandsynligheden for, at <span class="math inline">\(c\)</span> ikke er kontekst til <span class="math inline">\(w\)</span>, er givet ved <span class="math display">\[
\begin{aligned}
\label{eq:pneg}
1-P(c\mid w) &amp;= 1-f(\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}) = f(-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c})
= \frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}}}
\end{aligned}
\]</span> hvor vi brugte, at <span class="math inline">\(1-f(x)=f(-x)\)</span>. Dette vises i boksen nedenfor.</p>
<section id="eksempel-2" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="eksempel-2">Eksempel 2</h3>
<p>I Eksempel 1 fandt vi, at <span class="math inline">\(\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte} = 1.75\)</span>. Sandsynligheden for, at "hund" har "lufte" som kontekst, er derfor <span class="math display">\[
P(\text{ lufte } \mid \text{ hund }) = \frac{1}{1+e^{-\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte}}}=\frac{1}{1+e^{-1.75}} \approx  0.852.
\]</span> Sandsynligheden for, at "hund" ikke har "lufte" som kontekst, er <span class="math display">\[
1- P(\text{ lufte } \mid \text{ hund }) = \frac{1}{1+e^{\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte}}}=\frac{1}{1+e^{1.75}} \approx 0.148.
\]</span></p>
<p>begynd box1</p>
<p>Man kan også finde værdimængden for <span class="math inline">\(f\)</span> ved at kigge på forskriften for <span class="math inline">\(f\)</span>. Da både tæller og nævner i <a href="#eq:sigmoid" data-reference-type="eqref" data-reference="eq:sigmoid">[eq:sigmoid]</a> er positive, får vi at <span class="math display">\[
f(x)=\frac{1}{1+e^{-x}}&gt;0
\]</span> Da eksponentialfunktionen kun kan antage positive værdier, er <span class="math inline">\(1&lt;1+e^{-x}\)</span>, så <span class="math display">\[
f(x)=\frac{1}{1+e^{-x}}&lt; \frac{1+e^{-x}}{1+e^{-x}} =1
\]</span> Vi har derfor, at værdimængden for <span class="math inline">\(f\)</span> er indeholdt i <span class="math inline">\(]0,1[\)</span>, hvilket kan skrives som <span class="math display">\[
\text{Vm}(f) \subseteq ]0,1[
\]</span> For at vise, at hele intervallet <span class="math inline">\(]0,1[\)</span> er med i værdimængden, viser vi, at funktionsværdierne kan komme så tæt på 0 og 1, som vi ønsker. Vi ser derfor på, hvad der sker med <span class="math inline">\(f(x)\)</span> når <span class="math inline">\(x\)</span> nærmer sig <span class="math inline">\(-\infty\)</span> og <span class="math inline">\(\infty\)</span>.</p>
<p>Figur ? viser grafen for <span class="math inline">\(e^{-x}\)</span>:</p>
<p>indsæt graf</p>
<p>Da <span class="math inline">\(e^{-x}\)</span> er en aftagende eksponentialfunktion, har vi at <span class="math display">\[
e^{-x} \to \infty \text{ når } x\to -\infty
\]</span> og derfor <span class="math display">\[
f(x)=\frac{1}{1+e^{-x}}\to 0 \text{ når } x\to -\infty
\]</span> Desuden vil <span class="math display">\[
e^{-x} \to 0 \text{ når } x\to \infty
\]</span> og derfor <span class="math display">\[
f(x)=\frac{1}{1+e^{-x}}\to 1 \text{ når } x\to \infty
\]</span> Alt i alt har vi altså vist, at <span class="math inline">\(f\)</span> har værdimængde <span class="math inline">\(]0,1[\)</span>, nærmer sig 0, når <span class="math inline">\(x\to -\infty\)</span>, og nærmer sig 1, når <span class="math inline">\(x\to \infty\)</span>.</p>
<p>Hvis man differentierer <span class="math inline">\(f\)</span>, får man <span class="math display">\[
f'(x)=\frac{e^{-x}}{(1+e^{-x})^2}
\]</span> Vi ser, at både tæller og nævner er positive, så <span class="math inline">\(f'(x)&gt;0\)</span> for alle <span class="math inline">\(x\)</span>. Da den afledte er positiv, er <span class="math inline">\(f\)</span> en strengt voksende funktion.</p>
<p>slut box1</p>
<p>begynd box2</p>
<p>Vi viser nu, at <span class="math inline">\(1-f(x) = f(-x)\)</span>. <span class="math display">\[
\label{eq:1-f}
1-f(x) = 1-\frac{1}{1+e^{-x}} = \frac{1+e^{-x}}{1+e^{-x}}-\frac{1}{1+e^{-x}}=\frac{e^{-x}}{1+e^{-x}}=\frac{1}{1+e^{x}} =f(-x)
\]</span> hvor vi forlængede brøken med <span class="math inline">\(e^x\)</span> i det sidste lighedstegn. det kunne man evt også vise på en figur?</p>
<p>slut box2</p>
</section>
<section id="opgaver-1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="opgaver-1">Opgaver</h3>
<ul>
<li><p>Lad <span class="math inline">\(f\)</span> være sigmoidfunktionen. Vis at <span class="math display">\[
f'(x)=\frac{e^{-x}}{(1+e^{-x})^2}
\]</span></p></li>
<li><p>Antag at vi har fundet vektorrepræsentationer af ord, og at inputvektoren for "hund" og kontekstvektorerne for "pels" og "fjer" er givet ved <span class="math display">\[
\begin{aligned}
\overrightarrow{v}_{hund}=\begin{pmatrix} 0.5\\2\\1\\-1\end{pmatrix} ,\quad
\overrightarrow{k}_{pels}=\begin{pmatrix} 0\\3\\2\\-2\end{pmatrix},\quad
\overrightarrow{k}_{fjer}=\begin{pmatrix} 1\\-2\\1.5\\0.5\end{pmatrix}
\end{aligned}
\]</span> Hvad er sandsynligheden for at henholdsvis "pels" og "fjer" er kontekst til "hund"? Hvad er sandsynligheden for at "fjer" ikke er kontekst til "hund"?</p></li>
</ul>
</section>
</section>
<section id="estimation-af-vektorer" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="estimation-af-vektorer">Estimation af vektorer</h2>
<p>Hvordan bestemmer vi så vektorrepræsentationerne <span class="math inline">\(\overrightarrow{v}_{w}\)</span> og <span class="math inline">\(\overrightarrow{k}_{c}\)</span> i praksis? Her får vi brug for det datasæt, som vi lavede ud fra vores tekstkorpus. Hver række i datasættet bestod af et inputord <span class="math inline">\(w\)</span>, et positivt kontekstord <span class="math inline">\(c^{+}\)</span>, som forekom som kontekst til <span class="math inline">\(w\)</span> i datasættet, og tre negative eksempler på kontekstord, som vi vil betegne <span class="math inline">\(c_{1}^-,c_{2}^-,c_{3}^-\)</span>. Vores datasæt har altså formen</p>
<table class="table">
<caption>Eksempel på datatabel<span data-label="tab:data"></span></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Positiv</th>
<th style="text-align: center;">Negativ</th>
<th style="text-align: center;">Negativ</th>
<th style="text-align: center;">Negativ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(w\)</span></td>
<td style="text-align: center;"><span class="math inline">\(c^{+}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(c^-_{1}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(c_{2}^-\)</span></td>
<td style="text-align: center;"><span class="math inline">\(c_{3}^-\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;">hund</td>
<td style="text-align: center;">og</td>
<td style="text-align: center;">palme</td>
<td style="text-align: center;">synger</td>
</tr>
<tr class="odd">
<td style="text-align: center;">sorte</td>
<td style="text-align: center;">logrer</td>
<td style="text-align: center;">bage</td>
<td style="text-align: center;">fra</td>
<td style="text-align: center;">tredive</td>
</tr>
<tr class="even">
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
<td style="text-align: center;">⋮</td>
</tr>
</tbody>
</table>
<p>Lad os sige, at der er <span class="math inline">\(M\)</span> rækker i vores træningsdata. Vi betegner data i den <span class="math inline">\(j\)</span>te række med <span class="math inline">\((w_j,c_j^+,c_{j1}^-,c_{j2}^-,c_{j3}^-)\)</span> for <span class="math inline">\(j=1,\ldots,M\)</span>.</p>
<p>Vi ser nu på <span class="math inline">\(j\)</span>te række. Hvis vores model <a href="#eq:pcw" data-reference-type="eqref" data-reference="eq:pcw">[eq:pcw]</a> for sandsynligheden for kontekstord er god, skulle det positive eksempel, vi har observeret, gerne have høj sandsynlighed. Vi vil altså gerne vælge vores vektorer således, at <span class="math inline">\(P(c_j^+\mid w_j)\)</span> er høj. Disse sandsynligheder vil være meget små i praksis, da der kan være mange meningsfulde kontekstord til <span class="math inline">\(w_j\)</span>. For at undgå numeriske problemer, tager vi derfor den naturlige logaritme og får <span class="math inline">\(\ln (P(c_j^+\mid w_j))\)</span>. Fodnote: Husk på, at hvis vi har et tal meget tæt på nul, fx <span class="math inline">\(10^{-a}\)</span>, og vi tager den naturlige logaritme, så får vi <span class="math inline">\(\ln(10^{-a}) = -a\cdot \ln(10)\)</span>, som er et tal på en mere normal skala, som en computer bedre kan finde ud af at regne på. Da den naturlige logaritme er en strengt voksende funktion, svarer en høj sandsynlighed <span class="math inline">\(P(c_j^+\mid w_j)\)</span> til, at <span class="math inline">\(\ln(P(c_j^+\mid w_j))\)</span> er høj. Vi vil derfor gerne have, at <span class="math inline">\(-\ln(P(c_j^+\mid w_j))\)</span> er lav. Malene indsætter graf. Fodnote: Vores sandsynligheder vil være tal mellem 0 og 1, så når vi tager den naturlige logaritme, får vi et negativt tal, se Figur ?. Minus den naturlige logaritme bliver derfor positiv.</p>
<p>Samtidig vil vi gerne have, at sandsynligheden for, at vores negative eksempler ikke er kontekstord, er høj. Altså skal <span class="math inline">\(1-P(c_{ji}^-\mid w_j)\)</span> være høj for <span class="math inline">\(i=1,2,3\)</span>. Igen svarer det til, at <span class="math inline">\(-\ln(1-P(c_{ji}^-\mid w_j))\)</span> skal være lav.</p>
<p>For at sikre, at både <span class="math inline">\(-\ln(P(c_j^+\mid w_j))\)</span> og <span class="math inline">\(-\ln(1-P(c_{ji}^-\mid w_j))\)</span> for <span class="math inline">\(i=1,2,3\)</span> er lave, vil vi kræve at deres sum er lav. Vi kræver altså at <span class="math display">\[
L_j = -\ln(P(c_j^+\mid w_j)) -\ln(1-P(c_{j1}^-\mid w_j)) -\ln(1-P(c_{j2}^-\mid w_j)) -\ln(1-P(c_{j3}^-\mid w_j))
\]</span> er lav. Formlerne <a href="#eq:pcw" data-reference-type="eqref" data-reference="eq:pcw">[eq:pcw]</a> og <a href="#eq:pneg" data-reference-type="eqref" data-reference="eq:pneg">[eq:pneg]</a> giver <span class="math display">\[
\begin{aligned}
L_j =&amp; -\ln(P(c_j^+\mid w_j)) -\ln(1-P(c_{j1}^-\mid w_j))\\
&amp;-\ln(1-P(c_{j2}^-\mid w_j)) -\ln(1-P(c_{j3}^-\mid w_j))
\\=&amp; -\ln \bigg(\frac{1}{1+e^{-\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c^+_j}}}\bigg) - \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j1}^-}}}\bigg)\\
&amp;- \ln \bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j2}^-}}}\bigg) - \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j3}^-}}}\bigg)
\end{aligned}
\]</span> Her kan vi bruge logaritmeregnereglen <span class="math inline">\(\ln(\tfrac{1}{a}) = -\ln(a)\)</span> til at skrive <span class="math inline">\(L_j\)</span> lidt pænere. <span class="math display">\[
\begin{aligned}
L_j =&amp;\ln \Big({1+e^{-\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c^+_j}}}\Big) + \ln\Big({1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j1}^-}}}\Big)\\
&amp;+ \ln \Big({1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j2}^-}}}\Big) + \ln\Big({1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j3}^-}}}\Big)
\end{aligned}
\]</span> Vores <span class="math inline">\(L_j\)</span> afhænger altså af, hvordan vores input- og kontekstvektorer er valgt. Vi ønsker at vælge vektorerne således, at <span class="math inline">\(L_j\)</span> er lav.</p>
<p>Dette kan vi gentage for hver række i vores træningsdata. Vi får et <span class="math inline">\(L_j\)</span> for hver række <span class="math inline">\(j=1,\ldots,M\)</span>. Da vi gerne vil have, at alle <span class="math inline">\(L_j\)</span> er lave, kræver vi, at deres sum <span class="math display">\[L=L_1+ L_2 +\dotsm + L_M\]</span> er lav. Vi kalder <span class="math inline">\(L\)</span> for vores <em>tabsfunktion</em>. Da hvert <span class="math inline">\(L_j\)</span> afhænger af input- og kontekstvektorerne, kommer <span class="math inline">\(L\)</span> også til at afhænge af disse vektorer. Vi ønsker derfor at vælge input- og kontekstvektorerne således, at de minimerer <span class="math inline">\(L\)</span>. Hvordan finder man minimum for <span class="math inline">\(L\)</span> i praksis? Det kan man fx gøre ved hjælp af <em>gradientnedstigning</em>, som du kan læse mere om her.</p>
<p>Der er rigtig mange ord i det danske sprog. For hvert af dem skal vi finde både en input- og en kontekstvektor, der hver har <span class="math inline">\(m\)</span> koordinater. Alt i alt giver det rigtig mange koordinater, der skal bestemmes. Antallet af ord på dansk afhænger lidt af, hvad man forstår ved et ord, men 200.000 er et fornuftigt bud. Hvis vi vil repræsentere dem ved vektorer af dimension <span class="math inline">\(m=100\)</span>, får man brug for at bestemme 40.000.000 koordinater. For at kunne gøre det, er man også nødt til at have enormt store mængder træningsdata til rådighed, dvs.&nbsp;antallet <span class="math inline">\(M\)</span> af positive eksempler i træningsdataet skal være gigantisk stort. Bemærk dog, at 40.000.000 koordinater stadig er væsentlig færre, end hvis vi skulle bestemme en sandsynlighed for hvert eneste af de <span class="math inline">\(200.000^2\)</span> mulige kombinationer af input- og kontekstord. Dette er en anden fordel ved Word2Vec.</p>
<p>Nu har vi set, hvordan man kan repræsentere et ord <span class="math inline">\(w\)</span> ved en inputvektor <span class="math inline">\(\overrightarrow{v}_{w}\)</span> og en kontekstvektor <span class="math inline">\(\overrightarrow{k}_{w}\)</span>. Vektoren <span class="math inline">\(\overrightarrow{v}_{w}\)</span> er den, der viser, hvordan <span class="math inline">\(w\)</span> forholder sig til sin kontekst, så det er den, der repræsenterer betydningen af <span class="math inline">\(w\)</span>. Normalt vil man derfor arbejde videre med <span class="math inline">\(\overrightarrow{v}_{w}\)</span>, mens <span class="math inline">\(\overrightarrow{k}_{w}\)</span> smides væk. Vi mangler dog stadig at se, hvordan vektorerne <span class="math inline">\(\overrightarrow{v}_{w}\)</span>, som vi har fundet med Word2Vec, kan bruges i forbindelse med kunstig intelligens.</p>
<section id="eksempel-3" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="eksempel-3">Eksempel 3</h3>
<p>Eges børnebog</p>
</section>
</section>
<section id="fra-vektorer-til-tekstgenerering" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="fra-vektorer-til-tekstgenerering">Fra vektorer til tekstgenerering</h2>
<p>I det følgende ser vi på, hvordan vektorrepræsentationerne, som vi fandt med Word2Vec, kan bruges til at lave en algoritme, der kan generere ny tekst. De fleste tekstgenereringsalgoritmer fungerer ved, at de danner teksten et ord ad gangen. Givet den tekst der allerede er dannet, prøver algoritmen hele tiden at gætte, hvad det næste ord skal være. Det kan gøres i to trin:</p>
<ul>
<li><p>Først benyttes Word2Vec til at oversætte alle ordene i sproget til vektorer.</p></li>
<li><p>Dernæst genereres teksten. Givet den tekst, der allerede er dannet, bruger vi en (kompliceret) funktion, der tager vektorrepræsentationerne af de hidtil genererede ord som input. Som output giver funktionen det mest sandsynlige næste ord (eller et af de mest sandsynlige).</p></li>
</ul>
<p>Hvis vi fx har genereret teksten <span class="math display">\[
\text{"Hunden spiser sit ---"}
\]</span> så skal vi prøve at gætte, hvilket ord der kommer efter "sit". Vi oversætter derfor ordene "Hunden", "spiser" og "sit" til vektorerne <span class="math inline">\(\overrightarrow{v}_{Hunden}\)</span>, <span class="math inline">\(\overrightarrow{v}_{spiser}\)</span> og <span class="math inline">\(\overrightarrow{v}_{sit}\)</span>. Disse tre vektorer giver vi funktionen som input, og som output får vi et nyt ord, måske "kødben". Den funktion, der bruges i punkt 2., ville typisk være et <em>neuralt netværk</em>. Du kan læse mere om, hvordan det fungerer i "Tekstgenerering med neurale netværk".</p>
<p>Det smarte ved at bruge vektorrepræsentationerne er, hvis vi for eksempel vil generere næste ord i <span class="math display">\[
\text{"Jeg skal huske, at katten skal have ---"}
\]</span> Det skulle gerne give "mad" som muligt næste ord. Men måske har sprogmodellen aldrig set sætningen "katten skal have mad". Hvis den til gengæld har set "hunden skal have mad", og modellen ved at "hund" og "kat" tit har samme kontekst, og dermed har næsten samme vektorrepræsentation, så vil man alligevel få "mad" som muligt næste ord.</p>
<p>Kan man også få det til at virke for Eges børnebog?</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/aimat\.dk");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>