\documentclass{article}
\usepackage{graphicx, hyperref,xcolor} % Required for inserting images
\usepackage{amsmath,amssymb,amsthm,bm,mathtools,xspace,booktabs,tikz}
\newcommand{\am}[1]{{\color{red} #1}}
\newcommand{\gr}[1]{{\color{green} #1}}
\DeclareMathOperator{\cossim}{cossim}

\title{AIMath Word2Vec}
\author{}
\date{January 2025}
\usepackage{amsmath} 
\begin{document}

\am{Planen er at lave 5 noter, som kan læses i følgende rækkefølge: 
\begin{itemize}
\item Introduktion til sprogmodeller (anbefales), 
\item simple sprogmodeller (blød introduktion, kan udelades), 
\item Word2Vec (central), 
\item Neurale netværk til tekstgenerering (avanceret, kan udelades)
\item Transformeren (avanceret, kan udelades, bygger på "neurale netværk til tekstgenerering")
\end{itemize}
Nedenfor får I de tre første.}

\section{Introduktion til sprogmodeller}

Mange former for kunstig intelligens skal kunne håndtere sprog. Det gælder selvfølgelig ChatGPT, franske Le Chat og andre chat-bots og generative AI-programmer. Men I kender det også fra fx oversættelsesprogrammer, tekstbehandlingsprogrammer, der retter grammatik, mobiltelefonens forslag til næste ord i en besked og mailprogrammer, der klassificerer emails som spam ud fra indholdet. 

 Sprog er komplekst. Der er flere hundrede tusinde ord i det danske sprog, afhængigt af hvordan man tæller. Disse ord kan sættes sammen til sætninger på et utal af måder. Men et sprog har samtidig struktur. Det har en \emph{syntaks}, som er regler for, hvilke ord man må bruge, og hvordan sætninger er sat sammen. Noget vanskeligere så har det også en \emph{semantik}, der handler om betydningen af sætninger. Mens syntaksen langt hen ad vejen følger regler, som man ville kunne programmere en computer til at kende, så er semantikken meget mindre regelret. 
Hvordan kan en computer fx vide om "en god kost" refererer til mad eller en fejekost? Vi mennesker afgør det normalt ud fra sammenhængen. 
Vi skal se, at man kan lære en computer at gøre det samme ved hjælp af en \emph{sprogmodel}. Eftersom denne model skal indgå i et computerprogram, er der naturligvis tale om en matematisk model. 

\subsection*{Prædiktion af næste ord}
Fælles for mange sprogmodeller er, at de virker ved at gætte (\emph{prædiktere}) det næste ord i en sætning ud fra de foregående. Generativ AI virker fx ved at bygge sætninger op et ord ad gangen, hvor hvert ord vælges på baggrund af de foregående ord. Hvis en sætning starter med 
$$\text{"Jeg går en tur i ---"}$$
så skal sprogmodellen kunne komme med et gæt på det  næste ord. Det kunne være ordet "skoven". 

Hvordan kommer sprogmodellen så med et godt gæt? Jo, først skal modellen \emph{trænes} på store mængder af tilgængelig tekst, et \emph{tekstkorpus}.  På baggrund af dette tekstkorpus skal sprogmodellen så lære, hvilket ord, der sandsynligvis kommer efter "Jeg går en tur i". Det gør den konkret ved at bygge en funktion, der som input tager starten på en sætning og som output giver  næste ord. Mere præcist giver funktionen for hvert ord i dens ordforråd sandsynligheden for, at netop dette ord er det næste. Sprogmodellen gætter så på et af de mest sandsynlige ord som det næste. I noten om "Simple sprogmodeller" \am{link} ser vi på, hvordan man lidt naivt kunne gøre det ved at tælle, hvor hyppigt forskellige ordkombinationer forekommer, og hvorfor det ikke er en god idé.


\subsection*{Store sprogmodeller}

I stedet må man have fat i en mere avanceret sprogmodel, en såkaldt \emph{stor sprogmodel} (large language model, LLM).
Enhver sprogmodel bygger på viden og hypoteser om, hvordan sprog fungerer. En helt grundlæggende hypotese er, at betydningen af ord kan forstås ved at se på, hvilke sammenhænge, også kaldet \emph{kontekst}, et ord optræder i. Det er en idé, der bl.a. skyldes den danske sprogforsker \href{https://lex.dk/Louis_Hjelmslev}{Louis Hjelmslev} (1899-1965). Vi vil gerne have en model, som har indbygget information om, at 
\begin{itemize}
    \item ordet "mus" kan være et lille pattedyr eller en computermus alt efter sammenhængen,
    \item ordene "søvn", "sove", "seng", "senge" er relaterede, fordi de ofte indgår i de samme sammenhænge,
    \item "op" og "ned" er relaterede ord, selvom de er hinandens modsætninger.
\end{itemize}
I "Word2Vec" ser vi på en matematisk model for sproget, hvor alle ord bliver repræsenteret som vektorer, der inkorporerer netop denne slags information.
Fordelen ved disse vektorer er, at en computer kan regne med dem, samtidig med at de indeholder information om betydning af ordene. 

For at kunne bruge disse vektorer til at gætte næste ord, skal der stadig bygges en funktion, der tager vektorerne som input og giver sandsynligheder for næste ord som output. Da det kræver en meget kompliceret funktion, benyttes der, som i så mange andre avancerede AI-algoritmer, et \emph{neuralt netværk}. Det er emnet i noten om "Tekstgenerering med neurale netværk" \am{link}. 


\section{Simple sprogmodeller}

\subsection*{Sandsynligheden for næste ord}

Vi tager udgangspunkt i en sprogmodel, der skal kunne gætte næste ord i en sætning.
Hvis vi fx ved, at en sætning starter med 
$$\text{"Jeg går en tur i ---"}$$
så skal sprogmodellen prøve at gætte det næste ord. Den skulle gerne vælge et af de mest sandsynlige næste ord.  Vi skriver sandsynligheden for at næste ord er "skoven", når  sætningen starter med "Jeg går en tur i", som 
\begin{equation*}
P(\text{skoven} \mid \text{Jeg går en tur i}).
\end{equation*}

\am{Start box: betingede sandsynligheder}

Her har vi brugt notationen for betingede sandsynligheder.
Hvis $A$ og $B$ er to hændelser og $P(B)>0$, så betegner $P(A\mid B)$ \emph{den betingede sandsynlighed for $A$ givet $B$}, som er givet ved formlen
\begin{equation}\label{eq:betinget}
P(A\mid B) = \frac{P(A\cap B)}{P(B)}
\end{equation}
Her er $A\cap B$ \emph{fælleshændelsen}, det vil sige hændelsen, at $A$ og $B$  forekommer samtidig. Vi fortolker $P(A\mid B)$  som sandsynligheden for, at hændelsen $A$ 
 indtræffer, hvis vi ved, at hændelsen  $B$
 er indtruffet. Dette giver mening i forhold til  \eqref{eq:betinget}, idet brøken angiver, hvor stor en andel af sandsynligheden for $B$, der udgøres af sandsynligheden for, at $A$
 indtræffer samtidig med $B$.

 I vores eksempel ovenfor, er $B$ hændelsen, at sætningen starter med "Jeg går en tur i", mens $A$ er hændelsen, at næste ord er "skoven". Den betingede sandsynlighed $P(A\mid B)$ er så sandsynligheden for at næste ord er "skoven", når vi ved, at sætningen starter med "Jeg går en tur i". Den er givet ved
 \begin{equation*}
P(A\mid B) = P(\text{skoven} \mid \text{Jeg går en tur i}) = \frac{P(\text{Jeg går en tur i skoven})}{P(\text{Jeg går en tur i})}
\end{equation*}

 \am{slut box}

Hvordan kan vi så finde denne sandsynlighed? Jo, her får vi brug for vores korpus af tilgængelig tekst. Vi tæller, hvor mange gange ordkombinationerne "Jeg går en tur i" og "Jeg går en tur i skoven" optræder i teksten. Vi har så, at andelen af gange, "Jeg går en tur i" efterfølges af "skoven", er givet ved
$$P(\text{skoven} \mid \text{Jeg går en tur i})=\frac{\text{Antal gange "Jeg går en tur i skoven" optræder}}{\text{Antal gange "Jeg går en tur i" optræder}}$$
Her opstår et problem: det er slet ikke sikkert, at kombinationen "jeg går en tur i" findes i vores tekstkorpus.\am{Malene laver fodnote} Selv hvis den gør, er det ikke sikkert, at alle relevante fortsættelser ("byen", "parken", "haven", ...) er repræsentereret. Man er derfor nødt til at gøre noget smartere. Det kan gøres på mange måder, og vi ser først på en naiv tilgang kaldet \emph{$N$-gram sprogmodeller}. 

\subsection*{$N$-gram sprogmodeller}
I \emph{$N$-gram sprogmodeller} forsøger man også at bestemme sandsynligheden for næste ord, men nu baserer man sig kun på de $N-1$ foregående ord. Hvis vi skal gætte næste ord efter "Jeg går en tur i", ville en $4$-gram sprogmodel basere sig på sandsynligheder på formen
$$P(\text{skoven} \mid \text{en tur i})=\frac{\text{Antal gange "en tur i skoven" optræder}}{\text{Antal gange "en tur i" optræder}}$$
Vi tager altså ikke starten på sætningen "Jeg går" i betragtning, fordi en 4-gram sprogmodel udelukkende kigger på de sidste tre ord "en tur i".

Som et eksempel på, hvordan $N$-gram sprogmodeller fungerer, ser vi på 
 et meget lille tekstkorpus bestående af fire sætninger med seks forskellige ord:
\begin{align*}
&\text{"En hund løber efter en kat.}\\
&\text{Løber en hund efter en kat?}\\
&\text{En kat løber ikke efter en hund.}\\
&\text{Efter en kat løber en hund."}
\end{align*}
Der er 25 ord i teksten, men kun 6 forskellige: 
en (8), hund (4), løber (4), efter (4), kat (4), ikke (1), hvor tallet i parentes angiver antal gange, ordet optræder i vores tekst. Vi ignorerer tegnsætning og skelner ikke mellem små og store  bogstaver.

Et \emph{bigram} er et par af ord, der forekommer efter hinanden i teksten. I Tabel 1 nedenfor er der lavet en hyppighedstabel over alle bigram i vores tekst. Vi  lader som om, teksten starter forfra, så det sidste "hund" efterfølges af "En".

\begin{table}[h]
    \centering
    \begin{tabular}{c|cccccc}
         
         & en & hund & løber & efter &  kat & ikke\\
         \hline
        en & 0 & 4 & 0 & 0 & 4 & 0\\
        hund&1 & 0 & 1 & 2 & 0 & 0\\
         løber& 2 & 0 & 0 & 1 & 0 & 1 \\
      efter   & 4 & 0  & 0 & 0 & 0 & 0\\
      kat  & 1 & 0 & 3 & 0 & 0 & 0\\
       ikke  & 0 & 0 & 0 &1  & 0 & 0\\
    \end{tabular}
    \caption{Hyppighed af bigram.}
    \label{tab:my_label}
\end{table}

Tabellen viser hyppigheden af forskellige bigram i vores tekst.  Ordet i venstre søjle er første ord i bigrammet, og ordet i første række er andet ord i bigrammet. Fx betyder de to 4-taller i anden række, at "en hund" og "en kat" hver optræder 4 gange. Der forekommer 12 forskellige bigram i teksten. Det er dem, hvor der ikke står 0 i tabellen. Hvis der står 0, svarer det til et bigram, der ikke forekommer.

Hvis det sidste ord i vores sætning er "ord$_1$", kan vi beregne sandsynligheden for, at næste ord vil være "ord$_2$". Det skriver vi som
$$P(\text{ord}_2\mid \text{ord}_1) = \frac{\text{Antal gange bigrammet "ord$_1$ ord$_2$" optræder}}{\text{Antal gange "ord$_1$"  optræder}}$$
Hvis fx det seneste ord i vores sætning er "løber", så er sandsynligheden for at næste ord er "efter" 
$$P(\text{efter} \mid \text{løber})=\frac{1}{4},$$
idet "løber" forekommer 4 gange, og 1 af gangene er "efter" det næste ord.

Lad os illustrere, hvordan man kan bruge bigram til at danne nye sætninger.
\begin{itemize}
\item Vælg et begyndelsesord - lad os sige "En". 
\item Bigramtabellen siger nu, at der i vores tekst er to muligheder for næste ord, nemlig "hund" og "kat". De er lige sandsynlige - de optræder begge 4 gange ud af 8. Vi slår plat og krone for at finde næste ord. Det blev "kat".
\item Vi har nu "En kat" og kigger i bigramtabellen efter et ord, der står efter "kat". Der er to muligheder: "løber" (3 gange)  og "en" (1 gang). Vi vælger nu "løber" med sandsynlighed 3/4 og "en" med sandsynlighed 1/4. Det blev "løber".
\item Nu har vi "En kat løber". Hvad kommer efter "løber"? Det gør "en" (2 gange), "efter" (1 gang) og "ikke" (1 gang). Vi vælger et af ordene "en", "efter" og "ikke" med sandsynlighed hhv 2/4, 1/4 og 1/4. Vi får "en".
\item Vi har "En kat løber en". Næste mulige ord er "hund" og "kat", som er lige sandsynlige. Vi får "kat". 
\item Vi slutter her og ender med sætningen "En kat løber en kat". Det giver ikke så megen mening.
\end{itemize}

Det startede meget godt med at danne vores sætning, men på et tidspunkt holdt den op med at give mening.
Problemet er, at man ikke får ret meget af sammenhængen med, når man kun bruger det sidste ord til at gætte det næste udfra. Derfor får man let genereret nogle ret underlige sætninger. Man kan forbedre det ved at kigge på \emph{trigram}. Et trigram er en sekvens på tre ord efter hinanden. I Tabel 2 nedenfor er vist hyppigheden af trigram i vores lille tekst.  

\begin{table}[h]
    \centering
    \begin{tabular}{c|cccccc}
         &  en&hund  & løber & efter & kat & ikke\\
         \hline
      en hund   & 1 &  0& 1 & 2 & 0 & 0\\
      en kat   & 1 &0  &3  &0  &0  &0 \\
       hund en   & 0 & 1 & 0 & 0 & 0 &0 \\
       hund løber  & 0 & 0 & 0 & 1 & 0 &0 \\
      hund efter   &2  &0  &0  & 0 & 0 & 0\\
       løber en  &0  & 2 & 0 & 0 & 0 & 0\\
       løber efter  & 1 & 0 &0  & 0 & 0 & 0\\
       løber ikke  & 0 & 0 & 0 &1  & 0 & 0\\
      efter en   & 0 & 2 & 0 & 0 & 2 & 0\\
        kat en   & 0 & 0 & 0 & 0 & 1 & 0\\
       kat løber  &2  & 0 & 0 & 0 & 0 &1 \\
      ikke efter   & 1 & 0 & 0 & 0 & 0 & 0\\
    \end{tabular}
    \caption{Hyppighed af trigram.}
    \label{tab:my_label}
\end{table}
 Søjlen til venstre i Tabel 2 er de første to ord i trigrammet, mens første række angiver tredje ord. Bemærk, at kun bigram, der faktisk forekommer i vores tekstkorpus er vist i første søjle. Anden række viser således, at bigrammet "en hund" optræder med "en", "løber" og "efter" som næste ord. Trigrammet "en hund efter" optræder 2 gange i vores tekstkorpus.

Man kan nu finde sandsynlighederne for trigrammet "ord$_1$ ord$_2$ ord$_3$" når vi ved at de to første ord er "ord$_1$ ord$_2$". Dette skriver vi igen som en betinget sandsynlighed
$$P(\text{ord}_3\mid \text{ord}_1\text{ ord}_2) = \frac{\text{Antal gange trigrammet "ord$_1$ ord$_2$ ord$_3$" optræder}}{\text{Antal gange bigrammet "ord$_1$ ord$_2$" optræder}}$$

Bruger man informationen i trigram til at generere ny tekst, er det mere restriktivt, hvad man kan skrive. Man kan ikke skrive "En hund en kat en hund", fordi "hund en kat" ikke er et trigram i vores tekstkorpus.  Man kunne derimod godt skrive "En hund en hund en hund", da både "hund en hund" og "en hund en" er trigram i vores tekstkorpus. 
\begin{itemize}
    \item Lad os begynde som i bigram-eksemplets Trin 2, hvor vi har "En kat". Der er igen to muligheder: "en kat løber" og "en kat en". Lad os sige, vores tilfældige valg giver "en kat løber"
    \item Nu skal vi finde en mulighed til "kat løber ..." og der kan vi vælge "en" eller "ikke". Vi kan ikke vælge "efter", som vi kunne i bigram-modellen. Vi rammer måske "En kat løber ikke"
    \item Vi skal finde et ord til "løber ikke ..." og der er nu kun "efter" at vælge.
    \item Vi har "en kat løber ikke efter" og har kun muligheden "ikke efter en"
    \item Fra "en kat løber ikke efter en", kan vi vælge "hund" eller "kat". Vælger vi "hund", får vi "en kat løber ikke efter en hund". 
\end{itemize}
Det er et meget lille tekstkorpus, vi har, og det giver naturligvis problemer. Man kan ikke generere fornuftige sætninger som "Løber en kat efter en hund?" ("kat efter" er end ikke et bigram i vores lille tekstkorpus). Et andet problem opstår, hvis vi ønsker at starte en sætning med "Efter kat". Dette bigram forekommer ikke i vores tekstkorpus, så vi kan slet ikke komme videre. 

Selv hvis vi havde store mængder tekst i vores tekstkorpus, er to ord ikke meget at gætte næste ord ud fra, da meningen med teksten nemt går tabt, når man kun betragter de sidste to ord. For at få et bedre indtryk af meningen med sætningen kan man bruge \emph{$N$-gram}, som består af $N$ på hinanden følgende ord. Man prøver så at gætte det $N$te ord ud fra de $N-1$ foregående. Her er der igen et problem med, at mange $(N-1)$-gram slet ikke findes i vores tekstkorpus. Starter vi med sådan et $(N-1)$-gram, kan vi  ikke komme til at gætte videre. Jo større $N$ er, desto større bliver problemet med manglende $N$-gram.

\am{\subsubsection*{EGES BØRNEBOG som eksempel - app-??}

Her kunne det være fedt med et bigram for Eges korpus
Evt. en app, der kan generere næste ord...
Måske opgaver baseret på børnebogen? }

\subsection*{Mere avancerede sprogmodeller}
For at kunne gætte næste ord med en $N$-gram sprogmodel, har vi brug for et stort $N$ for at få meningen med. Det giver rigtig mange mulige kombinationer af de $N-1$ ord, som vi prædikterer næste ord ud fra. Mange af dem vil ikke være repræsenteret i vores tekstkorpus, og vi kan derfor ikke benytte $N$-gram modellen. Men hvad nu, hvis vores tekstkorpus indeholder en ordsekvens, hvis betydning minder om? Måske ønsker vi at prædiktere næste ord i sætningen
$$\text{"Min hund har en blød ---"}$$
Og måske står denne kombination af ord ikke noget sted i vores tekstkorpus. Til gengæld er der måske et sted hvor der står
$$\text{"Min kat har en blød pels"}$$
Hvis vi nu ved, at ordene "hund" og "kat" tit indgår i sammenhænge, der ligner hinanden, så kan vi måske erstatte sætningen "Min hund har en blød" med "Min kat har en blød" og bruge det til at gætte, at næste ord skal være "pels". For at udføre denne idé i praksis, får vi brug for en stor sprogmodel, som en model for sprogets betydning. Som et eksempel på, hvordan det kan gøres, kan du læse om Word2Vec \am{her}. Kort fortalt er idéen at repræsentere hvert ord med en vektor, hvor ord, hvis betydning minder om hinanden svarer til vektorer, med nogenlunde samme retning og længde.
\am{Måske et simpelt plot med vektorer for "hund", "kat" og "æble" eller lignende...}
Når vi har lavet vektorrepræsentationer af alle ord i sproget, skal vi bruge dem til at lave prædiktioner af næste ord ud fra de foregående. Til det kan man benytte en form for \emph{neuralt netværk}, som du kan læse mere om \am{her}. I praksis bruges dog en \emph{transformer}\am{link}, som er endnu mere avanceret, og som i praksis er den sprogmodel, der virker bedst til tekstgenerering. 


\section{Word2Vec}
 Lad os igen se på en situation, hvor vi gerne vil kunne gætte næste ord i en sætning. Lad os sige, at vi har sætningen
$$\text{"Min hund har en blød ---"}$$
og vil gætte næste ord. Hvis vi har en stor mængde tekst til rådighed, et tekstkorpus, kan vi selvfølgelig lede efter ordsekvensen "Min hund har en blød" og se hvilket ord, der oftest kommer efter som beskrevet i \am{link}. Men hvis ikke sekvensen forekommer i vores korpus, så har vi et problem. I stedet kunne vi lede efter en sætning med en betydning, der minder om "Min hund har en blød" og se, hvad der kommer efter den. Men hvordan får vi en computer til at forstå betydningen af ord?

Den simpleste måde at repræsentere et ord på i en computer ville være ved at nummerere alle ordene i det danske sprog fra 1 til $V$, hvor $V$ er det samlede antal ord. Nummeret på et ord giver dog ikke megen information om ordets betydning. 

En anden nærliggende idé kunne være at repræsentere et ord ved bogstaverne i ordet. For at en computer skal kunne forstå det, kunne man give hvert bogstav et tal ud fra bogstavets nummer i alfabetet. Så ville "kat" blive til $(11,1,20)$ og "hund" til $(8,21,14,4)$. Stavemåden fortæller dog heller ikke meget om betydningen af et ord. Ordet "mund" staves næsten lige som "hund", men har en helt anden betydning. Omvendt betyder 
ordet "vovse" næsten det samme som "hund", men staves helt anderledes. 

I stedet vil vi gerne repræsentere hvert ord med en vektor. Idéen er, at ord, hvis betydning ligner hinanden, skal repræsenteres med vektorer, der peger i nogenlunde samme retning og har nogenlunde samme længde, som illustreret på Figur ?.

\am{FIGUR}

I kender vektorer i 2 eller 3 dimensioner og ved, at de kan skrives på formen
$$\begin{pmatrix} a_1\\a_2\end{pmatrix}\text{ og } \begin{pmatrix} a_1\\a_2 \\a_3\end{pmatrix}$$
hvor $a_1, a_2$ og (eventuelt) $a_3$ er reelle tal, der kaldes vektorens koordinater. Tre koordinater er dog ikke nok til at indfange betydningen af alle ord i sproget. Derfor bruger man i stedet en \emph{$m$-dimensional vektor}, som man kan tænke på som en liste af $m$ koordinater 
$$\begin{pmatrix} a_1\\a_2\\a_3\\ \vdots \\ a_m\end{pmatrix}$$
hvor $a_1,a_2,a_3,\ldots,a_m$ er reelle tal. 
I praksis vælger man $m$ stort, fx $m=100$. 
Man kan regne med $m$-dimensionale vektorer, lige som man gør i to eller tre dimensioner. Vi kommer for eksempel til at se, hvordan man kan finde skalarprodukter. Til gengæld har man ikke mulighed for at visualisere en $m$-dimensional vektor som en pil i et koordinatsystem, men det har vi heldigvis heller ikke brug for.

I denne note ser vi på algoritmen Word2Vec som et eksempel på, hvordan man kan oversætte ord til vektorer, der repræsenterer ordenes betydning. Algoritmen blev opfundet af en gruppe medarbejdere hos Google i 2013. I dag bruges diverse forfininger af algoritmen i mange store sprogmodeller.

\subsection*{Betydning og kontekst}
Hvilke egenskaber skal de vektorer, der repræsenterer ord, så have? Jo, idéen er, at vektorerne skal indfange betydningen af et ord i den forstand, at ord, hvis betydning minder om hinanden, svarer til vektorer, der ligner hinanden. Og hvordan ved vi så, om to ords betydning minder om hinanden? Betydningen af et ord har noget at gøre med, hvilke sammenhænge det optræder i, når man kigger i rigtig mange dokumenter. En sproglig sammenhæng kaldes også en \emph{kontekst}.
Ord, der betyder næsten det samme vil altså ofte  optræde i samme  {kontekst}. Ordene "hund" og "kat" er for eksempel forskellige, men de vil ofte optræde i sammenhænge, der ligner hinanden. Se for eksempel på sætningerne
$$\text{"Min --- har spist af sin madskål"}$$
og
$$ \text{"Sikken en blød pels, din --- har"}$$
Her ville der kunne stå "hund" eller "kat", men nok ikke "kælk" eller "badedragt". Betydningen af ordene "hund" og "kat" er tættere på hinanden end betydningen af "hund" og "kælk". På den anden side kunne der ikke stå 
"hund" i sætningen
$$\text{"Den lille --- har hvide knurhår"}$$
mens både "kat" og "mis" ville passe ind. Ordene "kat" og "hund" er altså tætte på hinanden, men ikke så tætte som "kat" og "mis".
Vi vil derfor gerne have, at vektorerne for "hund" og "kat" minder mere om hinanden end vektorerne for "hund" og "kælk", men ikke så meget som "kat" og "mis". Hvad vi forstår ved, at vektorer minder om hinanden, kommer vi tilbage til. 

Konteksten er særlig vigtig i forbindelse med ord med flere betydninger. Vi forstår fx betydningen af ordet "marsvin" forskelligt alt efter om ordet "hav" eller ordet "mælkebøtte" optræder i nærheden af det.

\subsection*{Træningsdata}
Vi har altså brug for at vide, hvilken kontekst ordene indgår i. For at lære, hvilken kontekst et ord forekommer i, tager vi udgangspunkt i et stort tilgængeligt tekstkorpus. Vi kalder dette korpus for vores \emph{træningsdata}.

Ved konteksten til et ord vil vi her forstå de ord, der står umiddelbart før og efter ordet. Mere præcist vælger vi et vindue, lad os sige på fem ord, hvor ordet i midten er det, vi gerne vil kende betydningen af. Vi vil kalde dette for \emph{inputordet}. De to første ord  og de to sidste ord i vinduet er inputordets \emph{kontekstord}.  Se fx på sætningen
$$\text{\fbox{Den sorte \textbf{hund} logrer med} halen}$$
hvor boksen angiver vores 5-ords vindue. Det midterste ord  "hund" er vores inputord, ordene "Den", "sorte", "logrer" og "med" er kontekstord til "hund". 

Vi starter med at placere vinduet omkring det første ord i vores datasæt og noterer dets fire kontekstord (hvoraf de to vil være blanke). Vi flytter nu vinduet mod højre et ord ad gangen, og hver gang noterer vi inputordet og  dets fire kontekstord. Vi gør det for al teksten i vores træningsdata og samler informationen i et datasæt som vist i Tabel \ref{tab:positiv}.
\begin{table}[h]
\begin{center}
\begin{tabular}{cc}
{Input} & {Kontekst} \\
\hline
\vdots & \vdots  \\
sorte &  \\
sorte & Den  \\
sorte & hund \\
sorte & logrer \\
hund & Den\\
hund & sorte \\
hund & logrer \\
hund & med \\
\vdots & \vdots
\end{tabular}
\end{center}
\caption{Positive eksempler på input- og kontekstord}\label{tab:positiv}
\end{table}
Hver række i tabellen består af et inputord og et af dets kontekstord. Sådan et par kalder vi \emph{positive eksempler}. (Fodnote: Når man vil gætte det næste ord i en sætning har man selvfølgelig kun lov til at bruge de ord, der kommer før ordet. Det er dog ikke det, vi er ude på, når vi laver Word2Vec. Vi er ude på at forstå, hvordan et ord forholder sig til dets kontekst, altså de omkringstående ord. Derfor er der ikke noget problem i, at vinduet både indeholder ord før og efter inputordet. )

Vi vil gerne kunne modellere sandsynligheden for, at et ord $w$ har ordet $c$ som kontekst. For at kunne gøre det, er vi nødt til at have noget at sammenligne med i form af eksempler på ord, der ikke er kontekstord. Og hvor får vi så dem fra? Ja, dem laver vi da bare selv.

For hvert positivt eksempel i datasættet vælger vi $n$ \emph{negative eksempler}, som er $n$  tilfældigt valgte ord blandt alle ordene i vores ordforråd undtagen kontekstordet i eksemplet. For eksempel kunne man sætte $n=3$. For parret bestående af inputordet "hund" og kontekstordet "sorte" vælger man så  tre tilfældige kontekstord, som ikke er "sorte". Det kunne være "og", "palme" og "synger". Det giver os et datasæt på formen vist i Tabel \ref{tab:negativ}.
\begin{table}[h]
\begin{center}
\begin{tabular}{ccccc}
{Input} & {Positiv} & Negativ & Negativ & Negativ \\
\hline
\vdots & \vdots & \vdots & \vdots &\vdots\\
sorte & hund & og & palme & synger \\
sorte & logrer & bage  & fra & tredive\\
hund & Den & mellem & jordbær & emne \\
\vdots & \vdots & \vdots & \vdots &\vdots\\
\end{tabular}
\end{center}\label{tab:negativ}
\caption{Positive og negative eksempler}
\end{table}
I praksis vælges alle de negative eksempler ikke med samme sandsynlighed. Hyppigt forekommende ord har en større sandsynlighed for at blive valgt end sjældnere ord. Hvis $h_i$ er hyppigheden af $i$te ord, så vælges det $i$te ord med sandsynligheden 
$$\frac{h_i^\alpha}{\sum_{j=1}^V h_j^\alpha}$$
hvor $V$ er antallet af ord i sproget. I praksis benyttes ofte $\alpha=3/4$. På den måde bliver sandsynlighederne ikke helt proportionale med hyppighederne. Vi giver en lille fordel til sjældne ord, men ikke så meget som hvis vi valgte alle ord med samme sandsynlighed.



Nu har vi fået lavet et datasæt bestående af både positive og negative eksempler på input- og kontekstord. I de næste to afsnit ser vi på, hvordan man kan modellere sandsynligheden for at et ord $c$ optræder som kontekst til et inputord $w$ ved hjælp af vektorer. Det vil vi sidenhen bruge til at vælge vektorerne, således at vores positive eksempler i datasættet bliver meget sandsynlige og de negative eksempler bliver meget usandsynlige. 

\subsection*{Input- og kontekstvektorer}
I første omgang vil vi lade hvert ord $w$ være repræsenteret af to vektorer, $\overrightarrow{v}_{w}$ og $\overrightarrow{k}_{w}$, hvor $\overrightarrow{v}_{w}$ repræsenterer ordet, når det optræder som input, mens $\overrightarrow{k}_{w}$ repræsenterer ordet, når det optræder som kontekst.  Betydningen af $w$ afgøres som nævnt af hvordan ordet forholder sig til konteksten. Det vil vi oversætte matematisk til, hvordan $\overrightarrow{v}_{w}$ forholder sig til kontekstvektorerne $\overrightarrow{k}_{c}$ for diverse kontekstordord $c$. Vi vil derfor tænke på $\overrightarrow{v}_{w}$ som den vektor, der repræsenterer betydningen af ordet, og altså den vi er ude på at bestemme.

For at måle hvordan inputvektoren $\overrightarrow{v}_{w}$ for ordet $w$ forholder sig til kontekstvektoren $\overrightarrow{k}_{c}$ for ordet $c$, vil vi bruge \emph{skalarproduktet} $\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$.   Husk på, at man finder skalarproduktet mellem to vektorer i to dimensioner ved formlen
 $$\begin{pmatrix} a_1\\a_2\end{pmatrix}\cdot \begin{pmatrix} b_1\\b_2\end{pmatrix}=a_1b_1+a_2b_2$$ og i tre dimensioner ved formlen
$$\begin{pmatrix} a_1\\a_2\\a_3\end{pmatrix}\cdot \begin{pmatrix} b_1\\b_2\\b_3\end{pmatrix}=a_1b_1+a_2b_2+a_3b_3$$
 Tilsvarende kan man definere skalarproduktet mellem to  $m$-dimensionale vektorer ved
$$\begin{pmatrix} a_1\\a_2\\a_3\\ \vdots\\a_m\end{pmatrix}\cdot \begin{pmatrix} b_1\\b_2\\b_3\\ \vdots \\b_m\end{pmatrix}=a_1b_1+a_2b_2+a_3b_3+\dotsm + a_mb_m$$

Vi vil gerne have, at vores input- og kontekstvektorer skal opfylde, at hvis $w$ ofte har $c$ som kontekst, så er skalarproduktet  $\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ stort, mens en meget negativ værdi af $\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ indikerer, at $w$ sjældent har $c$ som kontekst.

Hvad fortæller skalarproduktet om, hvordan to vektorer forholder sig til hinanden? I 2 og 3 dimensioner kan vi give en geometrisk fortolkning af skalarproduktet ved hjælp af formlen 
\begin{equation}\label{eq:skalar}
\overrightarrow{a}\cdot\overrightarrow{b} = |\overrightarrow{a}||\overrightarrow{b}| \cos(\angle (\overrightarrow{a},\overrightarrow{b}))
\end{equation}
hvor $\angle (\overrightarrow{a},\overrightarrow{b})$ er vinklen mellem vektorerne $\overrightarrow{a}$ og $\overrightarrow{b}$, og $|\overrightarrow{a}|$ betegner længden af $\overrightarrow{a}$, som findes med formlen 
$$|\overrightarrow{a}|=\sqrt{\overrightarrow{a}\cdot\overrightarrow{a}}$$
Cosinus er en aftagende funktion på intervallet $[0,\pi]$, så jo større vinklen $\angle (\overrightarrow{a},\overrightarrow{b})$ er, desto mindre vil $\cos(\angle (\overrightarrow{a},\overrightarrow{b}))$ være.\am{figur}
 Det betyder, at $ \cos(\angle (\overrightarrow{a},\overrightarrow{b}))$ er størst når vinklen mellem $\overrightarrow{a}$ og $\overrightarrow{b}$ er 0, svarende til at vektorerne peger samme vej. Her er $ \cos(\angle (\overrightarrow{a},\overrightarrow{b}))=1$. Den mindste værdi af $\cos(\angle (\overrightarrow{a},\overrightarrow{b}))$ er $-1$, som antages ved en vinkel på $\pi$, hvor vektorerne peger i modsat retning. 



Jo mindre vinklen mellem $\overrightarrow{v}_{w}$ og $\overrightarrow{k}_{c}$ er, desto  større er deres skalarprodukt $\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ altså,  og jo oftere har ordet $w$ dermed $c$  som kontekst. Desuden viser \eqref{eq:skalar}, at lange vektorer tæller mere, både positivt og negativt, end korte vektorer. Ord, der er gode til at forudsige konteksten udfra, fx "logre", vil derfor blive repræsenteret med længere inputvektorer  end ord som "og" eller "er", der ikke indeholder megen information om konteksten.  Altså vil $|\overrightarrow{v}_{logre}|$ være større end $|\overrightarrow{v}_{og}|$ og $|\overrightarrow{v}_{er}|$.

Bemærk også, at hvis to ord  ofte optræder i samme kontekst, skal deres inputvektorer gerne have nogenlunde samme skalarprodukt med alle kontekstvektorer. Formlen \eqref{eq:skalar} viser, at det betyder, at de dels skal have nogenlunde samme længde og dels skal have nogenlunde samme vinkel med alle kontekstvektorerne. Sidstnævnte kræver, at de har nogenlunde samme retning. Ord, hvis betydning ligner hinanden, kommer derfor til at svare til vektorer, hvis længde og retning ligner hinanden.

I praksis bruger vi vektorer af højere dimension end 3. Det er måske ikke helt oplagt, hvad man skal forstå ved længden af en vektor eller vinklen mellem to vektorer i højere dimensioner, men det viser sig, at man stadigvæk godt kan give mening til formlen \eqref{eq:skalar}. Intuitionen fra to eller tre dimensioner er derfor god at have, selv om vi arbejder med højere dimensioner.

\subsubsection*{Eksempel 1}\am{Forslag til eksempel. Forslag til bedre ord modtages gerne, der kan nok også laves en flottere figur. }
Lad os sige, at vi har fundet en vektorrepræsentation, der opfylder det ønskede. Det gav vektorerne
\begin{align*}
&\overrightarrow{v}_{hund} =  \begin{pmatrix} 1.5\\ 1\end{pmatrix}, \overrightarrow{v}_{kat} =  \begin{pmatrix} 1\\ 1.5\end{pmatrix} \\
&\overrightarrow{k}_{madsk\aa l}  =  \begin{pmatrix} 1.5\\1.5\end{pmatrix}, \overrightarrow{k}_{badedragt} =  \begin{pmatrix} -1\\-0.75 \end{pmatrix}, \overrightarrow{k}_{lufte} =  \begin{pmatrix} 1\\0.25 \end{pmatrix} \end{align*}
Vektorerne er vist herunder, hvor inputvektorer er røde, og kontekstvektorer er blå:

\begin{center}
\begin{tikzpicture}
\draw[->] (0,-2) -- (0,3.5);
\draw[->] (-2,0) -- (3.5,0);
\draw[->,red] (0,0) -- (2,3);
\draw[->,red] (0,0) -- (3,2);
\draw[->,blue] (0,0) -- (3,3);
\draw[->,blue] (-0,0) -- (-2,-1.5);
\draw[->,blue] (-0,0) -- (2,0.5);
\node[red] at (1.5,3) {$\overrightarrow{v}_{kat}$};
\node[red] at (3,1.5) {$\overrightarrow{v}_{hund}$};
\node[blue] at (4,3) {$\overrightarrow{k}_{madsk\aa l}$};
\node[blue] at (2.5,0.5) {$\overrightarrow{k}_{lufte}$};
\node[blue] at (-2,-1) {$\overrightarrow{k}_{badedragt}$};
\end{tikzpicture}
\end{center}

Vi kan udregne skalarprodukterne
\begin{align*}
&\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l} = \begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} 1.5\\ 1.5\end{pmatrix} = 1\cdot 1.5 +  1.5\cdot 1.5 = 3.75 \\ 
&\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{badedragt} = \begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} -1\\ -0.75\end{pmatrix} = 1\cdot (-1) +  1.5\cdot (-0.75) = -2.125 
\end{align*}
Vi ser, at $\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l}$ er større end $\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{badedragt}$. Det svarer til, at ordet "kat" oftere har "madskål" som kontekst, end det har "badedragt". Det ses også ved, at  $\overrightarrow{k}_{madsk\aa l}$ peger i nogenlunde samme retning som $\overrightarrow{v}_{kat}$, mens $\overrightarrow{k}_{badedragt} $ peger i en helt anden retning.

Vi kan også udregne
\begin{align*}
&\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{madsk\aa l} = \begin{pmatrix} 1.5\\ 1\end{pmatrix}  \cdot  \begin{pmatrix} 1.5\\ 1.5\end{pmatrix} = 1.5\cdot 1.5 +  1\cdot 1.5 = 3.75 
\end{align*}
Vi ser, at $\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{madsk\aa l} =\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{madsk\aa l} =3.75$, svarende til at både "hund" og "kat" ofte har "madskål" som kontekst. Vi ser da også, at vektorerne $\overrightarrow{v}_{hund}$ og $\overrightarrow{v}_{kat} $ peger i nogenlunde samme retning og har nogenlunde samme længde, fordi de tit har samme kontekst. De to vektorer er dog ikke helt ens, da der også vil være nogle kontekstord, der er ikke er lige hyppige for "hund" og "kat". Vi kan fx udregne
\begin{align*}
&\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{lufte} = \begin{pmatrix} 1.5\\ 1\end{pmatrix}  \cdot  \begin{pmatrix} 1\\ 0.25\end{pmatrix} = 1.5\cdot 1 + 1\cdot 0.25 =1.75\\
&\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{lufte} =\begin{pmatrix} 1\\ 1.5\end{pmatrix}  \cdot  \begin{pmatrix} 1\\ 0.25\end{pmatrix} = 1\cdot 1 + 1.5\cdot 0.25 = 1.375 
\end{align*}
Her er $\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{lufte}$ større end $\overrightarrow{v}_{kat}\cdot \overrightarrow{k}_{lufte}$ svarende til, at "hund" oftere har "lufte" som kontekst, end "kat" har. \am{bedre ord end "lufte"?}

\am{begynd box}

Måske har du undret dig over, hvorfor vi har brug for to forskellige vektorer for hvert ord. Hvert positivt eksempel forekommer jo to gange i vores datasæt, hvor de to ord skiftes til at spille rollen som input og kontekst. Alligevel er der ikke symmetri mellem de to ord.  Hvis fx inputordet er "logre", er det ret sandsynligt, at "hund" er et af kontekstordene. Inputordet "hund" forekommer derimod i mange kontekster, der ikke involverer ordet "logre". Sandsynligheden for kontekstordet afhænger derfor af, om det er "hund" eller "logre", der er input. 
Hvis vi kun brugte én vektor for hvert ord, $\overrightarrow{u}_{hund}$ og $\overrightarrow{u}_{logre}$, ville vi få samme skalarprodukt $\overrightarrow{u}_{hund}\cdot \overrightarrow{u}_{logre} =  \overrightarrow{u}_{logre}\cdot \overrightarrow{u}_{hund}$, og dermed samme sandsynlighed, uanset hvilket ord der var input. 

\am{slut box}
 
\subsubsection*{Opgaver} 

\begin{itemize}
\item Antag at vi har lavet 4-dimensionale input- og kontekstvektorer således, at jo større skalarproduktet $\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ er, desto mere sandsynligt er det, at ordet $w$ har $c $ som kontekst. Inputvektoren for "hund"  og kontekstvektorerne for "pels" og "fjer" er
\begin{align*}
\overrightarrow{v}_{hund}=\begin{pmatrix} 0.5\\2\\1\\-1\end{pmatrix} ,\quad
\overrightarrow{k}_{pels}=\begin{pmatrix} 0\\3\\2\\-2\end{pmatrix},\quad
\overrightarrow{k}_{fjer}=\begin{pmatrix} 1\\-2\\1.5\\0.5\end{pmatrix}
\end{align*}
Udregn skalarprodukterne $\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{pels}$ og $\overrightarrow{v}_{hund}\cdot \overrightarrow{k}_{fjer}$. Passer det med, hvilket af ordene "pels" og "fjer" der er mest sandsynligt som kontekst til "hund"?

\item Antag at vi har lavet 3-dimensionale input- og kontekstvektorer som beskrevet i afsnittet ovenfor. Så skulle ord, der ofte har samme kontekst gerne have nogenlunde samme længde og retning, mens ord, der betyder noget helt forskelligt, kan have meget forskellig længde og retning. Antag at inputvektorerne for "kat", "hund", "mis" og "kælk" er
\begin{align*}
\overrightarrow{v}_{kat}=\begin{pmatrix}0\\2\\1 \end{pmatrix},\quad
\overrightarrow{v}_{hund}=\begin{pmatrix}0\\1\\1.7\end{pmatrix}, \quad
\overrightarrow{v}_{mis}=\begin{pmatrix}0.4\\2\\0.9\end{pmatrix},\quad
\overrightarrow{v}_{kælk}=\begin{pmatrix} 2\\-1\\0 \end{pmatrix}
\end{align*}
Find længden af de fire vektorer. Find vinklen mellem $\overrightarrow{v}_{kat}$ og de tre øvrige vektorer. Stemmer resultatet overens med hvilke ord der er tættest på "kat" i betydning? Tegn vektorerne ind i GeoGebra.

\end{itemize}

\subsection*{Model for sandsynligheder}
Som nævnt, vil vi gerne have, at jo større skalarproduktet $\overrightarrow{v}_{w}\cdot \overrightarrow{k}_{c}$ er, desto mere sandsynligt er det, at ordet $w$ har ordet $c$ som kontekst. 
Skalarprodukter kan imidlertid antage alle reelle værdier, så de egner sig ikke til at repræsentere en sandsynlighed, der jo skal være et tal mellem 0 og 1. Derfor anvender vi en funktion $f$ på skalarproduktet med definitionsmængde $\mathbb{R}$ og værdimængde $]0,1[$ for at få skalarprodukterne lavet om til tal mellem 0 og 1. Den funktion, vi vil bruge, er \emph{sigmoid-funktionen} (også nogle gange kaldet den logistiske funktion). Forskriften er 
\begin{equation}\label{eq:sigmoid}
f(x)=\frac{1}{1+e^{-x}}
\end{equation}
og grafen ses herunder
\am{indsæt graf}
Det ses af grafen, at $f$ er strengt voksende, og at værdimængden for $f$ er $]0,1[$.

Word2Vec-algoritmen modellerer sandsynligheden $P(c\mid w)$ for at ordet $c$ er kontekst til inputordet $w$ som (Fodnote: Bemærk at vi bruger notationen $P(c\mid w)$ lidt anderleds end i "Simple sprogmodeller". Der betød det sandsynligheden for at $c$ er næste ord efter $w$. Her betyder det sandsynligheden for, at $c$ er kontekst til $w$.)
\begin{equation}\label{eq:pcw}
P(c\mid w) = f(\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c})=\frac{1}{1+e^{-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}}}
\end{equation}
Denne sandsynlighed vil ligge i værdimængden for $f$, som var $]0,1[$. Da $f$ er strengt voksende, vil sandsynligheden $P(c \mid w)$ være større, jo større $\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}$ er. Bemærk desuden, at sandsynligheden for, at $c$ ikke er kontekst til $w$, er givet ved 
\begin{align}\label{eq:pneg}
1-P(c\mid w) &= 1-f(\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}) = f(-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c})
= \frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c}}}
\end{align}
hvor vi brugte, at $1-f(x)=f(-x)$. Dette vises i boksen nedenfor.

\subsubsection*{Eksempel 2}
I Eksempel 1 fandt vi, at $\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte} = 1.75$. Sandsynligheden for, at "hund" har "lufte" som kontekst, er derfor 
$$P(\text{ lufte } \mid \text{ hund }) = \frac{1}{1+e^{-\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte}}}=\frac{1}{1+e^{-1.75}} \approx  0.852.$$
Sandsynligheden for, at 
"hund" ikke har "lufte" som kontekst, er 
$$1- P(\text{ lufte } \mid \text{ hund }) = \frac{1}{1+e^{\overrightarrow{v}_{hund} \cdot \overrightarrow{k}_{lufte}}}=\frac{1}{1+e^{1.75}} \approx 0.148.$$

\am{begynd box1}

 Man kan også finde værdimængden for $f$ ved at kigge på forskriften for $f$. Da både tæller og nævner i \eqref{eq:sigmoid} er positive, får vi at 
$$f(x)=\frac{1}{1+e^{-x}}>0$$
Da eksponentialfunktionen kun kan antage positive værdier, er $1<1+e^{-x}$, så
$$ f(x)=\frac{1}{1+e^{-x}}< \frac{1+e^{-x}}{1+e^{-x}} =1$$
Vi har derfor, at værdimængden for $f$ er indeholdt i $]0,1[$, hvilket kan skrives som
$$\text{Vm}(f) \subseteq ]0,1[$$
For at vise, at hele intervallet $]0,1[$ er med i værdimængden, viser vi, at funktionsværdierne kan komme så tæt på 0 og 1, som vi ønsker. 
Vi ser derfor på, hvad der sker med $f(x) $ når $x$ nærmer sig $-\infty$ og $\infty$. 

Figur ? viser grafen for $e^{-x}$:

\am{indsæt graf}

Da $e^{-x}$ er en aftagende eksponentialfunktion, har vi at 
$$e^{-x} \to \infty \text{ når } x\to -\infty$$
og derfor
$$f(x)=\frac{1}{1+e^{-x}}\to 0 \text{ når } x\to -\infty$$
Desuden vil
$$e^{-x} \to 0 \text{ når } x\to \infty$$
og derfor
$$f(x)=\frac{1}{1+e^{-x}}\to 1 \text{ når } x\to \infty$$
Alt i alt har vi altså vist, at $f$ har værdimængde $]0,1[$, nærmer sig 0, når $x\to -\infty$, og nærmer sig 1, når $x\to \infty$. 

Hvis man differentierer $f$, får man
$$f'(x)=\frac{e^{-x}}{(1+e^{-x})^2}$$
Vi ser, at både tæller og nævner er positive, så $f'(x)>0$ for alle $x$. Da den afledte er positiv, er $f$ en strengt voksende funktion. 

\am{slut box1}

\am{begynd box2}

Vi viser nu, at $1-f(x) = f(-x)$.
\begin{equation}\label{eq:1-f}
1-f(x) = 1-\frac{1}{1+e^{-x}} = \frac{1+e^{-x}}{1+e^{-x}}-\frac{1}{1+e^{-x}}=\frac{e^{-x}}{1+e^{-x}}=\frac{1}{1+e^{x}} =f(-x)
\end{equation}
hvor vi forlængede brøken med $e^x$ i det sidste lighedstegn.
\am{det kunne man evt også vise på en figur?}

\am{slut box2}


\subsubsection*{Opgaver}
\begin{itemize}
\item Lad $f$ være sigmoidfunktionen. Vis at
$$f'(x)=\frac{e^{-x}}{(1+e^{-x})^2}$$
\item Antag at vi har fundet vektorrepræsentationer af ord, og at inputvektoren for "hund"  og kontekstvektorerne for "pels" og "fjer" er givet ved
\begin{align*}
\overrightarrow{v}_{hund}=\begin{pmatrix} 0.5\\2\\1\\-1\end{pmatrix} ,\quad
\overrightarrow{k}_{pels}=\begin{pmatrix} 0\\3\\2\\-2\end{pmatrix},\quad
\overrightarrow{k}_{fjer}=\begin{pmatrix} 1\\-2\\1.5\\0.5\end{pmatrix}
\end{align*}
Hvad er sandsynligheden for at henholdsvis "pels" og "fjer" er kontekst til "hund"? Hvad er sandsynligheden for at "fjer" ikke er kontekst til "hund"?
\end{itemize}

\subsection*{Estimation af vektorer}
Hvordan bestemmer vi så vektorrepræsentationerne $\overrightarrow{v}_{w}$ og $\overrightarrow{k}_{c}$ i praksis? Her får vi brug for det datasæt, som vi lavede ud fra vores tekstkorpus. Hver række i  datasættet bestod af et inputord $w$, et positivt kontekstord $c^{+}$, som forekom som kontekst til $w$ i datasættet, og tre negative eksempler på  kontekstord, som vi vil betegne  $c_{1}^-,c_{2}^-,c_{3}^-$.  Vores datasæt har altså formen
\begin{table}[h]
\begin{center}
\begin{tabular}{ccccc}
{Input} & {Positiv} & Negativ & Negativ & Negativ \\ \hline
$w$ & $c^{+}$ & $c^-_{1}$ & $c_{2}^-$ & $c_{3}^-$  \\
\hline
%\vdots & \vdots & \vdots & \vdots &\vdots\\
sorte & hund & og & palme & synger \\
sorte & logrer & bage  & fra & tredive\\
%hund & Den & mellem & jordbær & emne \\
\vdots & \vdots & \vdots & \vdots &\vdots\\
\end{tabular}
\end{center}
\caption{Eksempel på datatabel}\label{tab:data}
\end{table}

Lad os sige, at der er $M$ rækker i vores træningsdata. Vi betegner data i den $j$te række med   $(w_j,c_j^+,c_{j1}^-,c_{j2}^-,c_{j3}^-)$ for $j=1,\ldots,M$.

 Vi ser nu på $j$te række. Hvis vores model \eqref{eq:pcw} for sandsynligheden for kontekstord er god, skulle det positive eksempel, vi har observeret, gerne have høj sandsynlighed. Vi vil altså gerne vælge vores vektorer således, at $P(c_j^+\mid w_j)$ er høj. Disse sandsynligheder vil være meget små i praksis, da der kan være mange meningsfulde kontekstord til $w_j$. For at undgå numeriske problemer, tager vi derfor den naturlige logaritme og får $\ln (P(c_j^+\mid w_j))$. \am{Fodnote: Husk på, at hvis vi har et tal meget tæt på nul, fx $10^{-a}$, og vi tager den naturlige logaritme, så får vi $\ln(10^{-a}) = -a\cdot \ln(10)$, som er et tal på en mere normal skala, som en computer bedre kan finde ud af at regne på.} Da den naturlige logaritme er en strengt voksende funktion, svarer en høj sandsynlighed $P(c_j^+\mid w_j)$ til, at $\ln(P(c_j^+\mid w_j))$ er høj. Vi vil derfor gerne have, at $-\ln(P(c_j^+\mid w_j))$ er lav.\am{Malene indsætter graf. Fodnote: Vores sandsynligheder vil være tal mellem 0 og 1, så når vi tager den naturlige logaritme,  får vi et negativt tal, se Figur ?. Minus den naturlige logaritme bliver derfor positiv.}

Samtidig vil vi gerne have, at sandsynligheden for, at vores negative eksempler ikke er kontekstord, er høj. Altså skal $1-P(c_{ji}^-\mid w_j)$ være høj for $i=1,2,3$. Igen svarer det til, at $-\ln(1-P(c_{ji}^-\mid w_j))$ skal være lav. 

For at sikre, at både $-\ln(P(c_j^+\mid w_j))$ og $-\ln(1-P(c_{ji}^-\mid w_j))$ for $i=1,2,3$ er lave, vil vi kræve at deres sum er lav. Vi kræver altså at
$$L_j = -\ln(P(c_j^+\mid w_j)) -\ln(1-P(c_{j1}^-\mid w_j)) -\ln(1-P(c_{j2}^-\mid w_j)) -\ln(1-P(c_{j3}^-\mid w_j))$$
er lav. Formlerne \eqref{eq:pcw} og \eqref{eq:pneg}  giver
\begin{align*}
L_j =& -\ln(P(c_j^+\mid w_j)) -\ln(1-P(c_{j1}^-\mid w_j))\\
&-\ln(1-P(c_{j2}^-\mid w_j)) -\ln(1-P(c_{j3}^-\mid w_j))
\\=& -\ln \bigg(\frac{1}{1+e^{-\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c^+_j}}}\bigg) - \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j1}^-}}}\bigg)\\
&- \ln \bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j2}^-}}}\bigg) - \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j3}^-}}}\bigg)
\end{align*}
Her kan vi bruge logaritmeregnereglen $\ln(\tfrac{1}{a}) = -\ln(a)$ til at skrive $L_j$ lidt pænere.
\begin{align*}
L_j =&\ln \Big({1+e^{-\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c^+_j}}}\Big) + \ln\Big({1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j1}^-}}}\Big)\\
&+ \ln \Big({1+e^{\overrightarrow{v}_{w_j} \cdot \overrightarrow{k}_{c_{j2}^-}}}\Big) + \ln\Big({1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_{j3}^-}}}\Big)
\end{align*}
Vores $L_j$ afhænger altså af, hvordan vores input- og kontekstvektorer er valgt. Vi ønsker at vælge vektorerne således, at $L_j$ er lav.

Dette kan vi gentage for hver række i vores træningsdata. Vi får et $L_j$ for hver række $j=1,\ldots,M$. Da vi gerne vil have, at alle $L_j$ er lave, kræver vi, at deres sum 
$$L=L_1+ L_2 +\dotsm + L_M$$
er lav. Vi kalder $L$ for vores \emph{tabsfunktion}. Da hvert $L_j$ afhænger af input- og kontekstvektorerne, kommer $L$ også til at afhænge af disse vektorer. Vi ønsker derfor at vælge input- og kontekstvektorerne således, at de minimerer $L$. Hvordan finder man minimum for $L$ i praksis? Det kan man fx gøre ved hjælp af \emph{gradientnedstigning}, som du kan læse mere om \am{her}.





%Vi vil nu gerne finde sandsynligheden for, at $c^{+}$ er et positivt kontekstord til $w$, og $c_{1}^-$, $c_{2}^-$ og $c_{3}^-$ ikke er det. Den kalder vi $P(c^{+},c_{1}^-,c_{2}^-,c_{3}^-\mid w)$. Husk på, at sandsynligheden for at 
% $ c^{+}$ er et positivt kontekstord er $P(c^{+}\mid w)$. Sandsynligheden for, at $c_{i}^- $ ikke er et positivt kontekstord, er $1-P(c_{i}^-\mid w)$ for $i=1,2,3$. Da de positive og negative eksempler er valgt uafhængigt af hinanden, kan man finde den samlede sandsynlighed ved at gange de enkelte sandsynligheder sammen. Vi får derfor, at sandsynligheden for, at $c^{+}$ er et kontekstord, og at $c_{1}^-,c_{2}^-,c_{3}^-$ ikke er det, er givet ved\am{der er et eller andet ved dette argument, der skurrer lidt}
%\begin{align*}\nonumber
%&P(c^{+},c_{1}^-,c_{2}^-,c_{3}^-\mid w)\\ \nonumber
%&= P(c^{+}\mid w) \cdot (1-P(c_{1}^-\mid w)) \cdot (1- P(c_{2}^-\mid w))\cdot (1-P(c_{3}^-\mid w))
%\end{align*}
%Da $P(c^{+}\mid w)$ er givet ved \eqref{eq:pcw} og $1-P(c_{i}^-\mid w)$ for $i=1,2,3$ er givet ved \eqref{eq:pneg}, får vi
%\begin{align}\nonumber
%&P(c^{+},c_{1}^-,c_{2}^-,c_{3}^-\mid w)\\   \label{eq:P_samlet}
%&= \frac{1}{1+e^{-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c^+}}} \cdot \frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_1^-}}} \cdot \frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_2^-}}} \cdot \frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_3^-}}}
%\end{align}

%Da summer er nemmere at regne på end produkter, vælger vi at tage den naturlige  logaritme på begge sider af \eqref{eq:P_samlet}. Ved at bruge logaritmeregnereglen $\ln(ab) = \ln(a) + \ln(b)$ får vi, at
%\begin{align*}
%\ln (P(c^{+},c^{-}_1,c_{2}^-,c_{3}^-\mid w)) 
%&= \ln \bigg(\frac{1}{1+e^{-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c^+}}}\bigg) + \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_1^-}}}\bigg)\\&  + \ln \bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_2^-}}}\bigg) + \ln\bigg(\frac{1}{1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_3^-}}}\bigg)
%\end{align*}
%Her kan vi desuden benytte regnereglen $\ln(\tfrac{1}{a}) = - \ln(a)$ og få
%\begin{align}\nonumber
%\ln (P(c^{+},c_{1}^-,c_{2}^-,c_{3}^-\mid w))&= -\ln({1+e^{-\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c^+}}}) - \ln({1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_1^-}}}) \\ \label{eq:ln_samlet}
%& -\ln({1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_2^-}}} ) - \ln ({1+e^{\overrightarrow{v}_{w} \cdot \overrightarrow{k}_{c_3^-}}})
%\end{align}

%Hvis vores vektorer er valgt fornuftigt, skulle sandsynligheden \eqref{eq:P_samlet} for vores positive og negative eksempler gerne være høj. Da logaritmefunktionen er en strengt voksende funktion, svarer det til at kræve, at logaritmen til den samlede sandsynlighed \eqref{eq:ln_samlet} skal være høj. Lader vi
%$$L_{w,c^+,c_{1}^-,c_{2}^-,c_{3}^-} = -\ln (P(c^{+},c_{1}^-,c_{2}^-,c_{3}^-\mid w))$$
%vil vi altså gerne have at $L_{w,c^+,c_{1}^-,c_{2}^-,c_{3}^-}$ er lav. 

%Det samme kan vi gøre for hver eneste række i vores træningsdata svarende til en række i datatabellen i Tabel \ref{tab:data}. Hvis der er $M$ rækker i vores træningsdata, og vi kalder data i den $j$te række for   $(w_1,c_1^+,c_{j1}^-,c_{j2}^-,c_{j3}^-)$, kan vi udregne $L_{w_j,c_j^+,c_{j1}^-,c_{j2}^-,c_{j3}^-}$ for hver eneste række $j=1,\ldots,M$. Da vi gerne vil have alle $L_{w_j,c_j^+,c_{j1}^-,c_{j2}^-,c_{j3}^-}$ til at være lave på én gang, definerer vi \emph{tabsfunktionen}  
%$$L=L_{w_1,c_1^+,c_{11}^-,c_{12}^-,c_{13}^-} + L_{w_2,c_2^+,c_{21}^-,c_{22}^-,c_{23}^-} + \dotsm + L_{w_M,c_M^+,c_{M1}^-,c_{M2}^-,c_{M3}^-}$$
%Værdien af $L$ afhænger af, hvordan vi har valgt vores input- og kontekstvektorer.
%Vi ønsker derfor at vælge vektorerne, således at de minimerer $L$. Hvordan finder man minimum for $L$ i praksis? Det kan man fx gøre ved hjælp af \emph{gradientnedstigning}, som du kan læse mere om \am{her}.

Der er rigtig mange ord i det danske sprog. For hvert af dem skal vi finde både en input- og en kontekstvektor, der hver har $m$ koordinater.  Alt i alt giver det rigtig mange koordinater, der skal bestemmes. Antallet af ord på dansk afhænger lidt af, hvad man forstår ved et ord, men 200.000 er et fornuftigt bud. Hvis vi vil repræsentere dem ved vektorer af dimension $m=100$, får man brug for at bestemme 40.000.000 koordinater. For at kunne gøre det, er man også nødt til at have enormt store mængder træningsdata til rådighed, dvs.\ antallet $M$ af positive eksempler i træningsdataet skal være gigantisk stort. Bemærk dog, at 40.000.000 koordinater stadig er væsentlig færre, end hvis vi skulle bestemme en sandsynlighed for hvert eneste af de $200.000^2$ mulige kombinationer af input- og kontekstord. Dette er en anden fordel ved Word2Vec.

Nu har vi set, hvordan man kan repræsentere et ord $w$ ved en inputvektor $\overrightarrow{v}_{w}$ og en kontekstvektor $\overrightarrow{k}_{w}$. Vektoren $\overrightarrow{v}_{w} $ er den, der viser, hvordan $w$ forholder sig til sin kontekst, så det er den, der repræsenterer betydningen af $w$. Normalt vil man derfor arbejde videre med $\overrightarrow{v}_{w}$, mens $\overrightarrow{k}_{w}$ smides væk. Vi mangler dog stadig at se, hvordan vektorerne $\overrightarrow{v}_{w}$, som vi har fundet med Word2Vec, kan bruges i forbindelse med kunstig intelligens. 

\subsubsection*{Eksempel 3}
\am{Eges børnebog}

\subsection*{Fra vektorer til tekstgenerering}
I det følgende ser vi på, hvordan vektorrepræsentationerne, som vi fandt med Word2Vec, kan bruges til at lave en algoritme, der kan generere ny tekst. De fleste tekstgenereringsalgoritmer fungerer ved, at de danner teksten et ord ad gangen. Givet den tekst der allerede er dannet, prøver algoritmen hele tiden at gætte, hvad det næste ord skal være. Det kan gøres i to trin:
\begin{itemize}
\item[1.] Først benyttes Word2Vec til at oversætte alle ordene i sproget til vektorer.
\item[2.] Dernæst genereres teksten. Givet den tekst, der allerede er dannet, bruger vi en (kompliceret) funktion, der tager vektorrepræsentationerne af de hidtil genererede ord som input. Som output giver funktionen det mest sandsynlige næste ord (eller et af de mest sandsynlige).
\end{itemize}
Hvis vi fx har genereret teksten
$$\text{"Hunden spiser sit ---"} $$
så skal vi prøve at gætte, hvilket ord der kommer efter "sit". Vi oversætter derfor ordene "Hunden", "spiser" og "sit" til vektorerne $\overrightarrow{v}_{Hunden}$, $\overrightarrow{v}_{spiser}$ og $\overrightarrow{v}_{sit}$. Disse tre vektorer giver vi funktionen som input, og som output får vi et nyt ord, måske "kødben".
Den funktion, der bruges i punkt 2., ville typisk være  et \emph{neuralt netværk}.
Du kan læse mere om, hvordan det fungerer i \am{"Tekstgenerering med neurale netværk"}. 

Det smarte ved at bruge vektorrepræsentationerne er, hvis vi for eksempel vil generere næste ord i 
$$\text{"Jeg skal huske, at katten skal have ---"}$$
Det skulle gerne give "mad" som muligt næste ord. Men måske har sprogmodellen aldrig set sætningen "katten skal have mad". Hvis den til gengæld har set  "hunden skal have mad", og modellen ved at "hund" og "kat" tit har samme kontekst, og dermed har næsten samme vektorrepræsentation, så vil  man alligevel få "mad" som muligt næste ord. 


\am{Kan man også få det til at virke for Eges børnebog?}


\end{document}