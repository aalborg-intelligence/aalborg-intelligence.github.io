---
title: "Retningsafledede og gradientnedstigning"
image: "images/minMaxFunc.png"
date: ''
format:
    html:
      self-contained: true 
      toc: true
      toc-title: Indhold
      toc-location: left
      related-formats-title: "Andre formater"
      link-external-newwindow: true
#    pdf: default
reference-location: margin
editor_options: 
  chunk_output_type: console
crossref:
  fig-prefix: figur   # (default is "Figure")
  tbl-prefix: tabel    # (default is "Table")
  exm-prefix: eksempel
  thm-prefix: sætning
  sec-prefix: afsnit
  eq-prefix: ''
  fig-title: Figur
  exm-title: Eksempel
  thm-title: Sætning
  tbl-title: Tabel
label:
    fig: Figur
fig-cap-location: margin
tab-cap-location: margin
execute:
  echo: false
  warning: false
---

Denne note giver en introduktion til retningsafledede og gradientnedstigning.

# Introduktion

I det almindelige gymnasiepensum indgår nogle vigtige begreber indenfor infinitesimalregningen for funktioner af én variable. For funktioner af én variabel siges en funktion $f$ at være kontinuert i et punkt $x_{0}$, hvis funktionsværdien $f(x)$ nærmer sig funktionsværdien $f(x_{0})$ når $x$ nærmer sig $x_{0}$. Det vil sige, at

$$
\lim_{x \rightarrow x_{0}}{f\left( x \right) = f(x_{0})}
$$

Funktionen $f$ siges at være differentiabel i $x_{0}$, hvis hældningen af sekanterne gennem de to punkter $(x_{0},f\left( x_{0} \right))$ og $(x,f\left( x \right))$ på grafen for $f$ nærmer sig et fast tal $f'(x_{0})$, når $x$ nærmer sig $x_{0}$:

$$
\lim_{x \rightarrow x_{0}}{\frac{f\left( x \right) - f(x_{0})}{x - x_{0}} = f'(x_{0})}
$$ 


Intuitivt kan man tænke på egenskaben kontinuitet i et punkt, som at grafen for funktionen ikke har et spring i punktet, og på egenskaben differentiabilitet i et punkt, som at grafen for funktionen hverken har spring eller knæk i punktet. 

<!-- Se @fig-fkt_kont_diff, @fig-fkt_kont, og @fig-fkt_ingen_af_delene.  -->

<!-- ![Grafen for en funktion som både er kontinuert og differentiabel i punktet $P$.](images/fkt_kont_diff.png){#fig-fkt_kont_diff} -->

<!-- ![Grafen for en funktion som er kontinuert, men *ikke* differentiabel i punktet $P$ (grafen har et "knæk" i $P$, og funktionen kan derfor ikke være differentiabel i $P$).](images/fkt_kont.png){#fig-fkt_kont} -->

<!-- ![Grafen for en funktion som hverken er kontinuert eller differentiabel i punktet $P$ (grafen har et "spring" i $P$, og funktionen kan derfor hverken være kontinuert eller differentiabel i $P$).](images/fkt_ingen_af_delene.png){#fig-fkt_ingen_af_delene} -->


Hældningen $f'(x_{0})$ i punktet $(x_{0},f\left( x_{0} \right))$ på grafen er så en grænseværdi af nogle sekanthældninger, som hver for sig er gennemsnitshældninger for et mindre og mindre stykke af grafen. Har man studeret grænseværdibegrebet lidt nærmere, ved man, at den intuitive fortolkning ikke er særligt præcis og heller ikke helt rigtig, men alligevel er denne fortolkning god til at give en forståelse af gymnasiematematikken.

For [funktioner af to variable](../funktioner_af_flere_variable/funktioner_af_flere_variable.qmd) siges en funktion $f$ at være kontinuert i et punkt ${(x}_{0},y_{0})$, hvis funktionsværdien $f(x,y)$ nærmer sig funktionsværdien $f(x_{0},y_{0})$, når $(x,y)$ nærmer sig ${(x}_{0},y_{0})$:

$$
\lim_{(x,y) \rightarrow (x_{0},y_{0})}{f\left( x,y \right) = f(x_{0},y_{0})}
$$

Definitionen er direkte overført fra den tilsvarende definition for funktioner af én variabel. Det virker jo umiddelbart meget fornuftigt, men der er faktisk lidt at tænke over. Det kan du læse mere om [her](kontinuitet.qmd). 

Desværre kan man *ikke* tilsvarende genbruge differentiabilitetsbegrebet fra funktioner af én variabel på denne måde

$$
\lim_{(x,y) \rightarrow (x_{0},y_{0})}{\frac{f\left( x,y \right) - f\left( x_{0},y_{0} \right)}{\left( x,y \right) - {(x}_{0},y_{0})} = f'(x_{0},y_{0})}
$$

Her giver brøken på venstre side repræsenterende sekanthældningen *ikke mening*, da man ikke kan dividere med et punkt eller en vektor. I stedet tager man udgangspunkt i en alternativ definition af differentiabilitet for funktioner af én variabel, hvor man har kaldt skridtet fra $x_{0}$ til $x$ for $h$:

$$
\lim_{h \rightarrow 0}{\frac{f\left( x_{0} + h \right) - f(x_{0})}{h} = f'(x_{0})}
$$

Man bruger det til at definere de to første ordens partielle afledede for en funktion $f$ af to variable $$
\begin{aligned}
&\lim_{h \rightarrow 0} \frac{f\left( x_{0} + h,y_{0} \right) - f(x_{0},y_{0})}{h} = f_x(x_{0},y_{0})\\
&\lim_{h \rightarrow 0} \frac{f\left( x_{0},y_{0} + h \right) - f(x_{0},y_{0})}{h} = f_y(x_{0},y_{0})
\end{aligned}
$$ hvis grænserne eksisterer. Her tager man et skridt $h$ i enten $x$-aksens eller $y$-aksens retning ud fra punktet $(x_{0},y_{0})$ og beregner en hældning af grafen i den retning ved hjælp af en grænseværdi af sekanthældninger for snitfunktionerne $f(x,y_{0})$ og $f(x_{0},y)$, som hver for sig er almindelige funktioner af én variabel, da man kun ændrer enten $x$-koordinaten eller $y$-koordinaten.

Gradientvektoren defineres som vektoren med de partielle afledede som koordinater:

$$
\nabla f\left( x_{0},y_{0} \right) = \begin{pmatrix}
f_x\left( x_{0},y_{0} \right) \\
f_y\left( x_{0},y_{0} \right) \\
\end{pmatrix}
$$

I noten om [funktioner af flere variable](../funktioner_af_flere_variable/funktioner_af_flere_variable.qmd) har vi skrevet, at denne vektor angiver den retning, man skal bevæge sig væk fra punktet $(x_{0},y_{0})$, for at funktionsværdierne $f(x,y)$ vokser mest muligt. Vi vil i det følgende se nærmere på denne egenskab og bruge den til at løse optimeringsproblemer numerisk.

# Retningsafledede

Vi vil nu se på væksten i andre retninger end blot i aksernes retning. Vi angiver retningen med en enhedsvektor - det vil sige en vektor med længde 1:

$$
\vec{u} = \begin{pmatrix}
u_{1} \\
u_{2} \\
\end{pmatrix}
$$ hvor altså $\lvert \vec u \rvert = 1$.

Vi definerer nu den retningsafledede af $f$ i punktet $(x_{0},y_{0})$ i retningen $\vec{u}$ ved

$$
D_{\vec{u}}f\left( x_{0},y_{0} \right) = \lim_{h \rightarrow 0}\frac{f\left( x_{0} + hu_{1},y_{0} + hu_{2} \right) - f(x_{0},y_{0})}{h}
$$ {#eq-retningsafledede}

hvis grænsen eksisterer.

Bemærk, at hvis $\vec{u}$ peger i $x$-aksens retning, så bliver den retningsafledede til $f_x(x_{0},y_{0})$, og hvis den peger i $y$-aksens retning, bliver den til $f_y(x_{0},y_{0})$. Man udregner en sekanthældning ved at tage et skridt $h$ i $\vec{u}$'s retning og dividere den fundne funktionstilvækst med $h$. Derefter lader man $h$ gå mod 0. Det giver hældningen af grafen for $f$ i punktet $(x_{0},y_{0})$ i retningen $\vec{u}$.

Idéen med den retningsafledede er illustreret i figuren nedenfor. Til venstre ses en repræsentant for $\vec u$ i $xy$-planen. Man kan ændre på den retning, som $\vec u$ peger i, ved at trække i skyderen. Til højre ses grafen for en funktion $f$ af to variable, hvor et punkt $P(x_0,y_0,f(x_0,y_0))$ på grafen er indtegnet. Samtidig vises den snitkurve som fås, hvis man på grafen i punktet $P$ bevæger sig langs en linje i retningen $\vec u$. Denne snitkurve har i punktet $P$ en tangent, som også er indtegnet, og denne tangents hældning vil netop svarer til størrelsen af den retningsafledede $D_{\vec{u}}f\left( x_{0},y_{0} \right)$. Hvis man ændrer på den retning, som $\vec u$ peger i, kan man se, hvordan størrelsen af den retningsafledede ændrer sig.

{{< include _geogebra/_geogebra.qmd >}}

::: {#ggbApplet_retning1}
:::

Det viser sig, at man kan udregne de retningsafledede med et prikprodukt:

$$
D_{\vec{u}}f\left( x_{0},y_{0} \right) = \nabla f(x_{0},y_{0}) \cdot \vec{u}
$$

Vi vil nedenfor argumentere for formlen, men lad os først se på konsekvenserne af den. Vi ved fra almindelig vektorregning, at

$$
\vec{a} \cdot \vec{b} = \lvert \vec{a} \rvert \cdot \lvert \vec{b} \rvert \cdot \cos(v)
$$

hvor $v$ er vinklen mellem de to vektorer. Da $\lvert \vec{u} \rvert = 1$ betyder det, at

$$
D_{\vec{u}}f\left( x_{0},y_{0} \right) = \lvert \nabla f(x_{0},y_{0}) \rvert \cdot \cos(v)
$$

hvor $v$ er vinklen mellem gradientvektoren $\nabla f\left( x_{0},y_{0} \right)$ og den valgte retning $\vec{u}$.

Vi ved, at $-1 \leq \cos(v) \leq 1$ samt at $\cos(0^{{^\circ}})=1$ og $\cos(180^{{^\circ}})=-1$. Det følger derfor, at den retningsafledede er størst (og dermed at $f$ vokser mest), når $\vec{u}$ peger i $\nabla f(x_{0},y_{0})$'s retning. Og tilsvarende at den retningsafledede er mindst (og dermed at $f$ aftager mest), når $\vec{u}$ peger i $-\nabla f(x_{0},y_{0})$'s retning. Det vil sige, at den retningsaflededes størsteværdi er

$$
D_{\vec{u}}f\left( x_{0},y_{0} \right) = \ \ \ \lvert \nabla f(x_{0},y_{0}) \rvert
$$

når $v = 0^{{^\circ}}$ og retningsaflededes mindsteværdi er

$$
D_{\vec{u}}f\left( x_{0},y_{0} \right) = - \lvert \nabla f(x_{0},y_{0}) \rvert
$$

når $v = 180^{{^\circ}}$. Det var netop, hvad vi gerne ville vise.


Princippet er illustreret i figuren herunder. Gradientvektoren $\nabla f(x_{0},y_{0})$ er indtegnet (med blå) og man kan se, at den retningsafledede antager den største værdi, netop når $\vec u$ peger i gradientens retning (prøv at trække i skyderen). Og omvendt antager den retningsafledede den mindste værdi, når $\vec u$ peger i minus gradientens retning. 

::: {#ggbApplet_retning2}
:::


For at vise, at man kan udregne de retningsafledede med et prikprodukt:

$$
D_{\vec{u}}f\left( x_{0},y_{0} \right) = \nabla f(x_{0},y_{0}) \cdot \vec{u}
$$

kan du vælge enten at læse et bevis, som baserer sig på [geometriske argumenter](bevis_geometrisk_argument.qmd) eller [et bevis som er baseret på middelværdisætningen](bevis_vha_middelvaerdisaetningen.qmd).

# Optimering

Betragt en funktion $f$ givet ved forskriften $$
f\left( x,y \right) = \left( \left( x - 5 \right)^{2} + 3 \right) \cdot \left( 5 + \left( y - 10 \right)^{2} \right) + 30
$$

Hvis man ser lidt på forskriften, kan man måske overbevise sig selv om, at funktionen har et minimum på 45, som fås, når $\left( x,y \right) = (5,10)$.

Grafen ses herunder.

::: {#ggbApplet}
:::

Man kan lave en iterativ metode til at finde minimumspunktet ved at udnytte egenskaben ved gradientvektoren:

-   Vælg et startpunkt $(x_0,y_0)$ som et første gæt på et minimumspunkt.

Vi udnytter nu, at $- \nabla f(x_0,y_0)$ angiver den retning, hvor funktionsværdien falder mest i punktet $(x_0,y_0,f(x_0,y_0))$.

-   Gå derfor et lille skridt i retningen $- \nabla f(x_0,y_0)$. Det giver så det næste punkt $(x_1,y_1)$, som forhåbentlig er et bedre bud på et minimumspunkt.

-   Processen foregår i definitionsmængden, men på grafen svarer det til at gå et lille stykke den stejleste vej ned ad bakken.

-   Processen itereres så gentagne gange indtil man forhåbentlig når minimumspunktet.

Vælger vi med den konkrete funktion et startpunkt på

$$
(x_0,y_0) = ( - 3,4)
$$

og vælger vi i hvert skridt at lægge -0,001 gange den negative gradientvektor i punktet til, så kan nogle af de følgende $(x,y)$-punkter ses til venstre i @fig-gradientnedstigning. Læg her mærke til hvordan vi nærmer os det globale minimumssted i $(5,10)$. Til højre i @fig-gradientnedstigning ses det også hvordan vi ved hjælp af gradientnedstigning, nærmer os den globale minimumsværdi på $f(5,10)=45$.

::: {#fig-gradientnedstigning}
![](images/Numerisk_xy_vaerdier.png) ![](images/Numerisk_funktionsvaerdier.png)

Til venstre ses et udvalg af nogle af de $(x,y)$-punkter, som genereres i forbindelse med gradientnedstigning. Til højre ses et udvalg af nogle af de funktionsværdier, som genereres i forbindelse med gradientnedstigning.
:::

Vi ser, at den iterative gradientnedstigning faktisk nærmer sig det globale minimumspunkt. Så om ikke andet så virker metoden i hvert fald i dette konkrete tilfælde.

# Træning af neurale netværk

At lede efter et globalt minimumspunkt eller i det mindste et brugbart lokalt minimumspunkt for en funktion af rigtig mange variable er et problem, man står overfor, når man skal træne et neuralt netværk og have fastlagt en masse vægte i netværket.

Det kan ikke gøres analytisk, så derfor bruger man netop en iterativ proces baseret på gradientnedstigning som metode til at finde frem til minimumspunktet. Eksemplet ovenfor illustrerer derfor idéen bag en central del af træningen af et neuralt netværk.

Læs mere om hvordan gradientnedstigning konkret bruges her: [Perceptroner](../perceptron/perceptron.qmd) og [Kunstige neurale netværk](../neurale_net/neurale_net.qmd).

