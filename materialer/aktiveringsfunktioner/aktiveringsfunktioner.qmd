---
title: "Aktiveringsfunktioner"
image: ""
description: I opbygningen af kunstige neurale netværk er aktiveringsfunktioner helt centrale. Hvis ikke man bruger aktiveringsfunktioner i et kunstigt neuralt netværk, vil man faktisk bare bygge en stor lineær funktion af input-værdierne. Der findes mange forskellige aktiveringsfunktioner og i denne note vil vi beskrive nogle af de mest anvendte.
from: markdown+emoji
format:
    html:
      self-contained: true 
      toc: true
      toc-title: Indhold
      toc-location: left
      related-formats-title: "Andre formater"
      link-external-newwindow: true
#    pdf: default
reference-location: margin
editor_options: 
  chunk_output_type: console
crossref:
  fig-prefix: figur   # (default is "Figure")
  tbl-prefix: tabel    # (default is "Table")
  exm-prefix: eksempel
  thm-prefix: sætning
  sec-prefix: afsnit
  eq-prefix: ''
  fig-title: Figur
  exm-title: Eksempel
  thm-title: Sætning
  tbl-title: Tabel
label:
    fig: Figur
fig-cap-location: margin
tab-cap-location: margin
execute:
  echo: false
  warning: false
---

## Aktiveringsfunktioner, targetværdier og tabsfunktion


## Sigmoid



## Softsign

Sigmoid-funktionen er ét bud på en aktiveringsfunktion, men man kan sagtens bruge andre aktiveringsfunktioner. Det vigtige er, at værdimængden for aktiveringsfunktionen er på samme skala som targetværdierne. 

En ofte anvendt aktiveringsfunktion er *softsign*-funktion med forskrift

$$
f(x)=\frac{x}{1+|x|}
$$
Grafen for $f$ ses i @fig-softsign.

![Grafen for softsign-funktionen.](images/softsign.png){width=75% #fig-softsign}

Nu indgår den numeriske værdi af $x$ i forskriften, og man kunne få den tanke at $f$ måske hverken er kontinuert eller differentiabel i $0$. Men bruger vi definitionen på $|x|$, får vi

$$
f(x) = 
\begin{cases}
\frac{x}{1+x} & \textrm{hvis } x \geq 0 \\
\\
\frac{x}{1-x} & \textrm{hvis } x < 0 \\
\end{cases}
$$ {#eq-def_softsign}

For det første ser vi, at $f(0)=0/(1+0)=0$ og $f(x) \rightarrow 0$, når $x$ nærmer sig $0$ både fra højre og venstre. Det betyder, at $f$ *er* kontinuert i $0$.

Vi ser også, at for store positve værdier af $x$ vil
$$
f(x)= \frac{x}{1+x} \approx \frac{x}{x}=1
$$
og for store negative værdier af $x$ vil 
$$
f(x)= \frac{x}{1-x} \approx \frac{x}{-x}=-1
$$
Det betyder, at 
$$
f(x) \rightarrow 1 \quad \textrm{når} \quad x \rightarrow \infty
$$
og 

$$
f(x) \rightarrow -1 \quad \textrm{når} \quad x \rightarrow - \infty
$$
hvilket stemmer fint overens med @fig-softsign. 

Det vil sige, at hvis vi skal bruge softsign-funktionen som aktiveringsfunktion, så skal targetværdierne være $\pm 1$, og tabsfunktionen defineres da ved

$$
\begin{aligned}
E(w_0, w_1, &\dots, w_n) \\ &= \frac{1}{2} \sum_{m=1}^{M} \left (t_m-
f(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right)^2,
\end{aligned}
$$
hvor $f$ altså er softsign-funktionen.
Når de nye opdateringsregler for vægtene skal udledes, får vi brug for at differentiere aktiveringsfunktionen. Ved at bruge definitionen i (@eq-def_softsign) kan man vise, at $f$ også er differentiabel med afledt funktion 
$$
f'(x)=\frac{1}{\left ( 1+ |x| \right )^2}
$$

Beviset for dette samt udledningen af de nye opdateringsregler overlades trygt til læseren. :blush:

Man kan også vise, at
$$
f'(x)=1-f(|x|)=1-|f(x)|.
$$

Det vil sige, at den afledede softsign-funktion nemt kan beregnes ud fra funktionsværdien $f(x)$.

## Hyperbolsk tangens

## ReLU
