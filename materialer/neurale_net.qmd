---
##title: "Materialer om kunstige neurale netværk"
description: "Forskellige noter -- med forskelligt fokus og sværhedsgrad -- om kunstige neurale netværk."
image: "simple_neurale_net/images/long_simple_NN.png"
---

# Materialer om kunstige neurale netværk

På denne side finder du forskellige noter om kunstige neurale netværk. Noterne varierer i sværhedsgrad og i matematisk fokus. 

Noterne er skrevet til elever i gymnasiet og kan læses uafhængigt af hinanden. Til gengæld er den overordnede beskrivelse af noterne nedenfor nok nemmest at læse for lærerne. 

Sværhedsgraden af noterne er klassificeret fra \"forholdvis nem\" (\*) til \"svær\" (\*\*\*\*).



Helt overordnet kan man sige, at et neuralt netværk er en funktion, der, som input, tager et antal målte variable. Det kunne for eksempel være en vælgers svar på en kandidattest. Som output giver det en klasse/kategori, for eksempel et parti vælgeren bør stemme på. I et neuralt netværk indgår nogle ikke-lineære funktioner kaldet [aktiveringsfunktioner](undervisningsforloeb/aktiveringsfunktioner.qmd), som i sig selv er matematisk interessante. For at finde et neuralt netværk, der giver gode klassifikationer, minimeres en [tabsfunktion](materialer/tabsfunktioner/tabsfunktioner.qmd). Til det formål får man brug for at finde partielt afledte, som bruges i forbindelse med [gradientnedstigning](materialer/gradientnedstigning/gradientnedstigning.qmd). 

En kort videogennemgang, hvor vi fortæller lidt om, hvad AI er, kan findes [her](https://youtu.be/ivrBEopralQ).




<!-- Start på clickable section -->

<a href="perceptron/perceptron.html" class="quarto-grid-link">

::: {.boxed}

### Perceptroner

::: {.paddingleft}
![](perceptron/images/perceptron.png){style='float:right;'  width=40%}
:::

Noten beskriver først \"Perceptron Learning Algoritmen\", som kan bruges i forbindelse med binær klassifikation, men som kun konvergerer, hvis data er lineært separabel. Der gives ikke noget matematisk argument for dette, men det sandsynliggøres, hvorfor algoritmen virker. I behandlingen af  \"Perceptron Learning Algoritmen\" anvendes der ikke eksplicit en tabsfunktion.  

Dernæst beskrives \"ADALINE\", som også bruges til binær klassifikation, men som konvergerer -- også i det tilfælde, hvor data ikke er lineært separabel. Opdateringsreglerne til ADALINE algoritmen udledes ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. Den anvendte aktiveringsfunktion er her identiteten, hvilket gør denne udledning lettere end i de efterfølgende noter.


| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion |
|:---:|:---:|:---:|
| Binær | Ingen/identiteten | Ingen/*squared error* |
: {.bordered}

[Sværhedsgrad: \*]{style="color: #8086F2"}

:::

<!-- Slut på clickable section -->
</a>



<!-- Start på clickable section -->

<a href="kunstige_neuroner/kunstige_neuroner.html" class="quarto-grid-link">

::: {.boxed}

### Kunstige neuroner

::: {.paddingleft}
![](kunstige_neuroner/images/simplet_netvaerk2.png){style='float:right;'  width=40%}
:::

Noten behandler byggestenene til de mere generelle kunstige neurale netværk nemlig de *kunstige neuroner*. De kunstige neuroner bruges -- som de beskrives her -- til binær klassifikation. Opdateringsreglerne til justering af vægtene udledes ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. Til forskel fra ADALINE algoritmen anvendes her **sigmoid**-funktionen som aktiveringsfunktion, og der er også henvisning til en side, som beskriver hvilken fordel det har sammenlignet med ADALINE. 

| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion | 
|:---:|:---:|:---:|
| Binær | Sigmoid | *Squared error* | 
: {.bordered}

[Sværhedsgrad: \*\*]{style="color: #8086F2"}

:::

<!-- Slut på clickable section -->
</a>



<!-- Start på clickable section -->

<a href="simple_neurale_net/simple_neurale_net.html" class="quarto-grid-link">

::: {.boxed}

### Simple neurale netværk

::: {.paddingleft}
![](simple_neurale_net/images/long_simple_NN.png){style='float:right;'  width=40%}
:::

Denne noten behandler et egentlig kunstigt neuralt netværk dog med den væsentlige forsimpling, at der kun er to skjulte lag, hvor hver af disse lag kun består af én kunstige neuron. Det har den fordel, at det nu giver mening at tale om **feedforward** og **backpropagation**, men uden at sidstnævnte drukner i kædereglen af flere variable. Til gengæld bruges der tid på at forklare og anvende den kæderegel, som eleverne allerede kender. Også her udledes opdateringsreglerne ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. 


| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion |
|:---:|:---:|:---:|
| Binær | Sigmoid | *Squared error* | 
: {.bordered}

[Sværhedsgrad: \*\*\*]{style="color: #8086F2"}

:::

<!-- Slut på clickable section -->
</a>



<!-- Start på clickable section -->

<a href="softmax/softmax.html" class="quarto-grid-link">

::: {.boxed}

### Simple kunstige neurale netværk til multipel klassifikation

::: {.paddingleft}
![](softmax/images/netvaerk_generelt.png){style='float:right;'  width=25%}
:::

I denne noten behandles det tilfælde, hvor man ikke er interesseret i binær klassifikation, men derimod er ønsket at kunne prædiktere blandt mere end to klasser. Noten er holdt simpel på den måde, at det beskrevne netværk ikke indeholder nogle skjulte lag, men outputlaget består i stedet af flere neuroner. Det får den konsekvens, at den anvendte aktiveringsfunktion nu er den såkaldte **softmax** funktion, som kan se som en udvidelse af sigmoid-funktionen. Derudover behandles også *cross-entropy* tabsfunktionen, som minimeres ved hjælp af gradientnedstigning. 

| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion |
|:---:|:---:|:---:|
| Multipel | Softmax | *Cross-entropy* | 
: {.bordered}

[Sværhedsgrad: \*\*\*\*]{style="color: #8086F2"}

:::

<!-- Slut på clickable section -->
</a>



<!-- Start på clickable section -->

<a href="neurale_net/neurale_net.html" class="quarto-grid-link">

::: {.boxed}

### Kunstige neurale netværk

::: {.paddingleft}
![](neurale_net/images/simple_NN_weights.png){style='float:right;'  width=40%}
:::

Noten gennemgår et mere generelt kunstigt neuralt netværk med to skjulte lag, som hver består af to neuroner. Netværket bruges her til binær klassifikation, som aktiveringsfunktion anvendes igen sigmoid-funktionen og vægtene opdateres ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. 
Da de skjulte lag består af flere neuroner end én, må **kædereglen for funktioner af flere variable** bringes i spil for at finde de partielle afledede, som anvendes i forbindelse med gradientnedstigning. 

Desuden gives der i denne note også en forklaring på fordelene ved *cross-entropy* tabsfunktionen sammenlignet med *squared error* tabsfunktionen, der linkes til en side hvor opdateringsreglerne i et helt generelt kunstigt neuralt netværk forklares (ved hjælp af en masse indekser!) og endelig gives der en overordnet beskrivelse af *convolutional neural networks*, som bruges i forbindelse med prædiktion af billeder.


| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion | 
|:---:|:---:|:---:|
| Primært binær | Sigmoid | *Squared error* | 
: {.bordered}

[Sværhedsgrad: \*\*\*\*]{style="color: #8086F2"}

:::

<!-- Slut på clickable section -->
</a>


