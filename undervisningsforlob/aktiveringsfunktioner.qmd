---
title: "Aktiveringsfunktioner"
image: "aktiveringsfunktioner/aktiveringsfkt.jpg"
description-meta: "I opbygningen af kunstige neurale netværk er aktiveringsfunktioner helt centrale. Og aktiveringsfunktioner skal differentieres -- det handler dette forløb om."
categories:
  - A-niveau
  - Kort
---

::: {.callout-caution collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug
Forløbet kræver kendskab til:

+ Differentialregning herunder differentiation af sammensatte funktioner og produktreglen.

**Tidsforbrug:** Ca. 1-2 x 90 minutter alt efter hvor mange aktiveringsfunktioner, I ønsker at arbejde med. I kan også arbejde i grupper og lade hver gruppe arbejde med hver sin aktiveringsfunktion.

:::


::: {.purpose}

### Formål

I opbygningen af kunstige neurale netværk er aktiveringsfunktioner helt centrale. Hvis ikke man bruger aktiveringsfunktioner i et kunstigt neuralt netværk, vil man faktisk bare bygge en stor lineær funktion af inputværdierne. Og lineære funktioner kan man ikke prædiktere ret meget med!

Når man skal træne en [perceptron](../materialer/perceptron/perceptron.qmd), et [simpelt neuralt netværk](../materialer/simple_neurale_net/simple_neurale_net.qmd) eller et helt [generelt neuralt netværk](../materialer/neurale_net/neurale_net.qmd), skal man kunne differentiere de aktiveringsfunktioner, som indgår. I dette forløb vil vi arbejde med nogle af de mange forskellige aktiveringsfunktioner, som anvendes i den virkelige verden. Vi skal se på grafer og værdimængde -- og så skal vi mest af alt differentiere dem!

:::

## Introduktion

{{< include aktiveringsfunktioner/introduktion.qmd >}}

## Sigmoid

{{< include aktiveringsfunktioner/sigmoid.qmd >}}


## Softsign

{{< include aktiveringsfunktioner/softsign.qmd >}}


## Hyperbolsk tangens

{{< include aktiveringsfunktioner/hyperbolsk_tangens.qmd >}}



## ReLU

{{< include aktiveringsfunktioner/ReLU.qmd >}}



For yderligere læsning henviser vi til referencerne i afsnittet [videre læsning](aktiveringsfunktioner.qmd#sec-videre).

## Overblik

I tabellen herunder finder du et overblik over de forskellige aktiveringsfunktioner, som vi har behandlet ovenfor.

| Navn | $f(x)$ | Graf | $Vm(f)$ | $f'(x)$ |
|:------:|:------:|:------:|:------:|:------:|:------:|
| Sigmoid | $\frac{1}{1+\mathrm{e}^{-x}}$ | ![](aktiveringsfunktioner/sigmoid.png) | $(0,1)$ | $f(x)\cdot(1-f(x))$ |
| Softsign | $\frac{x}{1+|x|}$ | ![](aktiveringsfunktioner/softsign.png) | $(-1,1)$ | $(1-|f(x)|)^2$ | 
| Hyperbolsk tangens | $\frac{\mathrm{e}^x-\mathrm{e}^{-x}}{\mathrm{e}^x+\mathrm{e}^{-x}}$ | ![](aktiveringsfunktioner/tanh.png) | $(-1,1)$ | $1-\left ( \tanh(x) \right )^2$| 
| ReLU | $\begin{cases}
x & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}$ | ![](aktiveringsfunktioner/ReLU.png) | $[0,\infty)$ | $\begin{cases}
1 & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}$| 

## Videre læsning {#sec-videre}

* [Activation Functions in Neural Networks: With 15 examples](https://encord.com/blog/activation-functions-neural-networks/)
* [RELU and SIGMOID Activation Functions in a Neural Network](https://www.shiksha.com/online-courses/articles/relu-and-sigmoid-activation-function/)
