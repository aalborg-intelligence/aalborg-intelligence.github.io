---
title: "Til læreren"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Dette forløb bygger på noten om [kunstige neuroner](../../materialer/kunstige_neuroner/kunstige_neuroner.qmd), som er en fundamentale byggesten i kunstige neurale netværk. En generel kort introduktion til kunstig intelligens (som også benyttes i [del 1](kunstige_neuroner_del1.qmd)) kan ses her:

{{< video https://youtu.be/ivrBEopralQ >}}


Forløbet består af seks dele, og hver enkel del består typisk af en video, som skal ses (eller alternativt kan det tilhørende afsnit i noten læses). Herefter følger en række opgaver, som støtter op om teorien.

Det meste af forløbet er tænkt, så eleverne selv kan arbejde med stoffet. 

## Oversigt over de forskellige dele i forløbet

Her følger en kort gennemgang af de enkelte dele:

I [del 1](kunstige_neuroner_del1.qmd) behandles **aktiveringsfunktioner**, der er en særlig type af funktioner, som anvendes i en kunstig neuron. I forbindelse med træning af kunstige neuroner får man brug for at differentiere den valgte aktiveringsfunktion, hvilket er omdrejningspunktet for denne første del. Det vigtigste er her at arbejde med **sigmoid-funktionen** som aktiveringsfunktion. Resten af materialet i denne del kan bruges til undervisningsdifferentiering.

[Del 2](kunstige_neuroner_del2.qmd) handler om at forstå **gradientnedstigning**, som en metode til at bestemme minimum for en funktion af flere variable. Her arbejdes der konkret med at bestemme minimum for en funktion af to variable i GeoGebra, og som en del af opgaven visualiseres trinene, der anvendes i gradientnedstigning. 

I [del 3](kunstige_neuroner_del3.qmd) introduceres begrebet **tabsfunktion**, og eleverne arbejder med, hvordan vægtene, der indgår i tabsfunktionen, bestemmes ved hjælp af gradientnedstigning. Helt konkret behandles *squared error* tabsfunktionen, fordi den er nemmest at forstå. Del 3 sluttes af med en aktivitet, som handler om en væsentlig ulempe ved *squared error* tabsfunktionen -- nemlig **slow learning**. Denne aktivitet kan bruges til de særligt dygtige og/eller interesserede elever.

I [del 4](kunstige_neuroner_del4.qmd) arbejdes der med at træne en **kunstig neuron** ved hjælp af en app. Den sidste aktivitet handler igen om *slow learning*, som kan springes over, hvis eleverne ikke tidligere har arbejdet med det. 

[Del 5](kunstige_neuroner_del5.qmd) handler om, hvordan man vælger hvilke inputvariable, der skal indgå i den endelige model. Eleverne komme her til at arbejde med **krydsvalidering**. Igen handler den sidste aktivitet om *slow learning*, som eventuelt kan springes over.

[Del 6](kunstige_neuroner_del6.qmd) handler om, hvordan man ved at vælge en anden tabsfunktion kan løse problemet med *slow learning*. Hvis eleverne ikke tidligere har arbejdet med *slow learning* springes denne del over.

I [del 7](kunstige_neuroner_del7.qmd) arbejdes der med at få en forståelse for værdien af tabsfunktionen, hvor eleverne opdager, at der er en bestemt værdi, som tabsfunktionen som minimum skal komme under, hvis modellen skal være bedre end et godt gæt. 


## Hvad kan man nøjes med?

På nogle hold vil det være urealistisk, at alle elever når at arbejde med alt stoffet. Her kan man for eksempel fint nøjes med at arbejde med del 1-4, og i disse dele er der også aktiviteter, der enten helt kan springes over eller bruges som undervisningsdifferentiering (som angivet ovenfor). Del 5-7 kan være for særligt dygtige elever eller for elever, som ønsker at arbejde med kunstig intelligens i sin SRP. Desuden er det muligt at arbejde med del 5-7 i vilkårlig rækkefølge, og man behøver ikke at arbejde med alle tre dele, men kan fint nøjes med bare at vælge én af delene.