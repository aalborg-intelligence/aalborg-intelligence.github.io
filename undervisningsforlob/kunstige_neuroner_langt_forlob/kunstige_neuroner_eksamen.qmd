---
title: "Eksamensspørgsmål: Kunstige neurale netværk"
categories:
  - A-niveau
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

::: {.estimeret_tid}

Forventet tid ca. 2 x 60 min.

:::
{{< include ../space.qmd >}}

## Aktivitet

Forbered en fremlæggelse til et eller flere af nedenstående eksamensspørgsmål.

::: {.callout-tip collapse="false" appearance="minimal"}

## Differentialregning og kunstige neurale netværk

- Forklar, hvad der forstås ved en kunstig neuron, herunder hvordan sigmoid-funktionen kan bruges som aktiveringsfunktion.

- Gør rede for, hvordan sigmoid-funktionen

   $$\sigma(x)=\frac{1}{1+\mathrm{e}^{-x}}$$
   differentieres og vis, at
   
   $$\sigma'(x) = \sigma(x) \cdot (1-\sigma(x))$$

:::


::: {.callout-tip collapse="false" appearance="minimal"}

## Differentialregning og kunstige neurale netværk

- Forklar, hvad der forstås ved en kunstig neuron, herunder hvordan *squared error* tabsfunktionen defineres.

- Gør rede for, hvordan man i tilfældet med et enkelt træningseksempel differentierer tabsfunktionen
   $$E=\frac{1}{2} \left (t-\sigma(w_0 + w_1 \cdot x) \right )^2$$
   med hensyn til $w_1$, samt hvordan dette bruges til opdatering af $w_1$-vægten.

:::

Hvis I har arbejdet med *cross-entropy* tabsfunktionen:

::: {.callout-tip collapse="false" appearance="minimal"}

## Differentialregning og kunstige neurale netværk

- Forklar, hvad der forstås ved en kunstig neuron, herunder hvordan *cross-entropy* tabsfunktionen defineres.

- Gør rede for, hvordan man i tilfældet med et enkelt træningseksempel differentierer tabsfunktionen
   
   $$
   E= - \left (t \cdot \ln(o) + (1-t) \cdot \ln(1-o)  \right),
   $$
   hvor outputværdien $o$ er

   $$
   o = \sigma(w_0+w_1 \cdot x)
   $$
   
   med hensyn til $w_1$, samt hvordan dette bruges til opdatering af $w_1$-vægten.

:::

{{< include ../space.qmd >}}

[Tilbage til forsiden $\rightarrow$](kunstige_neuroner_forside.qmd){.next}


