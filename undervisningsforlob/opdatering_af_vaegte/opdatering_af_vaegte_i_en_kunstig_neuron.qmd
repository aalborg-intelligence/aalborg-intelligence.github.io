---
title: "Opdatering af vægte i en kunstig neuron"
description: 'I dette forløb arbejdes der med at opdatere vægtene i en kunstig neuron med brug af sigmoid aktiveringsfunktionen og gradientnedstigning.'
image: "../../materialer/kunstige_neuroner/images/simplet_netvaerk1.png"
categories:
  - A-niveau
  - Kort
---

::: {.callout-caution collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug

+ Kunstige neuroner.
+ Sigmoid aktiveringsfunktion.
+ Gradientnedstigning.

Forudsætningerne kan for eksempel dækkes ved hjælp af [noten om kunstige neuroner](../../materialer/kunstige_neuroner/kunstige_neuroner.qmd){target="_blank"}.

**Tidsforbrug:** Ca. 90 minutter.

:::

::: {.purpose}

### Formål

Formålet med dette forløb er gennem detaljerede beregninger at forstå, hvordan vægtene i en kunstig neuron opdateres med brug af sigmoid aktiveringsfunktionen og gradientnedstigning.

Dette kan ses som et skridt på vejen til at forstå, hvordan vægtene opdateres i et generelt neuralt netværk.

:::

## Et meget lille datasæt

I neurale netværk er der ofte rigtige mange inputvariable (features), rigtigt mange vægte og rigtig mange træningsdata. 

Vi vil se på følgende netværk:

![](../../materialer/kunstige_neuroner/images/simplet_netvaerk1.png){width=75% fig-align='center'}

For bedre at forstå, hvordan vægtene opdateres i et simpelt neuralt netværk, vil vi her se på et meget lille eksempel, så det manuelt er muligt at lave opdateringen af vægtene.

Konkret vil vi se på to features $x_1$ og $x_2$ og en targetværdi $t$ ud fra følgende tre træningseksempler:

|$x_1$ | $x_2$ | $t$ |
|:---:|:---:|:---:|
| -1 | 2 | ja = 1 |
| 0 | 4 | nej = 0 |
| 1 | 7 | ja = 1 |
: {.bordered}

Vi vælger en learning rate på 

$$
\eta = 0.1,
$$ 

*sigmoid-funktionen* som aktiveringsfunktion $$\sigma(x)=\frac{1}{1+\mathrm{e}^{-x}}$$ 
og *squared error* som tabsfunktion $$E(w_0, w_1, w_2) = \frac{1}{2} \sum_{m=1}^{3} \left (t^{(m)}-\sigma(w_0 + w_1 \cdot x_1^{(m)} + w_2 \cdot x_2^{(m)}) \right)^2$$ 
Endeligt vælger vi startvægtene 

$$w_0=0.1, \qquad w_1=0.1, \qquad w_2=0.1
$$

## Første opdatering af vægtene

Du skal nu gå gennem de enkelte beregninger, som skal til for at opdatere vægtene. Der er efterfølgende facit til hver del.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 1: Beregn værdien af tabsfunktionen
* Udregn først 

   $$s^{(1)} = w_0 + w_1 \cdot x_1^{(1)} + w_2 \cdot x_2^{(1)}$$ 

   for det første træningseksempel.

* Udregn så 

   $$o^{(1)} = \sigma (s^{(1)})$$

* Udregn derefter 

   $$\mathrm{e}^{(1)} = (t^{(1)} - o^{(1)})^2,$$ 

   hvilket giver det første træningseksempels bidrag til tabsfunktionen.

* Gentag for hver af de to øvrige træningseksempler.

* Læg de 3 beregnede værdier sammen og divider med 2, så 

  $$E=\frac{1}{2} \cdot (\mathrm{e}^{(1)}+\mathrm{e}^{(2)}+\mathrm{e}^{(3)}).$$

:::


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 2: Beregn de partielle afledede

* Beregn $$\frac{\partial E}{\partial w_0} = - \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \cdot 1$$
* Beregn $$\frac{\partial E}{\partial w_1} = - \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \cdot x_1^{(m)}$$
* Beregn $$\frac{\partial E}{\partial w_2} = - \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \cdot x_2^{(m)}$$

:::


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 3: Opdatér vægtene

Beregn de opdaterede vægte
$$
\begin{aligned}
w_0^{(\textrm{ny})} \leftarrow & w_0 - \eta \cdot \frac{\partial E}{\partial w_0}   \\
\\
w_1^{(\textrm{ny})} \leftarrow & w_1 - \eta \cdot \frac{\partial E}{\partial w_1}  \\
\\
w_2^{(\textrm{ny})} \leftarrow & w_2 - \eta \cdot \frac{\partial E}{\partial w_2} 
\end{aligned}
$$

:::


## Anden opdatering af vægtene
Overvej, om du kan strømline dine beregninger, for eksempel i Excel eller i dit CAS værktøj, så det bliver hurtigere at opdatere vægtene en gang mere på samme måde.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 4: Opdater vægtene anden gang

* Beregn tabsfunktionen.
* Beregn de opdaterede vægte.

:::

Bemærk, at værdien af tabsfunktionen er blevet lidt mindre. Formålet er jo netop at minimere den gennem gradientnedstigning, så som regel bør værdien bliver mindre, hver gang vægtene opdateres.


## App til simple neurale netværk
Med [kunstig neuron app'en](https://apps01.math.aau.dk/ai/neuron/){target="_blank"} kan du lave de samme beregninger bare automatisk.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 5: Afprøv kunstig neuron app'en

* Lav et Excelark med feature- og targetværdierne.
* Indlæs data fra Excelarket i app'en under fanen \"Data\".
* Vælg de korrekte værdier under fanen \"Variable\" og \"Træning\". 
* Under fanen \"Træning\" skal fluebenet ved \"Brug feature-skalering\" *fjernes*. Herefter trykkes der på \"Kør modellen\".
* Se om app'en giver samme resultater for tabsfunktionen og vægtene, som du selv fik efter 1 iteration og efter 2 iterationer.

:::

## Cross-entropy som tabsfunktion
Du kan læse om *cross-entropy* i [noten om tabsfunktioner](../../materialer/tabsfunktioner/tabsfunktioner.qmd#cross-entropy){target="_blank"}.

::: {.callout-note collapse="false" appearance="minimal"}
### (Ekstra) Opgave 6: Cross-entropy tabsfunktion 
* Gentag opgave 1-5 men med *cross-entropy* som tabsfunktion i stedet for *squared error*.

:::


## Delvis facitliste
[Facitliste](opdatering_af_vaegte_i_en_kunstig_neuron_loesninger.qmd){target="_blank"}.

