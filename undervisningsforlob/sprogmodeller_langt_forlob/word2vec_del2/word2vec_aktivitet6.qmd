Læs afsnittet [\"Estimatation af vektorrepræsentationer\" i noten Word2Vec](/materialer/sprogmodeller/word2vec.qmd#estimation-af-vektorrepræsentationer). 


Tabsfunktionen er en funktion af flere variable, nemlig alle vægtene. Vi skal finde de vægte, der minimerer tabsfunktionen. I ved, at for at finde minimum for en funktion af én variabel, skal man se på, hvornår den afledte funktion er nul. For funktioner af flere variable gælder tilsvarende, at minimum skal findes i et punkt hvor alle de *partielle afledede* er nul. Hvis du ikke har hørt om partielle afledede før, kan du læse mere i boksen herunder.

::: {.callout-tip collapse="true" appearance="minimal"}

### Partielle afledede

Funktionen

$$
f(x,y) = x^2+3xy+\mathrm{e}^y
$$

afhænger ikke kun af én, men af to variable nemlig $x$ og $y$. Man kan derfor differentiere $f$ både med hensyn til $x$ (hvor man betragter $y$ som en konstant) og med hensyn til $y$ (hvor man betragter $x$ som en konstant). Disse afledede kaldes for partielle afledede og betegnes med

$$
\frac{\partial f}{\partial x} \quad \textrm{og} \quad \frac{\partial f}{\partial y}
$$

Når man for eksempel skal finde $\frac{\partial f}{\partial x}$, så differentierer man $f(x,y)$, hvor man tænker på $x$ som den variable og $y$ som en konstant. Tilsvarende med $\frac{\partial f}{\partial y}$. I dette eksempel giver det:

$$
\frac{\partial f}{\partial x} = \frac{\partial }{\partial x} (x^2) + \frac{\partial }{\partial x} (3xy) + \frac{\partial }{\partial x} (\mathrm{e}^y) = 2x + 3y + 0 = 2x + 3y
$$

og 

$$
\frac{\partial f}{\partial y} = \frac{\partial }{\partial y} (x^2) + \frac{\partial }{\partial y} (3xy) + \frac{\partial }{\partial y} (\mathrm{e}^y) = 0 + 3x + \mathrm{e}^y
$$

:::

Da Softmax-funktionen indgår i tabsfunktionen, får man brug for at finde partielle afledte af denne funktion. Husk på, at Softmax-funktionen er defineret sådan her:

$$
z_i = \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} =\mathrm{e}^{y_i} \cdot \frac{1}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} 
$$

For at diffentiere denne funktion kan man enten bruge produktreglen eller kvotientreglen, som vi lige genopfrisker her:

::: {.highlight }

**Produktreglen for differentiation**

$$
\left ( f \cdot g\right)'(x) = f'(x)\cdot g(x) + f(x) \cdot g'(x)
$$
\

**Kvotientreglen for differentiation **

$$
\left ( \frac{f}{g}\right)'(x) = \frac{f'(x) \cdot g(x)-f(x) \cdot g'(x)}{(g(x))^2}, \quad g(x) \neq 0
$$
:::



::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 9: Partielle afledte af Softmax-funktionen

    
* Vis følgende egenskaber ved de partielle afledte af Softmax-funktionen:

   $$\frac{\partial z_i}{\partial y_i} = z_i(1-z_i) \quad \quad \textrm{og} \quad\quad \frac{\partial z_i}{\partial y_j} = -z_iz_j, \quad i\neq j$$

   Husk på, at

   $$
   \frac{\partial }{\partial y_i} (\mathrm{e}^{y_i}) = \mathrm{e}^{y_i} 
   $$

   mens

   $$
   \frac{\partial }{\partial y_j} (\mathrm{e}^{y_i}) = 0   
   $$

   hvis $i \neq j$.

:::

Bemærk, at ovenstående opgaver giver, at man ikke behøver at kende værdien af $\vec{y}$ i det punkt, hvor man differentierer, men kun funktionsværdien $\vec{z}$. Det viser sig at have meget store beregningsmæssige fordele, når man skal finde minimum for tabsfunktionen, så det er faktisk en overordentlig vigtig egenskab!
