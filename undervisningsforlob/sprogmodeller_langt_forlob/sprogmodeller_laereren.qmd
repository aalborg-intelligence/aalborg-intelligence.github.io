---
title: "Til læreren"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Dette forløb bygger på noterne om [sprogmodeller](../../materialer/sprogmodeller.qmd.qmd) samt de tilhørende videoer. Hver enkel del i forløbet består typisk af en eller flere videoer, som skal ses (eller alternativt kan det tilhørende afsnit i noten læses). Herefter følger en række opgaver, som støtter op om teorien.

Det meste af forløbet er tænkt, så eleverne kan arbejde selvstændigt med stoffet. 

Her følger en kort gennemgang af de enkelte dele:

I [del 1](sprogmodeller_del1.qmd) gives en introduktion til en meget simpel sprogmodel. Matematikken i denne del er på et forholdsvis lavt niveau.

I [del 2](sprogmodeller_del2.qmd) arbejdes der med **Word2Vec** algoritmen, hvor målet er, at alle ord i et ordforråd bliver præsenteret ved vektorer på en sådan måde, at alle ord, hvis betydning ligner hinanden, repræsenteres med vektorer, der peger i nogenlunde samme retning og har nogenlunde samme længde. Her er skalarproduktet centralt. Desuden indføres den såkaldte Softmax-funktion

$$ 
z_i = \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}}
$$

som eleverne skal[^1] bestemme de partielle afledede for ved hjælp af kvotientreglen. Det er ikke en forudsætning, at man har arbejdet med funktioner af flere variable inden, idet der gives en kort introduktion til partielle afledede. Hvis man ønsker at arbejde med funktioner af flere variable, henviser vi til vores [materiale om emnet](../../materialer/funktioner_af_flere_variable/funktioner_af_flere_variable.qmd). 

Bruges **produktreglen** på

$$
z_i = \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} =\mathrm{e}^{y_i} \cdot \frac{1}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} 
$$

fås:

$$
\begin{aligned}
\frac{\partial z_i}{\partial y_i} &= \mathrm{e}^{y_i} \cdot \frac{1}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} + \mathrm{e}^{y_i} \cdot  \frac{-1}{( \mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V})^2} \cdot \mathrm{e}^{y_i} \\
& = \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} - \left ( \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \right )\cdot \left ( \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \right )\\
&= \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \cdot \left ( 1 - \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \right ) \\
&= z_i \cdot (1-z_i)
\end{aligned}
$$
Differentierer vi i stedet med hensyn til $y_j$ hvor $i \neq j$, får vi:

$$
\begin{aligned}
\frac{\partial z_i}{\partial y_j} &= 0 \cdot \frac{1}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} + \mathrm{e}^{y_i} \cdot  \frac{-1}{( \mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V})^2} \cdot \mathrm{e}^{y_j} \\
&= - \left ( \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \right )\cdot \left ( \frac{\mathrm{e}^{y_j}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \right ) \\
&= - z_i \cdot z_j
\end{aligned}
$$
Anvendes i stedet **kvotientreglen**, får vi:

$$
\begin{aligned}
\frac{\partial z_i}{\partial y_i} &= \frac{\mathrm{e}^{y_i} \cdot (\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V})-\mathrm{e}^{y_i} \cdot \mathrm{e}^{y_i} }{(\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V})^2} \\
&= \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \cdot \frac{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} - \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \cdot \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \\
&= \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \cdot 1  - \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \cdot \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \\
&= \frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \cdot \left (1-\frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \right) \\
&= z_i \cdot (1-z_i)
\end{aligned}
$$

Tilsvarende kan vi bruge kvotientreglen til at differentiere med hensyn til $y_j$ for $i \neq j$:
$$
\begin{aligned}
\frac{\partial z_i}{\partial y_j} &=  \frac{0 \cdot (\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}) - \mathrm{e}^{y_i} \cdot \mathrm{e}^{y_j}}{(\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V})^2} \\
&= -\frac{\mathrm{e}^{y_i}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \cdot \frac{\mathrm{e}^{y_j}}{\mathrm{e}^{y_1} + \dotsm + \mathrm{e}^{y_V}} \\
&= - z_i \cdot z_j
\end{aligned}
$$

[^1]: Man kan vælge at springe denne aktivitet over, men det er til gengæld det stof, der danner udgangspunkt for det foreslåede mundtlige eksamensspørgsmål.

I [del 3](sprogmodeller_del3.qmd) går vi lidt mere i dybden med at estimere kontekst- og fokusvektorer i Word2vec med udgangspunkt i et meget lille tekstkorpus.

[Del 4](sprogmodeller_del4.qmd) handler om, hvordan man kan bruge de vektorer, som **Word2Vec** algoritmen giver, til at lave tekstgenerering. Her kommer eleverne blandt andet til at stifte bekendtskab med *cross-entropy* tabsfunktionen.

[Del 5](sprogmodeller_del5.qmd) handler om **transformeren**, som er den algoritme, der i dag anvendes i de store sprogmodeller. Det er temmelig kompliceret matematik, som ligger bag transformeren, men eleverne får en overordnet introduktion til teorien og lærer blandt andet, hvordan og hvorfor alle ord nu bliver repræsenteret ved tre vektorer.