---
title: "Del 4: Transformeren"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

::: {.estimeret_tid}

Forventet tid ca. 90-120 min.


:::

{{< include ../space.qmd >}}

## Aktivitet 1 - Vektorregning {.aimat}

Læs afsnittene [Opmærksomhed](../../materialer/sprogmodeller/transformeren.qmd#opmærksomhed) og [Vektorregning](../../materialer/sprogmodeller/transformeren.qmd#vektorregning). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 1: Vektorregning

Lad $\vec{v}_1 = \begin{pmatrix} 1\\ 3\end{pmatrix}$ og $\vec{v}_2 = \begin{pmatrix} 3\\1\end{pmatrix}$.

* Beregn vektorerne 
  $$
  \begin{aligned}
  \vec{w}_1 = \frac{1}{3} \vec{v}_1 + \frac{2}{3} \vec{v}_2 \\ \\
  \vec{w}_2 = \frac{1}{2} \vec{v}_1 + \frac{1}{2} \vec{v}_2 \\ \\
  \vec{w}_3 = \frac{2}{3} \vec{v}_1 + \frac{1}{3} \vec{v}_2 \\
  \end{aligned}
  $$

* Indtegn $\vec{w}_1$, $\vec{w}_2$ og $\vec{w}_3$ i et koordinatsystem sammen med $\vec{v}_1$ og $\vec{v}_2$.

* Passer resultatet med, at vektoren
  $$
  c_1\vec{v}_1 + c_2\vec{v}_2
  $$ 
  med
  $c_1,c_2\geq 0$ og $c_1+c_2=1$ ligger tættere på $\vec{v}_1$ jo større $c_1$ er?

Lad $\vec{v}_1 = \begin{pmatrix} 1\\ 2\\ 0\\ -3\end{pmatrix}$ og $\vec{v}_2 = \begin{pmatrix} 1\\-1\\2\\ 4\end{pmatrix}$.
    
* Beregn
  $$
  2\vec{v}_1+\vec{v}_2 
  $$

:::



{{< include ../space.qmd >}}

## Aktivitet 2 - Et opmærksomhedslag  {.aimat}

Læs afsnittet [Et opmærksomhedslag](../../materialer/sprogmodeller/transformeren.qmd#et-opmærksomhedslag). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 2: Vektorrepræsentationer

Betragt sætningen \"Der svømmer marsvinet\". Her har \"svømmer\" stor betydning for vores opfattelse af ordet \"marsvinet\", mens \"der\" ikke giver os ret meget ny information. Lad os sige, at de tre ord har vektorrepræsentationerne 

$$
\begin{aligned}
&\text{der}&&
\vec{q}_1 = \begin{pmatrix}0\\-5\end{pmatrix},&& \vec{k}_1=\begin{pmatrix}4\\-1\end{pmatrix},&&\vec{v}_1= \begin{pmatrix} -1\\ -1\end{pmatrix}\\
&\text{svømmer}&&
\vec{q}_2 = \begin{pmatrix}3\\-7\end{pmatrix},&& \vec{k}_2=\begin{pmatrix}1\\2\end{pmatrix},&&\vec{v}_2= \begin{pmatrix} -2\\ 2\end{pmatrix}\\
&\text{marsvinet}&&
\vec{q}_3 = \begin{pmatrix}0\\1\end{pmatrix}, &&\vec{k}_3=\begin{pmatrix}-1\\2\end{pmatrix},&&\vec{v}_3= \begin{pmatrix} 1\\ 2\end{pmatrix}\\
\end{aligned}
$$

Vi vil nu se, at den opdaterede vektor $\vec{w}_3$ for \"marsvinet\" bliver trukket meget i retning af \"svømmer\", men i mindre grad i retning af \"der\".

-   Beregn $\vec{w}_3$ ved at følge nedenstående skridt:

    -   Beregn skalarprodukterne
        $\vec{k}_1\cdot \vec{q}_3$,
        $\vec{k}_2\cdot \vec{q}_3$ og
        $\vec{k}_3\cdot \vec{q}_3$.

    -   Brug softmax til at beregne konstanterne $c_{1,3}$, $c_{2,3}$ og
        $c_{3,3}$.
        
    -   Passer det med at store værdier af $c_{1,3}$, $c_{2,3}$ og
        $c_{3,3}$ svarer til ord, der har meget at sige om betydningen af \"marsvinet\"?

    -   Beregn
        $$\vec{w}_3 = c_{1,3}\vec{v}_1 + c_{2,3}\vec{v}_2 + c_{3,3}\vec{v}_3$$

-   Tegn vektorerne $\vec{v}_1$, $\vec{v}_2$,     $\vec{v}_3$ og $\vec{w}_3$ ind i et koordinatsystem og overvej, hvordan $\vec{w}_3$ ligger i forhold til $\vec{v}_1$, $\vec{v}_2$ og $\vec{v}_3$.

-   Tegn vektorerne $\vec{k}_1$, $\vec{k}_2$, $\vec{k}_3$ og $\vec{q}_3$ ind i et koordinatsystem. Passer det med, at kontekstvektoren for de ord, der har mest at sige om betydningen af ordet \"marsvinet\", peger mest i retning af $\vec{q}_3$?

::: 

{{< include ../space.qmd >}}

## Aktivitet 3 - Transformernetværket  {.aimat}

Læs afsnittene [En transformerblok](../../materialer/sprogmodeller/transformeren.qmd#en-transformerblok) og [Transformernetværket](../../materialer/sprogmodeller/transformeren.qmd#transformernetværket). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 3: Vægte i en transformernetværket

I denne opgave giver vi et overslag over størrelsen af den transformer, der blev brugt i GPT-2[^1], som er forgængeren for den GPT-4 teknologi, der bruges i ChatGPT.

[^1]: GPT-2 kommer i fire forskellige størrelser. Den vi regner på her er den mindste, som kaldes GPT-2 Small. 

Vi ser først på en enkelt transformerblok:

1. GPT-2 brugte vektorer af dimension $64$ til at repræsentere ord.  Hvert af de $V$ ord i sproget skal repræsenteres af $3$ forskellige vektorer. Hvor mange vægte skal der bruges til at lave vektorrepræsentationer af alle $V$ ord i sproget?

2. I et trin, som vi ikke gennemgik, reduceres de $V$ ord i sproget til $768$-dimensionale vektorer, som derefter oversættes til tre vektorer. Ved at sætte $V=768$ i 1., får du det reelle antal vægte, der skal bruges for at lave de tre vektorrepræsentationer. 

3. I GPT-2 var der $K=12$ parallelle opmærksomhedslag. Gang med $12$ i ovenstående beregning for at få antallet af vægte, der samlet set skal bruges til at lave vektorrepræsentationerne i alle opmærksomhedslagene.

4. Outputtet fra hvert opmærksomhedslag er en $64$-dimensional vektor.  Outputtet fra de $12$ opmærksomhedslag sættes i forlængelse af hinanden til en vektor $\vec{x}_i$. Hvilken dimension har $\vec{x}_i$? 

5. Som det næste bliver $\vec{x}_i$ brugt som input i et neuralt netværk. GPT-2 bruger $d=4\cdot 768$ neuroner i det skjulte lag og outputtet er en ny vektor af dimension $768$. Hvor mange vægte skal der i alt bruges til det neurale netværk (brug eventuelt din beregning fra opgave 1 i [del 3](sprogmodeller_del3.qmd#aktivitet-1---antal-parametre-i-modellen)).

6. Læg  antallet af vægte i opmærksomhedslagene, som du fandt i 3., sammen med antallet af vægte i det neurale netværk, som du fandt i 5., for at få det samlede antal vægte i en transformerblok.

Nu ser vi på det samlede transformernetværk.

7. GPT-2 brugte $N=12$ transformerblokke. Gang resultatet fra 6. med $12$ for at få det samlede antal vægte i transformerblokkene. 

8. Og så har vi endda  sprunget et par trin over. Helt i starten (se 2. ovenfor) blev de $V$ ord i ordforrådet oversat til $768$-dimensionale vektorer. I prædiktionslaget blev der brugt $V$ $768$-dimensionale vektorer til at udregne en score for hvert ord. Ved at bruge tokens i stedet for ord (se boksen under opgaven), kan GPT-2 nøjes med $V=50.257$ ord (eller rettere orddele). Hvor mange ekstra vægte giver det anledning til? 

9. Læg resultatet fra 7. og 8. sammen og sammenlign med det antal vægte, der ofte angives for GPT-2 Small i litteraturen, nemlig $117$ millioner (nogle kilder siger dog $124$ millioner).

:::

::: {.callout-tip collapse="true" appearance="minimal"}

### Tokens

Mange ord er opbygget af mindre dele, som går igen i mange forskellige ord. For eksempel er "lær" et ord i sig selv, men det kan også sættes sammen med andre orddele til at danne ord som "lær-e", "lær-er", "lær-ing" eller "lær-e-bog". Orddelene "e", "er" og "ing" forekommer i rigtig mange ord. Disse dele kaldes tokens. Ord adskilles af mellemrum, som er et token i sig selv.     
I stedet for at dele tekst op i ord, deler store sprogmodeller  tekst op i tokens. Fordelen er, at antallet af forskellige tokens, hvis de er valgt fornuftigt, er noget mindre end det samlede antal ord i sproget. Samtidig er de store nok til at indeholde  information om sprogets betydning. For eksempel forekommer tokenet "lær" ofte i ord relateret til læring, mens tokens som "er" eller "ing" ofte markerer en bestemt bøjning af et ord.

:::


{{< include ../space.qmd >}}



## Aktivitet 4 - Træning af netværket  {.aimat}

Læs afsnittet [Træning af netværket](../../materialer/sprogmodeller/transformeren.qmd#træning-af-netværket). 

Opgaver...???

{{< include ../space.qmd >}}

[$\leftarrow$ Forrige](sprogmodeller_del3.qmd){.prev}
[Næste $\rightarrow$](sprogmodeller_eksamen.qmd){.next}
