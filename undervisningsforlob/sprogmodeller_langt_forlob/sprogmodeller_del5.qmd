---
title: "Del 5: Transformeren"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

::: {.estimeret_tid}

Forventet tid ca. 90-120 min.


:::

{{< include ../space.qmd >}}

## Aktivitet 1 - Vektorregning {.aimat}

Læs afsnittene [Opmærksomhed](../../materialer/sprogmodeller/transformeren.qmd#opmærksomhed) og [Vektorregning](../../materialer/sprogmodeller/transformeren.qmd#vektorregning). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 1: Vektorregning

Lad $\vec{v}_1 = \begin{pmatrix} 1\\ 3\end{pmatrix}$ og $\vec{v}_2 = \begin{pmatrix} 3\\1\end{pmatrix}$.

* Beregn vektorerne 
  $$
  \begin{aligned}
  \vec{w}_1 = \frac{1}{3} \vec{v}_1 + \frac{2}{3} \vec{v}_2 \\ \\
  \vec{w}_2 = \frac{1}{2} \vec{v}_1 + \frac{1}{2} \vec{v}_2 \\ \\
  \vec{w}_3 = \frac{2}{3} \vec{v}_1 + \frac{1}{3} \vec{v}_2 \\
  \end{aligned}
  $$

* Indtegn $\vec{w}_1$, $\vec{w}_2$ og $\vec{w}_3$ i et koordinatsystem sammen med $\vec{v}_1$ og $\vec{v}_2$.

* Passer resultatet med, at vektoren
  $$
  c_1\vec{v}_1 + c_2\vec{v}_2
  $$ 
  med
  $c_1,c_2\geq 0$ og $c_1+c_2=1$ ligger tættere på $\vec{v}_1$ jo større $c_1$ er?

Lad $\vec{v}_1 = \begin{pmatrix} 1\\ 2\\ 0\\ -3\end{pmatrix}$ og $\vec{v}_2 = \begin{pmatrix} 1\\-1\\2\\ 4\end{pmatrix}$.
    
* Beregn
  $$
  2\vec{v}_1+\vec{v}_2 
  $$

:::



{{< include ../space.qmd >}}

## Aktivitet 2 - Et opmærksomhedslag  {.aimat}

Læs afsnittet [Et opmærksomhedslag](../../materialer/sprogmodeller/transformeren.qmd#et-opmærksomhedslag). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 2: Vektorrepræsentationer

Betragt sætningen \"Der svømmer marsvinet\". Her har \"svømmer\" stor betydning for vores opfattelse af ordet \"marsvinet\", mens \"der\" ikke giver os ret meget ny information. Lad os sige, at de tre ord har vektorrepræsentationerne 

$$
\begin{aligned}
&\text{der}&&
\vec{q}_1 = \begin{pmatrix}0\\-5\end{pmatrix},&& \vec{k}_1=\begin{pmatrix}4\\-1\end{pmatrix},&&\vec{v}_1= \begin{pmatrix} -1\\ -1\end{pmatrix}\\
&\text{svømmer}&&
\vec{q}_2 = \begin{pmatrix}3\\-7\end{pmatrix},&& \vec{k}_2=\begin{pmatrix}1\\2\end{pmatrix},&&\vec{v}_2= \begin{pmatrix} -2\\ 2\end{pmatrix}\\
&\text{marsvinet}&&
\vec{q}_3 = \begin{pmatrix}0\\1\end{pmatrix}, &&\vec{k}_3=\begin{pmatrix}-1\\2\end{pmatrix},&&\vec{v}_3= \begin{pmatrix} 1\\ 2\end{pmatrix}\\
\end{aligned}
$$

Vi vil nu se, at den opdaterede vektor $\vec{w}_3$ for \"marsvinet\" bliver trukket meget i retning af \"svømmer\", men i mindre grad i retning af \"der\".

-   Beregn $\vec{w}_3$ ved at følge nedenstående skridt:

    -   Beregn skalarprodukterne
        $\vec{k}_1\cdot \vec{q}_3$,
        $\vec{k}_2\cdot \vec{q}_3$ og
        $\vec{k}_3\cdot \vec{q}_3$.

    -   Brug softmax til at beregne konstanterne $c_{1,3}$, $c_{2,3}$ og
        $c_{3,3}$.
        
    -   Passer det med at store værdier af $c_{1,3}$, $c_{2,3}$ og
        $c_{3,3}$ svarer til ord, der har meget at sige om betydningen af \"marsvinet\"?

    -   Beregn
        $$\vec{w}_3 = c_{1,3}\vec{v}_1 + c_{2,3}\vec{v}_2 + c_{3,3}\vec{v}_3$$

-   Tegn vektorerne $\vec{v}_1$, $\vec{v}_2$,     $\vec{v}_3$ og $\vec{w}_3$ ind i et koordinatsystem og overvej, hvordan $\vec{w}_3$ ligger i forhold til $\vec{v}_1$, $\vec{v}_2$ og $\vec{v}_3$.

-   Tegn vektorerne $\vec{k}_1$, $\vec{k}_2$, $\vec{k}_3$ og $\vec{q}_3$ ind i et koordinatsystem. Passer det med, at kontekstvektoren for de ord, der har mest at sige om betydningen af ordet \"marsvinet\", peger mest i retning af $\vec{q}_3$?

::: 

{{< include ../space.qmd >}}

## Aktivitet 3 - Transformernetværket  {.aimat}

Læs afsnittene [En transformerblok](../../materialer/sprogmodeller/transformeren.qmd#en-transformerblok) og [Transformernetværket](../../materialer/sprogmodeller/transformeren.qmd#transformernetværket). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 3: Vægte i en transformernetværket

I denne opgave giver vi et overslag over størrelsen på den transformer, der blev brugt i GPT-2[^7], som er forgængeren for den GPT-4 teknologi, der bruges i ChatGPT. I praksis inddeler GPT-2 ikke tekst i ord, men i *tokens*, som er mindre orddele (se boksen under opgaven). Derfor arbejder GPT-2 med et ordforråd på $V=50.257$ tokens. Brug denne værdi af $V$ i opgaven nedenunder.

[^7]: GPT-2 kommer i fire forskellige størrelser. Den vi regner på her er den mindste, som kaldes GPT-2 Small. 

Vi ser på de enkelte trin i transformeren:

1. Helt i starten  bliver hvert af de $V$ ord oversat til $768$-dimensionale vektorer (dette trin blev ikke gennemgået i noten). Hvor mange vægte skal der bruges? 

Nu ser vi på en enkelt transformerblok:

2. Det første der sker i transformerblokken, er, at vores ord  oversættes til 3 vektorer. I GPT-2 havde hver af de 3 vektorer  dimension $64$. Husk på, at hvert ord allerede er blevet oversat til en $768$-dimensional vektor i Punkt 1. For at oversætte  $768$-dimensionale vektorer
$$\overrightarrow{a}= \begin{bmatrix} a_1\\ \vdots\\ a_{768} \end{bmatrix}$$
videre til $64$-dimensionale vektorer
$$\overrightarrow{b}= \begin{bmatrix} b_1\\ \vdots\\ b_{64} \end{bmatrix}$$
skal der bruges $64\cdot 768$ vægte på formen $c_{ij}$, hvor $i=1,\ldots, 64$ og $j=1,\ldots, 768$.[^33]
Den $i$te koordinat i $\overrightarrow{b}$ beregnes ved
$$b_i=c_{i1}a_1 + c_{i2}a_2 + \dotsm + c_{i768}a_{768}.$$
Det skal så gøres tre gange for at lave de tre vektorer. Gang antal vægte med tre for at få det samlede antal vægte, der skal bruges i en transformerblok.

[^33]: Bemærk at det er de samme vægte $c_{ij}$, der benyttes for alle ord.

Transformerblokkene sættes sammen til et opmærksomhedslag:

3. I GPT-2 var der $K=12$ parallelle opmærksomhedslag. Gang resultatet fra 2. med $12$ for at få antallet af vægte, der samlet set skal bruges til at lave vektorrepræsentationerne i alle opmærksomhedslagene.

4. Outputtet fra hvert opmærksomhedslag er en $64$-dimensional vektor.  Outputtet fra de $12$ opmærksomhedslag sættes i forlængelse af hinanden til en vektor $\vec{x}_i$. Hvilken dimension har $\vec{x}_i$? 

5. Som det næste bliver $\vec{x}_i$ brugt som input i et neuralt netværk. GPT-2 bruger $d=4\cdot 768$ neuroner i det skjulte lag og outputtet er en ny vektor af dimension $768$. Hvor mange vægte skal der i alt bruges til det neurale netværk (brug eventuelt din beregning fra opgave 1 i [del 4](sprogmodeller_del4.qmd#aktivitet-1---antal-parametre-i-modellen)).

6. Læg  antallet af vægte i opmærksomhedslagene, som du fandt i 3., sammen med antallet af vægte i det neurale netværk, som du fandt i 5., for at få det samlede antal vægte i en transformerblok.

Nu ser vi på det samlede transformernetværk.

7. GPT-2 brugte $N=12$ transformerblokke. Gang resultatet fra 6. med $12$ for at få det samlede antal vægte i transformerblokkene. 

8. Til sidst blev der i prædiktionslaget brugt $V$ $768$-dimensionale vektorer til at udregne en score for hvert ord.  Hvor mange ekstra vægte giver det anledning til? 

9. Læg resultatet fra 1., 7. og 8. sammen for at få det samlede antal vægte. Sammenlign med det antal vægte, der ofte angives for GPT-2 Small i litteraturen, nemlig $117$ millioner (nogle kilder siger dog $124$ millioner).

:::

::: {.callout-tip collapse="true" appearance="minimal"}

### Tokens

Mange ord er opbygget af mindre dele, som går igen i mange forskellige ord. For eksempel er "lær" et ord i sig selv, men det kan også sættes sammen med andre orddele til at danne ord som "lær-e", "lær-er", "lær-ing" eller "lær-e-bog". Orddelene "e", "er" og "ing" forekommer i rigtig mange ord. Disse dele kaldes tokens. Ord adskilles af mellemrum, som er et token i sig selv.     
I stedet for at dele tekst op i ord, deler store sprogmodeller  tekst op i tokens. Fordelen er, at antallet af forskellige tokens, hvis de er valgt fornuftigt, er noget mindre end det samlede antal ord i sproget. Samtidig er de store nok til at indeholde  information om sprogets betydning. For eksempel forekommer tokenet "lær" ofte i ord relateret til læring, mens tokens som "er" eller "ing" ofte markerer en bestemt bøjning af et ord.

:::


{{< include ../space.qmd >}}



## Aktivitet 4 - Træning af netværket  {.aimat}

Læs afsnittet [Træning af netværket](../../materialer/sprogmodeller/transformeren.qmd#træning-af-netværket). 

Opgaver...???

{{< include ../space.qmd >}}

[$\leftarrow$ Forrige](sprogmodeller_del4.qmd){.prev}
[Næste $\rightarrow$](sprogmodeller_eksamen.qmd){.next}
