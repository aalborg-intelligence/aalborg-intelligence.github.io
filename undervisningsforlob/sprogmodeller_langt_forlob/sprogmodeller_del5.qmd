---
title: "Del 5: Transformeren"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

::: {.estimeret_tid}

Forventet tid ca. 90-120 min.


:::

{{< include ../space.qmd >}}

## Aktivitet 1 - Vektorregning {.aimat}

Læs afsnittene [Opmærksomhed](../../materialer/sprogmodeller/transformeren.qmd#opmærksomhed) og [Vektorregning](../../materialer/sprogmodeller/transformeren.qmd#vektorregning). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 1: Vektorregning

Lad $\vec{v}_1 = \begin{pmatrix} 1\\ 3\end{pmatrix}$ og $\vec{v}_2 = \begin{pmatrix} 3\\1\end{pmatrix}$.

* Beregn vektorerne 
  $$
  \begin{aligned}
  \vec{w}_1 = \frac{1}{3} \vec{v}_1 + \frac{2}{3} \vec{v}_2 \\ \\
  \vec{w}_2 = \frac{1}{2} \vec{v}_1 + \frac{1}{2} \vec{v}_2 \\ \\
  \vec{w}_3 = \frac{2}{3} \vec{v}_1 + \frac{1}{3} \vec{v}_2 \\
  \end{aligned}
  $$

* Indtegn $\vec{w}_1$, $\vec{w}_2$ og $\vec{w}_3$ i et koordinatsystem sammen med $\vec{v}_1$ og $\vec{v}_2$.

* Passer resultatet med, at vektoren
  $$
  c_1\vec{v}_1 + c_2\vec{v}_2
  $$ 
  med
  $c_1,c_2\geq 0$ og $c_1+c_2=1$ ligger tættere på $\vec{v}_1$ jo større $c_1$ er?

Lad $\vec{v}_1 = \begin{pmatrix} 1\\ 2\\ 0\\ -3\end{pmatrix}$ og $\vec{v}_2 = \begin{pmatrix} 1\\-1\\2\\ 4\end{pmatrix}$.
    
* Beregn
  $$
  2\vec{v}_1+\vec{v}_2 
  $$

:::



{{< include ../space.qmd >}}

## Aktivitet 2 - Et opmærksomhedslag  {.aimat}

Læs afsnittet [Et opmærksomhedslag](../../materialer/sprogmodeller/transformeren.qmd#et-opmærksomhedslag). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 2: Vektorrepræsentationer

Betragt sætningen \"Der svømmer marsvinet\". Her har \"svømmer\" stor betydning for vores opfattelse af ordet \"marsvinet\", mens \"der\" ikke giver os ret meget ny information. Lad os sige, at de tre ord har vektorrepræsentationerne 

$$
\begin{aligned}
&\text{der}&&
\vec{q}_1 = \begin{pmatrix}0\\-5\end{pmatrix},&& \vec{k}_1=\begin{pmatrix}4\\-1\end{pmatrix},&&\vec{v}_1= \begin{pmatrix} -1\\ -1\end{pmatrix}\\
&\text{svømmer}&&
\vec{q}_2 = \begin{pmatrix}3\\-7\end{pmatrix},&& \vec{k}_2=\begin{pmatrix}1\\2\end{pmatrix},&&\vec{v}_2= \begin{pmatrix} -2\\ 2\end{pmatrix}\\
&\text{marsvinet}&&
\vec{q}_3 = \begin{pmatrix}0\\1\end{pmatrix}, &&\vec{k}_3=\begin{pmatrix}-1\\2\end{pmatrix},&&\vec{v}_3= \begin{pmatrix} 1\\ 2\end{pmatrix}\\
\end{aligned}
$$

Vi vil nu se, at den opdaterede vektor $\vec{w}_3$ for \"marsvinet\" bliver trukket meget i retning af \"svømmer\", men i mindre grad i retning af \"der\".

-   Beregn $\vec{w}_3$ ved at følge nedenstående skridt:

    -   Beregn skalarprodukterne
        $\vec{k}_1\cdot \vec{q}_3$,
        $\vec{k}_2\cdot \vec{q}_3$ og
        $\vec{k}_3\cdot \vec{q}_3$.

    -   Brug softmax til at beregne konstanterne $c_{1,3}$, $c_{2,3}$ og
        $c_{3,3}$.
        
    -   Passer det med at store værdier af $c_{1,3}$, $c_{2,3}$ og
        $c_{3,3}$ svarer til ord, der har meget at sige om betydningen af \"marsvinet\"?

    -   Beregn
        $$\vec{w}_3 = c_{1,3}\vec{v}_1 + c_{2,3}\vec{v}_2 + c_{3,3}\vec{v}_3$$

-   Tegn vektorerne $\vec{v}_1$, $\vec{v}_2$,     $\vec{v}_3$ og $\vec{w}_3$ ind i et koordinatsystem og overvej, hvordan $\vec{w}_3$ ligger i forhold til $\vec{v}_1$, $\vec{v}_2$ og $\vec{v}_3$.

-   Tegn vektorerne $\vec{k}_1$, $\vec{k}_2$, $\vec{k}_3$ og $\vec{q}_3$ ind i et koordinatsystem. Passer det med, at kontekstvektoren for de ord, der har mest at sige om betydningen af ordet \"marsvinet\", peger mest i retning af $\vec{q}_3$?

::: 

{{< include ../space.qmd >}}

## Aktivitet 3 - Transformernetværket  {.aimat}

Læs afsnittene [En transformerblok](../../materialer/sprogmodeller/transformeren.qmd#en-transformerblok) og [Transformernetværket](../../materialer/sprogmodeller/transformeren.qmd#transformernetværket). 

::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 3: Vægte i et transformernetværk

I denne opgave giver vi et overslag over størrelsen på den transformer, der blev brugt i GPT-2[^7], som er forgængeren for den GPT-4 teknologi, der bruges i ChatGPT. I praksis inddeler GPT-2 ikke tekst i ord, men i *tokens*, som er mindre orddele (se boksen under opgaven). Derfor arbejder GPT-2 med et ordforråd på $V=50.257$ tokens. Brug denne værdi af $V$ i opgaven nedenunder.

[^7]: GPT-2 kommer i fire forskellige størrelser. Den vi regner på her er den mindste, som kaldes GPT-2 Small. 

Vi ser på de enkelte trin i transformeren:

1. Helt i starten  bliver hver af de $V$ tokens oversat til $768$-dimensionale vektorer (dette trin blev ikke gennemgået i noten). Hvor mange vægte skal der bruges? 

Vi ser først på en enkelt transformerblok:

2. Det første der sker i transformerblokken, er, at vores tokens  oversættes til de 3 vektorer $\overrightarrow{q}$, $\overrightarrow{k}$ og $\overrightarrow{v}$.  Husk på, at hver token allerede er blevet oversat til en $768$-dimensional vektor i 1., så i virkeligheden er det disse vektorer, der skal oversættes videre til de tre vektorer.  I GPT-2 havde hver af de 3 vektorer dimension $64$. 
For at lave $\overrightarrow{q}$-vektorerne, oversættes hver $768$-dimensional vektor
$$\overrightarrow{a}= \begin{bmatrix} a_1\\ \vdots\\ a_{768} \end{bmatrix}$$
til en $64$-dimensional vektor
$$\overrightarrow{q}= \begin{bmatrix} q_1\\ \vdots\\ q_{64} \end{bmatrix}$$
ved hjælp af $64\cdot 768$ vægte på formen $c_{ij}^q$, hvor $i=1,\ldots, 64$ og $j=1,\ldots, 768$.
Den $i$te koordinat i $\overrightarrow{q}$ beregnes ved
$$q_i=c_{i1}^qa_1 + c_{i2}^qa_2 + \dotsm + c_{i768}^qa_{768}.$$
Tilsvarende skal der bruges $64\cdot 768$ vægte $c_{ij}^k$ for at lave $\overrightarrow{k}$-vektorerne og $64\cdot 768$ vægte $c_{ij}^v$ for at lave $\overrightarrow{v}$-vektorerne.
Hvor mange vægte blev der brugt i alt (bemærk, at det er de samme vægte $c_{ij}^q$, $c_{ij}^k$ og $c_{ij}^v$, der benyttes for alle tokens).

Transformerblokkene sættes sammen til et opmærksomhedslag:

3. I GPT-2 var der $K=12$ parallelle opmærksomhedslag. Gang resultatet fra 2. med $12$ for at få antallet af vægte, der samlet set skal bruges til at lave vektorrepræsentationerne i alle opmærksomhedslagene.

4. Outputtet fra hvert opmærksomhedslag er en $64$-dimensional vektor.  Outputtet fra de $12$ opmærksomhedslag sættes i forlængelse af hinanden til en vektor $\vec{x}$. Hvilken dimension har $\vec{x}$? 

5. Som det næste bliver $\vec{x}$ brugt som input i et neuralt netværk. GPT-2 bruger $d=4\cdot 768$ neuroner i det skjulte lag, og outputtet er en ny vektor af dimension $768$. Hvor mange vægte skal der i alt bruges til det neurale netværk (brug eventuelt din beregning fra opgave 1 i [del 4](sprogmodeller_del4.qmd#aktivitet-1---antal-parametre-i-modellen)).

6. Læg  antallet af vægte i opmærksomhedslagene, som du fandt i 3., sammen med antallet af vægte i det neurale netværk, som du fandt i 5., for at få det samlede antal vægte i en transformerblok.

Nu ser vi på det samlede transformernetværk:

7. GPT-2 brugte $N=12$ transformerblokke. Gang resultatet fra 6. med $12$ for at få det samlede antal vægte i transformerblokkene. 

8. Til sidst var der et prædiktionslag. Her blev brugt en $768$-dimensional vektor for hver af de $V$ tokens for at udregne en score. Det er de samme vektorer, som blev brugt i 1., så der skal ikke bruges nye vægte til dette trin. 

9. Læg resultatet fra 1. og 7. sammen for at få det samlede antal vægte. Sammenlign med det antal vægte, der ofte angives for GPT-2 Small i litteraturen, nemlig $117$ millioner (nogle kilder siger dog $124$ millioner).

De resterende vægte skyldes mindre trin, som vi ikke har gennemgået.

:::

::: {.callout-tip collapse="true" appearance="minimal"}

### Tokens

Mange ord er opbygget af mindre dele, som går igen i  forskellige ord. For eksempel er "lær" et ord i sig selv, men det kan også sættes sammen med andre orddele til at danne ord som "lær-e", "lær-er", "lær-ing" eller "lær-e-bog". Orddelene "e", "er" og "ing" forekommer i rigtig mange ord. Disse dele kaldes tokens. Ord adskilles af mellemrum, som er et token i sig selv.     
I stedet for at dele tekst op i ord, deler store sprogmodeller  tekst op i tokens. Fordelen er, at antallet af forskellige tokens, hvis de er valgt fornuftigt, er noget mindre end det samlede antal ord i sproget. Samtidig er de store nok til at indeholde  information om sprogets betydning. For eksempel forekommer tokenet "lær" ofte i ord relateret til læring, mens tokens som "er" eller "ing" ofte markerer en bestemt bøjning af et ord.

:::


{{< include ../space.qmd >}}



## Aktivitet 4 - Træning af netværket  {.aimat}

Læs afsnittet [Træning af netværket](../../materialer/sprogmodeller/transformeren.qmd#træning-af-netværket). 

Opgaver...???

{{< include ../space.qmd >}}

[$\leftarrow$ Forrige](sprogmodeller_del4.qmd){.prev}
[Næste $\rightarrow$](sprogmodeller_eksamen.qmd){.next}
