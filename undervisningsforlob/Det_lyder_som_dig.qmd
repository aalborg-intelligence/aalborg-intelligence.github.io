---
title: "Det lyder som dig!"
description-meta: "Vi vil undersøge, hvordan AI kan genkende personer ud fra ganske få sekunders lydklip."
image: "Det_lyder_som_dig_filer/det_lyder_som_dig.jpg"
categories:
  - C-niveau
  - Kort
---


::: {.callout-caution collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug

- Lineær regression.


**Tidsforbrug:** 2-3 timer

:::

::: {.purpose}

### Formål
Vi vil sammen undersøge, hvor godt og hvordan AI kan genkende personer ud fra ganske få sekunders lydklip. Konkret med brug af lydklip med dig og dine klassekammerater.

Undervejs vil du blive introduceret til mange af de centrale begreber om træning af de såkaldte kunstige neurale netværk, som AI gør brug af. Dette sker blandt andet med programmet Orange.

Forløbet indeholder ikke som sådan noget matematik, men giver en god start på begreber og metoder indenfor kunstige neurale netværk. Matematikken bag kommer så i senere forløb.

:::

## Hvad er AI?

Start med at se de første 10:40 minutter af denne video.

{{< video https://youtu.be/ivrBEopralQ >}}

## Den grimme ælling - oplæst af fire forskellige personer

Inden vi kommer til jeres egne data, starter vi lige med at se på hele processen med brug af vores data. Til det formål har vi fire oplæsninger af den samme tekst. I kan høre lydfilerne fra oplæsningerne her: [Malene](Det_lyder_som_dig_filer/Malene.m4a), [Lisbeth](Det_lyder_som_dig_filer/Lisbeth.m4a), [Ege](Det_lyder_som_dig_filer/Ege.m4a) og [Jan](Det_lyder_som_dig_filer/Jan.m4a).

Alle lydfilerne er 2-2,5 minutter lange, men da vi gerne vil arbejde med meget korte lydfiler, opdeles hver lydfil i sekvenser af 3 sekunder. Du kan høre disse små sekvenser her: [korte sekvenser](Det_lyder_som_dig_filer/sekvenser.zip). 

### Kunstige neurale netværk

**Kunstige neurale netværk** arbejder ikke direkte på lyd, som mennesker gør -- men på såkaldte **features**. I denne sammenhæng skal vi bruge oplæserens navn, et ID og en række tal, som beskriver frekvenser med mere i lydfilen på en smart måde. Da det er fysik og ganske svært, vil vi ikke her for alvor gå i dybden med, hvordan det gøres, men måske du har haft eller får noget i fysik om analyse af lyd og svingninger. Du får dog lige en kort introduktion til, hvad det er.

Det, vi skal bruge, hedder **Mel-Frequency Cepstral Coefficients (MFCC)**, som er en metode, der bruges til at analysere og repræsentere lydsignaler, især tale og musik, ved at fange de vigtigste egenskaber ved lydsignalet, som det opfattes af det menneskelige øre. Mennesker opfatter lyd sådan, at vi er mere følsomme over for forskelle i lave frekvenser (for eksempel mellem 200 og 400 Hz) end i høje (for eksempel mellem 5000 og 5200 Hz). MFCC tager højde for dette ved at bruge en Mel-skala, som efterligner, hvordan øret opfatter frekvenser. Detaljerne er på SRP niveau, men hvis du har lyst til at vide mere, så kan du læse her: [MFCC](https://www.geeksforgeeks.org/nlp/mel-frequency-cepstral-coefficients-mfcc-for-speech-recognition/?utm_source=chatgpt.com).  

Vi har i forvejen lavet disse MFCC data for hver af de små lydsekvenser. Vi har desuden opdelt data i **træningsdata**, som vi vil bruge til at træne et kunstigt neuralt netværk, og **testdata**, som efterfølgende skal teste, hvor god modellen i det kunstige neurale netværk er blevet. Du kan se disse data her: [MFCC data](Det_lyder_som_dig_filer/MFCC_data.zip).

Til træningsdata har vi brugt $75\%$ af lydsekvenserne for hver af de fire oplæsere, mens de resterende $25\%$ er anvendt til testdata.

### Sammenligning med lineær regression
Det at træne et neuralt netværk minder om at lave lineær regression, som du kender.

I lineær regression har man en række punkter -- det er træningsdata.

Hvert punkt består af et $x$-koordinat og et $y$-koordinat. Her er $x$-koordinaten en **feature**, og der er kun den ene feature, mens $y$-koordinaten er **target** (den vi gerne vil \"ramme\").

Når vi laver lineær regression, er modellen^[Hvis det skal være helt korrekt er der også et normalfordelt støjled $N(\mu, \sigma),$ men det undlader man som regel at tage med på gymnasieniveau.]  $y=a\cdot x+b$, og det gælder om at bestemme $a$ og $b$, så modellen for featureværdierne ($x$-koordinaterne) samlet set bedst muligt rammer targetværdierne ($y$-koordinaterne). I forhold til et neuralt netværk vil $a$ og $b$ kaldes for **vægte**.

I et kunstigt neuralt netværk er modellen betydeligt mere kompliceret, og der er mange flere vægte end de to i lineær regression (i store netværk kan der være milliarder af vægte), men idéen er den samme. Man skal også i kunstige neurale netværk bestemme vægtene i modellen, så den for featureværdierne samlet set bedst muligt rammer targetværdierne. 

I lineær regression skal modellen komme så tæt på targetværdien $y$ for hver featureværdi $x$. I vores tilfælde er det anderledes, da vi ikke skal tæt på en talværdi, men i stedet skal vælge mellem de fire personer -- dette kaldes for **klassifikation**. I lineær regression handler det således om, at linjen (modellen) kommer tæt på punkterne, mens det i klassifikation handler om, at modellen skal forudsige den rigtige oplæser for så mange som muligt af de små lydklip. Man taler derfor om **klassifikationsnøjagtigheden (CA)**, som er andelen af korrekte forudsigelser. Så hvis $CA=0.85$, har modellen korrekt forudsagt $85\%$ af targetværdierne. Dermed er der altså $15\%$ forkerte forudsigelser.

### Det kunstige neurale netværk i Orange

Nok snak om teori -- vi skal have det afprøvet i praksis.

Vi vil anvende programmet [Orange Data Mining](https://orangedatamining.com/) til at træne og teste et neuralt netværk på MFCC data for lydfilerne.

Lav opgave 1 -- hvis noget driller, kan du måske få hjælp ved at se denne video 

{{< video https://youtu.be/vGvFpgrUv8o?si=4OZKShXkmrKHFEKK >}}


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 1: Installér og afprøv Orange

* Installér Orange.

* Download filen [Model i Orange](Det_lyder_som_dig_filer/Det_lyder_som_dig_Orange.zip) og åbn filen \"Det_lyder_som_dig_Orange.ows\". Den indeholder stukturen til at lave modellen.

* Hvis du ikke allerede har hentet [MFCC data](Det_lyder_som_dig_filer/MFCC_data.zip) , så hent dem og pak zip-filen ud, så du har adgang til filerne med træningsdata og testdata.

* Klik på \"Træningsdata\" i modellen i Orange og importer filen med træningsdata. 

* Sæt \"id\" til \"meta\" (modellen skal ignorere denne værdi), sæt \"category\" (\"Malene\", \"Lisbeth\", \"Ege\" eller \"Jan\") til \"target\", og sæt de 13 MFCC data til \"feature\" (det står de nok allerede som). 

* Gør det samme for \"Testdata\" i modellen i Orange.
:::


Dermed er data klar i Orange, og modellen i det kunstige neurale netværk er faktisk også trænet. Men hvor god er modellen blevet?

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 2: Undersøg modellen

* Klik på \"Test and Score\" i Orange. Hvad er **klassifikationsgraden (CA)**? 

* Under \"Test and Score\" kan man ude til venstre vælge forskellige måder at teste på. Sørg for, at den indtil videre står på \"Test on train data\".

* Klik på \"Confusion matrix\" for at se, hvilke input, der er klassificeret korrekt, og hvilke der er klassificeret forkert. 

* Sørg så for, at både \"Predictions\" og \"Probabilities\" er valgt ude til venstre og klik så på \"Select Misclassified\" i bunden. Luk vinduet ned.

* Klik derefter på \"Predictions - misclassified\" ude til højre i Orange. Her kan du for hver af de misklassificerede lydfiler se filens ID, hvem der har lavet lydfilen (under \"category\"), hvem modellen fejlagtigt mener, det er (under \"category(Neural Network)\"), og procenter for, hvor meget modellen tror, at filen er fra hver af de 4 muligheder. Der er lige nu kun ét misklassificeret klip. Notér \"category\" og \"id\" for dette lydklip.

* Hvis du ikke allerede har hente de små 3 sekunders lydklip, så gør det nu: [Korte sekvenser](Det_lyder_som_dig_filer/sekvenser.zip). 

* Find det misklassificerede klip og lyt til det.

:::

Når vi før valgte at teste på træningsdata, er det på en måde lidt snyd. Det betyder, at vi først lader modellen træne på træningsdata, og derved tilpasse de mange vægte i modellen^[Der er 124 vægte i dette neurale netværk, så det er betydeligt flere end i den lineære model, men stadig ikke mange, når man taler om neurale netværk.] så den passer bedst muligt til disse data -- og så derefter at se, hvor godt modellen passer til præcis samme data.

Vi bør i stedet træne modellen på **træningsdata**, for derefter at teste på nogle helt separate data, som derfor kaldes for **testdata**. Derved \"snyder\" vi ikke længere, da modellen faktisk skal forudsige targetværdier ud fra featureværdier, den ikke brugte til at træne på.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 3: Test på testdata

Gå tilbage til modellen i Orange.

* Vælg i stedet under \"Test and Score\" at teste på testdata (til venstre skal du vælge \"Test on test data\"). Hvad bliver klassifikationsgraden (CA) nu? Den vil som regel blive ringere end ved test på træningsdata.

* Se igen på \"Confusion Matrix\" og på \"Predictions -- misclassified\", hvor der nu er flere fejl. 

* Hør lydklippene for de misklassificerede.

:::

## Jeres egne data
Det bliver naturligvis først rigtigt interessant -- og sværere for modellen -- hvis der er flere end fire oplæsere. 

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 4: Alle læser samme tekst op

Denne opgave kan eventuelt være lavet hjemme i forvejen som en lektie, da det kan være upraktisk, hvis en hel klasse skal læse op samtidig!

* Jeres lærer vælger en tekst, som I alle skal læse op. Mindst 2 minutter, og gerne længere (I kan for eksempel vælge at læse forordet i jeres matematikbog, hvis det er passende i længde og indhold. Så slår I to fluer med ét smæk!).

* Optag en lydfil på din telefon eller computer (vigtigt, *ikke* en video, kun lyd!).
 
* Giv filen med din egen oplæsning dit navn.

* Saml alle lydfilerne på samme computer.

* Filerne skal være i \"wav\", \"flac\" eller \"m4a\" format. ^[Filerne vil typisk være på 2-4 MB hver, men kan godt med nogle optagere være lavet i højere kvalitet, så filen er ca. 10 MB. Hvis filen er tæt på 100 MB, har man nok i stedet fået lavet en video, hvilket ikke var meningen. Små filer gør det næste skridt hurtigere.]

:::

Som det næste skal I have opdelt lydfilerne i små sekvenser på 3 sekunder, og derefter have lavet MFCC data til hver sekvens. Det bliver lidt teknisk, da I skal bruge et Python-script^[Python er et meget anvendt programmeringssprog.]  i Googles Colab^[Colab er en gratis side online, som kan bruges til Python scripts, så man ikke selv skal igennem en lidt besværlig installation af Python.], men I kan heldigvis gøre det sammen. Hvis der blandt jer er en Python ekspert, kan I også bruge koden på en computer lokalt, hvilket er noget hurtigere.

Denne video viser, hvordan det foregår.

{{< video https://youtu.be/XZkxnXnfm_U?si=77trJ6UiF6dOQpAW >}}

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 5: Generer MFCC data
Alle behøver ikke at lave denne del, som er temmelig teknisk. Det er nok, at én person gør det, og derefter deler filerne med alle.

* Hent denne fil med et [Python-script](Det_lyder_som_dig_filer/Det_lyder_som_dig_Python.zip).

* Åbn siden [Google Colab](https://colab.research.google.com/) og log ind med en gmail.

* Importér Python-scriptet i Colab (vælg \"Fil\" $\rightarrow$ \"Upload notesbog\").

* Vælg \"Kør alle\" på siden:

  * Undervejs skal du vælge lydfilerne med jeres oplæsninger (det er i boksen, hvor der står \"# 1) Multi-upload af lydfiler...\", og du skal måske scrolle lidt ned, før den kommer til syne). Når det er gjort, tager det noget tid at uploade filerne.

  * Når filerne er hentet, skal du vælge, om du vil gemme de små 3 sekunders lydklip, så du kan høre dem hver for sig. Sig ja til det.

  * Afhængigt af, hvordan din Colab er sat op, kan det være lidt forskelligt, hvordan du får gemt de 3 filer: \"testdata.csv\", \"trainingdata.csv\" og en zip filen \"segments_export.zip\" med de små lydklip. Måske du bliver bedt om at tillade, at Colab gemmer flere filer direkte på din computer. Alternativt ender de 3 filer måske under \"Filer\" ikonet ude til venstre i Colab, hvorfra du kan downloade dem.

* Sørg for, at de 3 filer er tilgængelige for alle de øvrige.

:::

Nu bliver det spændende, om Orange kan lave en model, som kan kende forskel på jeres stemmer, eller om nogen af jer lyder for ens.


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 6: Model i et kunstigt neuralt netværk for jeres egne data

* Indlæs både trænings- og testdata fra jeres egne lydfiler i Orange.

* Kig på \"Confusion Matrix\" og \"Predictions -- misclassified\" både for \"Test on train data\" og \"Test on test data\". 

* Lyt til de lydklip, som modellen misklassificerer.

:::  

Indtil nu har vi talt meget lidt om det kunstige neurale netværk. Vi vil på ingen måde gå i detaljer, men dog se en lille smule på det. I det kunstige neurale netværk er der i første omgang 2 skjulte lag med hver 5 neuroner. Uden at forstå detaljerne, kan vi gøre modellen mere kompliceret, og dermed måske bedre, ved at øge antal neuroner i hvert lag eller ved at lave flere skjulte lag.

Desuden kan vi også beslutte, hvor lang tid Orange må bruge på at justere de mange vægte i modellen, så den passer bedst muligt til vores data. Dette kaldes for **iterationer**. Flere iterationer tager længere tid, men vil også ofte give et bedre resultat.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 7: Modellens størrelse og antal iterationer

* Klik på \"Neural Network\" i modellen i Orange og afprøv forskellige værdier for antal neuroner og antal skjulte lag. Skriv for eksempel under \"Neurons in hidden layers\": 10,10,10 for 3 lag med 10 neuroner i hver. Klik på \"Test and Score\" og vælg \"Test on test data\". Hvad giver en lav klassifikationsnøjagtighed (CA)? Hvad giver en høj klassifikationsnøjagtighed (CA)?

* Afprøv også både mindre og større værdier for antal iterationer (det vælges i \"Neural Network\" under \"Maximal number of iterations\"). Se igen på, hvad der giver lav eller høj klassifikationsnøjagtighed (CA).

* Er der stadig lydklip, som misklassificeres i den af dine modeller, som har bedst klassifikationsnøjagtighed (CA)? Lyt i så fald til disse lydklip.

:::  

Tillykke -- du kender nu nogle af de begreber, som indgår i kunstige neurale net, der er en meget vigtig del af AI, og du har en idé om, hvordan de anvendes i Orange. 

Hvis du vil se, hvordan Orange kan gøre noget tilsvarende med billeder, og undervejs lære lidt om matematikken bagved, så kan du fortsætte med [Overvågning i Monitobian](../undervisningsforlob/kNN_forlob_overvaagning.qmd).

Hvis du får lyst til at vide meget mere (og noget sværere), er følgende længere forløb et rigtigt godt sted at fortsætte: [Langt forløb om kunstig neuroner](../undervisningsforlob/kunstige_neuroner_langt_forlob/kunstige_neuroner_forside.qmd).