---
title: "Det lyder som dig!"
description-meta: "Vi vil undersøge, hvordan AI kan genkende personer ud fra ganske få sekunders lydklip"
image: ""
categories:
  - C-niveau
  - Kort
---


::: {.callout-caution collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug
- Lineær regression


**Tidsforbrug:** 90-120 minutter.

:::

::: {.purpose}

### Formål
Vi vil sammen undersøge, hvor godt og hvordan AI kan genkende personer ud fra ganske få sekunders lydklip. Konkret med brug af lydklip med dig og dine klassekammerater.

Undervejs vil du blive introduceret til mange af de centrale begreber om træning af de såkaldte neurale netværk, som AI gør brug af. Dette sker bl.a. med programmet Orange.

:::

## Den grimme ælling - oplæst af 4 forskellige personer

Inden vi kommer til jeres egne data, vil jeg gerne vise jer hele processen med brug af vores data. Til det formål har tre af mine kolleger og jeg alle læst den samme tekst op. Du kan høre lydfilerne fra vores oplæsning her: [Malene](Det_lyder_som_dig_filer/Malene.M4A), [Lisbeth](Det_lyder_som_dig_filer/Lisbeth.M4A), [Ege](Det_lyder_som_dig_filer/Ege.M4A) og [Jan](Det_lyder_som_dig_filer/Jan.M4A).

Da lydfilerne er 2-2,5 minutter lange, men vi vil arbejde med meget korte lydfiler, opdeles hver lydfil i sekvenser af 3 sekunder. Du kan høre disse små sekvenser her: [korte sekvenser](Det_lyder_som_dig_filer/sekvenser.zip). 

### Neurale netværk

**Neurale netværk** arbejder ikke direkte på lyd, som mennesker gør - men på såkaldte **features**. I denne sammenhæng skal vi bruge oplæserens navn, et ID og en række tal, som beskriver frekvenser m.m. i lydfilen på en smart måde. Da det er fysik og ganske svært, vil vi ikke her for alvor gå i dybden, men måske du har haft eller får noget i fysik om analyse af lyd og svingninger. Du får dog lige en kort introduktion til, hvad det er.

Det vi skal bruge hedder **Mel-Frequency Cepstral Coefficients (MFCC)**, som er en metode, der bruges til at analysere og repræsentere lydsignaler, især tale og musik, ved at fange de vigtigste egenskaber ved lydsignalet, som det opfattes af det menneskelige øre. Mennesker opfatter lyd ikke-lineært med frekvens – vi er mere følsomme over for forskelle i lave frekvenser (f.eks. mellem 200 og 400 Hz) end i høje (f.eks. mellem 5000 og 5200 Hz).
MFCC tager højde for dette ved at bruge en Mel-skala, som efterligner, hvordan øret opfatter frekvenser. Detaljerne er på SRP niveau, men hvis du har lyst til at vide mere, så kan du læse her: [MFCC](https://www.geeksforgeeks.org/nlp/mel-frequency-cepstral-coefficients-mfcc-for-speech-recognition/?utm_source=chatgpt.com)  

Jeg har i forvejen lavet disse MFCC data for hver af de små lydsekvenser. Jeg har desuden delt op i **træningsdata**, som vi vil bruge til at træne et neuralt netværk, og **testdata**, som efterfølgende skal teste, hvor god modellen i det neurale netværk er blevet. Du kan se disse data her: [MFCC data](Det_lyder_som_dig_filer/MFCC_data.zip)

Til træningsdata har jeg brugt 75% af lydsekvenserne for hver af de 4 oplæsere, mens de resterende 25% er anvendt til testdata.

### Sammenligning med lineær regression
Det at træne et neuralt netværk minder om at lave lineær regression, som du kender.

I lineær regression har man en række punkter - det er træningsdata.

Hvert punkt består af et x-koordinat og et y-koordinat - x-koordinaten er en **feature**, og der er kun den ene feature, og y-koordinaten er **target**.

I lineær sammenhæng er modellen $y=a\cdot x+b$ ^[Hvis det skal være helt korrekt er der også et normalfordelt støjled $N(\mu, \sigma)$, men det undlader man som regel at tage med på gymnasieniveau], og det gælder om at bestemme $a$ og $b$, så modellen for featureværdierne ($x$-koordinaterne) samlet set bedst muligt rammer targetværdierne ($y$-koordinaterne). I forhold til et neuralt netværk vil $a$ og $b$ kaldes for **vægte**.

I et neuralt netværk er modellen betydeligt mere kompliceret og der er mange flere vægte end de 2 i lineær regression (i store netværk kan der være milliarder af vægte), men ideen er den samme. Man skal også i neurale netværk bestemme vægtene i modellen, så den for featureværdierne samlet set bedst muligt rammer targetværdierne. 

I lineær regression skal modellen komme så tæt på targetværdien $y$ for hver featureværdi $x$. I vores tilfælde er det anderledes, da vi ikke skal tæt på en talværdi, men i stedet skal vælge mellem de 4 personer - dette kaldes for **klassifikation**. I lineær regression handler det således om, at linjen (modellen) kommer tæt på punkterne, mens det i klassifikation handler om, at modellen skal forudsige den rigtige oplæser for så mange som muligt af de små lydklip. Man taler derfor om **klassifikationsnøjagtigheden (CA)**, som er andelen af korrekte forudsigelser. Så hvis CA=0.85, har modellen korrekt forudsagt 85% af targetværdierne og dermed 15% forkert.

### Det neurale netværk i Orange
Nok snak om teori - vi skal have det afprøvet i praksis.

Vi vil anvende programmet [Orange Data Mining](https://orangedatamining.com/) til at træne og teste et neuralt netværk på MFCC data for lydfilerne.

Lav opgave 1 - hvis noget driller, kan du måske få hjælp ved at se denne video (video mangler).


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 1: Installér og afprøv Orange
* Installér Orange

* Åbn denne fil [Model i Orange](Det_lyder_som_dig_filer/Det_lyder_som_dig_Orange.zip). Den indeholder stukturen til at lave modellen.

* Hvis du ikke allerede har hentet [MFCC data](Det_lyder_som_dig_filer/MFCC_data.zip) , så hent dem og pak zip-filen ud, så du har adgang til filerne med træningsdata og testdata.

* Klik på **Træningsdata** i Orange og importer filen med træningsdata. 

* Sæt ID til meta (modellen skal ignorere denne værdi), sæt category (Marlene, Lisbeth, Ege eller Jan) til target, og sæt de 13 MFCC data til feature (det står de nok allerede som). 

* Gør det samme for **Testdata** i Orange.
:::


Dermed er data klar i Orange, og modellen i det neurale netværk er faktisk også trænet. Men hvor god er modellen blevet?

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 2: Undersøg modellen

* Klik på **Test and Score** i Orange. Hvad er klassifikationsgraden (CA)? 

* Under Test and score kan man ude til venstre vælge forskellige måder at teste på. Sørg for, at den indtil videre står på "test on train data".

* Klik på **Confusion matrix** for at se, hvilke input, der er klassificeret korrekt, og hvilke der er klassificeret forkert. 

* Sørg så for, at både Predictions og Probabilities er valgt ude til venstre og klik så på Select Misclassified i bunden. 

* Klik derefter på **Predictions - misclassified** ude til højre i Orange. Her kan du for hver af de misklassificerede lydfiler se filens ID, hvem der har lavet lydfilen, hvem modellen fejagtigt mener, det skal være, og procenter for, hvor meget modellen tror, at filen er fra hver af de 4 muligheder. Der er lige nu kun ét misklassificeret klip. 

* Hvis du ikke allerede har hente de små 3 sekunders lydklip, så gør det nu. [korte sekvenser](Det_lyder_som_dig_filer/sekvenser.zip). 

* Find det misklassificerede klip og lyt til det.

:::

Når vi før valgte at teste på træningsdata, er det på en måde lidt snyd. Det betyder, at vi først lader modellen træne på træningsdata, og derved tilpasse de mange vægte i modellen ^[Der er 124 vægte i dette neurale netværk, så det er betydeligt flere end i den lineære model, men stadig ikke mange, når man taler om neurale netværk] så den passer bedst muligt til disse data - og så derefter at se, hvor godt modellen passer til præcis samme data.

Vi bør i stedet træne modellen på træningsdata, for derefter at teste på nogle helt seperate data, som derfor kaldes for testdata. Derved "snyder" vi ikke længere, da modellen faktisk skal forudsige targetværdier ud fra featureværdier, den ikke brugte til at træne på.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 3: Test på testdata
* Vælg i stedet under **Test and Score** at teste på testdata. Hvad bliver klassifikationsgraden (CA) nu? Det vil som regel blive ringere end ved test på træningsdata.

* Se igen på Confusion Matrix og på Predictions - Misclassified, hvor der nu er flere fejl. 

* Hør lydklip for de misklassificerede.

:::

## Jeres egne data
Det bliver naturligvis først rigtigt interessant - og sværere for modellen - hvis der er flere end 4 oplæsere. 

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 4: Alle læser samme tekst op.
* Jeres lærer vælger en tekst, som I alle skal læse op. Mindst 2 minutter, og gerne længere. ^[I kan f.eks. vælge at læse forordet i jeres matematikbog, hvis det er passende i længde og indhold. Så slår I to fluer med ét smæk]

* Optag en lydfil på din telefon eller din computer (vigtigt, ikke en video, kun lyd)
 ^[Opgaven kan evt. være lavet hjemme i forvejen som en lektie, da det kan være upraktisk, hvis en hel klasse skal læse op samtidig!]
 
* Giv filen med din egen oplæsning dit navn.

* Saml alle lydfilerne på samme computer.

* Filerne skal være i wav, flac eller m4a format. ^[Filerne vil typisk være på 2-4 MB hver, men kan godt med nogle optagere være lavet i højere kvalitet, så filen er ca. 10 MB. Hvis filen er tæt på 100 MB, har man nok i stedet fået lavet en video, hvilket ikke var meningen.Små filer gør det næste skridt hurtigere.]

:::

Som det næste skal I have opdelt lydfilerne i små sekvenser på 3 sekunder, og derefter lavet MFCC data til hver sekvens. Det bliver lidt teknisk, da I skal bruge et Python-script ^[Python er et meget anvendt programmeringssprog.]  i Googles Colab ^[Colabs er en gratis side online, som kan bruges til Python scripts, så man ikke selv skal igennem en lidt besværlig installation af Python.], men I kan heldigvis gøre det sammen. Hvis der blandt jer er en Python ekspert, kan I også bruge koden på en computer lokalt, hvilket er noget hurtigere.

(endnu en lille video, som ikke er lavet endnu)

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 5: Generer MFCC data
Alle behøver ikke at lave denne del, som er temmelig teknisk - det er nok, at én person gør det, og derefter deler fierne med alle.

* Hent denne fil med et [Python-script](Det_lyder_som_dig_filer/Det_lyder_som_dig_Python.zip)

* Åbn siden [Google Colab](https://colab.research.google.com/) og log ind med en gmail.

* Importér Python-scriptet i Colab.

* Vælg **Kør alle** på siden.

* Undervejs skal du vælge lydfilerne med jeres oplæsninger. Når det er gjort, tager det noget tid med at uploade filerne.

* Når filerne er hentet, skal du vælge, om du vil gemme de små 3 sekunders lydklip, så du kan høre dem hver for sig. Sig ja til det.

* Afhængigt af, hvordan din Colab er sat op, kan det være lidt forskelligt, hvordan du får gemt de 3 filer - testdata, trainingdata og en zip fil med de små lydklip. Måske du bliver bedt om at tillade, at Colab gemmer flere filer direkte på din computer. Alternativt ender de 3 filer måske under Filer ikonet ude til venstre i Colab, hvorfra du kan downloade dem.

* Sørg for, at de 3 filer er tilgængelige for alle de øvrige.

:::

Nu bliver det spændende, om Orange kan lave en model, som kan kende forskel på jeres stemmer, eller om nogen af jer lyder for ens.


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 6: Model i et neuralt netværk for jeres egne data

* Indlæs både trænings- og testdata fra jeres egne lydfiler i Orange.

* Kig på Confusion Matrix og Predictions - misclassified både for test on train data and test on test data. 

* Lyt til de lydklip, som modellen misklassificerer.

:::  

Indtil nu har vi talt meget lidt om det neurale netværk. Vi vil på ingen måde gå i detaljer, men dog se en lille smule på det. I det neurale netværk er der i første omgang 2 skjulte lag med hver 5 neuroner. Uden at forstå detaljerne, kan vi gøre modellen mere kompliceret, og dermed måske bedre, ved at øge antal neuroner i hvert lag eller ved at lave flere skjulte lag.

Desuden kan vi også beslutte, hvor lang tid Orange må bruge på at justere de mange vægte i modellen, så den passer bedst muligt til vores data. Dette kaldes for **iterationer**. Flere iterationer tager længere tid, men vil også ofte give et bedre resultat.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 7: Modellens størrelse og antal iterationer

* Klik på **Neural Network** i Orange.

* Afprøv forskellige tal for antal neuroner og antal skjulte lag (skriv f.eks. 10,10,10 for 3 lag med 10 neuroner hver) både med test on train data og test on test data. Hvad giver en lav klassifikationsgrad? Hvad giver en høj klassifikationsgrad?

* Afprøv også både mindre og større værdier for antal iterationer. Se igen på, hvad der giver lav eller høj klassifikationsgrad.

* Er der stadig lydklip, som misklassificeres i din model med størst klassifikationsgrad? Lyt i så fald til disse lydklip.

:::  

Tillykke - du kender nu nogle af de begreber, som indgår i neurale net, der er en meget vigtig del af AI, og har en ide om, hvordan de anvendes i Orange. Hvis du får lyst til at vide meget mere (og noget sværere), er følgende længere forløb et rigtigt godt sted at forsætte [Kunstig neuron](../undervisningsforlob/kunstige_neuroner_langt_forlob/kunstige_neuroner_forside.qmd).