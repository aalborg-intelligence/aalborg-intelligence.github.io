[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI - Aalborg Intelligence",
    "section": "",
    "text": "Her p√• sitet finder du materiale og undervisningsforl√∏b om kunstig intelligens rettet mod danske gymnasieelever.\n\n\n\n\n\n\n\n\n\n\nUndervisningsforl√∏b\n\n\nForskellige undervisningsforl√∏b til matematik i gymnasiet, som inddrager AI. Der findes forl√∏b til b√•de A-, B- og C-niveau.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaterialer\n\n\nNoter om diverse AI relaterede emner.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRO\n\n\nId√©er til hvordan AI kan inddrages i SRO.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRP\n\n\nId√©er til hvordan AI kan inddrages i SRP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI apps\n\n\nDiverse apps til tr√¶ning af kunstig intelligens.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferencer\n\n\nDiverse referencer til andre AI materialer.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nProjektet ‚ÄúAI - Aalborg Intelligence‚Äù er finansieret af Novo Nordisk Fonden og l√∏ber frem til 2026.\nSiden her er i et tidligt stadie med ganske f√• tilg√¶ngelige materialer p√• nuv√¶rende tidspunkt (2023), og mere materiale tilf√∏jes l√∏bende."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html",
    "title": "Hvem ligner du mest?",
    "section": "",
    "text": "Foruds√¶tninger og tidsforbrug\n\n\n\n\n\nForl√∏bet kr√¶ver kendskab til:\n\nAfstand mellem to punkter\n\nTidsforbrug: Ca. 2 x 90 minutter."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#introduktion",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#introduktion",
    "title": "Hvem ligner du mest?",
    "section": "Introduktion",
    "text": "Introduktion\nVi har apps p√• vores telefoner eller computere, som ud fra et billede kan genkende personer eller fra nogle f√• strofer kan genkende en sang. Der er scannere i lufthavne og andre steder, som kan genkende farlige ting, og biler har autopiloter, der selv holder afstanden til forank√∏rende. Vi har ogs√• apps, som g√•r den anden vej, og forvr√¶nger et billede af en person, s√• personen bliver sv√¶r at genkende, men ofte alligevel kan genkendes, selvom ansigtet er fordrejet.\nS√• alle steder og hele tiden foreg√•r der bevist eller ubevist en skelnen mellem forskellige kategorier, men hvordan foreg√•r denne skelnen i grunden? Hvis vi skulle svare fyldestg√∏rende p√• dette sp√∏rgsm√•l, om overhovedet muligt, ville det nok betyde et langt studie p√• universitetet og sikkert mere end dette, men lad os starte med et meget simpelt eksempel, og tage den derfra.\nVi f√•r brug for din viden om afstande mellem punkter, men kommer ogs√• til senere at se p√• andre former for afstande. Det er en fordel af lave opgaverne i grupper, da der kan blive en del af diskutere."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#video-hvem-ligner-du-mest",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#video-hvem-ligner-du-mest",
    "title": "Hvem ligner du mest?",
    "section": "VIDEO: Hvem ligner du mest?",
    "text": "VIDEO: Hvem ligner du mest?\nI denne video gives en kort introduktion til forl√∏bet."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-1-afstand-mellem-to-punkter",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-1-afstand-mellem-to-punkter",
    "title": "Hvem ligner du mest?",
    "section": "Case 1 ‚Äì Afstand mellem to punkter",
    "text": "Case 1 ‚Äì Afstand mellem to punkter\n\n\n\n\n\n\nOpgave 1\n\n\n\n\n\nP√• figur¬†1 nedenfor er der \\(10\\) r√∏de punkter, \\(10\\) bl√• punkter og \\(10\\) gr√∏nne punkter ‚Äì samt et enkelt gr√•t punkt \\(P\\) i midten. Et af de bl√• punkter er s√¶rligt markeret, men da det ligger ret langt fra \\(P\\) b√∏r det nok ikke betyde s√• meget for, hvilket farve \\(P\\) skal have, som de punkter, der ligger t√¶ttere p√• \\(P\\)\nVurd√©r ud fra de √∏vrige punkter i n√¶rheden af \\(P\\), om du synes, at \\(P\\) b√∏r v√¶re r√∏dt, bl√•t eller gr√∏nt s√• det mest ligner sine naboer.\n\n\n\n\n\n\n\n\n\nFigur¬†1: Koordinatsystem med \\(10\\) r√∏de punkter, \\(10\\) bl√• punkter og \\(10\\) gr√∏nne punkter ‚Äì samt et enkelt gr√•t punkt \\(P\\).\n\n\n\n\n\n\n\n\n\nOpgave 2\n\n\n\n\n\nP√• figur¬†2 er der andre \\(31\\) punkter, samt en cirkel med radius \\(10\\) omkring det gr√• punkt \\(P(15,15)\\).\nT√¶l hvor mange r√∏de, bl√• og gr√∏nne punkter, der ligger inde i cirklen. Kan det bruges til at beslutte, hvilken farve det gr√• punkt b√∏r have?\n\n\n\n\n\n\n\n\n\nFigur¬†2: Koordinatsystem med \\(31\\) punkter, samt en cirkel med radius \\(10\\) omkring det gr√• punkt \\(P(15,15)\\)\n\n\n\n\n\n\n\n\n\nOpgave 2, fortsat\n\n\n\n\n\nHvilket resultat giver det, hvis cirklens radius kun var halvt s√• stor? Brug app‚Äôen nedenfor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpgave 3\n\n\n\n\n\nDet g√•r nok ikke i l√¶ngden blot at ville vurdere p√• √∏jem√•l, s√• du m√• til at regne lidt. I tabellen herunder er koordinater og farver p√• de \\(10\\) punkter, der ligger t√¶ttest p√• det gr√• punkt i opgave 2.\nUdregn afstanden fra det gr√• punkt \\(P(15,15)\\) til hver af disse \\(10\\) punkter.\nAfg√∏r s√•, hvor mange af hver farve, der ligger indenfor en cirkel med radius \\(5\\) omkring det gr√• punkt. Det er nok specielt det bl√• punkt lige p√• kanten af cirklen, hvor beregningen er vigtig, men hvis en computer skal lave arbejdet automatisk, vil den jo beregne alle afstandene, da den ikke bare kan ‚Äúkigge p√• figuren‚Äù, som et menneske kan.\nHvilken farve tyder det p√•, at det gr√• punkt b√∏r have?\nTabellens data er punkter i kommatal. F.eks. er det f√∏rste bl√• punkt \\((13,8 ; 19,9)\\), s√• det har x-koordinaten \\(13,8\\) og y-koordinaten \\(19,9\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFarve\n1\n2\n3\n4\n\n\n\n\nBl√•\n\\((13,8 ; 19,9)\\)\n\\((8,2 ; 14,9)\\)\n\\((16,4 ; 14,1)\\)\n\\((15,5 ; 13,1)\\)\n\n\nR√∏d\n\\((10,6 ; 16,0)\\)\n\\((16,3 ; 15,2)\\)\n\\((15,6 ; 11,3)\\)\n\n\n\nGr√∏n\n\\((11,1 ; 18,6)\\)\n\\((16,4 ; 17,5)\\)\n\\((21,7 ; 13,4)\\)\n\n\n\n\n\n\n\n\n\n\nOpgave 4\n\n\n\n\n\nOvervej, hvad der sker, hvis cirklens radius v√¶lges som en meget lille v√¶rdi eller som en meget stor v√¶rdi i forhold til at bruge metoden til at bestemme, hvilken farve det gr√• punkt b√∏r have. Kan det give problemer?\n\n\n\n\n\n\n\n\n\nOpgave 5\n\n\n\n\n\n\nI app‚Äôen herunder er \\(27\\) andre punkter indsat i et koordinatsystem, og der er tegnet en cirkel med centrum i det gr√• punkt \\(P(15,15)\\). √Ündr p√• radius af cirklen og se, om det g√∏r en forskel.\nDiskut√©r, hvilken radius, der er bedst.\n\n\n\n\n\n\n\nAfslutning p√• case 1\nDet bliver nok klart, at der er brug for en metode til at beslutte, hvor stor radius skal v√¶re for at f√• det bedste resultat. Kort og lidt simpelt forklaret, involverer det noget, som man kalder tr√¶ningsdata og testdata. Man har f.eks. 1000 punkter, som man kender farven p√•. Man lader s√• som om, at man ikke kender farven p√• f.eks. 200 af punkterne (som man s√• kalder testdata). S√• bruger man de √∏vrige 800 punkter (tr√¶ningsdata) til at forudsige farven af hver af de 200 punkter i testdata. Dette g√∏r man for forskellige v√¶rdier af radius, hvorefter man v√¶lger den radius, der forudsiger flest af de 200 punkters farve korrekt. Hvis f.eks. radius 5 forudsiger 121 punkters farve korrekt, radius 10 furudsiger 135 punkters farve korrekt, og radius 20 kun forudsiger 87 punkters farve korrekt, s√• har radius 10 jo klaret sig bedst."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-2-manhattan-afstand",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-2-manhattan-afstand",
    "title": "Hvem ligner du mest?",
    "section": "Case 2 ‚Äì Manhattan afstand",
    "text": "Case 2 ‚Äì Manhattan afstand\nI nogle situationer giver den almindelige afstand mellem punkter ikke s√• god mening. F.eks. er de fleste veje p√• Manhattan i New York enten nord-syd eller √∏st-vest, s√• man kan ikke bare g√• eller k√∏re ‚Äúp√• skr√•‚Äù, men kun lodret eller vandret.\nHvis vi ser p√• punkterne \\(P(2,3)\\) og \\(Q(5,7)\\), s√• er den almindelige afstand \\(5\\) vha. Pythagoras, mens Manhattan afstanden er \\(3+4=7\\). Dette er illustreret p√• figur¬†3.\n\n\n\n\n\n\nFigur¬†3: Den almindelige afstand mellem punkterne \\(P(2,3)\\) og \\(Q(5,7)\\) er \\(5\\), mens Manhattan afstanden er \\(7\\).\n\n\n\n\n\n\n\n\n\nOpgave 6\n\n\n\n\n\nIndtegn punkterne \\(A(1,7)\\), \\(B(3,4)\\) og \\(C(5,6)\\) i et koordinatsystem.\nUdregn b√•de almindelig afstand og Manhattan afstand mellem hvert par af punkter.\n\n\n\n\n\n\n\n\n\nOpgave 7\n\n\n\n\n\nI eksemplet og i opgave 6 var Manhattan afstanden st√∏rre end den almindelige afstand, men kan Manhattan afstanden v√¶re mindre end den almindelige afstand eller kan de to afstande v√¶re ens?\n\n\n\n\n\n\n\n\n\nOpgave 8\n\n\n\n\n\nSe p√• figur¬†4 fra opgave 2 igen.\nPunkterne inde i cirklen har en almindelig afstand p√• under \\(10\\) til det gr√• punkt, men hvilke af punkterne har en Manhattan afstand p√• under \\(10\\) til det gr√• punkt?\nDen almindelige afstand giver en cirkel med det gr√• punkt i centrum, men hvilken figur giver Manhatten afstand omkring det gr√• punkt?\nHvilken farve b√∏r det gr√• punkt derfor have, hvis Manhattan afstanden benyttes? Giver det samme resultatet, som du fik i opgave 2?\n\n\n\n\n\n\n\n\n\nFigur¬†4: Koordinatsystem med \\(31\\) punkter, samt en cirkel med radius \\(10\\) omkring det gr√• punkt \\(P(15,15)\\)\n\n\n\n\n\n\n\n\n\nOpgave 9\n\n\n\n\n\nOpstil en formel for den almindelige afstand mellem to punkter med koordinaterne \\((x_1, y_1)\\) og \\((x_2, y_2)\\). Det er en formel, du kender i forvejen.\nOpstil tilsvarende en formel for Manhattan afstanden. Det er nok ikke en formel, du har set f√∏r.\nDu kan l√¶se mere om Manhattan afstanden her."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-3-hvor-ens-er-to-tekster",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-3-hvor-ens-er-to-tekster",
    "title": "Hvem ligner du mest?",
    "section": "Case 3 ‚Äì Hvor ens er to tekster?",
    "text": "Case 3 ‚Äì Hvor ens er to tekster?\nI forbindelse med at unders√∏ge om en tekst, f.eks. en dansk stil, er plagiat, bliver det relevant at sammenligne, hvor ens to tekster er. Helt s√• avanceret bliver det dog ikke her.\nVi vil kun se meget simpelt p√• ord med \\(5\\) bogstaver, og hvordan man f.eks. kan m√•le afstande mellem forskellige ord. Vi vil se p√• alle kombinationer af \\(5\\) bogstaver, ogs√• f.eks. xtmsp, selvom de ikke er normale ord.\n\n\n\n\n\n\nOpgave 10\n\n\n\n\n\nI tabellen herunder ses ordet ‚Äúnedes‚Äù sammen med ordene ‚Äúmodel‚Äù, ‚Äúmetal‚Äù og ‚Äúnudts‚Äù.\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\ne\nd\ne\ns\n\n\n\n\n\n\nm\no\nd\ne\nl\n\n\n\n\nm\ne\nt\na\nl\n\n\n\n\nn\nu\nd\nt\ns\n\n\n\n\nHvilket af ordene ‚Äúmodel‚Äù, ‚Äúmetal‚Äù og ‚Äúnudts‚Äù synes du, at ordet ‚Äúnedes‚Äù ligner mest. Begrund dit svar.\n\n\n\n\n\n\n\n\n\nOpgave 11\n\n\n\n\n\nHvis vi v√¶lger, at afstanden mellem to ord er antallet af bogstaver, som er forskellige incl.¬†placering, s√• er afstanden mellem ‚Äúxtmsp‚Äù og ‚Äúxmtsq‚Äù \\(3\\), da kun \\(2\\) af de \\(5\\) bogstaver matcher incl.¬†placering i de to ord, nemlig x og s.\nUdregn med den metode afstanden mellem ‚Äúnedes‚Äù og hver af de \\(3\\) ord i opgave 10. Var det s√•dan du allerede havde gjort det i opgave 10, eller gav dette et andet resultat?\n\n\n\n\n\n\n\n\n\nOpgave 12\n\n\n\n\n\nOvervej og diskuter andre m√•der at regne afstand mellem to ord p√• hver \\(5\\) bogstaver. Det kunne f.eks. v√¶re noget, hvor ombytning af to nabobogstaver giver mindre afstand end helt tilf√¶ldige andre bogstaver, s√• f.eks. ‚Äúkolon‚Äù og ‚Äúkloon‚Äù er t√¶ttere p√• hinanden end ‚Äúkolon‚Äù og ‚Äúkston‚Äù."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-4-dna-strenge-og-alignment",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-4-dna-strenge-og-alignment",
    "title": "Hvem ligner du mest?",
    "section": "Case 4 ‚Äì DNA-strenge og alignment",
    "text": "Case 4 ‚Äì DNA-strenge og alignment\nUden i √∏vrigt at komme ind p√• biologien repr√¶senteres DNA som meget lange tekststrenge. N√•r mennesker og chimpanser er meget ens, kommer det til udtryk ved, at DNA-strengen for et menneske ligner den for en chimpanse meget, der er alts√• en kort afstand mellem DNA for et menneske og DNA for en chimpanse. Indenfor biologien kaldes dette for alignment. I stedet for at sammenligne p√• DNA niveau, sammenlignes ogs√• nogle gange p√• aminosyre niveau, hvilket vi vil bruge her,\nF√∏lgende eksempel, der viser et meget lille udsnit af s√•danne koder fra mus, rotter, mennesker og g√¶r er taget fra Tema12-Link5.pdf (nucleus.dk), der kan anbefales, hvis man √∏nsker at arbejde mere med alignment.\n\n\n\nDyr\nKode\n\n\n\n\nMus\nS W A W A E G W T R Y G P\n\n\nRotte\nK W V W A E G W T R Y G P\n\n\nMenneske\nA W A W A E G W T R Y G P\n\n\nG√¶r\nE W L R K P G W V K Y V P\n\n\n\nHvis afstanden her regnes som antal bogstaver, der er forskellige, ses det at afstanden mellem mus og rotte er p√• \\(2\\), som vist nedenfor.\n\n\n\nDyr\nKode\n\n\n\n\nMus\nS W A W A E G W T R Y G P\n\n\nRotte\nK W V W A E G W T R Y G P\n\n\n\n\n\n\n\n\n\nOpgave 13\n\n\n\n\n\nUdregn p√• tilsvarende vis afstandene mellem mus-menneske, mus-g√¶r, rotte-menneske, rotte-g√¶r og menneske-g√¶r.\n\n\n\nN√•r resultatet sikkert virker overraskende, skyldes det, at vi kun har set p√• et meget lille udsnit af DNA for de fire. I figur¬†5 har man set p√• hele det protein, som udsnittet stammer fra, og her bliver resultatet mere som forventet.\n\n\n\n\n\n\nFigur¬†5: De faktiske afstande mellem mus-menneske, mus-g√¶r, rotte-menneske, rotte-g√¶r og menneske-g√¶r."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-5-hvilken-politiker-er-du-mest-enig-med",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-5-hvilken-politiker-er-du-mest-enig-med",
    "title": "Hvem ligner du mest?",
    "section": "Case 5 ‚Äì Hvilken politiker er du mest enig med?",
    "text": "Case 5 ‚Äì Hvilken politiker er du mest enig med?\nOp til b√•de folketingsvalg og kommunal- og regionalvalg kan man svare p√• en r√¶kke sp√∏rgsm√•l, hvorefter ens svar bliver sammenlignet med politikernes svar p√• de samme sp√∏rgsm√•l. Herefter kan man s√• se, hvem man er mest enig med.\nHer er et eksempel fra kommunal- og regionalvalget i 2021.\nTag kandidattesten Kommunalvalg 2021 - Altinget - Alt om politik: altinget.dk\n\n\n\n\n\n\nOpgave 14\n\n\n\n\n\nKlik p√• linket ovenfor, v√¶lg din egen kommune i testen og besvar sp√∏rgsm√•lene. Se derefter, hvem dine svar er mest enige med, og hvor mange procent enige I er.\n\n\n\nMen hvordan virker det mon? Hvordan vurderes, hvilken kandidat du er mest enig med, og hvordan udregnes, hvor mange procent enige I er?\nTil hvert sp√∏rgsm√•l kan der svares ‚Äúhelt uenig‚Äù, ‚Äúuenig‚Äù, ‚Äúenig‚Äù eller ‚Äúhelt enig‚Äù, men desuden er der en skjult ‚Äúneutral‚Äù mulighed i midten, som man ikke kan v√¶lge.\n\n\n\nHelt uenig\nUenig\nNeutral\nEnig\nHelt enig\n\n\n\nAfstanden mellem to svar regnes som antal ‚Äúfelter‚Äù i tabellen, s√• afstanden mellem Uenig og Enig er \\(2\\), mens afstanden mellem Enig og Helt enig er \\(1\\), og den st√∏rste afstand er \\(4\\).\nI figur¬†6 ses en persons svar og et partis svar p√• \\(23\\) sp√∏rgsm√•l til kommunalvalget i 2021. Ved et partis svar forst√•s det svar, som flest af kandidaterne fra partiet har givet (ved lighed afgjort ud af, hvilken kandidat, der st√•r f√∏rst p√• listen). S√• det er typetallet (typesvaret), der er anvendt for partierne, ikke gennemsnittet af svarene fra partiets kandidater. Figuren er fra testen p√• Altinget. Bem√¶rk, at antallet af sp√∏rgsm√•l kan variere fra kommune til kommune, s√• du har m√•ske f√¶rre eller flere sp√∏rgsm√•l.\n\n\n\n\n\n\nFigur¬†6: En persons svar (sort) og et partis svar (r√∏d) p√• \\(23\\) sp√∏rgsm√•l til kommunalvalget i 2021.\n\n\n\nAfstanden i det f√∏rste sp√∏rgsm√•l er \\(1\\), afstanden i det andet sp√∏rgsm√•l ogs√• \\(1\\) og afstanden i det tredje sp√∏rgsm√•l er \\(2\\) pga. den skjulte ‚Äúneutral‚Äù mulighed i midten.\n\n\n\n\n\n\nOpgave 15\n\n\n\n\n\nDe 3 f√∏rste afstand er alts√• 1, 1 og 2. Udregn afstanden for hver af de √∏vrige \\(20\\) sp√∏rgsm√•l.\nL√¶g s√• afstandene sammen, hvilket svarer til en form for Manhattan afstand, da afstanden regnes for hvert enkelt sp√∏rgsm√•l for sig. Hvilken samlet afstand giver det?\n\n\n\n\n\n\n\n\n\nOpgave 16\n\n\n\n\n\nHvad er den mindst mulig samlede afstand for de \\(23\\) sp√∏rgsm√•l? Hvordan skal svarene for partiet og for v√¶lgeren se ud fra at f√• denne afstand?\nHvad er den st√∏rst mulige afstand, og hvordan skal svarene s√• se ud?\n\n\n\n\n\n\n\n\n\nOpgave 17\n\n\n\n\n\nP√• figur¬†7 ses det, at siden angiver, at enigheden med Socialdemokratiet er p√• \\(79 \\%\\).\nOvervej, hvordan den afstand du udregnede i opgave 15 og den st√∏rste mulige afstand, som du fandt i opgave 16, kan betyde, at enigheden er \\(79 \\%\\).\n\n\n\n\n\n\n\n\n\nFigur¬†7: P√• siden angives det, at enigheden med Socialdemokratiet er p√• \\(79 \\%\\).\n\n\n\n\n\n\n\n\n\nOpgave 18\n\n\n\n\n\nVend tilbage til dine egne svar p√• testen.\nBeregn afstand og procent i forhold til den kandidat, du var mest enig med, og den kandidat, du var mest uenig med.\nBeregn desuden afstand og procent til det parti, du var mest enig med, og til det parti, du var mest uenig med.\nPasser dine udregninger med sidens procenter?\n\n\n\n\n\n\n\n\n\nOpgave 19 (sv√¶r)\n\n\n\n\n\nDen kandidat, som svarene i opgave 15 var mest enig med, giver en procent p√• \\(76 \\%\\), s√• procenten for samtlige kandidater fra Socialdemokratiet er alts√• lavere end procenten for selve partiet.\nDet virker m√•ske umiddelbart underligt. Overvej, hvorfor det faktisk er korrekt ud fra den metode, som siden anvender til beregningerne.\nDiskut√©r derefter, om procenten for partiet kunne v√¶re beregnet anderledes."
  },
  {
    "objectID": "undervisningsforloeb/kNN_forlob_overvaagning.html",
    "href": "undervisningsforloeb/kNN_forlob_overvaagning.html",
    "title": "Overv√•gning i Monitorbian",
    "section": "",
    "text": "Foruds√¶tninger og tidsforbrug\n\n\n\n\n\nForl√∏bet kr√¶ver kendskab til:\n\nKoordinatsystemer\nPunkter og afstande mellem punkter\nProcentregning\n\nTidsforbrug: ca. 90 minutter (uden brug af Orange)."
  },
  {
    "objectID": "undervisningsforloeb/kNN_forlob_overvaagning.html#velkommen-til-monitorbian",
    "href": "undervisningsforloeb/kNN_forlob_overvaagning.html#velkommen-til-monitorbian",
    "title": "Overv√•gning i Monitorbian",
    "section": "Velkommen til Monitorbian",
    "text": "Velkommen til Monitorbian\nLandet Monitorbian √∏nsker at blive en vaske√¶gte overv√•gningsstat! Men efterretningstjenesten i Monitorbian ved meget lidt om overv√•gning. Derfor har de ansat jer som intelligence officerer til at l√∏se denne opgave. Tillykke med jeres nye job! Lad os sm√∏ge √¶rmerne op og komme i gang! üòÑ\nI Monitorbian findes der to forskellige slags indbyggere: Nogle nedstammer fra Anders And, mens andre nedstammer fra Fedtmule. P√• figur¬†1 kan du se, hvordan de forskellige indbyggere ser ud.\n\n\n\n\n\n\nFigur¬†1: Billede af de to slags indbyggere i Monitorbian. Indbyggeren til venstre nedstammer fra Anders And, mens indbyggeren til h√∏jre nedstammer fra Fedtmule.\n\n\n\n\nFeatures\nFor at overv√•ge indbyggerne er vi n√∏dt til at identificere nogle egenskaber ved indbyggerne, som kan bruges til at adskille dem fra hinanden. S√•dan en egenskab kaldes for en feature. En feature kunne for eksempel v√¶re en indbyggers v√¶gt. Det vil v√¶re en god feature, hvis de to forskellige slags indbyggere har forholdsvis forskellig v√¶gt. En anden feature kunne v√¶re √∏jenfarve, men hvis det ikke p√• en eller anden m√•de kan v√¶re med til at skelne de to slags indbyggere fra hinanden, s√• vil √∏jenfarve v√¶re et d√•rlig valg af feature i denne sammenh√¶ng.\n\n\n\n\n\n\nOpgave 1: Features\n\n\n\n\n\nSe p√• billedet i figur¬†1 og find p√• nogle flere features.\n\n\n\n\n\nTr√¶ningsdata\nSom I lige har set i opgave 1, er der rigtig mange egenskaber ved indbyggerne, der kan bruges som features. Men som intelligence officerer er vi n√∏dt til at tr√¶ffe et valg og beslutte os for, hvad vi vil g√• videre med. I har derfor netop v√¶ret til m√∏de i sikkerhedsudvalget, hvor det er blevet besluttet, at h√∏jde (m√•lt i \\(cm\\)) og fodareal (m√•lt i \\(cm^2\\)) er de to features, som I skal arbejde videre med. Disse to features er forholdsvis nemme at scanne, og fremadrettet bliver det derfor s√•dan, at hver gang en indbygger i Monitorbian g√•r ind i en offentlig bygning, s√• bliver vedkommende scannet og h√∏jde og fodareal bliver m√•lt.\nI skal nu have lavet en algoritme1, som kan forudsige, hvilken slags indbygger der er tale om ‚Äì alene baseret p√• viden om en given indbyggers h√∏jde og fodareal. Man siger, at vi gerne vil klassificere indbyggerne ‚Äì her i to klasser: Anders And og Fedtmule.\n1¬†T√¶nk p√• en algoritme som en slags opskrift, som kan bruges til at forudsige hvilken slags indbygger, der er tale om.2¬†Man siger ogs√•, at vi gerne vil pr√¶diktere hvilken slags indbygger, der er tale om.For at g√∏re det har vi brug for tr√¶ningsdata. Tr√¶ningsdata best√•r af en masse data fra forskellige indbyggere, hvor de to features er blevet m√•lt samtidig med, at det for hver indbygger er angivet om vedkommende nedstammer fra Anders And eller fra Fedtmule. Denne sidste oplysning er jo lige pr√¶cis den oplysning, som vi gerne fremadrettet vil kunne forudsige2. I tr√¶ningsdata angiver vi alts√• den v√¶rdi, som vi gerne vil pr√¶diktere. Derfor kalder man ogs√• denne v√¶rdi for en targetv√¶rdi.\n\n\n\n\n\n\nOpgave 2: Tr√¶ningsdata\n\n\n\n\n\nNedenst√•ende viser en tabel med tr√¶ningsdata, men targetv√¶rdien mangler. Angiv targetv√¶rdien:\n\n\n\n\n\n\n\n\n\nIndbygger\nFodareal (\\(cm^2\\))\nH√∏jde (\\(cm\\))\nTargetv√¶rdi\n\n\n\n\n\n197\n123\n\n\n\n\n214\n155\n\n\n\n\n255\n115\n\n\n\n\n297\n96\n\n\n\n\n213\n74\n\n\n\n\n173\n138\n\n\n\n\n272\n115\n\n\n\n\n235\n94\n\n\n\n\n311\n99\n\n\n\n\n334\n116\n\n\n\n\n276\n151\n\n\n\n\n283\n92\n\n\n\n\n234\n132\n\n\n\n\n172\n97\n\n\n\n\n278\n74\n\n\n\n\n241\n75\n\n\n\n\n220\n62\n\n\n\n\n249\n86\n\n\n\n\n138\n96\n\n\n\n\n252\n93\n\n\n\n\n\n\n\n\n\nN√¶rmeste naboer (kNN)\nDer findes en lang r√¶kke af metoder til at lave klassifikation, som er det, vi har brug for her. Nogle af dem bruger meget avanceret matematik og enorme computerkr√¶fter og kan anvendes til diagnosticering af sygdomme, klassificere dokumenter i forskellig typer, genkende objekter i billeder og videoer. Helt s√• avancerede metoder f√•r vi dog ikke brug for her.\nVi vil i stedet fokusere p√• √©n af de simpleste, men alligevel effektive metoder til at klassificere observationer. Metoden kaldes p√• engelsk k-nearest neighbors eller p√• dansk k-n√¶rmeste naboer, og forkortes ofte som kNN. kNN er baseret p√• den simple antagelse, at observationer, som er t√¶t p√• hinanden, ogs√• ligner hinanden. I vores eksempel vil det v√¶re, at indbyggere, som har cirka samme h√∏jde og fodareal, vil vi antage, h√∏rer til i den samme klasse.\nFor at bestemme hvilke naboer, der ligger t√¶t p√• hinanden, er vi n√∏dt til at kunne beregne afstanden mellem to punkter. Du husker nok, at afstanden \\(d\\) mellem punktet \\(P(x_1,y_1)\\) og punktet \\(Q(x_2,y_2)\\) er\n\\[\nd = \\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\n\\tag{1}\\]\nP√• figur¬†2 nedenfor ser I de tr√¶ningsdata, som I selv angav en targetv√¶rdi for i den foreg√•ende opgave. M√•ske var det for nogle af indbyggerne sv√¶rt at afg√∏re oprindelsen alene ved at se p√• billedet ‚Äì og m√•ske var det nemt nok, men her ses i hvert tilf√¶lde den korrekte klassificering3.\n3¬†Man kan forestille sig, at en s√•dan klassificering er baseret p√• yderligere test og unders√∏gelser, som man normalvis ikke vil have til r√•dighed.\n\n\n\n\n\nFigur¬†2: Datas√¶ttet med fodareal ud af \\(x\\)-aksen og h√∏jde op af \\(y\\)-aksen. De r√∏de punkter svarer til Fedtmule-indbyggere, mens de bl√• svarer til Anders And-indbyggere.\n\n\n\n\n\n\n\n\n\nOpgave 3: Afstande\n\n\n\n\n\nBeregn afstanden fra det gr√• punkt til de syv punkter, som er markeret i figur¬†3 herunder. S√∏rg for at skrive de beregnede afstande ned ‚Äì du skal bruge dem senere!\n\n\n\n\n\n\n\n\n\nFigur¬†3: Et udsnit af data med et nyt gr√•t punkt indsat. De r√∏de punkter svarer til Fedtmule-indbyggere, mens de bl√• svarer til Anders And-indbyggere.\n\n\n\nN√•r \\(k\\)-n√¶rmeste naboer bruges til at klassificere en ny indbygger benyttes en flertalsafstemning (ogs√• kaldet majoritetsbeslutning). Det vil sige, at en ny indbygger bliver pr√¶dikteret til at tilh√∏re den klasse, som de fleste af indbyggerens \\(k\\)-n√¶rmeste naboer tilh√∏rer. Hvis for eksempel \\(k=5\\), og vi har en ny indbygger, som vi gerne vil afg√∏re klassen for, s√• ser vi simpelthen p√• de 5 n√¶rmeste naboer til denne indbygger og t√¶ller op, hvilke klasser de tilh√∏rer. Hvis to af dem nedstammer fra Anders And og tre fra Fedtmule, s√• vil en flertalsafstemning sige, at den nye indbygger nok er i Fedtmule-klassen. Man kan komme ud for, at pr√¶cis halvdelen af de \\(k\\) n√¶rmeste naboer nedstammer fra Anders And, mens pr√¶cis den anden halvdel nedstammer fra Fedtmule. I det tilf√¶lde vil vi sige, at klassifikationen er uafklaret.\nDenne id√© vil vi nu bruge.\n\n\n\n\n\n\nOpgave 4: Afstand til ny og ukendt indbygger\n\n\n\n\n\nI figur¬†3 svarer det gr√• punkt til en ny indbygger, som skal klassificeres, og de n√¶rmeste naboer svarer til de punkter, som du lige har beregnet afstanden til. Baseret p√• en flertalsafstemning blandt de fem n√¶rmeste naboer (det vil sige, at \\(k=5\\)) vil du s√• sige, at den nye indbygger stammer fra Fedtmule eller fra Anders And?\n\n\n\n\n\n\n\n\n\nOpgave 5: Flertalsafstemning for forskellige v√¶rdier af \\(k\\)\n\n\n\n\n\nDer er ingen, som siger, at vi skal se p√• de fem n√¶rmeste naboer. Vi kan lige s√• godt se p√• d√©n n√¶rmeste nabo, p√• de to n√¶rmeste naboer eller p√• de tre n√¶rmeste naboer. Vi vil nu se p√•, hvad der sker, hvis vi √¶ndrer p√• antallet af n√¶rmeste naboer \\(k\\).\nSe igen p√• figur¬†3 og de afstande, som du beregnede i opgave 3. Du skal nu for forskellige v√¶rdier af \\(k\\) afg√∏re, om den nye indbygger skal klassificeres som en Fedtmule- eller en Anders And-indbygger.\nUdfyld en tabel som nedenst√•ende (med \"andel\" mener vi den andel, som afg√∏r flertalsafstemningen ‚Äì hvis for eksempel der er tre ud af fire punkter, som er bl√• eller r√∏de, skal andelen s√¶ttes til 3/4).\n\n\n\n\\(k\\)\nBl√•/R√∏d/Uafklaret (pr√¶diktion)\nAndel\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\nValg af \\(k\\) og testdata\nSom du har set ovenfor, vil forskellige valg af \\(k\\) give forskellige resultater. S√• hvordan v√¶lger vi mon den bedst mulige v√¶rdi af \\(k\\)? For at afg√∏re det vil vi opdele vores data i to dele: tr√¶ningsdata og testdata. Det kunne for eksempel v√¶re s√•dan, at af alle de data vi har, s√• bruger vi 80% som tr√¶ningsdata. Det er tr√¶ningsdata, som vi bruger til at lave pr√¶diktionen med. De resterende 20% af data vil vi lade v√¶re testdata, hvor vi bruger testdata til at m√•le, hvor n√∏jagtig vores algoritme er.\nId√©en er nu denne:\n\nVi v√¶lger en v√¶rdi af \\(k\\) ‚Äì det kunne for eksempel v√¶re \\(k=5\\).\nVi ser s√• p√• hver eneste indbygger i testdata og lader som om, at vi ikke kender denne indbyggers oprindelse. Det vil sige, at vi lader som om, at vi ikke kender targetv√¶rdien. Vi vil nu bruge tr√¶ningsdata til at lave en pr√¶diktion for denne indbygger baseret p√• den valgte v√¶rdi af \\(k\\). Men da vi jo i virkeligheden godt kender denne indbyggers oprindelse, s√• f√•r vi nu mulighed for at afg√∏re, om pr√¶diktionen er rigtig eller forkert.\n\nLad os forestille os, at vi har 500 data i alt, og at vi lader 20% af disse v√¶re testdata. Det vil sige, at testdata best√•r af 100 datapunkter. For hvert af disse datapunkter laver vi en pr√¶diktion. S√• enten pr√¶dikterer vi, at datapunktet er bl√•t eller r√∏dt baseret p√• en flertalsafstemning af de \\(k\\) n√¶rmeste naboer i tr√¶ningsdatas√¶ttet. Holder vi denne pr√¶diktion op mod den faktiske v√¶rdi, kan vi opstille en s√•kaldt confusion tabel. Et eksempel p√• en s√•dan ses her:\n\n\n\n\nPr√¶dikteret bl√•\nPr√¶dikteret r√∏d\n\n\n\n\nFaktisk bl√•\n56\n9\n\n\nFaktisk r√∏d\n7\n68\n\n\n\n\nDer er 140 datapunkter i alt, og vi kan her se, at 56 datapunkter blev pr√¶dikteret til at v√¶re bl√• og faktisk ogs√• er bl√•. Tilsvarende er 68 af datapunkterne pr√¶dikteret til at v√¶re r√∏de, mens de faktiske ogs√• er r√∏de. I alt 7+9=16 datapunkter har f√•et pr√¶dikteret en forkert farve sammenlignet med deres faktiske farve. Alts√• kan vi her se, at med den valgte v√¶rdi af \\(k\\) har vores kNN algoritme lavet en fejl i \\[\n\\frac{16}{140} \\approx 11.4 \\%\n\\] af tilf√¶ldene, mens den har pr√¶dikteret korrekt i \\[\n\\frac{56+68}{140} = \\frac{124}{140} \\approx 88.6 \\%\n\\] af tilf√¶ldene. Vi kan nu lave tilsvarende beregninger for forskellige v√¶rdier af \\(k\\) og helt enkelt v√¶lge den v√¶rdi af \\(k\\), som giver den mindste fejlprocent, n√•r vi tester p√• vores testdata.\nVi ser nu igen p√• vores simple datas√¶t, og deler det op i et tr√¶ningsdatas√¶t og et testdatas√¶t. P√• figur¬†4 er testdata markeret med et kryds. Vi vil for forskellige v√¶rdier af \\(k\\) pr√¶diktere farven p√• testdata (samtidig med at vi jo godt kender den faktiske v√¶rdi).\n\n\n\n\n\n\nFigur¬†4: Testdata er markeret med et kryds.\n\n\n\n\n\n\n\n\n\nOpgave 6: Valg af \\(k\\)\n\n\n\n\n\n\nBrug app‚Äôen herunder til at afg√∏re den pr√¶dikterede v√¶rdi af hvert testdata for \\(k=1, 2, 3, 4, 5\\). Du kan for hvert testdatapunkt f√• tegnet en cirkel rundt om (hvor du kan justere p√• radius), som kan hj√¶lpe dig med at finde de n√¶rmeste naboer. Udfyld en tabel som nedenst√•ende (husk at den pr√¶dikterede farve kan v√¶re bl√•, r√∏d eller uafklaret) ‚Äì skriv den enten ned p√• et stykke papir eller brug Pr√¶diktion for forskellige v√¶rdier af k.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestdata\nFaktisk\nPr√¶diktion \\(k=1\\)\nPr√¶diktion \\(k=2\\)\nPr√¶diktion \\(k=3\\)\nPr√¶diktion \\(k=4\\)\nPr√¶diktion \\(k=5\\)\n\n\n\n\n1\nR√∏d\n\n\n\n\n\n\n\n2\nR√∏d\n\n\n\n\n\n\n\n3\nBl√•\n\n\n\n\n\n\n\n4\nBl√•\n\n\n\n\n\n\n\n\n\nUdfyld for hver v√¶rdi af \\(k\\) en confusion tabel. Hvis den pr√¶dikterede farve er uafklaret, skal det t√¶lle som en fejl. Skriv igen ned p√• papir eller brug Confusion tabeller.\nUdregn for hver v√¶rdi af \\(k\\) fejlprocenten. Hvilken v√¶rdi af \\(k\\) giver den mindste fejlprocent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonusopgave (sv√¶r og kan springes over): Hvilke muligheder er der?\n\n\n\n\n\nSe p√• tabellen i opgave 5 Flertalsafstemning for forskellige v√¶rdier af \\(k\\)  ovenfor.\n\nF√•r man altid den samme pr√¶diktion for alle v√¶rdier af \\(k\\)?\n\nVi forestiller os nu, at vi har et nyt datas√¶t, som vi ikke har lavet beregninger p√•.\n\nEr det muligt, at der kommer til at st√• bl√• ved \\(k=1\\) og samtidig r√∏d ved \\(k=2\\)?\nFor hvilke v√¶rdier af \\(k\\) kan der st√• uafklaret?\nHvilke mulige andele kan optr√¶de i tabellen for \\(k=1, 2, 3, 4, 5\\)?\nPr√∏v at opstille alle muligheder for tabeller (for \\(k=1, 2, 3\\)).\n\n\n\n\n\n\nkNN i programmet Orange\nHer til sidst skal vi lege lidt med et program, som hedder Orange. Du kan f√• hj√¶lp til at installere programmet her.\nStart med at se denne video:\n\n\n\n\n\n\n\nOpgave 7: kNN i Orange baseret p√• features\n\n\n\n\n\n\nInstaller Orange.\nOpbyg modellen, som det er vist i videoen ovenfor. For at g√∏re det f√•r du brug for de tr√¶ningsdata og testdata, som vi har brugt i det foreg√•ende.\nPr√∏v at √¶ndre p√• v√¶rdien af \\(k\\) (\\(k=1, 2, 3, 4, 5\\)) og se om du i Orange kan genskabe resultaterne fra opgave 6.\n\n\n\n\nDet er ogs√• muligt at bruge kNN uden selv at udv√¶lge features. I stedet kan man bruge billederne fra tabellen i opgave 2 direkte. Se her hvordan man g√∏r det:\n\n\n\n\n\n\n\nOpgave 8: kNN i Orange baseret p√• billeder\n\n\n\n\n\n\nFind selv p√• en v√¶rdi som du gerne vil pr√¶diktere ud fra billeder.\nTag billeder som kan bruges som tr√¶ningsdata og opbyg en model, som det er vist i videoen ovenfor.\n\n\n\n\n\n\nVidere overvejelser\nDer er flere ting, man kunne overveje at arbejde videre med. For eksempel kunne man sagtens forestille sig, at det kunne give mening med mere end to features. Afstandsformlen i (1) kan faktisk sagtens udvides til flere dimensioner end to. Man finder eksempelvis afstanden mellem to punkter \\((x_1,y_1,z_1)\\) og \\((x_2,y_2,z_2)\\) i et tredimentionelt koordinatsystem p√• denne m√•de: \\[\nd = \\sqrt{(x_2-x_1)^2+(y_2-y_1)^2+(z_2-z_1)^2}\n\\] Og det er nok ikke sv√¶rt at forestille sig, at man kan udvide denne formel yderligere. Det betyder, at man stadig kan bruge \\(k\\) n√¶rmeste naboer som metode i det tilf√¶lde, hvor man har mere end to features.\nEn anden ting at overveje er den skala, vi m√•ler features p√•. Vi har for eksempel valgt at m√•le i \\(cm^2\\) og i \\(cm\\). Men hvad hvis vi havde m√•lt i \\(m^2\\) og i \\(m\\)? Det er faktisk ikke helt ligegyldigt hvilken skala, man bruger, og derfor v√¶lger man som regel ogs√• at skalere alle ens data, s√• de kommer p√• en sammenlignelig skala. Det kan du l√¶se meget mere om i vores materiale om feature-skalering."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html",
    "href": "undervisningsforloeb/polynomium.html",
    "title": "AI og r√∏dder i andengradspolynomier",
    "section": "",
    "text": "Foruds√¶tninger og tidsforbrug\n\n\n\n\n\nForl√∏bet kr√¶ver kendskab til:\n\nRette linjer.\nAndengradspolynomier og r√∏dder.\n\nTidsforbrug: Ca. 90 minutter.\nVi anbefaler, at I i dette forl√∏b arbejder i grupper p√• 3-4 elever."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html#andengradspolynomier-og-r√∏dder",
    "href": "undervisningsforloeb/polynomium.html#andengradspolynomier-og-r√∏dder",
    "title": "AI og r√∏dder i andengradspolynomier",
    "section": "Andengradspolynomier og r√∏dder",
    "text": "Andengradspolynomier og r√∏dder\nLad os lige starte med at minde om, at et andengradspolynomium er en funktion med en forskrift p√• formen \\[\nf(x)=ax^2 + bx + c, \\quad a \\neq 0\n\\] Grafen for et andengradspolynomium kaldes som bekendt for en parabel. I figur¬†1 ses tre eksempler p√• s√•danne parabler.\n\n\n\n\n\n\nFigur¬†1: Graferne for tre forskellige andengradspolynomier.\n\n\n\nHvis vi l√∏ser andengradsligningen \\[\nf(x)=ax^2 + bx + c=0\n\\] finder vi andengradspolynomiets r√∏dder. Men at l√∏se \\(f(x)=0\\), svarer netop til at bestemme, hvor den tilh√∏rende parabel sk√¶rer \\(x\\)-aksen. I figur¬†1 kan vi se, at den gr√∏nne parabel sk√¶rer \\(x\\)-aksen to steder. Det vil sige, at det tilh√∏rende andengradspolynomium har to r√∏dder. Den r√∏de parabel sk√¶rer \\(x\\)-aksen √©t sted ‚Äì det tilh√∏rende andengradspolynomium har alts√• √©n rod. Endelig kan vi se, at den bl√• parabel slet ikke sk√¶rer \\(x\\)-aksen, og det tilh√∏rende andengradspolynomium har derfor ingen r√∏dder.\nDu husker nok, hvordan man bestemmer antallet af r√∏dder i et andengradspolynomium. Vi har brug for diskriminanten \\(d\\):\n\\[\nd = b^2-4ac\n\\tag{1}\\]\nOg der g√¶lder s√•, at \\[\n\\begin{aligned}\n&d&lt;0: \\quad f \\text{ har ingen r√∏dder} \\\\\n&d=0: \\quad f \\text{ har √©n rod} \\\\\n&d&gt;0: \\quad f \\text{ har to r√∏dder} \\\\\n\\end{aligned}\n\\]\nId√©en er nu at unders√∏ge, om vi kan bruge kunstig intelligens til at afg√∏re1, om et andengradspolynomium overhovedet har nogle r√∏dder alene ude fra de tre koefficienter \\(a\\), \\(b\\) og \\(c\\) ‚Äì og helt uden at kende noget til diskriminantformlen i (1)!\n1¬†Det er klart, at der er intet nyt under solen her. Vi kan jo bare selv beregne diskriminanten og svare p√• sp√∏rgsm√•let. Men form√•let er her at l√¶re lidt om, hvad det vil sige at bruge kunstig intelligens i et tilf√¶lde, hvor vi allerede selv kender svaret. Desuden findes der ingen lukkede l√∏sningsformler for at bestemme r√∏dder i et polynomium, s√• snart graden af polynomiet er \\(5\\) eller derover. S√• id√©en kan generaliseres, og s√• er den m√•ske slet ikke s√• tosset endda!Inden vi g√•r i gang, vil vi starte med at indse, at i stedet for at l√∏se ligningen\n\\[\na x^2 + bx +c = 0\n\\tag{2}\\]\nS√• kan vi lige s√• godt l√∏se en ligning p√• formen\n\\[\nx^2 + bx +c =0\n\\] hvor alts√• \\(a=1\\). Det virker m√•ske som en forsimpling, men da vi har antaget, at \\(a \\neq 0,\\) s√• kan vi i ligningen i (2) dividere igennem med \\(a\\) og f√•\n\\[\n\\begin{aligned}\n\\frac{a}{a} x^2 + \\frac{b}{a} x + \\frac{c}{a} &= \\frac{0}{a} \\quad \\Leftrightarrow \\\\\nx^2 + \\frac{b}{a} x + \\frac{c}{a} &= 0\n\\end{aligned}\n\\]\nDet betyder, at n√•r vi skal bestemme r√∏dder i andengradspolynomier, s√• er det tilstr√¶kkeligt, at betragte andengradspolynomier med en forskrift p√• formen\n\\[\nf(x)=x^2+bx+c\n\\] fordi man simpelthen bare tager sit oprindelige andengradspolynomium og dividerer igennem med \\(a\\). Lad os illustrere det med et eksempel.\n\nEksempel 1 Betragt andengradspolynomiet med forskriften\n\\[\nf(x)=-4x^2+8x+12\n\\] Her har vi \\(a=-4, b=8\\) og \\(c=12\\). L√∏ser vi ligningen \\(f(x)=0\\), finder vi ud af, at \\(f\\) har to r√∏dder nemlig \\(-1\\) og \\(3\\). Dividerer vi forskriften for \\(f\\) igennem med \\(a=-4\\) f√•s et nyt andengradspolynomium \\(g\\) med forskrift\n\\[\ng(x)=x^2-2x-3\n\\] Her er koefficienterne \\(a=1, b=-2\\) og \\(c=-3\\). Men \\(g\\) har pr√¶cis samme r√∏dder som \\(f\\) ‚Äì nemlig \\(-1\\) og \\(3\\). Dette ses ogs√• illustreret i figur¬†2, hvor grafen for \\(f\\) og \\(g\\) begge sk√¶rer \\(x\\)-aksen i \\(-1\\) og \\(3\\).\n\n\n\n\n\n\nFigur¬†2: Grafen for \\(f(x)=-4x^2+8x+12\\) (den bl√•) og \\(g(x)=x^2-2x-3\\) (den gr√∏nne), som begge sk√¶rer \\(x\\)-aksen samme sted. Det vil sige, at \\(f\\) og \\(g\\) har de samme r√∏dder. I dette tilf√¶lde \\(-1\\) og \\(3\\)."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html#tr√¶ningsdata",
    "href": "undervisningsforloeb/polynomium.html#tr√¶ningsdata",
    "title": "AI og r√∏dder i andengradspolynomier",
    "section": "Tr√¶ningsdata",
    "text": "Tr√¶ningsdata\nI dette eksempel vil vi n√∏jes med at se p√•, om vi kan bruge en metode fra kunstig intelligens, s√• vi forh√•bentlig kan f√• svar p√•, om et givent andengradspolynomium enten har ingen eller en eller to r√∏dder. Vi vil alts√• gerne finde en metode, som for en given parabel kan svare p√•, om parablen sk√¶rer \\(x\\)-aksen eller ej (og alts√• ikke hvor mange gange den eventuelt sk√¶rer \\(x\\)-aksen).\n\n\n\n\n\n\nOpgave 1: R√∏dder eller ej?\n\n\n\n\n\nOvervej f√∏lgende:\n\nHvordan laver man et andengradspolynomium, der har √©n eller to r√∏dder?\nHvordan laver man et andengradspolynomium, som ingen r√∏dder har?\n\n\n\n\nFor at bruge kunstig intelligens skal vi have lavet en masse eksempler p√• forskellige andengradspolynomier (det vil her sige med forskellige v√¶rdier af \\(b\\) og \\(c\\)) samtidig med, at vi ogs√• finder ud af, om det tilh√∏rende andengradspolynomium har r√∏dder eller ej. Den v√¶rdi, der angiver om et polynomium har r√∏dder eller ej, kalder man for en targetv√¶rdi. T√¶nk p√• det som en lille label du s√¶tter p√• hvert eksempel, hvor du fort√¶ller, hvad det rigtige svar er ‚Äì ‚Äúdet er alts√• det her, jeg gerne vil have, at du l√¶rer!‚Äù. Samlet set kalder man de forskellige eksempler inklusiv targetv√¶rdien for tr√¶ningsdata.\n\n\n\n\n\n\nOpgave 2: Tr√¶ningsdata\n\n\n\n\n\nAlle i gruppen skal nu:\n\nFinde et andengradspolynomium som ikke har nogle r√∏dder (husk at \\(a=1\\)). Not√©r din v√¶rdi af \\(b\\) og \\(c\\) og s√¶t her targetv√¶rdien \\(t\\) til \\(-1\\).\nFinde et andengradspolynomium som har √©n eller to r√∏dder (husk at \\(a=1\\)). Not√©r din v√¶rdi af \\(b\\) og \\(c\\) og s√¶t her targetv√¶rdien \\(t\\) til \\(1\\).\n\nI skal nu inds√¶tte jeres forskellige v√¶rdier for \\(b, c\\) og \\(t\\) i et regneark. I videoen herunder er det vist, hvordan man g√∏r i GeoGebra:\n\nDisse data er nu pr√¶cis det, man kalder for tr√¶ningsdata.\n\nIndtast jeres v√¶rdier i et regnark, som det er vist i videoen.\nIndtegn dine v√¶rdier af \\(b\\) og \\(c\\) i et koordinatsystem, hvor v√¶rdien af \\(b\\) er p√• \\(x\\)-aksen, og v√¶rdien af \\(c\\) er p√• \\(y\\)-aksen. Hvis \\((b,c)\\)-punktet svarer til et andegradspolynomium, som har r√∏dder, farves punktet r√∏dt og ellers farves det bl√•t. Dette er ogs√• vist i videoen ovenfor."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html#tr√¶ning-af-kunstig-intelligens",
    "href": "undervisningsforloeb/polynomium.html#tr√¶ning-af-kunstig-intelligens",
    "title": "AI og r√∏dder i andengradspolynomier",
    "section": "Tr√¶ning af kunstig intelligens",
    "text": "Tr√¶ning af kunstig intelligens\nEn simpel metode inden for kunstig intelligens er at pr√∏ve at bestemme en ret linje, som kan bruges til at adskille de r√∏de punkter fra de bl√• punkter i det punktplot, som I har lavet i opgave 2.\nHvis man skal have en computer til at g√∏re det, s√• vil man typisk starte med en hel tilf√¶ldig ret linje med en ligning p√• formen\n\\[y=a \\cdot x+b\\]\nog s√• pr√∏ve at opdatere h√¶ldningen \\(a\\) og sk√¶ring med \\(y\\)-aksen \\(b\\), s√•dan at linjen bliver bedre og bedre til at adskille de r√∏de punkter fra de bl√•.\nNu ser vi jo p√• andengradspolynomier med en forskrift p√• formen \\(f(x)=x^2+bx+c\\). Det vil sige, at \\(b\\) allerede har en betydning. Derfor er det ikke s√• hensigtsm√¶ssigt at bruge \\(b\\) igen i ligningen for en ret linje. Derfor v√¶lger vi her at beskrive den rette linje med en ligning p√• formen\n\\[\ny = w_1 \\cdot x + w_0\n\\] Det vil alts√• sige, at \\(w_1\\) er linjens h√¶ldning, og \\(w_0\\) er linjens sk√¶ring med \\(y\\)-aksen.\n\n\n\n\n\n\nOpgave 3: Bestemmelse af en linje som kan adskille de r√∏de punkter fra de bl√•\n\n\n\n\n\n\nI inputfeltet i GeoGebra skal du taste: y=w1*x+w0.\nN√•r GeoGebra sp√∏rger, om du vil oprette skydere for w0 og w1, svarer du \"Opret skydere\".\nTr√¶k i skyderne for w0 og w1 og pr√∏v om du kan finde en ret linje, som kan adskille de r√∏de punkter fra de bl√•.\n\n\n\n\n\n\n\n\n\n\nOpgave 4: Flere tr√¶ningsdata\n\n\n\n\n\n\nAfg√∏r om f√∏lgende andengradspolynomier har r√∏dder og tilf√∏j dem til dit tr√¶ningsdata (husk ingen r√∏dder svarer til at \\(t=-1\\) og det tilh√∏rende punkt farves bl√•t, mens √©n eller to r√∏dder svarer til \\(t=1\\) og punktet farves r√∏dt):\n\n\\[\n\\begin{aligned}\nf_1(x) &= x^2 + 10x + 26 \\\\\nf_2(x) &= x^2 + 10x + 24\\\\\nf_3(x) &= x^2 + 5x + 6\\\\\nf_4(x) &= x^2 + 5x + 7 \\\\\nf_5(x) &= x^2 + 2x + 1\\\\\nf_6(x) &= x^2 + 2x + 2 \\\\\n\\end{aligned}\n\\]\n\nKan det lade sig g√∏re at adskille de to grupper med den rette linje, du fandt i opgave 3?\nHvis ikke kan du s√• bestemme en ny ret linje, som kan adskille de r√∏de punkter fra de bl√•?\n\n\n\n\nSom du netop har opdaget, er det en umulig opgave, vi har givet os selv! Vi kan ikke finde en ret linje, som i alle tilf√¶lde kan bruges til at adskille de r√∏de punkter fra de bl√•. üòï\nLad os se p√• hvorfor. Som tidligere n√¶vnt har vores linje en ligning p√• formen\n\\[\ny=w_1 \\cdot x + w_0\n\\]\nMen nu har vi \\(b\\)-v√¶rdier ud af \\(x\\)-aksen og \\(c\\)-v√¶rdier op af \\(y\\)-aksen, s√• i virkeligheden ser ligningen s√•dan her ud:\n\\[\nc=w_1 \\cdot b + w_0\n\\tag{3}\\]\nhvor \\(b\\) og \\(c\\) jo svarer til koefficinter i forskellige andengradspolymonier med forskrift \\(f(x)=x^2+bx+c\\).\nVi husker nu p√• formlen for diskriminanten \\(d=b^2-4ac=b^2-4c\\), da \\(a=1\\) i vores eksempel. Skillelinjen for om andengradspolynomiet har ingen eller flere r√∏dder, g√•r netop ved \\(d=0\\). Det vil sige\n\\[\nb^2-4c =0\n\\tag{4}\\]\nsom kan omskrives til\n\\[\nc = \\frac{1}{4}b^2\n\\tag{5}\\]\nMen vi kan ikke finde nogle v√¶rdier af \\(w_0\\) og \\(w_1\\), s√• udtrykket i (3) kommer til at svare til udtrykket i (5). Det er fordi, at i (3) indg√•r der kun et \\(b\\), mens der i (5) indg√•r et \\(b^2\\). Denne observation giver os imidlertid ogs√• en l√∏sning p√• vores problem. I stedet for at lade tr√¶ningsdata best√• af \\(b\\)- og \\(c\\)-v√¶rdier, s√• vil vi i stedet lade tr√¶ningsdata best√• af \\(b^2\\)- og \\(c\\)-v√¶rdier! Det vil sige, at vi ud af \\(x\\)-aksen vil afs√¶tte \\(b^2\\) og op af \\(y\\)-aksen, vil vi afs√¶tte \\(c.\\)\n\n\n\n\n\n\nOpgave 5: Transformerede tr√¶ningsdata\n\n\n\n\n\n\nBrug dit regneark fra tidligere og udregn \\(b^2\\).\nIndtegn dine v√¶rdier af \\(b^2\\) og \\(c\\) i et nyt koordinatsystem, hvor v√¶rdien af \\(b^2\\) er p√• \\(x\\)-aksen, og v√¶rdien af \\(c\\) er p√• \\(y\\)-aksen. Hvis \\((b^2,c)\\)-punktet svarer til et andegradspolynomium, som har r√∏dder, farves punktet r√∏dt og ellers farves det bl√•t.\nHvilken linje kan du v√¶lge til at adskille de to grupper?\n\n\n\n\nId√©en med at pr√∏ve at adskille to grupper af punkter med en ret linje bruges blandt andet i den AI-metode, som kaldes for perceptroner. Metoden kan bruges, n√•r man gerne vil kunne adskille to grupper af punkter fra hinanden baseret p√• en r√¶kke forskellige v√¶rdier ‚Äì disse v√¶rdier kalder man for features. Du kender m√•ske kandidattests, hvor man p√• baggrund af svarene fra en r√¶kke sp√∏rgsm√•l gerne vil kunne forudsige, om en person vil stemme p√• r√∏d eller bl√• blok til det n√¶ste valg. Det kunne man for eksempel bruge en perceptron til at hj√¶lpe med at afg√∏re, og det kan du l√¶se meget mere om her.\nHvis du vil pr√∏ve at bruge den metode p√• dine data om andengradspolynomier, kan du g√∏re det her."
  },
  {
    "objectID": "teacher.html",
    "href": "teacher.html",
    "title": "For l√¶rerne",
    "section": "",
    "text": "Her kommer information om hvordan materialerne kan bruges i forskellige typer forl√∏b i gymnasiet.\nVi har p√• nuv√¶rende tidspunkt erfaring med SRO i matematik og samfundsfag samt forskellige typer af SRP. Kontakt Ege Rubak for n√¶rmere information."
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "AI apps",
    "section": "",
    "text": "ADALINE perceptron app\n\n\nHer kan du tr√¶ne en perceptron med dine egne data.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "apps/perceptron_app.html",
    "href": "apps/perceptron_app.html",
    "title": "ADALINE perceptron app",
    "section": "",
    "text": "G√∏r f√∏lgende\n\nUpload data (.xlsx eller .csv format).\nV√¶lg den kolonne som angiver targetv√¶rdien (skal v√¶re kodet +/- 1).\nV√¶lg de forklarende variable (feature-/input-v√¶rdier).\n√Ündr eventuelt p√• startv√¶gte, learning rate, stopkriterium og/eller det maksimale iterationer ‚Äì eller behold default v√¶rdierne.\nV√¶lg antal af fold i \\(k\\)-folds krydsvalidering (default er 5).\nTryk p√• \"K√∏r ADALINE!\"\n\n\n\nOutput fra algoritmen\n\nInformation om hvad du har valgt som features og targetv√¶rdi.\nV√¶rdien af de estimerede v√¶gte.\nEn figur, som viser, hvordan v√¶gtene har √¶ndret sig for hver iteration. Hvis graferne for alle v√¶gtene er fladet ud, er det tegn p√•, at algoritmen er konvergeret.\nResultatet af krydsvalidering her angivet som klassifikationsn√∏jagtigheden (den gennemsnitlige andel som er klassificeret korrekt ved \\(k\\)-folds krydsvalidering).\nTil sidst en tabel med data."
  },
  {
    "objectID": "materialer/logistisk/data/figur_til_www.html",
    "href": "materialer/logistisk/data/figur_til_www.html",
    "title": "Figur til www",
    "section": "",
    "text": "## Data\ndatadir &lt;- here::here(\"materialer\", \"logistisk\", \"data\")\ndat_navn &lt;- file.path(datadir, \"blodtryk.xlsx\")\ndat &lt;- readxl::read_xlsx(dat_navn)\ndat &lt;- head(dat, n = 200)\nfit &lt;- glm(y~x, family = binomial(), data = dat)\na_mle &lt;- coef(fit)[2]\nb_mle &lt;- coef(fit)[1]\np &lt;- function(x,a,b){exp(a*x + b)/(1+exp(a*x+ b))}\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\np_dat &lt;- tibble(x = seq(50, 300, by = .2), \n                p = p(x, a_mle, b_mle))\ndat |&gt; \n  ggplot(aes(x = x)) +\n  geom_point(aes(y=y), col = \"blue\", size = 2, alpha = .5) + \n  geom_line(data = p_dat, aes(y = p)) +\n  labs(x = \"\", y = \"\") +\n  theme_grey()"
  },
  {
    "objectID": "materialer/afstande/MetrikDetAbstrakteAfstandsBegreb.html",
    "href": "materialer/afstande/MetrikDetAbstrakteAfstandsBegreb.html",
    "title": "Definition af en metrik ‚Äì det abstrakte afstandsbegreb",
    "section": "",
    "text": "Man har ikke frit valg til at bestemme, hvad man vil bruge som afstandsm√•l. Hvis det skal give mening, skal man have en metrik ‚Äì det betyder, at afstanden skal opfylde nogle betingelser:\nEn metrik p√• en m√¶ngde \\(M\\) er en funktion \\(d\\) fra \\(M\\times M\\) til \\(\\mathbb{R}\\) ‚Äì alts√• en funktion, som tager to elementer i \\(M\\) og giver et reelt tal.\nHvis en funktion \\(d\\) skal v√¶re en metrik, s√• vil vi kr√¶ve, at den opfylder f√∏lgende fire betingelser:\nFor alle \\(p,q,r\\) i \\(M\\) skal der g√¶lde, at\n\n\\(d(p,q)\\geq 0\\). Med ord: Alle afstande er positive eller \\(0\\).\n\\(d(p,p)=0\\) og \\(d(p,q)=0\\) hvis og kun hvis \\(p=q\\). Med ord: Afstanden fra et punkt til sig selv er \\(0\\), og ingen andre afstande er \\(0\\).\n\\(d(p,q)=d(q,p)\\). Det vil sige, at afstanden er symmetrisk. Med ord: Der er lige s√• langt fra \\(p\\) til \\(q\\) som fra \\(q\\) til \\(p\\).\n\\(d(p,q)+d(q,r)\\geq d(p,r)\\). Det kaldes for trekantsuligheden. Med ord: Der er mindst lige s√• langt fra \\(p\\) til \\(r\\) via \\(q\\), som direkte fra \\(p\\) til \\(r\\).\n\nLad os tage et velkendt eksempel.\n\nEksempel 1 (Euklidisk afstand som metrik) Lad \\(M\\) v√¶re alle punkter i planen og lad metrikken v√¶re den euklidiske afstand, som vi kender. Funktionen \\(d\\) vil s√• tage to punkter \\(P(x_1,y_1)\\) og \\(Q(x_2,y_2)\\) i planen og give et reelt tal som output svarende til den euklidiske afstand mellem \\(P\\) og \\(Q\\). Det vil sige, at \\[ d(P,Q) = \\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\\] Vi vil senere vise, at denne funktion opfylder betingelserne for en metrik, som defineret ovenfor.\n\nDet er en meget kort definition. Og meget, meget generel. \\(M\\) er en m√¶ngde - der er en strengt logisk m√•de at g√• til m√¶ngder p√•, men lad os her sige en samling af objekter, som vi ogs√• kalder elementer af m√¶ngden. L√¶g m√¶rke til, at vi her bare graver problemet lidt l√¶ngere ned i sandet ‚Äì fejer det ind under gulvt√¶ppet ‚Äì for hvad er \"objekter\"? Det kommer vi ikke n√¶rmere her.\nDet er ret nemt at acceptere, at de tre krav er rimelige. Men er det nok? Og er det nu alligevel rimeligt? Hvad med symmetrien? Der er vel l√¶ngere \\(10\\) km op ad bakke end \\(10\\) km ned ad bakke, hvis man t√¶nker p√• arbejdsindsats. S√• m√•ske giver det ikke altid mening?1\n1¬†Hvis funktionen \\(d\\) opfylder 1,2,4, er det en quasimetrik. Opfylder den 1,2,3, er det en semimetrik. Opfylder den 1, 3 og 4, og f√∏rste del af 2 (\\(d(p,p)=0\\), men der kan v√¶re andre afstande, der er \\(0\\)) er det en pseudometrik. Der findes s√•m√¶nd ogs√• pr√¶metrikker, metametrikker, pseudoquasimetrikker og sikkert andre ‚Äì \"falske metrikker\".2¬†Ordet \"rum\" skal man ikke l√¶gge for meget i. Der er ikke anden information i det end definitionen. Intuition skal man v√¶re varsom med.Definitionen af metrik som her, er den, vi bruger i matematik. Den har vist sig nyttig. Der er en skov af artikler og b√∏ger, hvor man kan se, hvad man ved, n√•r man har en metrik. En m√¶ngde med en metrik kaldes et metrisk rum.2\n\nEksempel 2 (Den diskrete metrik) P√• en m√¶ngde \\(M\\) er funktionen \\(d\\) givet ved.\n\n\\(d(p,p)=0\\)\nHvis \\(p\\neq q\\) er \\(d(p,q)=1\\).\n\nDet er en metrik ‚Äì den opfylder definitionen ovenfor. Men det er ikke nogen specielt nyttig metrik. Alle elementer ligger lige t√¶t p√• alle andre, s√• der er ikke ny information ‚Äì udover, om to elementer er ens eller ej.\n\n\nEksempel 3 (Euklidisk afstand som metrik, fortsat) Vi vil vise, at den euklidiske afstand mellem to punkter rent faktisk opfylder betingelserne for en metrik, som vi definerede dem ovenfor:\n\nDen f√∏rste betingelse er opfyldt, da \\[d(P,Q)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2} \\geq 0\\]\nI den anden betingelse er der to ting at vise. For det f√∏rste ses det nemt, at \\[d(P,P)=\\sqrt{(x_1-x_1)^2+(y_1-y_1)^2} = \\sqrt{0}=0\\] For det andet ‚Äì hvis \\[d(P,Q)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}=0\\] s√• kan det kun lade sig g√∏re, hvis b√•de \\[(x_2-x_1)^2=0 \\quad \\textrm{og} \\quad (y_2-y_1)^2=0\\] Det kan igen kun lade sig g√∏re3, hvis \\[x_1=x_2 \\quad \\textrm{og} \\quad y_1=y_2\\] Det vil sige, at \\(P=Q\\), og den anden betingelse er s√•ledes ogs√• opfyldt.\nDa \\((a-b)^2=(b-a)^2\\) f√•r vi, at \\[d(P,Q)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}=d(Q,P)\\] og den tredje betingelse er opfyldt.\nDet kr√¶ver lidt mere at bevise trekantsuligheden, men intuitivt virker det fornuftigt nok. Hvis du i trekant \\(PQR\\), skal fra \\(P\\) til \\(R\\), s√• bliver turen dertil ikke kortere, hvis du f√∏rst g√•r om \\(Q\\).\n\n3¬†Brug nulreglen.\n\n\n\n\n\n\nOpgave: Levenshteinafstanden\n\n\n\n\n\nVis, at Levenshteinafstanden giver en metrik.\n\nHvilken m√¶ngde er det mon en metrik p√•? Her kan man v√¶lge ‚Äì hvilke bogstaver m√• bruges? Vil I begr√¶nse l√¶ngden p√• de ord, der kan optr√¶de?\nOvervej, at afstanden mellem to ord er l√¶ngden af den (eller rettere en - der kan v√¶re flere veje, som er lige lange) korteste mulige vej fra det ene til det andet i et netv√¶rk (en graf). \n\nNu skulle det v√¶re til at indse, at de fire betingelser er opfyldt.\n\n\n\n\nEksempel 4 (Ikke-metrik) En elev er tr√¶t af kvadratr√∏dder og t√¶nker, at man vel kan droppe den euklidiske afstand og i stedet definere en afstand mellem to punkter \\(p(x_1,y_1)\\) og \\(q(x_2,y_2)\\) i planen som f√∏lger:\n\\[D(p,q)=(x_2-x_1)^2+(y_2-y_1)^2 \\tag{1}\\]\nDer er bare et lille problem: \\(D\\) er ikke en metrik! Den opfylder nemlig ikke trekantsuligheden. Men hvordan kan man se det? Husk p√•, at vi bare skal finde √©t eksempel ‚Äì det vil sige tre punkter \\(p,q,r\\), hvor trekantsuligheden ikke holder. S√• har vi vist, at \\(D\\) ikke er en metrik.\nEt konkret eksempel: \\(p=(0,0)\\), \\(q=(2,0)\\), \\(r=(4,0)\\). Se figur¬†1.\n\n\n\n\n\n\nFigur¬†1: Koordinatsystem med punkterne \\(p=(0,0)\\), \\(q=(2,0)\\) og \\(r=(4,0)\\).\n\n\n\nAfstanden fra \\(p\\) til \\(r\\) er \\(D(p,r)=4^2+0^2=16\\), mens afstanden fra \\(p\\) til \\(q\\) er \\(D(p,q)=2^2+0^2=4\\) og det samme g√¶lder afstanden fra \\(q\\) til \\(r\\): \\(D(q,r)=2^2+0^2=4\\) s√• \\[D(p,q)+D(q,r)=8\\] mens \\[D(p,r)=16\\] Alts√• er \\[ D(p,q)+D(q,r) \\ngeq D(p,r) \\]\nEt andet eksempel, som ligner en rigtig trekant: \\(p=(0,0)\\) \\(q=(2,1)\\), \\(r=(4,0)\\). Se figur¬†2.\n\n\n\n\n\n\nFigur¬†2: Koordinatsystem med punkterne \\(p=(0,0)\\), \\(q=(2,1)\\) og \\(r=(4,0)\\).\n\n\n\nHer er \\(D(p,q)=2^2+1^2=5\\) og \\(D(q,r)=(4-2)^2+1^2=5\\) s√• \\[D(p,q)+D(q,r)=10\\] mens \\[D(p,r)=4^2+0^2=16\\] Igen er det med dette afstandsm√•l kortere at g√• fra \\(p\\) til \\(r\\) via \\(q\\) end at g√• direkte. Og det er alts√• derfor ikke en metrik.\n\n\n\n\n\n\n\nOpgave: Ikke-metrik\n\n\n\n\n\nBrug funktionen\n\\[D(p,q)=(x_2-x_1)^2+(y_2-y_1)^2\\]\nfra eksempel¬†4. Vi vil unders√∏ge, hvorn√•r \\(D(p,q)+D(q,r) \\geq D(p,r)\\), s√•ledes at trekantsuligheden er opfyldt.\nHer regner vi p√• trekanter \\(pqr\\) med: \\(p=(0,0)\\), \\(q=(2,y)\\) og \\(r=(4,0)\\), hvor midterpunktet \\(q\\) flyttes l√¶ngere v√¶k fra f√∏rsteaksen. Brug app‚Äôen nedenfor og find det \\(y\\), hvor \\(D(p,q)+D(q,r)=D(p,r)\\).\n\nHvad er \\(\\angle pqr\\), n√•r denne ligning er opfyldt?\nKunne man have indset det uden at regne?\nHvad skal \\(\\angle pqr\\) v√¶re for at trekantsuligheden er opfyldt: \\(D(p,q)+D(q,r) \\geq D(p,r)\\)?"
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html",
    "title": "Afstande mellem punkter i planen",
    "section": "",
    "text": "Du har ikke n√∏dvendigvis t√¶nkt over det f√∏r, men hvordan m√•ler man egentlig afstanden mellem to punkter i planen? ‚ÄúJa, man finder afstanden mellem dem‚Äù, vil du m√•ske sige, men det er jo ikke rigtigt et brugbart svar p√• sp√∏rgsm√•let. Den afstand, du t√¶nker p√•, er formentlig l√¶ngden af det linjestykke, som forbinder de to punkter, men der findes faktisk mange andre m√•der at definere afstanden p√•. Det vil vi give et par eksempler p√• her.\nFor at blive lidt mere pr√¶cis forestiller vi os, at vi har to punkter i planen, som vi kalder for \\(P(x_1,y_1)\\) og \\(Q(x_2,y_2)\\)."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-euklidisk_afstand",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-euklidisk_afstand",
    "title": "Afstande mellem punkter i planen",
    "section": "Euklidisk afstand",
    "text": "Euklidisk afstand\nDen euklidisk afstand mellem \\(P\\) og \\(Q\\) er \\[\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\\] Det er det, vi kender mest ‚Äì og som formentlig er den afstand, du lige har t√¶nkt p√•. Formlen ovenfor fremkommer ved at bruge Pythagoras.\nI app‚Äôen herunder er den euklidiske afstand illustreret. Du kan flytte rundt p√• punkterne og se, hvordan afstanden √¶ndrer sig."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-manhattan_afstand",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-manhattan_afstand",
    "title": "Afstande mellem punkter i planen",
    "section": "Manhattanafstanden",
    "text": "Manhattanafstanden\nManhattanafstanden er den afstand, man f√•r, n√•r man er tvunget til at bev√¶ge sig langs akserne, som vi kender det fra vejene i mange amerikanske byer, herunder p√• Manhattan. Den kaldes ogs√• taxi-afstanden. Formlen for at bestemme Manhattanafstanden er: \\[|x_2-x_1|+|y_2-y_1|\\] I app‚Äôen herunder er Manhattanafstanden illustreret. Du kan flytte rundt p√• punkterne og se, hvordan afstanden √¶ndrer sig."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#max-afstanden",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#max-afstanden",
    "title": "Afstande mellem punkter i planen",
    "section": "Max-afstanden",
    "text": "Max-afstanden\nMax-afstanden er maksimum mellem den vandret og lodrette afstand mellem \\(P\\) og \\(Q\\). Det vil sige, maksimum af \\(|x_2-x_1|\\) og \\(|y_2-y_1|\\).\nDen kaldes ogs√• skak-konge afstanden. Kongen i skak kan g√• diagonalt eller langs de to akser. Et diagonalt move fra \\((a,b)\\) til \\((a+k,b+k)\\) t√¶nkes at have l√¶ngde \\(k\\) ‚Äì som i skak. Skal man fra eksempelvis \\(A(1,4)\\) til \\(B(3,7)\\) kan skakkongen g√• fra \\(A(1,4)\\) til \\(C(3,6)\\) ‚Äì det stykke har l√¶ngde \\(2\\) og derefter fra \\(C(3,6)\\) til \\(B(3,7)\\) langs \\(y\\)-aksen - et stykke p√• l√¶ngde \\(1\\). Samlet afstand er \\(3\\), maksimum af \\(|3-1|\\) og \\(|7-4|\\). Id√©en er illustreret i figur¬†1.\n\n\n\n\n\n\nFigur¬†1: Kongen i skak skal fra \\(A\\) til \\(B\\) via \\(C.\\) Bem√¶rk, at der ogs√• er andre veje fra \\(A\\) til \\(B\\) med afstand \\(3\\). For eksempel kan kongen g√• fra \\(A(1,4)\\) til \\((1,5)\\) (det giver en afstand p√• \\(1\\)) og dern√¶st lave et diagonalt move fra \\((1,5)\\) til \\(B(3,7)\\) med en afstand p√• \\(2\\). Den samlede afstand bliver igen \\(1+2=3\\). Der er alts√• flere korteste veje fra \\(A\\) til \\(B\\).\n\n\n\nI app‚Äôen herunder er max-afstanden illustreret. Du kan flytte rundt p√• punkterne og se, hvordan afstanden √¶ndrer sig."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#posthusafstanden",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#posthusafstanden",
    "title": "Afstande mellem punkter i planen",
    "section": "Posthusafstanden",
    "text": "Posthusafstanden\nPosthusafstanden1 mellem \\(P\\) og \\(Q\\) finder man, ved at t√¶nke p√•, at der ligger et posthus i origo \\(O(0,0)\\), og vi skal sende et brev fra \\(P\\) til \\(Q\\). Det bliver transporteret fra \\(P\\) til posthuset f√∏rst og derefter fra posthuset til \\(Q\\). Hvis man anvender Pythagoras to gange, kan man se, at formlen for denne afstand er: \\[\\sqrt{x_1^2+y_1^2}+\\sqrt{x_2^2+y_2^2}\\] I app‚Äôen herunder er posthusafstanden illustreret. Du kan flytte rundt p√• punkterne og se, hvordan afstanden √¶ndrer sig.\n1¬†Den hedder ogs√• British Rail afstanden eller, hvis man er fransk, SNCF (Soci√©t√© Nationale des Chemins de fer Fran√ßais) -afstanden. Man t√¶nker sig, at man altid skal rejse via London (eller Paris) for at komme med tog fra et sted til et andet."
  },
  {
    "objectID": "materialer/afstande/index_afstande.html",
    "href": "materialer/afstande/index_afstande.html",
    "title": "Afstande og feature-skalering",
    "section": "",
    "text": "Det er ikke altid helt klart, hvordan man skal bestemme afstanden mellem to datapunkter, hvis koordinaterne i hvert datapunkter beskriver vidt forskellige ting. Det er faktisk ikke en gang entydigt, hvad man overhovedet skal forst√• ved en afstand ‚Äì eller det som man i matematik vil kalde for en metrik. Her p√• siden behandler vi nogle af disse problemstillinger. Vi vil se n√¶rmere p√•, hvilke problemer, der kan opst√•, hvis man ikke t√¶nker sig om ‚Äì og hvad man kan g√∏re for at l√∏se dem.\nL√¶s mere i noterne herunder.\n\n\n\n\n\n\n\n\n\n\nFeature-skalering\n\n\nHvis de data, man arbejder med, m√•ler vidt forskellige ting ‚Äì m√•ske endda p√• vidt forskellige skalaer ‚Äì s√• vil man som oftest have brug for at ‚Äújustere‚Äù data, s√• de er p√•‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstande mellem punkter i planen\n\n\nAfstande mellem punkter i planen er ikke entydigt ‚Äì faktisk kan man m√•le afstanden mellem to punkter i planen p√• mange forskellige m√•der.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstande mellem ord\n\n\nHvordan m√•ler man mon afstanden mellem ord ‚Äì alts√• s√•dan helt generelt?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand mellem DNA- og RNA-strenge\n\n\nHvordan m√•ler man afstanden mellem DNA- og RNA-strenge?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition af en metrik ‚Äì det abstrakte afstandsbegreb\n\n\nHvad skal der til for, at man kan tale om, at et afstandsm√•l rent faktisk m√•ler noget, som giver mening?\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemDNA.html",
    "href": "materialer/afstande/AfstandeMellemDNA.html",
    "title": "Afstand mellem DNA- og RNA-strenge",
    "section": "",
    "text": "RNA er strenge med bogstaverne U (uracil), G (guanin), C (cytosin), A, (adenin). DNA har ikke U, men i stedet T (thymin) og DNA er dobbelt. Bogstaverne U, G, C, A og T kaldes for nukleotider.\nVi ser her p√• afstande mellem DNA (eller RNA), som bygger p√• antallet af mutationer for at n√• fra den ene til den anden og desuden, hvor hyppige disse mutationer er - hvis man ved, en mutation sker ofte, er afstanden mellem en streng uden mutationen og en med mutationen ikke s√• lang, som hvis mutationen er meget sj√¶lden. Udover regler for, hvilke √¶ndringer, man tillader, giver man derfor en omkostning ved √¶ndringen ‚Äì afstanden er ikke bare antal √¶ndringer, men summen af, hvor \"dyre\" disse √¶ndringer er.\nAfstand mellem DNA bruges til at analysere sl√¶gtskab og hvilke dyr, herunder mennesket, der nedstammer fra hvilke andre dyr ‚Äì det kaldes for fylogenetiske tr√¶er ‚Äì se mere her.\nI den sammenh√¶ng kalder man skift mellem A og G eller mellem C og T for transitioner1. De fire andre mulige skift mellem A og C, mellem A og T, mellem G og T, mellem G og C, kaldes for transversioner. Transitioner er hyppigere mutationer end transversioner, s√• afstanden mellem \\(GATTACA\\) og \\(GATTACG\\) er mindre end afstanden mellem \\(GATTACA\\) og \\(GATTACC\\). Den slags udskiftning af et bogstav (et basepar) kaldes en punktmutation.\n1¬†A og G er puriner, mens C og T er pyrimediner. Transition bytter en purin med en purin eller en pyrimedin med en pyrimedin.Indel mutationer er inds√¶tning (\"In\") eller fjernelse (\"Del\" for delete\") af et eller flere basepar. Det er mindre hyppigt og svarer til l√¶ngere afstand. I kilden ovenfor bruges f√∏lgende omkostninger og alts√• afstande mellem DNA-strenge. Bem√¶rk, at det er et valg - der er mange andre muligheder:\n\nTransition: 1\nTransversion: 2\nGap √•bning: 9 (inds√¶t eller fjern pr√¶cis en base - alts√• et bogstav)\nGap forl√¶ngelse: 4 (inds√¶t eller fjern en base p√• samme sted, som er √•bnet)\n\nMan kan samle de to sidste og sige, at det koster \\(5+4L\\) at inds√¶tte eller fjerne en delstreng med \\(L\\) bogstaver midt i et ord (overvej, at I forst√•r, at det er samme regel).\nVi tilf√∏jer forl√¶ngelse/forkortelse: Det koster \\(4L\\) at inds√¶tte eller fjerne \\(L\\) bogstaver i start eller slut af et ord. Alt i alt:\n\nTransition: 1\nTransversion: 2\nInds√¶t eller fjern delord med \\(L\\) bogstaver midt i et ord: \\(5+4L\\)\nForl√¶ng/forkort: Inds√¶t eller fjern \\(L\\) bogstaver i start eller slut af et ord: \\(4L\\)\n\nAfstand mellem to strenge er s√• den kortest mulige m√•de, man kan komme fra den ene til den anden med de tilladte moves v√¶gtet som her.\n\n\n\n\n\n\n\nI det f√∏lgende bruger vi meget korte strenge. Det er naturligvis ikke realistisk. Vi vil finde afstanden fra \\(AGT\\) til \\(ATG\\). Der er mange muligheder for, hvordan man kan komme fra \\(AGT\\) til \\(ATG\\), alts√• hvordan mutationerne kunne have set ud. For eksempel kunne det v√¶re:\n\\[AGT \\to ATGT \\to ATG\\] Det vil sige, inds√¶t \\(T\\) mellem \\(A\\) og \\(G\\) og fjern s√• det sidste \\(T\\). Det koster \\(9+4 =13\\). Alts√• er l√¶ngden af denne vej \\(13\\). En anden mulighed er\n\\[AGT\\to ATT \\to ATG\\]\nHer er der to punktmutationer og begge er transversioner (fra \\(T\\) til \\(G\\) eller omvendt), s√• det koster \\(2+2=4\\). Det er den korteste vej, s√• afstanden er \\(4\\). At denne vej faktisk er den kortest, kr√¶ver mere eftertanke.\nHavde vi brugt samme omkostning/v√¶gt for alle tilladte √¶ndringer, ville begge de to veje have samme l√¶ngde.\nHvad med fra \\(AGT\\) til \\(TGA\\)? Jo, det er faktisk nemmere. Det er i virkeligheden samme DNA-sekvens ‚Äì man har bare l√¶st den fra den anden ende...\nMed lange strenge, som er ens p√• lange stykker, finder man afstande ved f√∏rst at \"aligne\". Det vil sige, at man anbringer strengene, s√• de passer sammen p√• flest mulige pladser. Og derefter udregner man afstande, men det er stadig ikke nemt ‚Äì der skal algoritmer til. Her er et eksempel.\nStreng 1: \\(TCGTAGG\\)\nStreng 2: \\(TCTGTATCGA\\)\nF√∏rste alignment: \\[\\begin{matrix}T&C&G&-&-&-&T&A&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] Det koster:\n\nInds√¶ttelse af \\(GTA\\): \\(5+4\\cdot 3=17\\)\nTo transversioner \\(G\\leftrightarrow T\\) og \\(A\\leftrightarrow C\\) samt en transition \\(G\\leftrightarrow A\\).\nI alt \\(17+4+1=22\\).\n\nHvis man i stedet v√¶lger denne alignment \\[\\begin{matrix}T&C&-&-&-&G&T&A&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] er transversionen mellem \\(G\\) og \\(T\\) erstattet med en transition \\(G\\leftrightarrow A\\) og omkostningen falder med \\(1\\) til \\(21.\\)\nMan indser ret let, at prisen for at klippe g√∏r, at man ikke vil klippe to gange og bruge \\[\\begin{matrix}T&C&-&G&-&-&T&A&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] hvor man kun sparer en enkelt transition.\nMen hvad med: \\[\\begin{matrix}T&C&-&G&T&A&-&-&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] Her er omkostningen \\(9\\) for det f√∏rste gap og \\(13\\) for det andet. Og der er en transition i sidste plads \\(G\\leftrightarrow A,\\) s√• omkostningen er \\(23\\), men det er ikke helt s√• klart, at det er for dyrt at klippe to gange. I kan nok finde p√• eksempler, hvor det kan svare sig at klippe flere steder."
  },
  {
    "objectID": "materialer/sigmoid_neuron/sigmoid_neuron.html",
    "href": "materialer/sigmoid_neuron/sigmoid_neuron.html",
    "title": "Sigmoid neuroner",
    "section": "",
    "text": "I noten om perceptroner beskrev vi perceptron learning algoritmen, som altid konvergerer, hvis data er line√¶r separable. Men verden er sj√¶ldent line√¶r separabel, og derfor introducerede vi ADALINE algoritmen, som ogs√• virker, selvom data ikke er line√¶r separable. Det virker jo alt sammen super godt ‚Äì men et enkelt lille eksempel afsl√∏rer alligevel, at ADALINE ikke altid er s√• smart, som man kunne tro.\nVi vil se p√• data i nedenst√•ende tabel\n\n\n\n\\(x_1\\)\n\\(x_2\\)\nTargetv√¶rdi\n\n\n\n\n\\(-0.5\\)\n\\(0.5\\)\n\\(1\\)\n\n\n\\(-0.3\\)\n\\(0.3\\)\n\\(1\\)\n\n\n\\(-0.1\\)\n\\(0.7\\)\n\\(1\\)\n\n\n\\(0.1\\)\n\\(0.4\\)\n\\(-1\\)\n\n\n\\(-0.1\\)\n\\(0.2\\)\n\\(-1\\)\n\n\n\\(0.1\\)\n\\(-0.1\\)\n\\(-1\\)\n\n\n\nI figur¬†1 har vi indtegnet punkterne \\((x_1,x_2)\\) og farvet punkterne med en targetv√¶rdi p√• \\(1\\) bl√• og dem med en targetv√¶rdi p√• \\(-1\\) r√∏de.\n\n\n\n\n\n\nFigur¬†1: Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de.\n\n\n\nDet er tydeligt, at punkterne er line√¶r separable og den indtegnede linje er ogs√• den som ADALINE giver1. Du kan selv pr√∏ve ADALINE her. De estimerede v√¶gte er \\(w_0=-0.9346, w_1=-2.838\\) og \\(w_2=1.668\\).Det vil sige, at den indtegnede linje har ligning\n1¬†Her er alle startv√¶gte sat til \\(0\\), learning rate er p√• \\(0.1\\), stop-kriterie er p√• \\(0.000001\\) og maksimalt antal iterationer er sat til \\(50000\\).\\[\n-0.9346-2.838 x_1 + 1.668x_2=0\n\\]\nDet er alt sammen meget fint, men lad os nu pr√∏ve at indtegne et nyt r√∏dt punkt:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\nTargetv√¶rdi\n\n\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\nDet nye punkt er indtegnet i figur¬†2 sammen med de √∏vrige seks punkter. Det er tydeligt, at data stadig er line√¶r separable.\n\n\n\n\n\n\nFigur¬†2: Et nyt r√∏dt punkt er indtegnet og data er stadig line√¶r separable.\n\n\n\nHvis vi pr√∏ver at k√∏re ADALINE algoritmen f√•s linjen, som er indtegnet i figur¬†3. Vi kan allerede se nu, at det er helt sk√∏rt. Data er line√¶r separable, men alligevel er der et r√∏dt punkt, som bliver klassificeret forkert ‚Äì faktisk var den oprindelige linje fra figur¬†1 bedre.\n\n\n\n\n\n\nFigur¬†3: Et nyt r√∏dt punkt er indtegnet og den linje, som ADALINE finder.\n\n\n\nDet er jo ikke ligefrem super overbevisende. Data er line√¶r separable og alligevel kan ADALINE ikke finde ud af at finde en rette linje, som kan adskille de r√∏de punkter fra de bl√•!\nHvis vi skal forst√•, hvad der sker, m√• vi se lidt n√¶rmere p√• den tabsfunktion, som ADALINE fors√∏ger at mininere. Fra afsnittet om ADALINE ved vi, at tabsfunktionen er\n\\[\n\\begin{aligned}\nE(w_0, w_1, &\\dots, w_n)\\\\ &= \\frac{1}{2} \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\end{aligned}\n\\]\nhvor det \\(m\\)‚Äôte tr√¶ningseksempel er \\[(x_{m,1}, x_{m,2}, \\dots, x_{m,n}, t_m)\\]\nDet vil sige, at det \\(m\\)‚Äôte tr√¶ningseksempel giver et bidrag til tabsfunktionen p√•\n\\[\n\\left ( t_m- (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\]\nFor et bl√•t punkt med \\(t_m=1\\) vil det sige, at bidraget til tabsfunktionen er pr√¶cis \\(0,\\) hvis \\[\n1- (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})=0\n\\] og for et r√∏dt punkt med \\(t_m=-1\\) er bidraget til tabsfunktionen pr√¶cis \\(0,\\) hvis \\[\n-1- (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})=0\n\\] Nu er \\(1- (w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n)=0\\) og \\(-1- (w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n)=0\\) jo bare ligninger for rette linjer. Disse linjer ses indtegnet p√• figur¬†4 (som henholdsvis en bl√• og r√∏d stiplet linje) sammen med de oprindelige seks punkter og den linje, som ADALINE fandt baseret p√• disse seks punkter. Samtidig er det for hvert punkt markeret, hvor meget dette punkt bidrager til tabsfunktionen.\n\n\n\n\n\n\nFigur¬†4: Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de. Den sorte linje med ligning \\(w_0+w_1 x_1 + w_2 x_2=0\\) svarer til den ADALINE fandt. Den bl√• stiplede linje har ligning \\(1-(w_0+w_1 x_1 + w_2 x_2)=0\\), mens den r√∏de stiplede linje har ligning \\(-1-(w_0+w_1 x_1 + w_2 x_2)=0\\).\n\n\n\nDet ses nu p√• figur¬†4, at bl√• punkter, som ligger t√¶t p√• den bl√• stiplede linje, bidrager mindst til tabsfunktionen, mens r√∏de punkter, som ligger t√¶t p√• den r√∏de stiplede linje, ligeledes bidrager mindst til tabsfunktionen. Den samlede v√¶rdi af tabsfunktionen er her \\(0.75\\).\nLaver vi nu samme √∏velse med det ekstra punkt f√•s resultat i figur¬†5.\n\n\n\n\n\n\nFigur¬†5: I alt syv punkter sammen med den sorte linje med ligning \\(w_0+w_1 x_1 + w_2 x_2=0\\) svarer til den ADALINE finder. Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de. Den bl√• stiplede linje har ligning \\(1-(w_0+w_1 x_1 + w_2 x_2)=0\\), mens den r√∏de stiplede linje har ligning \\(-1-(w_0+w_1 x_1 + w_2 x_2)=0\\). Den samlede v√¶rdi af tabsfunktionen er her \\(2.00\\).\n\n\n\nIgen ser vi, at bl√• punkter t√¶t p√• den bl√• stiplede linje bidrager mindst til tabsfunktionen og tilsvarende for de r√∏de punkter, som ligger t√¶t p√• den r√∏de stiplede linje. Det nye punkts bidrag til tabsfunktionen bliver derfor her det mindste bidrag blandt alle de r√∏de punkter. Den samlede v√¶rdi af tabsfunktionen er her \\(2.00\\).\nHvis vi i stedet pr√∏ver at bruge vores egen oprindelige linje (baseret p√• de seks f√∏rste punkter), som rent faktisk kunne adskille de bl√• punkter fra de r√∏de, s√• f√•s det resultat, som ses i figur¬†6.\n\n\n\n\n\n\nFigur¬†6: I alt syv punkter sammen med den sorte linje, som ADALINE giver baseret p√• de oprindelige seks punkter. Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de. Den bl√• stiplede linje har ligning \\(1-(w_0+w_1 x_1 + w_2 x_2)=0\\), mens den r√∏de stiplede linje har ligning \\(-1-(w_0+w_1 x_1 + w_2 x_2)=0\\). Den samlede v√¶rdi af tabsfunktionen er her \\(10.60\\).\n\n\n\nDet er nu tydeligt, at det nye r√∏de punkter ligger s√• langt v√¶k fra den stiplede r√∏de linje, at det bidrager betydeligt til tabsfunktionen. Derfor er den samlede v√¶rdi af tabsfunktionen \\(10.60\\) ‚Äì og derfor v√¶lger ADALINE linjen i figur¬†5 til at adskille punkterne. Ikke fordi, det er den linje, som giver den laveste andel af korrekt klassificerede, men fordi det er den linje, som minimerer tabsfunktionen! Det er jo faktisk helt sk√∏rt!"
  },
  {
    "objectID": "materialer/sigmoid_neuron/sigmoid_neuron.html#kan-vi-g√∏re-det-bedre-end-adaline",
    "href": "materialer/sigmoid_neuron/sigmoid_neuron.html#kan-vi-g√∏re-det-bedre-end-adaline",
    "title": "Sigmoid neuroner",
    "section": "",
    "text": "I noten om perceptroner beskrev vi perceptron learning algoritmen, som altid konvergerer, hvis data er line√¶r separable. Men verden er sj√¶ldent line√¶r separabel, og derfor introducerede vi ADALINE algoritmen, som ogs√• virker, selvom data ikke er line√¶r separable. Det virker jo alt sammen super godt ‚Äì men et enkelt lille eksempel afsl√∏rer alligevel, at ADALINE ikke altid er s√• smart, som man kunne tro.\nVi vil se p√• data i nedenst√•ende tabel\n\n\n\n\\(x_1\\)\n\\(x_2\\)\nTargetv√¶rdi\n\n\n\n\n\\(-0.5\\)\n\\(0.5\\)\n\\(1\\)\n\n\n\\(-0.3\\)\n\\(0.3\\)\n\\(1\\)\n\n\n\\(-0.1\\)\n\\(0.7\\)\n\\(1\\)\n\n\n\\(0.1\\)\n\\(0.4\\)\n\\(-1\\)\n\n\n\\(-0.1\\)\n\\(0.2\\)\n\\(-1\\)\n\n\n\\(0.1\\)\n\\(-0.1\\)\n\\(-1\\)\n\n\n\nI figur¬†1 har vi indtegnet punkterne \\((x_1,x_2)\\) og farvet punkterne med en targetv√¶rdi p√• \\(1\\) bl√• og dem med en targetv√¶rdi p√• \\(-1\\) r√∏de.\n\n\n\n\n\n\nFigur¬†1: Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de.\n\n\n\nDet er tydeligt, at punkterne er line√¶r separable og den indtegnede linje er ogs√• den som ADALINE giver1. Du kan selv pr√∏ve ADALINE her. De estimerede v√¶gte er \\(w_0=-0.9346, w_1=-2.838\\) og \\(w_2=1.668\\).Det vil sige, at den indtegnede linje har ligning\n1¬†Her er alle startv√¶gte sat til \\(0\\), learning rate er p√• \\(0.1\\), stop-kriterie er p√• \\(0.000001\\) og maksimalt antal iterationer er sat til \\(50000\\).\\[\n-0.9346-2.838 x_1 + 1.668x_2=0\n\\]\nDet er alt sammen meget fint, men lad os nu pr√∏ve at indtegne et nyt r√∏dt punkt:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\nTargetv√¶rdi\n\n\n\n\n\\(1\\)\n\\(-1\\)\n\\(-1\\)\n\n\n\nDet nye punkt er indtegnet i figur¬†2 sammen med de √∏vrige seks punkter. Det er tydeligt, at data stadig er line√¶r separable.\n\n\n\n\n\n\nFigur¬†2: Et nyt r√∏dt punkt er indtegnet og data er stadig line√¶r separable.\n\n\n\nHvis vi pr√∏ver at k√∏re ADALINE algoritmen f√•s linjen, som er indtegnet i figur¬†3. Vi kan allerede se nu, at det er helt sk√∏rt. Data er line√¶r separable, men alligevel er der et r√∏dt punkt, som bliver klassificeret forkert ‚Äì faktisk var den oprindelige linje fra figur¬†1 bedre.\n\n\n\n\n\n\nFigur¬†3: Et nyt r√∏dt punkt er indtegnet og den linje, som ADALINE finder.\n\n\n\nDet er jo ikke ligefrem super overbevisende. Data er line√¶r separable og alligevel kan ADALINE ikke finde ud af at finde en rette linje, som kan adskille de r√∏de punkter fra de bl√•!\nHvis vi skal forst√•, hvad der sker, m√• vi se lidt n√¶rmere p√• den tabsfunktion, som ADALINE fors√∏ger at mininere. Fra afsnittet om ADALINE ved vi, at tabsfunktionen er\n\\[\n\\begin{aligned}\nE(w_0, w_1, &\\dots, w_n)\\\\ &= \\frac{1}{2} \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\end{aligned}\n\\]\nhvor det \\(m\\)‚Äôte tr√¶ningseksempel er \\[(x_{m,1}, x_{m,2}, \\dots, x_{m,n}, t_m)\\]\nDet vil sige, at det \\(m\\)‚Äôte tr√¶ningseksempel giver et bidrag til tabsfunktionen p√•\n\\[\n\\left ( t_m- (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\]\nFor et bl√•t punkt med \\(t_m=1\\) vil det sige, at bidraget til tabsfunktionen er pr√¶cis \\(0,\\) hvis \\[\n1- (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})=0\n\\] og for et r√∏dt punkt med \\(t_m=-1\\) er bidraget til tabsfunktionen pr√¶cis \\(0,\\) hvis \\[\n-1- (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})=0\n\\] Nu er \\(1- (w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n)=0\\) og \\(-1- (w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n)=0\\) jo bare ligninger for rette linjer. Disse linjer ses indtegnet p√• figur¬†4 (som henholdsvis en bl√• og r√∏d stiplet linje) sammen med de oprindelige seks punkter og den linje, som ADALINE fandt baseret p√• disse seks punkter. Samtidig er det for hvert punkt markeret, hvor meget dette punkt bidrager til tabsfunktionen.\n\n\n\n\n\n\nFigur¬†4: Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de. Den sorte linje med ligning \\(w_0+w_1 x_1 + w_2 x_2=0\\) svarer til den ADALINE fandt. Den bl√• stiplede linje har ligning \\(1-(w_0+w_1 x_1 + w_2 x_2)=0\\), mens den r√∏de stiplede linje har ligning \\(-1-(w_0+w_1 x_1 + w_2 x_2)=0\\).\n\n\n\nDet ses nu p√• figur¬†4, at bl√• punkter, som ligger t√¶t p√• den bl√• stiplede linje, bidrager mindst til tabsfunktionen, mens r√∏de punkter, som ligger t√¶t p√• den r√∏de stiplede linje, ligeledes bidrager mindst til tabsfunktionen. Den samlede v√¶rdi af tabsfunktionen er her \\(0.75\\).\nLaver vi nu samme √∏velse med det ekstra punkt f√•s resultat i figur¬†5.\n\n\n\n\n\n\nFigur¬†5: I alt syv punkter sammen med den sorte linje med ligning \\(w_0+w_1 x_1 + w_2 x_2=0\\) svarer til den ADALINE finder. Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de. Den bl√• stiplede linje har ligning \\(1-(w_0+w_1 x_1 + w_2 x_2)=0\\), mens den r√∏de stiplede linje har ligning \\(-1-(w_0+w_1 x_1 + w_2 x_2)=0\\). Den samlede v√¶rdi af tabsfunktionen er her \\(2.00\\).\n\n\n\nIgen ser vi, at bl√• punkter t√¶t p√• den bl√• stiplede linje bidrager mindst til tabsfunktionen og tilsvarende for de r√∏de punkter, som ligger t√¶t p√• den r√∏de stiplede linje. Det nye punkts bidrag til tabsfunktionen bliver derfor her det mindste bidrag blandt alle de r√∏de punkter. Den samlede v√¶rdi af tabsfunktionen er her \\(2.00\\).\nHvis vi i stedet pr√∏ver at bruge vores egen oprindelige linje (baseret p√• de seks f√∏rste punkter), som rent faktisk kunne adskille de bl√• punkter fra de r√∏de, s√• f√•s det resultat, som ses i figur¬†6.\n\n\n\n\n\n\nFigur¬†6: I alt syv punkter sammen med den sorte linje, som ADALINE giver baseret p√• de oprindelige seks punkter. Punkter med en targetv√¶rdi p√• \\(1\\) er bl√• og dem med en targetv√¶rdi p√• \\(-1\\) er r√∏de. Den bl√• stiplede linje har ligning \\(1-(w_0+w_1 x_1 + w_2 x_2)=0\\), mens den r√∏de stiplede linje har ligning \\(-1-(w_0+w_1 x_1 + w_2 x_2)=0\\). Den samlede v√¶rdi af tabsfunktionen er her \\(10.60\\).\n\n\n\nDet er nu tydeligt, at det nye r√∏de punkter ligger s√• langt v√¶k fra den stiplede r√∏de linje, at det bidrager betydeligt til tabsfunktionen. Derfor er den samlede v√¶rdi af tabsfunktionen \\(10.60\\) ‚Äì og derfor v√¶lger ADALINE linjen i figur¬†5 til at adskille punkterne. Ikke fordi, det er den linje, som giver den laveste andel af korrekt klassificerede, men fordi det er den linje, som minimerer tabsfunktionen! Det er jo faktisk helt sk√∏rt!"
  },
  {
    "objectID": "materialer/sigmoid_neuron/sigmoid_neuron.html#sigmoid-neuron-og-aktiveringsfunktioner",
    "href": "materialer/sigmoid_neuron/sigmoid_neuron.html#sigmoid-neuron-og-aktiveringsfunktioner",
    "title": "Sigmoid neuroner",
    "section": "Sigmoid neuron og aktiveringsfunktioner",
    "text": "Sigmoid neuron og aktiveringsfunktioner\nProblemet med ADALINE, som vi har set i eksemplet ovenfor, opst√•r fordi, et ekstremt punkt f√•r lov til at \"tr√¶kke\" uforholdsm√¶ssigt meget i den linje, som ADALINE finder, for at dette punkts bidrag til tabsfunktionen ikke skal blive alt for stort.\nVi s√• det i figur¬†5 og figur¬†6. I figur¬†5 brugte vi den linje, som ADALINE gav, og her var det ekstreme punkts bidrag til tabsfunktionen p√• \\(0.32\\). I figur¬†6 valgte vi en linje, som oplagt er bedre til at adskille de bl√• punkter fra de r√∏de, men her er det ekstreme punkts bidrag til tabsfunktionen helt oppe p√• \\(19.72\\).\nFor at forst√• det lidt bedre skal vi m√•ske lige repetere, hvordan man finder afstanden fra et punkt \\(P(x_1,y_1)\\) til en linje \\(l\\) med ligning \\(ax+by+c=0\\):\n\\[\n\\textrm{dist}(P,l)=\\frac{|a x_1 + b y_1 +c|}{\\sqrt{a^2+b^2}}\n\\]\nDenne afstandsformel kan generaliseres, s√• afstanden fra et punkt \\(P(x_{m,1}, x_{m,2}, \\dots, x_{m,n})\\) (i et \\(n\\)-dimensionalt rum!) til linjen \\(l\\) med ligning \\(w_0+w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n=0\\) er:\n\\[\n\\textrm{dist}(P,l)=\\frac{|w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}|}{\\sqrt{w_1^2 + w_2^2 + \\cdots + w_n^2}}\n\\]\nDet vil sige, at udtrykket i t√¶lleren \\(|w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}|\\) bliver et m√•l for hvor langt v√¶k punktet \\(P(x_{m,1}, x_{m,2}, \\dots, x_{m,n})\\) ligger fra linjen. Det forklarer, hvordan et ekstremt punkt kan give et meget stort bidrag til tabsfunktionen:\n\\[\n\\left ( t- (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\tag{1}\\]\nHvis punktet ligger langt v√¶k fra den linje, som m√•ske umiddelbart ser fornuftig ud, s√• vil punktet give et stort bidrag til tabsfunktionen, fordi v√¶rdien af \\(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}\\) bliver stor, og dermed vil bidraget til tabsfunktionen i (1) ogs√• blive stort!\nAlt det her leder os frem til, at valget af tabsfunktion m√•ske i virkeligheden ikke er super smart. Problemet opst√•r grundl√¶ggende, fordi targetv√¶rdien \\(t\\) og udtrykket i den inderste parentes i (1) er p√• to helt vidt forskellige skalaer. Targetv√¶rdien er enten \\(-1\\) eller \\(1\\), mens udtrykket \\(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}\\) kan antage en hvilken som helst reel v√¶rdi ‚Äì en v√¶rdi som kan blive v√¶ldig stor, hvis punktet \\(P(x_{m,1}, x_{m,2}, \\dots, x_{m,n})\\) ligger langt v√¶k fra linjen. Derfor bliver ADALINE n√∏dt til at tvinge linjen med ligningen\n\\[\nw_0 + w_1 \\cdot x_{1} + \\cdots + w_n \\cdot x_{n}=0\n\\]\nover mod et ekstremt punkt, s√•dan at dette punkts bidrag til tabsfunktionen ikke bliver alt for stort.\nProblemet kan l√∏ses ved at bruge det, man kalder for en aktiveringsfunktion. Helt grundl√¶ggende handler det om, at targetv√¶rdi \\(t,\\) og det udtryk som vi beregner p√• baggrund af punktet \\(P(x_{m,1}, x_{m,2}, \\dots, x_{m,n})\\) skal v√¶re p√• samme skala. Vi vil her illustrere det ved at bruge en aktiveringsfunktion, som ofte benyttes, og som kaldes for \"sigmoid\"-funktionen \\(\\sigma\\). Forskriften for sigmoid-funktionen er:\n\\[\n\\sigma(x)=\\frac{1}{1+e^{-x}}\n\\tag{2}\\]\nGrafen for sigmoid-funktionen ses i figur¬†7.\n\n\n\n\n\n\nFigur¬†7: Grafen for sigmoid-funktionen med forskrift \\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\).\n\n\n\nDefinitionsm√¶ngden for sigmoid-funktionen er alle reele tal, mens v√¶rdim√¶ngden er intervallet \\((0,1)\\). Det kan skrives s√•dan her:\n\\[\n\\sigma: \\mathbb{R} \\rightarrow (0,1)\n\\]\nDet vil vi udnytte og nu omdefinere targetv√¶rdien \\(t\\) p√• denne m√•de:\n\\[\nt=\n\\begin{cases}\n1 & \\textrm{hvis punktet er bl√•t} \\\\\n0 & \\textrm{hvis punktet er r√∏dt} \\\\\n\\end{cases}\n\\]\nS√• targetv√¶rdierne er nu \\(0\\) eller \\(1\\) i stedet for \\(-1\\) og \\(1\\). Vi definerer nu tabsfunktionen s√•dan her:\n\\[\n\\begin{aligned}\nE(w_0, w_1, &\\dots, w_n) \\\\ &= \\frac{1}{2} \\sum_{m=1}^{M} \\left (t_m-\n\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\end{aligned}\n\\tag{3}\\]\nBem√¶rk, at problemet med de to skalaer nu er l√∏st. Targetv√¶rdien er enten \\(0\\) eller \\(1\\) samtidig med, at \\(\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\\) ogs√• ligger mellem \\(0\\) og \\(1\\). Vi sammenligner alts√• ikke l√¶ngere p√¶rer med bananer! N√•r sigmoid-funktionen p√• denne m√•de indg√•r i en tabsfunktion, s√• kalder man den ogs√• for en aktiveringsfunktion. Og den \"perceptron\" som minimerer tabsfunktionen i (3) kaldes for en sigmoid neuron.\nLad os nu unders√∏ge hvordan vi f√•r en lille v√¶rdi af tabsfunktionen. Hvis punktet er bl√•t (svarende til en targetv√¶rdi p√• \\(1\\)), s√• vil bidraget\n\\[\n\\left (1 - \\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\]\nv√¶re lille (alts√• s√¶t p√• \\(0\\)), hvis \\(\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\\) er t√¶t p√• \\(1\\). Og omvendt hvis targetv√¶rdien er \\(0\\) (hvilket svarer til et r√∏dt punkt), s√• vil bidraget\n\\[\n\\left (0 - \\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\]\nv√¶re lille (alts√• s√¶t p√• \\(0\\)), hvis \\(\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\\) er t√¶t p√• \\(0\\). Vi leder alts√• efter v√¶gte \\(w_0, w_1, \\dots, w_n\\), som netop har denne egenskab, fordi de s√• vil minimere tabsfunktionen.\nDette har ogs√• en anden fordel. Vi kan nemlig t√¶nke p√• v√¶rdi\n\\[\n\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\n\\]\nsom en sandsynlighed for at punktet \\(P(x_{m,1}, x_{m,2}, \\dots, x_{m,n})\\) er bl√•t. Vi kan derfor bruge denne v√¶rdi til at forudsige om et nyt punkt er bl√•t eller r√∏dt. Det vil vi g√∏re p√• denne m√•de:\n\\[\n\\textrm{Nyt punkt er }=\n\\begin{cases}\n\\textrm{bl√•t} & \\textrm{hvis } \\sigma(w_0 +  \\cdots + w_n \\cdot x_{m,n}) \\geq 0.5\\\\\n\\textrm{r√∏dt} & \\textrm{hvis } \\sigma(w_0 +  \\cdots + w_n \\cdot x_{m,n}) &lt; 0.5\\\\\n\\end{cases}\n\\tag{4}\\]\nSkillelinjen for hvorn√•r et punkt \\((x_1, x_2, \\dots, x_n)\\) er bl√•t eller r√∏dt f√•s netop, n√•r\n\\[\n\\sigma(w_0 + w_1 \\cdot x_{1} + \\cdots + w_n \\cdot x_{n}) = 0.5\n\\tag{5}\\]\nSer vi p√• definitionen af sigmoid-funktionen i (2) svarer det til at l√∏se\n\\[\n\\sigma(x)=\\frac{1}{1+e^{-x}} = 0.5\n\\]\nDette er netop opfyldt, hvis \\(e^{-x}=1\\), hvilket kun kan lade sig g√∏re, n√•r \\(x=0\\). Sammenligner vi dette med (5), s√• ser vi alts√• at skillelinjen er\n\\[\nw_0 + w_1 \\cdot x_{1} + \\cdots + w_n \\cdot x_{n} = 0\n\\]\nDen nye m√•de at t√¶nke tabsfunktionen p√• giver os alts√• stadigv√¶k en ret linje, som kan bruges til at adskille de r√∏de punkter fra de bl√•.\nLad os pr√∏ve f√∏rst at illustrere det med datas√¶ttet best√•ende af de seks punkter i figur¬†1. Resultat af at bruge ADALINE (fuldt optrukken linje) og en sigmoid neuron (stiplet linje) ses i figur¬†8. Det er her tydeligt, at begge metoder kan bruges til at finde en linje, som adskiller de bl√• punkter fra de r√∏de og der er i det hele taget ikke den store forskel p√• de to metoder.\n\n\n\n\n\n\nFigur¬†8: Stiplet linje svarer til ADALINE ‚Äì fuldt optrukken linje svarer til sigmoid neuron.\n\n\n\nBruger vi nu ADALINE og en sigmoid neuron p√• data fra figur¬†2 f√•s resultatet i figur¬†9. Igen svarer ADALINE til fuldt optrukket linje og sigmoid til stiplet linje. Vi kan nu se, at sigmoid neuron pr√¶cis g√∏r det, som vi havde h√•bet p√•: Den adskiller de bl√• punkter fra de r√∏de ogs√• selvom √©t af punkterne er ekstremt.\n\n\n\n\n\n\nFigur¬†9: Stiplet linje svarer til ADALINE ‚Äì fuldt optrukken linje svarer til sigmoid neuron.\n\n\n\nV√¶gtene fra sigmoid neuronen i det sidste eksempel er \\(w_0=-6.046\\), \\(w_1=-16.69\\) og \\(w_2=10.94\\) svarende til linjen med ligning \\[\n-6.046 - 16.69x_1+10.94x_2=0\n\\] Udregner vi \\[\n\\sigma(-6.046 - 16.69x_1+10.94x_2)\n\\] f√•r vi alts√• sandsynligheden for at et punkt er bl√•t. G√∏r vi det f√•s resultatet i nedenst√•ende tabel:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\nTargetv√¶rdi\nSandsynlighed\n\n\n\n\n\\(-0.5\\)\n\\(0.5\\)\n\\(1\\)\n\\(1.00\\)\n\n\n\\(-0.3\\)\n\\(0.3\\)\n\\(1\\)\n\\(0.90\\)\n\n\n\\(-0.1\\)\n\\(0.7\\)\n\\(1\\)\n\\(0.96\\)\n\n\n\\(0.1\\)\n\\(0.4\\)\n\\(0\\)\n\\(0.03\\)\n\n\n\\(-0.1\\)\n\\(0.2\\)\n\\(0\\)\n\\(0.10\\)\n\n\n\\(0.1\\)\n\\(-0.1\\)\n\\(0\\)\n\\(0.00\\)\n\n\n\\(1\\)\n\\(-1\\)\n\\(0\\)\n\\(0.00\\)\n\n\n\nDer er her fin overensstemmelse mellem targetv√¶rdien og den beregende sandsynlighed. L√¶g ogs√• m√¶rke til at det ekstreme punkt har en beregnet sandsynlighed p√• \\(0.00\\) og dermed bliver pr√¶dikteret til klart at v√¶re et ikke bl√•t ‚Äì det vil sige et r√∏dt ‚Äì punkt."
  },
  {
    "objectID": "materialer/sigmoid_neuron/sigmoid_neuron.html#nye-opdateringsregler",
    "href": "materialer/sigmoid_neuron/sigmoid_neuron.html#nye-opdateringsregler",
    "title": "Sigmoid neuroner",
    "section": "Nye opdateringsregler",
    "text": "Nye opdateringsregler\nI noten om perceptroner forklarede vi i afsnittet om gradientnedstigning, hvordan v√¶gtene i ADALINE algoritmen blev opdateret. Vi vil her p√• tilsvarende vis udlede de nye opdateringsregler for sigmoid neuronen.\nVi har nu tabsfunktionen\n\\[\n\\begin{aligned}\nE(w_0, w_1, &\\dots, w_n) \\\\ &= \\frac{1}{2} \\sum_{m=1}^{M} \\left (t_m-\n\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2\n\\end{aligned}\n\\]\nsom vi igen vil minimere ved at bruge gradientnedstigning. Det vil sige, at v√¶gtene opdateres ved at g√• et lille stykke i den negative gradients retning:\n\\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 - \\eta \\cdot \\frac{\\partial E }{\\partial w_0} \\\\\nw_1 \\leftarrow & w_1 - \\eta \\cdot \\frac{\\partial E }{\\partial w_1} \\\\\n&\\vdots  \\\\\nw_n \\leftarrow & w_n - \\eta \\cdot \\frac{\\partial E }{\\partial w_n} \\\\\n\\end{aligned}\n\\]\nhvor \\(\\eta\\) er en learning rate.\nVi bestemmer nu den partielle afledede for den \\(i\\)‚Äôte v√¶gt. Ved at bruge sum- og k√¶dereglen f√•r vi:\n\\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_i} &= \\frac{1}{2} \\sum_{m=1}^{M} \\frac{\\partial}{\\partial w_i}\\left (t_m-\n\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2 \\\\\n&= \\frac{1}{2} \\sum_{m=1}^{M} 2 \\cdot \\left (t_m-\n\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\\\\n& \\quad  \\quad \\quad  \\quad  \\cdot \\frac{\\partial}{\\partial w_i} \\left (t_m-\n\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n} ) \\right) \\\\\n\\end{aligned}\n\\tag{6}\\]\nBetragter vi den sidste faktor og bruger k√¶dereglen igen f√•r vi\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial w_i} (t_m - \\sigma(w_0 + & w_1 \\cdot x_{m,1} + \\cdots   + w_n \\cdot x_{m,n} )) = \\\\\n&- \\sigma'(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\\cdot \\\\ &\\frac{\\partial}{\\partial w_i} \\left (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n} \\right)\n\\end{aligned}\n\\] Nu er \\[\n\\frac{\\partial}{\\partial w_i} \\left (w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_i \\cdot x_{m,i} + \\cdots  + w_n \\cdot x_{m,n} \\right) = x_{m,i}\n\\]\nS√• \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial w_i} (t_m -\n\\sigma(w_0 &+ w_1 \\cdot x_{m,1}  + \\cdots  + w_n \\cdot x_{m,n} )) = \\\\ &- \\sigma'(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\\cdot x_{m,i}\n\\end{aligned}\n\\]\nVi mangler nu bare at finde den afledede sigmoid-funktion. Man kan vise ‚Äì og dette overlades trygt til l√¶seren ‚Äì at\n\\[\n\\sigma'(x)=\\sigma(x)\\cdot(1-\\sigma(x))\n\\]\nDerfor bliver\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial w_i} (t_m -\n\\sigma(w_0 &+ w_1 \\cdot x_{m,1} + \\cdots  + w_n \\cdot x_{m,n} )) = \\\\\n&- \\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\cdot \\\\ & (1-\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots  + w_n \\cdot x_{m,n} )) \\cdot x_{m,i}\n\\end{aligned}\n\\]\nFor at g√∏re det lidt mere l√¶sevenligt definerer vi\n\\[\no_m = \\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\n\\]\nHer st√•r \\(o_m\\) for outputv√¶rdien h√∏rende til det \\(m\\)‚Äôte tr√¶ningseksempel. Hvis vi sammenligner med (4) ses det, at det netop er denne outputv√¶rdi, som bruges til at afg√∏re om et nyt punkt er r√∏dt eller bl√•t. Vi f√•r s√•\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial w_i} (t_m -\n\\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots  &+ w_n \\cdot x_{m,n} )) = \\\\ &- o_m\\cdot (1-o_m) \\cdot x_{m,i}\n\\end{aligned}\n\\]\nInds√¶ttes dette i (6) f√•r vi og bruger at \\(o_m = \\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\\) f√•s\n\\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_i} = - \\sum_{m=1}^{M} \\left (t_m-o_m \\right) \\cdot o_m\\cdot (1-o_m) \\cdot x_{m,i}\n\\end{aligned}\n\\]\nDette g√¶lder for \\(i \\in \\{1, 2, \\dots, n \\}\\). N√•r \\(i=0\\) er det ikke sv√¶rt at overbevise sig selv om, at\n\\[\n\\frac{\\partial E}{\\partial w_0} = - \\sum_{m=1}^{M} \\left (t_m-o_m \\right) \\cdot o_m\\cdot (1-o_m) \\cdot 1\n\\]\nDerfor ender vi med:\n\n\n\n\n\n\nOpdateringsregler for sigmoid neuronen\n\n\n\n\\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 + \\eta \\cdot \\sum_{m=1}^{M} \\left (t_m-o_m \\right) \\cdot o_m\\cdot (1-o_m) \\\\\nw_1 \\leftarrow & w_1 + \\eta \\cdot \\sum_{m=1}^{M} \\left (t_m-o_m \\right) \\cdot o_m\\cdot (1-o_m) \\cdot x_{m,1}\\\\\n&\\vdots  \\\\\nw_n \\leftarrow & w_n + \\eta \\cdot \\sum_{m=1}^{M} \\left (t_m-o_m \\right) \\cdot o_m\\cdot (1-o_m) \\cdot x_{m,n}\n\\end{aligned}\n\\]\nhvor \\(o_m = \\sigma(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\\).\n\n\nI praktisk vil man blive ved med at opdatere v√¶gtene, indtil v√¶rdien af tabsfunktionen ikke √¶ndrer sig s√¶rlig meget."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html",
    "href": "materialer/krydsvalidering/krydsvalidering.html",
    "title": "Overfitting, modeludv√¶lgelse og krydsvalidering",
    "section": "",
    "text": "Denne note handler om, hvad man kan g√∏re, n√•r man har flere forskellige modeller for data at v√¶lge imellem og gerne vil v√¶lge den bedste. Noten introducerer f√∏rst polynomiel regression, der bruges som gennemg√•ende eksempel. Mod slutningen diskuteres, hvordan de samme principper kan bruges i forbindelse med nogle af de andre algoritmer, der er gennemg√•et her p√• siden."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#polynomiel-regression",
    "href": "materialer/krydsvalidering/krydsvalidering.html#polynomiel-regression",
    "title": "Overfitting, modeludv√¶lgelse og krydsvalidering",
    "section": "Polynomiel regression",
    "text": "Polynomiel regression\n\nLine√¶r regression\nFra gymnasieundervisningen kender I line√¶r regression. Lad os sige, at vi har datapunkter \\((x_i,y_i)\\), hvor \\(i=1,2,\\ldots,n\\). Vi vil gerne finde den rette linje, der bedst beskriver punkterne. I denne note kalder vi linjens sk√¶ring for \\(a_0\\) og h√¶ldningen for \\(a_1\\). Linjen har alts√• funktionsforskriften1\n1¬†Du er vant til, at forskriften for en line√¶r funktion er p√• formen \\(f(x)=ax+b\\). Men lige om lidt viser skrivem√•den \\(f(x)=a_0+a_1x\\) sig nyttig. I forhold til det, du kender, svarer det til, at \\(a_0=b\\) og \\(a_1=a\\).\\[f(x) = a_0 + a_1x.\\]\nFor at finde den bedste linje til at beskrive vores data, s√∏ger vi de v√¶rdier \\(a_0\\) og \\(a_1\\), som g√∏r, at \\(a_0 + a_1x_i\\) er s√• t√¶t p√• \\(y_i\\) som muligt. Vi vil alts√• gerne g√∏re afvigelserne fra linjen \\(y_i - (a_0 + a_1 x_i)\\) s√• sm√• som muligt. Bem√¶rk, at disse afvigelser svarer til residualerne\n\\[\nr_i=y_i - (a_0 + a_1 x_i).\n\\]\nSom et samlet m√•l for hvor store disse afvigelser er for alle vores punkter, kigger vi p√• kvadratsummen af afvigelserne/residualerne\n\\[\n\\begin{aligned}\nE &= \\left(y_1 - (a_0 + a_1 x_1) \\right)^2 + \\left(y_2 - (a_0 + a_1 x_2) \\right)^2 + \\cdots + \\left(y_n - (a_0 + a_1 x_n) \\right)^2 \\\\\n& = r_1^2 + r_2^2 + \\cdots + r_n^2\n\\end{aligned}\n\\]\nNu er det lidt omst√¶ndeligt at skrive summen ud, som vi har gjort det ovenfor. I matematik vil man ofte skrive en s√•dan sum lidt mere kompakt ved hj√¶lp af et summationstegn. G√∏r vi det, ser det s√•dan her ud:\n\\[\n\\begin{aligned}\nE &=\\sum_{i=1}^n \\left(y_i - (a_0 + a_1x_i) \\right)^2 \\\\\n&= \\sum_{i=1}^n r_i^2 .\n\\end{aligned}\n\\]\nVi v√¶lger s√• de v√¶rdier \\(a_0\\) og \\(a_1\\), der g√∏r \\(E\\) mindst mulig. Dette kaldes mindste kvadraters metode.\n\n\nKvadratisk regression\nHvad nu hvis det slet ikke ligner, at der er en line√¶r sammenh√¶ng, n√•r vi tegner vores datapunkter ind i et koordinatsystem? Er det s√• overhovedet en god id√© at fors√∏ge med en line√¶r regression? P√• figur¬†1 ser det for eksempel ikke ud til at punkterne f√∏lger en ret linje.\n\n\n\n\n\n\n\n\nFigur¬†1: Til venstre ses et punktplot af et datas√¶t. Til h√∏jre er den bedste rette linje indtegnet.\n\n\n\n\n\n\n\n\n\n\n\nDatas√¶ttet\n\n\n\n\n\nDatas√¶ttet fra figur¬†1 ses i tabellen herunder, hvis du selv vil pr√∏ve at lave line√¶r regression p√• data. Data kan ogs√• hentes som en Excel-fil her.\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(y\\)\n\n\n\n\n0.125\n1.71\n\n\n0.25\n1.95\n\n\n0.375\n1.877\n\n\n0.5\n1.914\n\n\n0.625\n2.341\n\n\n0.75\n1.692\n\n\n0.875\n2.473\n\n\n1\n2.217\n\n\n1.125\n2.199\n\n\n1.25\n1.962\n\n\n1.375\n2.125\n\n\n1.5\n2.595\n\n\n1.625\n2.021\n\n\n1.75\n1.894\n\n\n1.875\n1.309\n\n\n2\n1.545\n\n\n2.125\n0.7685\n\n\n2.25\n0.638\n\n\n2.375\n0.7456\n\n\n2.5\n0.1396\n\n\n\n\n\n\n\n\nDet fremg√•r ogs√• af residualplottet i figur¬†2. Her kan vi tydeligt se, at der er et m√∏nster i den m√•de, residualerne fordeler sig omkring \\(x\\)-aksen. Det er alts√• tegn p√•, at en ret linje ikke er velegnet til at beskrive datapunkterne.\n\n\n\n\n\n\n\n\nFigur¬†2: Residualplottet for den bedste rette linje indtegnet i figur¬†1.\n\n\n\n\n\nN√•r \\(x\\) ligger mellem 0 og 1, kunne der godt se ud til at v√¶re en svagt stigende tendens i figur¬†1, mens der ser ud til at v√¶re en aftagende tendens for \\(x&gt;1.5\\). Det svarer til, at residualerne i figur¬†2 f√∏rst er negative, s√• positive og dern√¶st negative igen. Den rette linje i figur¬†1 ser heller ikke ud til at f√∏lge punkterne s√¶rlig godt. M√•ske en parabel passer bedre p√• data?\n\n\n\n\n\n\n\n\nFigur¬†3: Datas√¶ttet fra figur¬†1, men nu med en parabel indtegnet.\n\n\n\n\n\nDet ser ud til, at parablen i figur¬†3 f√∏lger datapunkterne langt bedre. Vi kunne s√•ledes pr√∏ve at modellere \\(y\\) ved hj√¶lp af et andengradspolynomium i \\(x\\). Lad \\(f\\) betegne andengradspolynomiet2\n2¬†I gymnasiet skriver vi som regel forskriften for et andengradspolynomium p√• formen \\(f(x)=ax^2+bx+c\\). Med notationen, som vi bruger her, svarer det til, at \\(a_0=c, a_1=b\\) og \\(a_2=a\\).\\[\nf(x) = a_0 + a_1x + a_2x^2\n\\]\nmed koefficienter \\(a_0,a_1,a_2\\in \\mathbb{R}\\).\nHvordan finder man s√• det andengradspolynomium, der bedst beskriver datapunkterne? Tilgangen er faktisk den samme som den mindste kvadraters metode, I kender fra line√¶r regression. Vi s√∏ger de v√¶rdier \\(a_0, a_1\\) og \\(a_2\\), som g√∏r, at \\(f(x_i)\\) kommer s√• t√¶t p√• \\(y_i\\) som muligt. Vi vil alts√• gerne g√∏re forskellene \\(y_i - f(x_i)\\) s√• sm√• som muligt. Vi kigger derfor p√• kvadratsummen af disse forskelle \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2 = \\sum_{i=1}^n \\left(y_i - (a_0 + a_1x_i + a_2x_i^2)\\right)^2.\\] Vi s√∏ger s√• de v√¶rdier \\(a_0,a_1\\) og \\(a_2\\), der minimerer \\(E\\).\nG√∏r man det i vores lille dataeksempel, f√•s netop den parabel, der er tegnet ind i koordinatsystemet i figur¬†3. Vi ser, at den beskriver data langt bedre end den rette linje.\nEksemplet viser vigtigheden af at tegne et residualplot for at vurdere anvendeligheden af den line√¶re model. Ellers kan man nemt komme til at overse en eventuel ikke-line√¶r sammenh√¶ng.\n\n\nPolynomiel regression generelt\nMen hvordan kan vi nu vide, at et andengradspolynomium er det bedste til at beskrive data? M√•ske et polynomium af endnu h√∏jere grad ville v√¶re bedre? Man kan tilpasse tredje- og h√∏jeregradspolynomier til data p√• en helt tilsvarende m√•de. Vi kan for eksempel pr√∏ve at tilpasse et tredjegradspolynomium\n\\[\nf(x) = a_0 + a_1x + a_2x^2 +a_3x^3.\n\\]\nDet bedste tredjegradspolynomium er igen det, der minimerer kvadratsummen \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2 = \\sum_{i=1}^n \\left(y_i - (a_0 + a_1x_i + a_2x_i^2 + a_3x_i^3 )\\right)^2.\\] Grafen for det bedste tredjegradspolynomium er indtegnet med gr√∏n for vores dataeksempel i figur¬†4. Andengradspolynomiet er indtegnet med r√∏d til sammenligning.\n\n\n\n\n\n\n\n\nFigur¬†4: Et andengradspolynomium (r√∏d) og et tredjegradspolynomium (gr√∏n) fittet til data.\n\n\n\n\n\nDet er ikke s√• let at se forskel. De to polynomier ser ud til at passe nogenlunde lige godt p√• vores data. Men i figur¬†5 har vi zoomet ud p√• figuren ovenfor, og her er der en klar forskel:\n\n\n\n\n\n\n\n\nFigur¬†5: Plottet fra figur¬†4, men hvor der nu er zoomet lidt ud. Her ses det tydeligt, at der i ‚Äúenderne‚Äù bliver stor forskel p√• anden- og tredjegradspolynomiet.\n\n\n\n\n\nSelv om der ikke var stor forskel p√• anden- og tredjegradspolynomiet p√• intervallet \\([0;2,5]\\) hvor alle \\(x\\)-v√¶rdierne i vores datas√¶t l√•, s√• er der stor forskel, n√•r vi kommer uden for dette interval. Man skal derfor passe p√• med at drage konklusioner om \\(x\\)-v√¶rdier uden for intervallet, hvor \\(x\\)-v√¶rdierne i vores datas√¶t ligger (det kaldes at ekstrapolere), da disse kan v√¶re meget f√∏lsomme over for, hvilken grad vi har valgt for vores polynomium. I det hele taget er det en ulempe ved polynomiel regression, at polynomierne har en tendens til at opf√∏re sig vildt i enderne."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#overfitting",
    "href": "materialer/krydsvalidering/krydsvalidering.html#overfitting",
    "title": "Overfitting, modeludv√¶lgelse og krydsvalidering",
    "section": "Overfitting",
    "text": "Overfitting\nDet er alts√• sv√¶rt at afg√∏re med det blotte √∏je, om anden- eller tredjegradspolynomiet passer bedst til punkterne. Hvordan v√¶lger vi s√•, hvad der er bedst? Som m√•l for, hvor t√¶t polynomiet er p√• data, kan vi kigge p√• kvadratsummen af afvigelserne \\(y_i - f(x_i)\\), alts√• \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2.\\] For andengradspolynomiet f√•r vi en kvadratsum p√• \\(E=1.14\\), mens vi f√•r \\(E=1.10\\) for tredjegradspolynomiet. Tredjegradspolynomiet kommer alts√• t√¶ttere p√• data end andengradspolynomiet. Det er p√• den anden side ikke s√• overraskende, for ved at s√¶tte \\(a_3=0\\) i et tredjegradspolynomium f√•s et andengradspolynomium. Andengradspolynomier er alts√• specialtilf√¶lde af tredjegradspolynomier. Vi vil derfor altid kunne tilpasse data mindst lige s√• godt med et tredjegradspolynomium som med et andengradspolynomium.\nKan det s√• altid betale sig at bruge et polynomium af h√∏jere grad? Lad os pr√∏ve med et syvendegradspolynomium. Vi s√∏ger \\[f(x) = a_0 + a_1x + a_2x^2 +a_3x^3 + a_4x^4 + a_5x^5 +a_6x^6 +a_7x^7,\\] der minimerer kvadratsummen \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2 .\\] Det bedste syvendegradspolynomium i vores lille dataeksempel er indtegnet med bl√• p√• figur¬†6 nedenfor:\n\n\n\n\n\n\n\n\nFigur¬†6: Et andengradspolynomium (r√∏d) og et syvendegradspolynomium (bl√•) fittet til data.\n\n\n\n\n\nKvadratsummen er p√• kun \\(E=0.90\\), s√• umiddelbart virker det til at v√¶re en meget bedre model. Der er dog visse problemer. Det ses, at grafen bugter sig meget for at komme s√• t√¶t som muligt p√• datapunkterne. Dels virker det urealistisk, at den faktiske sammenh√¶ng mellem \\(x\\) og \\(y\\) skulle v√¶re s√• kompliceret. Dels opst√•r der et problem, hvis vi kommer med nye datapunkter. I figur¬†7 er polynomierne fra f√∏r tegnet sammen med 20 nye datapunkter i gr√∏n (som stammer fra den samme underliggende fordeling). Nu beskriver syvendegradspolynomiet pludselig ikke datapunkterne s√• godt l√¶ngere.\n\n\n\n\n\n\n\n\nFigur¬†7: Andengradspolynomiet (r√∏d) og syvendegradspolynomiet (bl√•) fra figur¬†6 sammen med 20 nye datapunkter (gr√∏n), som kommer fra den samme underliggende fordeling, som de sorte datapunkter fra figur¬†6.\n\n\n\n\n\nDet, der sker her, er et eksempel p√• det f√¶nomen, der kaldes overfitting: syvendegradspolynomiet havde tilpasset sig for godt til lige netop de sorte datapunkter. N√•r graden bliver for h√∏j, begynder polynomiet at tilpasse sig nogle strukturer i data, som i virkeligheden bare skyldes tilf√¶ldigheder. Det fungerer rigtig godt til at beskrive det oprindelige data, men til geng√¶ld er det d√•rligt til at forudsige nye datav√¶rdier.\nJo h√∏jere grad man v√¶lger, at polynomiet skal have, desto bedre kan man tiln√¶rme data. Med \\(n\\) datapunkter (som alle have forskellige \\(x\\)-v√¶rdier), kan man faktisk altid finde et polynomium af grad \\(n-1\\), der g√•r igennem alle datapunkterne, men det er klart, at nye datapunkter ikke n√∏dvendigvis f√∏lger dette polynomium s√¶rlig godt.\n\nModelfleksibilitet\nDet, vi s√• ovenfor, var, at vi havde forskellige modeller for data (polynomier af forskellig grad). Modellerne havde forskellig fleksibilitet (h√∏j grad gjorde polynomiet meget fleksibelt). N√•r vi brugte en model med for lav fleksibilitet (line√¶r regression), kunne vi ikke tilpasse modellen godt nok til data. N√•r vi valgte en model med for h√∏j fleksibilitet (polynomium af grad syv), opstod der problemer med overfitting, og modellen var ikke god til at beskrive nye data.\nDet tilsvarende problem opst√•r ogs√• i andre sammenh√¶nge, n√•r man har flere forskellige modeller at v√¶lge imellem. Nogle vil v√¶re for ufleksible til at beskrive data ordentligt. Andre vil v√¶re for fleksible og f√∏re til overfitting. S√• hvordan finder vi et godt kompromis? Det handler det f√∏lgende om."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#tr√¶nings--og-testdata",
    "href": "materialer/krydsvalidering/krydsvalidering.html#tr√¶nings--og-testdata",
    "title": "Overfitting, modeludv√¶lgelse og krydsvalidering",
    "section": "Tr√¶nings- og testdata",
    "text": "Tr√¶nings- og testdata\nN√•r vi har et datas√¶t og pr√∏ver at tilpasse en polynomiel regressionsmodel, siger vi, at vi tr√¶ner modellen. Datas√¶ttet, vi bruger til at tr√¶ne modellen, kaldes tr√¶ningsdata. Som vi s√• ovenfor, indeb√¶rer det en risiko for overfitting, n√•r vi tr√¶ner modellen. Hvis vi kommer med et nyt datas√¶t af samme type, passer modellen ikke n√∏dvendigvis s√¶rlig godt.\nFor at vurdere hvilken grad af polynomiet der passer bedst, kan vi se p√•, hvilken model der er bedst til at forudsige (ogs√• kaldet pr√¶diktere) \\(y\\)-v√¶rdierne i et nyt datas√¶t. Det nye datas√¶t kaldes testdata. Lad os kalde testdatapunkterne for \\((x_i^{test},y^{test}_i)\\), hvor \\(i=1,\\ldots,m\\). Man kan m√•le, hvor godt modellen forudsiger testdata ved at se p√• forskellene \\(y_i^{test}-f(x_i^{test})\\) mellem de observerede v√¶rdier \\(y_i^{test}\\) og dem, der forudsiges af polynomiet \\(f(x_i^{test})\\). Som samlet m√•l for, hvor godt modellen forudsiger testdata, beregner vi kvadratsummen af disse forskelle \\[E^{test} = \\sum_{i=1}^{m} \\left(y_i^{test} - {f}\\left(x_i^{test}\\right)\\right)^2.\\] Man kalder \\(E^{test}\\) for tabsfunktionen.\nI praksis har man typisk kun et datas√¶t til r√•dighed, og man er derfor n√∏dt til f√∏rst at dele data i to. Hele processen med at inddele data og f√∏rst tr√¶ne modellen og derefter teste den er, som f√∏lger:\n\nV√¶lg en polynomiumsgrad \\(p\\).\nDatas√¶ttet inddeles i to dele, √©n del der bruges som tr√¶ningsdata, og √©n del der bruges som testdata. Vi vil betegne punkterne i tr√¶ningsdata med \\((x_i^{tr√¶n},y_i^{tr√¶n})\\), \\(i=1,\\ldots,n\\), og punkterne i testdata med \\((x_i^{test},y_i^{test})\\), \\(i=1,\\ldots,m\\).\nVi tr√¶ner modellen p√• tr√¶ningsdatas√¶ttet og finder det \\(p\\)‚Äôte-gradspolynomium \\[f(x)=a_0 + a_1 x + \\dotsm + a_px^p\\] der passer bedst p√• data. Mindste kvadraters metode benyttes til at bestemme \\(a_0,\\ldots,a_p\\) som de tal, der minimerer \\[E^{tr√¶n}(p)=\\sum_{i=1}^{n} (y_i^{tr√¶n} - {f}(x_i^{tr√¶n}))^2.\\] Det bedste polynomium kalder vi \\(\\hat{f}\\).\nN√•r vi har valgt funktionen \\(\\hat{f}\\) p√• baggrund af tr√¶ningsdataet, tester vi den p√• testdataet ved at beregne \\[E^{test}(p)=\\sum_{i=1}^{m} (y_i^{test} - \\hat{f}(x_i^{test}))^2.\\] Jo mindre \\(E^{test}(p)\\) er, des bedre passer \\(\\hat{f}\\) p√• testdata.\n\nDenne procedure kan gentages for forskellige v√¶rdier af polynomiumsgraden \\(p\\). Det \\(p\\), der giver den mindste v√¶rdi af \\(E^{test}(p)\\) svarer alts√• til den model, der er bedst til at forudsige v√¶rdierne i vores testdata, og vi v√¶lger derfor at bruge dette \\(p\\).\nI vores eksempel ovenfor f√•r vi \\(E^{test}(2)=1.82\\) og \\(E^{test}(7)=2.29\\), n√•r vi bruger de sorte datapunkter som tr√¶ningsdata og de gr√∏nne datapunkter som testdata. Andengradspolynomiet er alts√• bedre end syvendegradspolynomiet til at forudsige testdata. Bem√¶rk, at i begge tilf√¶lde er \\(E^{test}\\) langt st√∏rre end \\(E^{tr√¶n}\\), fordi modellen kun er tilpasset til tr√¶ningsdata.\nN√•r det bedst mulige \\(p\\) er valgt, kan man s√• tr√¶ne modellen igen p√• alt dataet, b√•de test- og tr√¶ningsdata, for at f√• et mere pr√¶cist bud p√• det bedste polynomium. Dette er s√• vores endelige model for data. I vores eksempel finder man, at \\(p=2\\) giver lavest \\(E^{test}\\). Vi sl√•r s√• de sorte og gr√∏nne datapunkter sammen til et datas√¶t og bruger dem til at finde det bedste andengradspolynomium. Det giver vores endelige model for sammenh√¶ngen mellem \\(x\\) og \\(y\\), som bliver \\[\nf(x) = 1.36 + 1.72x - 0.87x^2.\n\\]"
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#sec-krydsvalidering",
    "href": "materialer/krydsvalidering/krydsvalidering.html#sec-krydsvalidering",
    "title": "Overfitting, modeludv√¶lgelse og krydsvalidering",
    "section": "Krydsvalidering",
    "text": "Krydsvalidering\nDer er et problem med tilgangen ovenfor. N√•r man tr√¶ner en model, er det altid en fordel at have s√• meget data som muligt, da man s√• har mulighed for at tr√¶ne modellen meget pr√¶cist. Problemet er, at hvis man bruger det meste af data som tr√¶ningsdata, er der ikke meget tilbage til at teste p√•, og vi risikerer overfitting.\nKrydsvalidering l√∏ser dette problem p√• snedig vis ved at gentage tr√¶nings- og testproceduren flere gange. For at lave \\(k\\)-fold krydsvalidering deler man data op i \\(k\\) lige store og tilf√¶ldige dele. I f√∏rste fold tr√¶ner man modellen p√• alt data undtagen den f√∏rste del og bruger f√∏rste del som testdata. Det given en tabsfunktion \\(E_1(p)\\). Dette gentages s√• \\(k\\) gange, hvor man i den \\(i\\)‚Äôte fold bruger den \\(i\\)‚Äôte del af data som testdata og resten som tr√¶ningsdata og f√•r en tabsfunktion \\(E_i(p)\\). Id√©en er illustreret i figur¬†8.\n\n\n\n\n\n\n\n\nFigur¬†8: Illustration af id√©en ved \\(5\\)-fold krydsvalidering.\n\n\n\n\n\nSom et samlet m√•l for, hvor god modellen er, bruges summen af tabsfunktionerne fra de \\(k\\) fold \\[E(p)=E_1(p) + E_2(p) + \\dotsm + E_k(p).\\] Man v√¶lger s√• den model, der giver den mindste v√¶rdi af \\(E(p)\\).\nFordelen ved krydsvalidering er, at man i hver fold bruger det meste af data til at tr√¶ne modellen p√•. Samtidig bliver hvert datapunkt alt i alt brugt pr√¶cis √©n gang til at teste p√•. P√• den m√•de f√•r man udnyttet data bedre end, hvis man bare laver en enkelt opdeling af data. Typisk v√¶lger man \\(k=5\\) eller \\(k=10\\).\n\nKrydsvalidering i andre sammenh√¶nge\nKrydsvalidering kan bruges i et v√¶ld af andre sammenh√¶nge, hvor der skal v√¶lges mellem flere forskellige pr√¶diktionsmodeller.\nHvis det, der skal pr√¶dikteres, er en talv√¶rdi, kan man g√∏re som ovenfor. Algoritmen tr√¶nes p√• tr√¶ningsdataet og bruges derefter til lave pr√¶diktioner \\(\\hat{y}_i^{test}\\), \\(i=1,\\ldots ,m\\), af v√¶rdierne i testdatas√¶ttet. Disse sammenlignes med de faktiske v√¶rdier \\(y_i^{test}\\) ved at se p√• forskellene \\(y_i^{test} - \\hat{y}_i^{test}\\) og beregne tabsfunktionen \\[E^{test} = \\sum_{i=1}^m(y_i^{test}-\\hat{y}_i^{test})^2.\\] Modellen med lavest \\(E^{test}\\) er bedst til at lave nye pr√¶diktioner.\nHvis der derimod er tale om et klassifikationsproblem, hvor der skal pr√¶dikteres en klasse (fx mand/kvinde, r√∏d blok/bl√• blok, almindelig mail/spam), skal man definere tabsfunktionen lidt anderledes. Som f√∏r tr√¶nes algoritmen p√• tr√¶ningsdataet og derefter bruges den til lave pr√¶diktioner af klasserne \\(\\hat{y}_i^{test}\\), \\(i=1,\\ldots ,m\\), i testdatas√¶ttet. Disse sammenholdes med de faktiske klasser, og vi bruger s√• fejlraten som tabsfuntion. Fejlraten angiver andelen af observationerne i testdataet, der bliver klassificeret forkert, det vil sige\n\\[\nE^{test} =  \\frac{1}{m}\\cdot (\\text{antal fejlklassifikationer i testdata}).\n\\tag{1}\\]\nModellen med lavest \\(E^{test}\\) har f√¶rrest fejlklassifikationer og v√¶lges derfor som den bedste.\nEksempler her fra siden, hvor krydsvalidering kan benyttes:\n\nI forl√∏bet Hvem ligner du mest sammenligner man et gr√•t punkt med alle punkter inden for en radius \\(r\\) for at forudsige farven. Det er ikke oplagt, hvordan man skal v√¶lge denne radius. En mulighed er at komme med nogle gode bud \\(r_1,\\ldots, r_N\\) p√• radier og s√• bruge krydsvalidering til at v√¶lge den bedste. √ân mulighed er at v√¶lge \\(k=n\\). S√• f√•r man det, der kaldes leave-one-out krydsvalidering. I hver fold bliver der s√• kun et datapunkt i testdata, mens resten bruges som tr√¶ningsdata. Det ene testdatapunkt farves gr√•t, og farven pr√¶dikteres ud fra de √∏vrige datapunkter, der ligger inden for den valgte radius. Pr√¶diktionen sammenlignes med punktets rigtige farve. Dette gentages for alle datapunkter og fejlraten (1) beregnes til sidst. Vi v√¶lger s√• den radius, der har lavest fejlrate.\nI noten om Kunstige neurale nerv√¶rk beskrives det, hvordan man tr√¶ner et kunstigt neuralt netv√¶rk. Men n√•r man g√∏r det, skal man p√• forh√•nd have besluttet sig for, hvor mange skjulte lag der skal v√¶re i netv√¶rket, og hvor mange neuroner, der skal v√¶re i hvert skjult lag. Jo flere skjulte lag og jo flere neuroner ‚Äì desto st√∏rre fleksibilitet. Her vil krydsvalidering v√¶re oplagt til at afg√∏re, hvor fleksibelt netv√¶rket skal v√¶re samtidig med, at man undg√•r overfitting."
  },
  {
    "objectID": "materialer/retningsafledede/kontinuitet.html",
    "href": "materialer/retningsafledede/kontinuitet.html",
    "title": "Kontinuitet for funktioner af to variable",
    "section": "",
    "text": "Vi har lige p√•st√•et, at en funktion \\(f\\) af to variable siges at v√¶re kontinuert i \\((x_0,y_0)\\), hvis f√∏lgende g√¶lder\n\\[\n\\lim_{(x,y) \\rightarrow (x_{0},y_{0})}{f\\left( x,y \\right) = f(x_{0},y_{0})}\n\\]\nMen der er faktisk grund til at dv√¶le lidt ved denne definition, for hvad vil det overhovedet sige, at \\((x,y) \\rightarrow (x_{0},y_{0})\\)? Forestil dig at du har v√¶ret i byen, og at \\((x_0,y_0)\\) er dit hjem. S√• kan man jo g√• hjem p√• rigtig mange m√•der. Det kan v√¶re, at man g√•r langs en ret linje, det kan v√¶re, at man g√•r i zig-zag hjem eller noget helt tredje. Ovenst√•ende definition p√• kontinuitet giver kun mening, hvis \\(f(x,y)\\) n√¶rmer sig \\(f(x_0,y_0)\\) uanset p√• hvilken m√•de \\((x,y)\\) n√¶rmer sig \\((x_0,y_0)\\).\nVi skal nu se p√• et eksempel, hvor en funktion \\(f\\) ikke er kontinuert i \\((0,0)\\), fordi man kan ‚Äúg√• hjem‚Äù p√• nogle m√•der, s√• \\(f(x,y)\\) ikke altid n√¶rmer sig \\(f(0,0)\\)! Det kan godt v√¶re lidt sv√¶rt at forestille sig, men her kommer eksemplet."
  },
  {
    "objectID": "materialer/retningsafledede/kontinuitet.html#skiferien",
    "href": "materialer/retningsafledede/kontinuitet.html#skiferien",
    "title": "Kontinuitet for funktioner af to variable",
    "section": "Skiferien",
    "text": "Skiferien\nVi forestiller os, at grafen for funktionen \\(f(x,y)\\) beskriver et landskab. St√•r vi p√• en flad mark er \\(f(x,y)\\) bare \\(0\\) for alle v√¶rdier af \\((x,y)\\), og det er jo √¶rlig talt lidt kedeligt. Men nu er din klasse taget p√• skiferie i et sp√¶ndende land, hvor skibakkerne kan beskrives som grafen for funktionen \\(f\\) med f√∏lgende forskrift:\n\\[\nf(x,y)=\n\\begin{cases}\n\\frac{y \\cdot x^2}{y^2+x^4} \\quad \\textrm{hvis } (x,y) \\neq (0,0) \\\\\n0 \\quad \\textrm{hvis } (x,y) = (0,0)\n\\end{cases}\n\\] Jeres hotel ligger i origo ‚Äì det vil sige i punktet \\((0,0,0)\\). I kan se skibakken i app‚Äôen herunder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI skal nu i gang med at unders√∏ge, om denne funktion er kontinuert i \\((0,0)\\). I g√∏r det ved at stille jer forskellige steder p√• skibakken og g√• af forskellige ruter hjem. De fleste af jer g√•r langs rette linjer (i \\(xy\\)-planen) og rapporterer, at funktionen er kontinuert (og faktisk ogs√• differentiabel) i \\((0,0)\\). I kan se nogle af de forskellige ruter herunder:\n\n\n\nNu er der et par elever, der rapporterer, at de kom vandrende ind mod origo p√• grafen og hele tiden var i h√∏jde \\(1/2\\) lige indtil, de faldt i et hul, da de n√•ede origo. Hvis det er rigtigt betyder det, at funktionen ikke er kontinuert i origo. De elever, der faldt i et hul, er kendt for ikke at g√• den lige vej hjem. Denne gang afsl√∏rer de, at de gik p√• grafen, mens de i \\(xy\\)-planen fulgte parablen med ligning \\(y=x^2\\). Du kan se disse elevers rute i app‚Äôen herunder.\n\n\n\nMen kan alle eleverne mon have ret? Vi pr√∏ver at regne p√• det.\n\n\n\n\n\n\nOpgave 1: G√• langs \\(y=x\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs linjen med ligning \\(y=x\\). Det vil sige bestem \\(f(x,x)\\).\nHvilken v√¶rdi g√•r denne snitfunktion imod, n√•r \\(x\\) g√•r mod \\(0\\)?\n\n\n\n\n\n\n\n\n\n\nOpgave 2: G√• langs \\(y=ax\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs linjen med ligning \\(y=ax\\). Det vil sige bestem \\(f(x,ax)\\).\nHvilken v√¶rdi g√•r denne snitfunktion imod, n√•r \\(x\\) g√•r mod \\(0\\)?\n\n\n\n\n\n\n\n\n\n\nOpgave 3: G√• langs \\(y\\)-aksen\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs \\(y\\)-aksen.\nHvilken v√¶rdi g√•r denne snitfunktion imod, n√•r \\(x\\) g√•r mod \\(0\\)?\n\n\n\n\nI de tre foreg√•ende opgaver skulle du gerne komme frem til at \\(f(x,y) \\rightarrow f(0,0)=0\\), n√•r \\((x,y) \\rightarrow (0,0)\\), s√• l√¶nge vi g√•r langs rette linjer. Vi skal nu unders√∏ge, hvad der sker, hvis vi g√•r langs parabler.\n\n\n\n\n\n\nOpgave 4: G√• langs parablen med ligning \\(y=x^2\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs parablen med ligning \\(y=x^2\\). Det vil sige bestem \\(f(x,x^2)\\).\nHvilken v√¶rdi g√•r denne snitfunktion imod, n√•r \\(x\\) g√•r mod \\(0\\)?\n\n\n\n\nHvis du har regnet rigtigt i ovenst√•ende opgave, s√• har du f√•et, at\n\\[\n\\lim_{(x,y) \\rightarrow (0,0)}{f\\left( x,y \\right)} = 1/2\n\\]\nn√•r \\((x,y) \\rightarrow (0,0)\\) langs parablen med ligning \\(y=x^2\\). Da \\(f(0,0)=0\\) har vi alts√• fundet en m√•de at n√¶rme os \\((0,0)\\) s√•\n\\[\n\\lim_{(x,y) \\rightarrow (0,0)}{f\\left( x,y \\right) \\neq f(0,0)}\n\\]\nog derfor er \\(f\\) ikke kontinuert i \\((0,0)\\), selvom det i f√∏rste omgang s√• s√•dan ud (da vi gik langs rette linjer)!\nFor sjov skyld kan vi jo pr√∏ve at unders√∏ge, om det g√¶lder langs alle parabler.\n\n\n\n\n\n\nOpgave 5: G√• langs parablen med ligning \\(y=ax^2\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs parablen med ligning \\(y=ax^2\\), hvor \\(a \\neq 0\\). Det vil sige bestem \\(f(x,ax^2)\\).\nHvilken v√¶rdi g√•r denne snitfunktion imod, n√•r \\(x\\) g√•r mod \\(0\\)?\n\n\n\n\nHvis du har regnes rigtig i ovenst√•ende, har du f√•et, at \\[\n\\lim_{(x,y) \\rightarrow (0,0)}{f\\left( x,y \\right)} = \\frac{a}{a^2+1} \\neq 0\n\\] n√•r \\(a \\neq 0\\). Igen har vi alts√• set, at\\(f\\) ikke kontinuert i \\((0,0)\\)."
  },
  {
    "objectID": "materialer/kmeans/kmeans.html",
    "href": "materialer/kmeans/kmeans.html",
    "title": "Clustering med K-means",
    "section": "",
    "text": "K-means\nN√•r \\(K\\)-means metoden bruges, er m√•let at inddele nogle observationer i grupper, s√• observationerne i hver gruppe minder meget om hinanden.\n\n\n\n\n\n\nFigur¬†1: Til venstre ses en r√¶kke observationer, som √∏nskes inddelt i \\(3\\) grupper. Til h√∏jre ses et bud p√• en s√•dan inddeling.\n\n\n\nP√• figur¬†1 til venstre ses en r√¶kke punkter, hvor vi √∏nsker at inddele punkterne i 3 grupper. Man kan nok godt f√• en id√© om, hvordan grupperne kan laves alene ved at se p√• billedet til venstre. P√• figur¬†1 til h√∏jre ses et bud p√• en l√∏sning, som ser fornuftig ud, men ved nogle punkter t√¶nker man nok alligevel lidt, om de nu skulle have v√¶ret i den orange eller bl√• gruppe. N√•r vi arbejder med \\(K\\)-means, s√• er id√©en, at vi ikke p√• forh√•nd har nogle observationer, hvor vi ved hvilken gruppe, de tilh√∏rer. Med andre ord har vi alts√• ikke et tr√¶ningsdatas√¶t at g√• ud fra her. Derfor taler man ogs√• om unsupervised learning. Det eneste, vi ved om vores punkter i figur¬†1, er deres \\(x\\)- og \\(y\\)-koordinat og ud fra det, skal vi s√• pr√∏ve at danne nogle grupper. Antallet af grupper ved man faktisk heller ikke n√∏dvendigvis noget om ‚Äì s√• her er det et valg, at vi har besluttet at pr√∏ve at inddele data i 3 grupper. Det kunne i princippet lige s√• godt have v√¶ret 2 eller 4 grupper eller noget helt andet!\nObservationerne vil vi her kalde for \\(\\vec{x_1}, \\vec{x_2},....,\\vec{x_n}\\), s√• der ialt er \\(n\\) observationer. Hver observation er et punkt med \\(d\\) koordinater (som dog behandles, som var det vektorer/stedvektorer), og som udgangspunkt benyttes euklidisk afstand til at bestemme afstand mellem punkter. I eksemplet i figur¬†1 er \\(d=2\\), fordi alle punkter i planen har 2 koordinater.\nGivet et heltal \\(k\\), s√• √∏nsker vi at opdele de \\(n\\) observationer \\(\\vec{x_1}, \\vec{x_2},....,\\vec{x_n}\\) i \\(k\\) grupper, som vi kalder for \\(S_1,S_2,....,S_k\\). Antallet af observationer i gruppen1 \\(S_i\\) betegnes med \\(|S_i|\\).\n1¬†En gruppe \\(S_i\\) er egentlig en m√¶ngde, og \\(|S_i|\\) er kardinaliteten af denne m√¶ngde ‚Äì alts√• antallet af elementer i m√¶ngden.Hele id√©en i \\(K\\)-means metoden er, at det skal v√¶re s√•dan, at observationerne i samme gruppe ligger t√¶t p√• hinanden. Det er ogs√• s√•dan, at vi har farvet punkterne til h√∏jre i figur¬†1.\nHvis man skal overs√¶tte det til matematik, s√• betyder det, at vi √∏nsker at minimere f√∏lgende sum (som vi kalder for \\(SUMPAR\\))\n\\[SUMPAR=\\sum_{i=1}^{k}\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2\\] Det ser m√•ske lidt voldsomt ud, men lad os pr√∏ve at nedbryde ovenst√•ende lidt. Vi forestiller os, at vi har de \\(k\\) grupper \\(S_1, S_2, \\dots , S_k\\). Vi ser f√∏rst p√• et punkt \\(\\vec p \\in S_i\\). Den kvadrerede afstand til et andet punkt \\(\\vec q\\) i samme gruppe er givet ved udtrykket\n\\[\n\\|\\vec p- \\vec q\\|^2\n\\]\nDet vil sige, den euklidiske afstand mellem \\(\\vec p\\) og \\(\\vec q\\) opl√∏ftet i anden. Den gennemsnitlige kvadrerede afstand til alle punkter i samme gruppe \\(S_i\\) vil derfor v√¶re\n\\[\\frac{1}{|S_i|} \\sum_{\\vec q \\in S_i} \\|\\vec p- \\vec q\\|^2 \\]\nDet er alts√• den gennemsnitlige kvadrerede afstand fra √©t punkt \\(\\vec p\\) til alle andre punkter i samme gruppe ‚Äì inklusiv punktet selv. Vi vil nu l√¶gge alle disse gennemsnitlige kvadrerede afstande sammen for alle punkter i \\(S_i\\). G√∏r vi det, f√•r vi:\n\\[ \\sum_{\\vec p\\in S_i}\\frac{1}{|S_i|}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2 \\]\nDa st√∏rrelsen \\(\\frac{1}{|S_i|}\\) indg√•r i alle led i den yderste sum, kan vi s√¶tte \\(\\frac{1}{|S_i|}\\) uden for det yderste sumtegn2. Derfor kan vi omskrive ovenst√•ende til\n2¬†Det svarer bare til at s√¶tte uden for en parentes.\\[\n\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2\n\\]\nDet her vil vi gerne g√∏re for alle grupper, og derfor ender vi samlet set med\n\\[\nSUMPAR=\\sum_{i=1}^{k}\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2\n\\tag{1}\\]\nAlt i alt f√•r vi alts√•, at \\(SUMPAR\\) giver summen af hvert punkts gennemsnitlige kvadrerede afstand til alle punkter i samme gruppe som sig selv (inklusiv sig selv).\nId√©en er s√• nu, at vi vil pr√∏ve at bestemme grupperne \\(S_1, S_2, \\dots, S_k\\) s√•dan, at denne sum bliver s√• lille s√• muligt. Det vil nemlig svare til, at de punkter, der ligger t√¶t p√• hinanden, kommer i samme gruppe, og punkter, som ligger langt v√¶k fra hinanden, kommer i forskellige grupper.\nDet er desv√¶rre ikke lige til at finde den optimale l√∏sning p√• dette problem, men her angives en metode/algoritme, som forh√•bentlig finder en god l√∏sning.\n\n\nAlgoritme\nVi vil nu se p√• en metode til at finde en god l√∏sning til \\(K\\)-means problemet. Vi f√•r her brug for ‚Äúmidterpunktet‚Äù for hver gruppe, som vi vil kalde for \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\). Ved midterpunktet vil vi simpelthen bare forst√• gennemsnittet af alle punkter i den p√•g√¶ldende gruppe.\nI algoritmen vil vi pr√∏ve at minimere f√∏lgende sum\n\\[\nSUMMIDT=\\sum_{i=1}^{k}\\sum_{\\vec p\\in S_i}\\|\\vec p-\\vec{\\mu_i}\\|^2\n\\tag{2}\\]\nHer summeres alts√• den kvadrerede afstand fra hvert punkt til midterpunktet for gruppen, som punktet er i. Og det g√∏r man s√• for alle grupper og l√¶gger alle de kvadredede afstande sammen. Senere vil vi se p√• sammenh√¶ngen mellem summen \\(SUMPAR\\) og summen \\(SUMMIDT\\).\nSp√∏rgsm√•let er nu, hvordan man kommer igang med at fastl√¶gge grupper og midterpunkter, for vi kender ikke m√¶ngderne \\(S_1,S_2,....,S_k\\) og dermed heller ikke midterpunkterne \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\).\nFor at l√∏se det problem vil vi bruge f√∏lgende fremgangsm√•de/algoritme:\n\nStart med at tage hver eneste observation og tilf√∏j den til en tilf√¶ldig gruppe (der skal mindst v√¶re √©n observation i hver gruppe).\nMidterpunkterne bestemmes ved at lade\n\n\\[\n\\vec{\\mu_i}=\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\vec p\n\\]\n\nFor hver af de \\(n\\) observationer findes det midterpunkt, der har den mindste afstand til punktet. Hvis det for en observation \\(\\vec{x_i}\\) er midterpunktet \\(\\vec{\\mu_a}\\), der er n√¶rmest, skal \\(\\vec{x_i}\\) v√¶re i m√¶ngden \\(S_a\\).\nGentag trin 2 og 3 indtil vi kommer til et tidspunkt, hvor ingen punkter kommer til at skifte til en anden gruppe.\n\nDet virker jo meget rimeligt. S√• er sp√∏rgsm√•let bare, om denne fremgangsm√•de virkelig fungerer! Det vil vi se n√¶rmere p√• i afsnittet om sammenh√¶ngen mellem SUMPAR og SUMMIDT. Men lad os starte med at se p√• et par eksempler.\n\n\nEksempel p√• beregning af midterpunkter\nF√∏rst kunne det m√•ske v√¶re rart at f√• en fornemmelse af, hvorfor \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\) betegnes som midterpunkter.\n\nEksempel 1 Vi forestiller os, at vi har to grupper med f√∏lgende punkter:\n\nGruppe 1 med punkterne \\((3,9)\\) og \\((7,11)\\).\nGruppe 2 med punkterne \\((10,30)\\), \\((17,34)\\), \\((12,27)\\) og \\((11,32)\\).\n\nSom n√¶vnt tidligere kan vi t√¶nke p√• hvert punkt som stedvektoren til punktet.3 Vi kan nu finde midterpunktet for den f√∏rste gruppe:\n3¬†Husk at et punkt og stedvektoren til punktet har samme koordinater.\\[\\vec{\\mu_1}=\\frac{1}{|S_1|}\\sum_{\\vec p\\in S_1}\\vec p = \\frac{\\begin{pmatrix}\n3 \\\\ 9\\end{pmatrix} + \\begin{pmatrix}\n7 \\\\ 11\\end{pmatrix}}{2}= \\frac{\\begin{pmatrix}\n10 \\\\ 20\\end{pmatrix} }{2} = \\begin{pmatrix}\n5 \\\\ 10\\end{pmatrix}\\]\nMidterpunktet for den f√∏rste gruppe har alts√• koordinats√¶t \\((5,10)\\).\nMidterpunktet for den anden gruppe bliver tilsvarende\n\\[\\vec{\\mu_2}=\\frac{1}{|S_2|}\\sum_{\\vec p\\in S_2}\\vec p = \\frac{\\begin{pmatrix}\n10 \\\\ 30\\end{pmatrix} + \\begin{pmatrix}\n17 \\\\ 34\\end{pmatrix} + \\begin{pmatrix}\n12 \\\\ 27\\end{pmatrix} + \\begin{pmatrix}\n11 \\\\ 32\\end{pmatrix}}{4}= \\frac{\\begin{pmatrix}\n50 \\\\ 123 \\end{pmatrix} }{4} = \\begin{pmatrix}\n12.5 \\\\ 30.75 \\end{pmatrix}\\]\nMidterpunktet for den anden gruppe har s√• koordinats√¶t \\((12.5, 30.75)\\).\nDette er illustreret i figur¬†2.\n\n\n\n\n\n\nFigur¬†2: To grupper af punkter (orange og bl√•) sammen med de tilh√∏rende midterpunkter \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\).\n\n\n\nP√• figur¬†2 bliver det tydeligt, hvorfor det er fornuftigt at v√¶lge midterpunkterne, som det sker i trin 2 i algoritmen ‚Äì midterpunkterne ligger simpelthen i ‚Äúmidten‚Äù af hver gruppe.\n\n\n\nEksempel p√• algoritmen\nLad os nu pr√∏ve at bruge algoritmen p√• punkterne fra eksempel¬†1. I figur¬†3 ses punkterne indtegnet, men uden angivelse af hvilken gruppe hvert enkelt punkt tilh√∏rer.\n\n\n\n\n\n\n\n\nFigur¬†3: Illustration af punkter som √∏nskes inddelt i 2 grupper.\n\n\n\n\n\nI trin 1 skal vi tilf√∏je hver observation i en tilf√¶ldig gruppe. Et s√•dant valg ses i figur¬†4.\n\n\n\n\n\n\n\n\nFigur¬†4: Tilf√¶ldig inddeling af punkterne i 2 grupper.\n\n\n\n\n\nVi skal nu have beregnet midtpunkterne i hver af de to grupper. G√∏r man det f√•s:\n\\[\n\\vec{\\mu_1} = \\begin{pmatrix} 8.33 \\\\ 22.0 \\end{pmatrix} \\quad \\textrm{og} \\quad \\vec{\\mu_2} = \\begin{pmatrix} 11.7 \\\\ 25.7 \\end{pmatrix}\n\\] Disse to midtpunkter er indtegnet i figur¬†5 og markeret med et plus.\n\n\n\n\n\n\n\n\nFigur¬†5: Tilf√¶ldig inddeling af punkterne i 2 grupper og med tilh√∏rende midtpunkter, som her er markeret med et plus.\n\n\n\n\n\nI trin 3 skal vi have beregnet afstand fra hver af de 6 punkter til hver af de to midtpunkter. For eksempel bliver afstanden \\(d\\) fra punktet \\((3,9)\\) til punktet med stedvektor \\(\\vec{\\mu_1}\\) v√¶re:\n\\[\nd=\\sqrt{(3-8.33)^2+(9-22.0)^2}=14.05\n\\] Resultatet af at beregne alle afstande p√• denne m√•de ses i tabel¬†1.\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand til \\(\\vec{\\mu_1}\\)\nAfstand til \\(\\vec{\\mu_2}\\)\n\n\n\n\n14.05\n18.79\n\n\n11.08\n15.39\n\n\n8.172\n4.643\n\n\n14.8\n9.894\n\n\n6.2\n1.374\n\n\n10.35\n6.368\n\n\n\n\n\n\nTabel¬†1: Afstanden fra de 6 datapunkter til hvert af midtpunkterne \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\).\n\n\n\n\nVi skal nu afg√∏re hvilken gruppe, de enkelte punkter skal tilh√∏re, ved at se p√• hvilket midtpunkt som hvert enkelt punkt ligger t√¶ttest p√•. For eksempel kan vi i tabel¬†1 se, at det f√∏rste punkt \\((3,9)\\) ligger t√¶ttest p√• \\(\\vec{\\mu_1}\\), og det punkt skal derfor (fortsat) h√∏rer til gruppe 1.\nI tabel¬†2 ses den oprindelige gruppe samt den nye gruppe for hvert punkt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand til \\(\\vec{\\mu_1}\\)\nAfstand til \\(\\vec{\\mu_2}\\)\nOpr. gruppe\nNy gruppe\n\n\n\n\n14.05\n18.79\n1\n1\n\n\n11.08\n15.39\n2\n1\n\n\n8.172\n4.643\n1\n2\n\n\n14.8\n9.894\n2\n2\n\n\n6.2\n1.374\n1\n2\n\n\n10.35\n6.368\n2\n2\n\n\n\n\n\n\nTabel¬†2: Afstanden fra de 6 datapunkter til hvert af midtpunkterne \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\) samt en angivelse af, hvilken gruppe punktet oprindeligt tilh√∏rte, og hvilken gruppe punktet tilh√∏rer efter trin 3.\n\n\n\n\nI figur¬†6 ses punkterne indtegnet med en angivelse af den nye inddeling (men stadig med de f√∏rst beregnede midterpunkter).\n\n\n\n\n\n\n\n\nFigur¬†6: Inddeling af punkterne i 2 grupper efter f√∏rste genneml√∏b af algoritmen.\n\n\n\n\n\nVi skal nu i gang med det n√¶ste genneml√∏b af algoritmen, og vi bestemmer derfor f√∏rst de nye midtpunkter. G√∏r man det f√•s:\n\\[\n\\vec{\\mu_1} = \\begin{pmatrix} 5.00 \\\\ 10.0 \\end{pmatrix} \\quad \\textrm{og} \\quad \\vec{\\mu_2} = \\begin{pmatrix} 12.5 \\\\ 30.8 \\end{pmatrix}\n\\]\nDe to nye midtpunkter ses indtegnet i figur¬†7 ‚Äì igen markeret med et plus.\n\n\n\n\n\n\n\n\nFigur¬†7: Inddeling af punkterne i 2 grupper efter f√∏rste genneml√∏b af algoritmen og med de nye tilh√∏rende midtpunkter indtegnet.\n\n\n\n\n\nVi kan nu igen udregne afstande fra alle punkter til de to nye midtpunkter og finde ud af om nogle af punkterne eventuelt skal skifte gruppe. Resultatet ses i tabel¬†3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand til \\(\\vec{\\mu_1}\\)\nAfstand til \\(\\vec{\\mu_2}\\)\nOpr. gruppe\nNy gruppe\n\n\n\n\n2.236\n23.73\n1\n1\n\n\n2.236\n20.5\n1\n1\n\n\n20.62\n2.61\n2\n2\n\n\n26.83\n5.551\n2\n2\n\n\n18.38\n3.783\n2\n2\n\n\n22.8\n1.953\n2\n2\n\n\n\n\n\n\nTabel¬†3: Afstanden fra de 6 datapunkter til hvert af de nye midtpunkterne \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\) samt en angivelse af, hvilken gruppe punktet oprindeligt tilh√∏rte, og hvilken gruppe punktet tilh√∏rer efter trin 3 (andet genneml√∏b af algoritmen).\n\n\n\n\nVi kan nu se, at ingen af punkterne har skiftet gruppe, og algoritmen stopper derfor. Den endelige inddeling i grupper bliver derfor som vist i figur¬†7, hvilket nok ogs√• er den inddelingen, som vi ville have valgt bare ved at kigge p√• punkterne med det blotte √∏je.\n\n\nFornuftigt valg af midterpunkter og grupper\nVi vil nu argumentere for, hvorfor algoritmen virker. Vi starter med at se p√•, hvorfor det er fornuftigt at v√¶lge midterpunkterne, som vi g√∏r i trin 2 i algoritmen.\nDa vi med algoritmen √∏nsker, at summen i (2) kaldet \\(SUMMIDT\\) skal minimeres, vil vi se, at valget af \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\) netop minimerer denne sum, n√•r vi t√¶nker, at grupperne er fastlagt.\nVed summen \\(SUMMIDT\\) har \\(\\vec{\\mu_i}\\) kun en effekt p√• delen h√∏rende til gruppen \\(S_i\\), alts√• \\[\\sum_{\\vec p\\in S_i}\\|\\vec p-\\vec{\\mu_i}\\|^2\\]\nFor en vektor \\(\\vec v\\), har vi f√∏lgende sammenh√¶ng mellem l√¶ngde og skalarprodukt/prikprodukt:\n\\[\\|\\vec v\\|^2=\\vec v\\cdot \\vec v \\tag{3}\\]\nDette g√∏r, at vi kan omskrive vores sum for gruppen \\(S_i\\) til\n\\[\\sum_{\\vec p\\in S_i}{(\\vec  p-\\vec{\\mu_i})\\cdot (\\vec p-\\vec{\\mu_i})}\\] Skalarproduktet udregnes ved at tage summen af produktet af tilsvarende koordinater for vektorerne. Hvis vi lader \\(p_m\\) og \\(\\mu_{i,m}\\) betegne det \\(m\\)‚Äôte koordinat af henholdsvis \\(\\vec p\\) og \\(\\vec{\\mu_i}\\), s√• kan ovenst√•ende sum skrives som\n\\[\\sum_{\\vec p\\in S_{i}}\\sum_{m=1}^{d}{(p_m-\\mu_{i,m})\\cdot (p_m-\\mu_{i,m})} = \\sum_{m=1}^{d} \\sum_{\\vec p\\in S_{i}} {(p_m-\\mu_{i,m})\\cdot (p_m-\\mu_{i,m})}\\] Her vil valget af \\(\\mu_{i,m}\\) kun have effekt p√• \\[\\sum_{\\vec  p\\in S_{i}}{(p_m-\\mu_{i,m})\\cdot (p_m-\\mu_{i,m})}\\] I denne sum har vi ikke l√¶ngere vektorer, og vi kan derfor benytte anden kvadrats√¶tning til at f√•\n\\[\\sum_{\\vec p\\in S_i}{(p_m^2-2\\cdot p_m\\cdot \\mu_{i,m}+\\mu_{i,m}^2)}\\] For at finde ud af hvordan \\(\\mu_{i,m}\\) skal v√¶lges for at lave summen mindst mulig, differentieres ovenst√•ende udtryk med hensyn til \\(\\mu_{i,m}\\) og udtrykket s√¶ttes lig med \\(0\\):\n\\[\\begin{align}\n\\frac{\\partial}{\\partial \\mu_{i,m}} \\sum_{\\vec p\\in S_i}{(p_m^2-2\\cdot p_m\\cdot \\mu_{i,m}+\\mu_{i,m}^2)}\n&= \\sum_{\\vec p\\in S_i} \\frac{\\partial}{\\partial \\mu_{i,m}} {(p_m^2-2\\cdot p_m\\cdot \\mu_{i,m}+\\mu_{i,m}^2)}\\\\\n&= \\sum_{\\vec p\\in S_i}{(-2\\cdot p_m+2\\cdot \\mu_{i,m})}=0\n\\end{align}\\]\nDen sidste ligning kan omskrives til \\[ \\sum_{\\vec p\\in S_i}{2\\cdot \\mu_{i,m}} = \\sum_{\\vec p\\in S_i}{2\\cdot p_m}\\] Ved division med \\(2\\) f√•s \\[ \\sum_{\\vec p\\in S_i}{\\mu_{i,m}} = \\sum_{\\vec p\\in S_i}{p_m}\\]\nVi kan nu udnytte at hvert led i den f√∏rste sum slet ikke afh√¶nger af \\(\\vec p\\), og da summen best√•r af\\(|S_i|\\) led f√•s \\[|S_i| \\cdot  \\mu_{i,m}= \\sum_{\\vec p\\in S_i}{p_m}\\]\nAlts√• er \\[\\mu_{i,m}=\\frac{1}{|S_i|} \\sum_{\\vec p\\in S_i}p_m\\] Hvis dette valg tages for alle koordinater for \\(\\vec{\\mu_i}\\) svarer det til \\[\\vec{\\mu_{i}}=\\frac{1}{|S_i|} \\sum_{\\vec p\\in S_i} \\vec p\\] som netop er den m√•de \\(\\vec{\\mu_i}\\) v√¶lges p√• ved trin \\(2\\) i algoritmen.\nHer glemte vi at argumentere for, at valget af \\(\\mu_{i,m}\\) rent faktisk gav et lokalt minimum, men lidt l√∏st kan man sige, at hvis \\(\\mu_{i,m}\\) enten v√¶lges alt for lille eller stor, vil afstanden og dermed ogs√• den kvadrerede afstand til punkterne i gruppen \\(S_i\\) blive store. Det kan selvf√∏lgelig ogs√• bevises helt formelt.\nLad os nu se p√• valget af grupper ved trin \\(3\\) i algoritmen. For en punkt \\(\\vec p\\) v√¶lges den gruppe \\(S_i\\), hvor midtpunktet \\(\\vec{\\mu_i}\\) er t√¶ttest p√• \\(\\vec p\\). Derved er det oplagt, at denne proces minimerer summen \\(SUMMIDT\\) i (2), n√•r vi har fastholdt midterpunkterne \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\).\nN√•r et punkt skifter gruppe vil \\(SUMMIDT\\) ikke l√¶ngere v√¶re optimal i forhold til \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\) f√∏r de bliver opdateret igen. I algoritmen bliver disse to trin netop gentaget, indtil ingen punkter skifter gruppe, hvorved \\(SUMMIDT\\) har ramt et lokalt minimum i forhold til valg af gruppe for det enkelt punkt og valg af midterpunkt for hver gruppe.\n\n\nSammenh√¶ngen mellem SUMPAR og SUMMIDT\nNu vil vi endelig se p√• sammenh√¶ngen mellem de to summer \\(SUMMIDT\\) i (2) og \\(SUMPAR\\) i (1). Vi vil vise, at \\[SUMPAR=2\\cdot SUMMIDT\\] n√•r midterpunkterne er valgt p√• denne m√•de\n\\[\n\\vec{\\mu_{i}}=\\frac{1}{|S_i|} \\sum_{\\vec p\\in S_i}\\vec p \\quad \\quad \\textrm{og derved} \\quad \\quad |S_i|\\cdot \\vec{\\mu_{i}}=\\sum_{\\vec p\\in S_i} \\vec p\n\\tag{4}\\]\nDet betyder, at hvis vi minimerer summen \\(SUMMIDT\\), s√• har vi ogs√• minimeret summen \\(SUMPAR\\), som var det vi oprindeligt √∏nskede.\nVi starter med at se p√• summen \\(SUMPAR\\) i (1) dog kun for en af grupperne \\(S_a\\) og uden faktoren \\(\\frac{1}{|S_i|}\\). Alts√• ser vi p√• summen \\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p-\\vec q\\|^2\n\\tag{5}\\]\nVed at bruge sammenh√¶ngen mellem l√¶ngde af vektor og skalarprodukt f√•r vi\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}(\\vec p-\\vec q)\\cdot(\\vec p-\\vec q)\n\\]\nHer bruger vi nu, hvad der svarer til anden kvadrats√¶tning for vektorer og vi omskriver tilbage til l√¶ngder ved at bruge (3)\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}(\\|\\vec p\\|^2+\\|\\vec q\\|^2-2\\cdot \\vec p\\cdot \\vec q)\n\\]\nDenne dobbeltsum opdeles nu i tre dobbeltsummer og \\(-2\\) kan tr√¶kkes ud af den ene\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2+\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec q\\|^2-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a} \\vec p\\cdot \\vec q\n\\]\nDe to f√∏rste dobbeltsummer er faktisk ens og derfor f√•r vi\n\\[\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q\n\\tag{6}\\]\nFor at komme videre med ovenst√•ende v√¶lger vi at se p√• dobbeltsummen\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q\n\\] Den inderste sum afh√¶nger ikke af \\(\\vec p\\) og derfor kan \\(\\vec p\\) s√¶ttes uden for sumtegnet4:\n4¬†Husk p√• at den distributive regel ogs√• g√¶lder for vektorer: \\(\\vec a \\cdot \\vec b + \\vec a \\cdot \\vec c = \\vec a \\cdot \\left (\\vec b + \\vec c \\right)\\)\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\sum_{\\vec p\\in S_a}\\vec p\\cdot \\left (\\sum_{\\vec q\\in S_a} \\vec q \\right )\n\\]\nFra valget af \\(\\vec{\\mu_a}\\) ved vi fra (4), at \\(|S_a|\\cdot \\vec{\\mu_{a}}=\\sum_{\\vec q \\in S_a} \\vec q\\). Bruger vi det f√•r vi\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\sum_{\\vec p\\in S_a}\\vec p\\cdot |S_a|\\cdot \\vec{\\mu_{a}}\n\\] S√¶tter vi \\(|S_a|\\cdot \\vec{\\mu_{a}}\\) uden for summmen5 og udnytter ovenst√•ende √©n gang til, f√•r vi:\n5¬†Bem√¶rk, at vi igen her benytter den distributive regel for vektorer.\\[\\begin{align}\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a} \\vec p\\cdot \\vec q &= |S_a|\\cdot \\vec{\\mu_{a}} \\cdot \\left ( \\sum_{\\vec p\\in S_a}\\vec p \\right ) \\\\\n&= \\left ( |S_a|\\cdot \\vec{\\mu_{a}} \\right ) \\cdot \\left ( |S_a|\\cdot \\vec{\\mu_{a}} \\right )\n\\end{align}\\]\nVi har nu et prikprodukt mellem to vektorer, som hver is√¶r er ganget med en skalar (her \\(|S_a|\\)). Bruger vi den kommutative lov6 for at gange med en skalar, f√•r vi\n6¬†Den kommutative lov siger, at \\(k \\cdot (\\vec a \\cdot \\vec b) =  (k \\cdot\\vec a) \\cdot (\\vec b) =  (\\vec a ) \\cdot (k \\cdot\\vec b)\\)\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a} \\vec p\\cdot \\vec q = |S_a|^2 \\cdot \\| \\vec{\\mu_{a}} \\|^2\n\\]\nDet m√• derfor betyde, at \\[\n|S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q=0 \\quad \\Leftrightarrow  \\quad\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q=0\n\\]\nDa dette giver \\(0\\), kan det tilf√∏jes til udtrykket i (6):\n\\[\\begin{align}\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2&-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\\\\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q &+\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\\\\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2 +\n2 \\cdot |S_a|^2 &\\cdot \\|\\vec{\\mu_a}\\|^2-4\\cdot\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q\n\\end{align}\\]\nI den sidste dobbeltsum kan \\(\\vec p\\) igen tages ud af den inderste sum og vi kan igen udnytte at \\(|S_a|\\cdot \\vec{\\mu_{a}}=\\sum_{\\vec q \\in S_a} \\vec q\\). Derved f√•r vi\n\\[\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2 +\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-4\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot |S_a|\\cdot \\vec{\\mu_{a}}\n\\]\nVed den f√∏rste dobbeltsum ses det, at leddene ikke afh√¶nger af \\(\\vec q\\) og derfor er \\(\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2 = |S_a| \\cdot \\|\\vec p\\|^2\\) (fordi der er \\(|S_a|\\) led i summen). Det vil sige, at vi kan omskrive til\n\\[\n2\\cdot \\sum_{\\vec p\\in S_a}|S_a| \\cdot \\|\\vec p\\|^2 +\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-4\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot |S_a|\\cdot \\vec{\\mu_{a}}\n\\]\nVi kan nu se, at \\(2 \\cdot |S_a|\\) indg√•r i alle led og vi kan derfor skrive:\n\\[\n2\\cdot |S_a| \\cdot \\left ( \\sum_{\\vec p\\in S_a} \\|\\vec p\\|^2  +\n|S_a|\\cdot \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot \\vec{\\mu_{a}} \\right )\n\\]\nHer kan \\(|S_a|\\cdot \\|\\mu_a\\|^2\\) laves om til en sum, hvor alle led er \\(\\|\\mu_a\\|^2\\). Det vil sige\n\\[\n2\\cdot |S_a| \\cdot \\left ( \\sum_{\\vec p\\in S_a} \\|\\vec p\\|^2  +\n\\sum_{\\vec p\\in S_a} \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot \\vec{\\mu_{a}} \\right )\n\\]\nHele udtrykket kan nu samles i √©n sum:\n\\[\n2\\cdot |S_a| \\sum_{\\vec p\\in S_a} \\left (  \\|\\vec p\\|^2  +\n\\|\\vec{\\mu_a}\\|^2-2\\cdot\\vec p\\cdot \\vec{\\mu_{a}} \\right )\n\\] Ved brug af anden kvadrats√¶tning for vektorer kan dette omskrives til\n\\[\n2\\cdot |S_a| \\sum_{\\vec p\\in S_a}   (\\vec p - \\vec{\\mu_a}) \\cdot (\\vec p - \\vec{\\mu_a}) = 2\\cdot |S_a| \\sum_{\\vec p\\in S_a} \\| \\vec p - \\vec{\\mu_a} \\|^2\n\\] Nu kan man jo godt have glemt, hvad det overhovedet var, vi var igang med at regne p√•! Men vi minder om, at det var udtrykket i (5). Det vil sige, at vi er kommet frem til, at\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p-\\vec q\\|^2 = 2\\cdot |S_a| \\sum_{\\vec p\\in S_a} \\| \\vec p - \\vec{\\mu_a} \\|^2\n\\] Eller skrevet p√• en anden m√•de:\n\\[\n\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2 = 2 \\sum_{\\vec p\\in S_i} \\| \\vec p - \\vec{\\mu_i} \\|^2\n\\] Summerer vi over alle \\(k\\) grupper f√•r vi: \\[\n\\sum_{i=1}^k \\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2 = 2 \\sum_{i=1}^k \\sum_{\\vec p\\in S_i} \\| \\vec p - \\vec{\\mu_i} \\|^2\n\\] Sammenligner vi med (1) og (2) har vi netop vist, at\n\\[\nSUMPAR = 2 \\cdot SUMMIDT\n\\]\nDet vil alts√• sige, at hvis vi minimerer summen \\(SUMMIDT\\), s√• har vi ogs√• minimeret summen \\(SUMPAR\\), hvilket pr√¶cis var, hvad vi oprindeligt √∏nskede.\n\n\nOpsummering/optimal l√∏sning\nNu har vi set p√• selve algoritmen og fundet ud af, at den finder et lokalt minimum for summen \\(SUMPAR\\), som man √∏nsker minimeret. Der er dog ingen garanti for, at man opn√•r et globalt minimum, eller hvor lang tid algoritmen er om at finde en l√∏sning.\nDet er egentlig heller ikke noget problem at f√• lavet en algoritme, der finder en optimal l√∏sning, problemet er blot, at den vil k√∏re alt for langsomt. En s√•dan optimal algoritme kan laves ved blot at unders√∏ge hver mulig inddeling i grupper og s√• finde den inddeling, der giver den mindste v√¶rdi af \\(SUMPAR\\). Dog vil det v√¶re s√•dan, at selv ved blot \\(2\\) grupper og \\(100\\) punkter vil der v√¶re \\(2^{99}\\) muligheder7, der skal tjekkes. At unders√∏ge s√• mange muligheder er ikke praktisk muligt ‚Äì selv ikke p√• en computer!\n7¬†Fordi for hvert punkt kan punktet enten v√¶re i den ene eller den anden gruppe. Det giver i f√∏rste omgang \\(2^{100}\\) grupper. Nu vil en inddeling hvor for eksempel punktet \\(A\\) og \\(B\\) er i gruppe \\(1\\), mens \\(C\\) er i gruppe \\(2\\) v√¶re den samme inddeling, som hvis \\(C\\) er i gruppe \\(1\\) og \\(A\\) og \\(B\\) er i gruppe \\(2\\). P√• grund af denne symmetri ender vi derfor samlet set med \\(2^{100}/2=2^{99}\\) grupper.\n\nK-means ikke blot med punkter\nIndtil videre har vi udelukkende set p√• data som v√¶rende punkter, hvor vi kan anvende euklidisk afstand for at m√•le afstanden mellem punkterne. Det kunne dog v√¶re langt mere interessant f.eks. at arbejde med mennesker og information om dem (f.eks. alder, k√∏n, forbrug og s√• videre) og stadigv√¶k med et √∏nske om at inddele disse mennesker i et bestemt antal grupper, hvor der er stor ligmed mellem dem indenfor samme gruppe. Her skal man selvf√∏lgelig have t√¶nkt lidt over, hvordan man kommer fra mennesker til punkter og efterf√∏lgende f√•r noget, der svarer til euklidisk afstand. Det kan man l√¶se meget mere om under feature-skalering."
  },
  {
    "objectID": "undervisningsforlob.html",
    "href": "undervisningsforlob.html",
    "title": "Undervisningsforl√∏b",
    "section": "",
    "text": "Siden er under opbygning, men vil komme til at indholde undervisningsforl√∏b til alle niveauer.\n\n\n\n\n\n\n\n\n\n\nHvem ligner du mest?\n\n\nEvnen til at skelne mellem forskellige kategorier er helt central for os som mennesker. Vi kan langt oftest kende forskel p√• et √¶ble og en p√¶re, p√• en cykel og en knallert‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverv√•gning i Monitorbian\n\n\nNogle lande overv√•ger deres borgere. Men hvordan g√∏r man mon det? I dette forl√∏b bliver I ansat af efterretningstjenesten i landet Monitorbian for at hj√¶lpe dem med at‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI og r√∏dder i andengradspolynomier\n\n\nEt andengradspolynomium kan have enten ingen, √©n eller to r√∏dder ‚Äì og m√•ske kan du ligefrem huske en metode til at bestemme antallet af r√∏dder. Men kan man mon bruge‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\nMilj√∏- og klimaudfodringer\n\n\nMilj√∏- og klimaudfordringer er allesteds-n√¶rv√¶rende ‚Äì kan kunstig intelligens hj√¶lpe med at l√∏se verdens klimaudfordringer? I dette tv√¶rfaglige forl√∏b med dansk arbejdes‚Ä¶\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "referencer.html",
    "href": "referencer.html",
    "title": "Referencer",
    "section": "",
    "text": "Websider\n\nDTU har lavet et undervisningsmateriale, som handler om, hvordan kunstig intelligens kan bruges til at genkende lyde: Regn lyden ud\nOnline kursus hvor man l√¶rer helt grundl√¶ggende om kunstig intelligens (men ikke s√• matematisk som materialet her p√• siden): Elements of AI\nDanske Gymnasier har afholdt en konference om betydningen af AI og ChatGPT. Her p√• siden kan man genh√∏re opl√¶g fra blandt andet Vincent Hendricks, Stefan Herman m.fl.\nDen Store Danske har skrevet lidt om, hvad kunstig intelligens er - herunder ogs√• et historisk oprids af udviklingen inden for kunstig intelligens. Kunstig intelligens\nP√• siden dataekspeditioner findes en r√¶kke forl√∏b hvoraf flere omhandler AI.\n\n\n\nVideoer\n\n3Blue1Brown har lavet en r√¶kke fine videoer, som forklarer, hvad et kunstigt neuralt netv√¶rk er, og hvordan det tr√¶nes (p√• engelsk): Neural networks\n\n\n\nB√∏ger\n\nOnline bog om kunstige neurale netv√¶rk af Michael Nielsen (p√• engelsk): Neural networks and deep learning"
  },
  {
    "objectID": "materialer.html",
    "href": "materialer.html",
    "title": "Materialer",
    "section": "",
    "text": "Perceptroner\n\n\nForl√∏beren til kunstige neurale netv√¶rk er perceptroner, som du kan l√¶se mere om her.\n\n\n\n\n\n\n\n\n\n\n\n\n\nKunstige neurale netv√¶rk\n\n\nEn grundig gennemgang af matematikken bag kunstige neurale netv√¶rk.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistisk regression\n\n\nI denne note skal vi se p√• logistisk regression. M√•ske har du allerede h√∏rt begrebet \"logistisk\" i gymnasieundervisningen i forbindelse med logistisk v√¶kst. Dette er et‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetningsafledede og gradientnedstigning\n\n\nEn introduktion til retningsafledede og gradientnedstigning.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaiv Bayes klassifier\n\n\nBeskrivelse af naiv Bayes klassifikation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering med K-means\n\n\nClustering med K-means er en metode, som kan bruges til at opdele observationer i et antal grupper.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting, modeludv√¶lgelse og krydsvalidering\n\n\nHvordan v√¶lger man den bedste model til beskrivelse af data? Skal man bare v√¶lge den mest komplicerede? Eller kan der mon g√• noget galt? Det handler overfitting og‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstande og feature-skalering\n\n\nLidt om afstande mellem punkter, ord, DNA-strenge og feature-skalering.\n\n\n\n\n\n\n\nNo matching items\n\n\n\nVideoer\nTil en del af materialerne findes en r√¶kke videoer, hvor teorien forklares. Der er linket til de relevante videoer under de respektive materialer, men en samlet liste findes ogs√• her."
  },
  {
    "objectID": "sro.html",
    "href": "sro.html",
    "title": "SRO",
    "section": "",
    "text": "I arbejdet med studieretningsopgaven kan matematik og AI indg√• i et samarbejde med en lang r√¶kke andre fag. Konkrete forl√∏b er beskrevet herunder."
  },
  {
    "objectID": "sro.html#samfundsfag-og-matematik",
    "href": "sro.html#samfundsfag-og-matematik",
    "title": "SRO",
    "section": "Samfundsfag og matematik",
    "text": "Samfundsfag og matematik\n\n\n\n\n\n\nUlighed\n\n\n\n\n\nDer tages afs√¶t i f√∏lgende holdninger til ulighed uden at s√¶tte partier p√•:\n\n‚ÄùBl√• blok‚Äù: √òget ulighed er en drivkraft for √∏get v√¶kst, som giver √∏get velstand for alle.\n‚ÄùR√∏d blok‚Äù: St√∏rre lighed er et kendetegn ved de bedst fungerende demokratier og lykkeligste samfund. Det giver samtidig de bedste muligheder for alle og samlet set de bedste rammevilk√•r for virksomheder.\n\n\nProblemformulering\nHvad er ulighed, og er det et problem i Danmark?\n\nRedeg√∏r kort for begrebet ulighed - herunder holdninger til ulighed.\nForklar hvordan kunstig intelligens kan bruges ved kandidattests i forbindelse med valg og lav en simpel kandidattest ud fra nogle f√• velvalgte sp√∏rgsm√•l om aspekter af ulighed, som skal give en anbefaling om at stemme p√• enten r√∏d eller bl√• blok.\nKom desuden ind p√• forskellige matematiske m√•l for ulighed herunder Gini-koefficienten.\nDiskussionssp√∏rgsm√•let er op til jer (M√•ske kan uligheden begr√¶nses? Skal den begr√¶nses? Hvordan kan den begr√¶nses? Fordele og ulemper ved ulighed og s√• videre).\n\n\n\nMaterialer\nNoten om perceptroner.\nPerceptron app - under udarbejdelse.\nJensby, Jakob & Br√∏ndum, Peter (2020): Ulighedens mange ansigter."
  },
  {
    "objectID": "forloeb.html",
    "href": "forloeb.html",
    "title": "AI forl√∏b",
    "section": "",
    "text": "AI forl√∏b"
  },
  {
    "objectID": "materialer/perceptron/perceptron.html",
    "href": "materialer/perceptron/perceptron.html",
    "title": "Perceptroner",
    "section": "",
    "text": "Perceptroner... Hvorfor nu det? Jo, for det er faktisk s√•dan nogle, du ikke vidste, at du ikke kunne leve uden! Nu skal du h√∏re hvorfor.\n\n\nDe senere √•r er det blevet popul√¶rt, at diverse medier laver forskellige kandidattests. S√•dan nogle tests kan laves p√• mange forskellige m√•der - man kunne blandt andet bruge perceptroner! Testene fungerer som regel p√• den m√•de, at man bliver stillet en r√¶kke forskellige sp√∏rgsm√•l og s√• skal man svare p√• en skala fra meget uenig til meget enig. Disse kategorier af svar kunne f.eks. overs√¶ttes til matematik p√• denne m√•de:\n\n\n\n\n\n\n\n\n\n\n\n\nHelt enig\nOvervejende enig\nHverken/eller\nOvervejende uenig\nHelt uenig\n\n\n\n\n2\n1\n0\n-1\n-2\n\n\n\n\n\nLad os pr√∏ve at g√∏re det helt simpelt. I stedet for at komme med et bud p√• hvem man skal stemme p√•, s√• vil vi blot fors√∏ge at komme med et bud p√•, om man skal stemme p√• r√∏d eller bl√• blok (det er sikkert en h√•bl√∏s simplificering, men det m√• du tale med din samfundsfagsl√¶rer om ).\nLad os sige at vi vil basere vores bud p√• to sp√∏rgsm√•l:\n\nJeg synes, at indkomstskatten skal s√¶ttes ned.\nJeg synes ikke, at danske virksomheder skal p√•l√¶gges en CO2-afgift.\n\nVi kan sikkert hurtigt blive enige om, at hvis man er meget enig i begge sp√∏rgsm√•l, s√• h√∏rer man formentlig til i bl√• blok og modsat, hvis man er meget uenig i begge sp√∏rgsm√•l, s√• h√∏rer man nok mere hjemme i r√∏d blok. S√• at lave en perceptron, som kan hj√¶lpe os med at forudsige det, er nok ikke raketvidenskab, men det kan ikke desto mindre hj√¶lpe os med at forst√• de bagvedliggende principper og hvordan disse sidenhen kan generaliseres.\nLad os pr√∏ve at blive lidt mere specifikke og indf√∏re to variable \\(x_1\\) og \\(x_2\\), hvor\n\n\\(x_1\\): svaret p√• Jeg synes, at indkomstskatten skal s√¶ttes ned angivet p√• en skala fra -2 til 2\n\n\\(x_2\\): svaret p√• Jeg synes ikke, at danske virksomheder skal p√•l√¶gges en CO2-afgift angivet p√• en skala fra -2 til 2.\n\nVores beslutning vil vi nu ogs√• kvantificere vha. en variabel \\(t\\), som kan antage to v√¶rdier, nemlig \\(-1\\) og \\(1\\). Hvis vi h√∏rer hjemme i bl√• blok, vil vi s√¶tte \\(t=1\\), mens vi vil s√¶tte \\(t=-1\\), hvis vi vil s√¶tte vores krydset ved et r√∏dt parti. Alts√•:\n\\[\n\\begin{aligned}\nt&=-1: &\\text{R√∏d blok} \\\\\nt&=1: &\\text{Bl√• blok} \\\\\n\\end{aligned}\n\\]\nNu forestiller vi os, at vi har bedt seks personer (som godt ved, hvem de vil stemme p√• - m√•ske er det ligefrem politikere vi har spurgt) om at svare p√• de to sp√∏rgsm√•l og samtidig tilkendegive, om de vil stemme p√• bl√• eller r√∏d blok. Lad os f.eks. sige, at den f√∏rste person er meget enig i at indkomstskatten skal s√¶ttes ned (dvs. \\(x_1=2\\)), og at denne person er overvejende enig i at danske virksomheder ikke skal p√•l√¶gges en CO2-afgift (dvs. \\(x_2=1\\)). Desuden oplyser denne person, at han/hun vil stemme p√• bl√• blok (dvs. \\(t=1\\)). Det kan udtrykkes s√•dan her: \\[\n(x_1,x_2)=(2,1) \\quad \\Rightarrow \\quad  t=1\n\\tag{1}\\]\nOg s√•dan kunne man opstille andre eksempler: \\[\n\\begin{aligned}\n&(x_1,x_2)=(-1,1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(-1,-1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(1,1) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(2,2) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(-2,-1) \\quad \\Rightarrow \\quad t=-1 \\\\\n\\end{aligned}\n\\] Det f√∏rste eksempel siger for eksempel, at en person har v√¶ret overvejende uenig i at s√¶tte indkomstskatten ned (\\(x_1=-1\\)), overvejende enig i at danske virksomheder ikke skal p√•l√¶gges en CO2-afgift (\\(x_2=1\\)) og samtidig vil denne person stemme p√• r√∏d blok (\\(t=-1\\)).\nVi kan pr√∏ve at indtegne \\((x_1,x_2)\\)-punkterne i et koordinatsystem og samtidig angive den tilh√∏rende v√¶rdi af \\(t\\) med en farve. Det vil se s√•dan her ud:\n\n\n\n\n\n\n\n\nFigur¬†1: Illustration af svaret p√• sp√∏rgsm√•l 1 (\\(1.\\) aksen) og sp√∏rgsm√•l 2 (\\(2.\\) aksen) med en markering af om man vil stemme p√• r√∏d eller bl√• blok.\n\n\n\n\n\nDet kunne godt se ud som om, at det vil v√¶re muligt at indtegne en ret linje p√• en s√•dan m√•de, at alle punkter som ligger over linjen skulle farves bl√• (svarende til \"her stemmer vi p√• bl√• blok\"), mens alle punkter under linjen skulle farves r√∏de (svarende til \"her stemmer vi p√• r√∏d blok\"). En tilf√¶ldig indtegnet linje ses p√• figur¬†2.\n\n\n\n\n\n\n\n\nFigur¬†2: Illustration af svaret p√• sp√∏rgsm√•l 1 (\\(1.\\) aksen) og sp√∏rgsm√•l 2 (\\(2.\\) aksen) med en markering af om man vil stemme p√• r√∏d eller bl√• blok. En tilf√¶ldig linje er indtegnet.\n\n\n\n\n\nHerunder ser du et bud p√• en linje, som ser ud til at v√¶re god til at adskille de bl√• punkter fra de r√∏de ‚Äì faktisk er der jo uendeligt mange linjer, som vil kunne adskille de bl√• punkter fra de r√∏de:\n\n\n\n\n\n\n\n\nFigur¬†3: Illustration af svaret p√• sp√∏rgsm√•l 1 (\\(1.\\) aksen) og sp√∏rgsm√•l 2 (\\(2.\\) aksen) med en markering af om man vil stemme p√• r√∏d eller bl√• blok. Her er indtegnet en linje, som kan separere de bl√• punkter fra de r√∏de.\n\n\n\n\n\nLinjen p√• figur¬†3 har ligning \\[\\begin{aligned}\ny=-1.2 \\cdot x+1.5.\\end{aligned}\\] Men nu kaldte vi jo faktisk ikke de to variable for \\(x\\) og \\(y\\), men derimod for \\(x_1\\) og \\(x_2\\). Med denne notation f√•r vi alts√•, at \\[\n\\begin{aligned}\nx_2=-1.2 \\cdot x_1+1.5\n\\end{aligned}\n\\] Hvis vi bruger denne ligning til at skelne imellem bl√• og r√∏de punkter, s√• vil vi sige, at alle punkter, som ligger over linjen skal v√¶re bl√•. Det vil v√¶re det samme som at sige, at alle de bl√• punkter opfylder uligheden \\[\n\\begin{aligned}\nx_2&gt;-1.2 \\cdot x_1+1.5.\n\\end{aligned}\n\\] Eller skrevet p√• en anden m√•de: \\[\n\\begin{aligned}\n1.2 \\cdot x_1+ 1 \\cdot x_2&gt;1.5.\n\\end{aligned}\n\\] Her kalder man v√¶rdi \\(1.5\\) p√• h√∏jreside for threshold v√¶rdien (p√• dansk: t√¶rskelv√¶rdi), fordi det er denne v√¶rdi, som afg√∏r, om vi skal farve et punkt r√∏dt eller bl√•t. V√¶rdierne \\(1.2\\) og \\(1\\) kaldes for v√¶gte, fordi de bestemmer, hvor meget inputv√¶rdierne \\(x_1\\) og \\(x_2\\) skal v√¶gtes i forhold til hinanden.\nEn helt tredje m√•de at skrive det samme p√• vil v√¶re \\[\n\\begin{aligned}\n-1.5+1.2 \\cdot x_1+ 1 \\cdot x_2&gt;0.\n\\end{aligned}\n\\] Nu kalder man s√• bare v√¶rdien \\(-1.5\\) for en bias, men i virkeligheden er det jo bare threshold v√¶rdien med modsat fortegn1.\n1¬†Der er forskellige overvejelser i forhold til valget af denne skrivem√•de. For det f√∏rste er vi g√•et v√¶k fra \\(x\\) og \\(y\\) og over til \\(x_1\\) og \\(x_2\\). Det giver mening, fordi vi ofte t√¶nker p√• \\(y\\) som den afh√¶ngige variabel og \\(x\\) som den uafh√¶ngige variabel. Denne fortolkning af de to variable giver ikke mening i denne sammenh√¶ng. Derudover kan vi beskrive en vilk√•rlig linje i planen ved hj√¶lp af ligningen \\(ax_1+bx_2+c=0\\) ‚Äì ogs√• de lodrette linjer. Holder vi derimod fast i \\(y=ax+b\\), s√• kan vi ikke ‚Äúfange‚Äù de lodrette linjer.Vi har nu faktisk udledt en regel, som for tid og evighed kan hj√¶lpe os med at afg√∏re, om vi skal stemme p√• r√∏d eller bl√• blok. Den kan opsummeres s√•dan her:\n\n\n\n\n\n\nHvem skal jeg stemme p√•?\n\n\n\nSvar p√• en skala fra -2 til 2 p√• f√∏lgende sp√∏rgsm√•l:\n\\(x_1\\): \"Jeg synes, at indkomstskatten skal s√¶ttes ned\"\n\\(x_2\\): \"Jeg synes ikke, at danske virksomheder skal p√•l√¶gges en CO2-afgift\"\nhvor 2 svarer til \"Meget enig\" og -2 svarer til \"Meget uenig\".\nBeregn nu \\(o\\) (for outputv√¶rdi) p√• denne m√•de \\[\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 \\geq 0 \\\\\n-1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 &lt; 0. \\\\\n\\end{cases}\\end{aligned}\\] Reglen er nu: \\[\\begin{aligned}\n&\\text{Hvis } o=1: \\quad &\\text{Stem bl√• blok.}\\\\\n&\\text{Hvis } o=-1: \\quad &\\text{Stem r√∏d blok.}\\\\\\end{aligned}\\]\n\n\nMan siger ogs√•, at man p√• baggrund af inputv√¶rdierne kan lave en klassificering (eller kategorisering). Det betyder, at vi p√• baggrund af inputv√¶rdierne kan beregne, om vi er i kategorien \"Bl√• blok\" (\\(o=1\\)) eller i kategorien \"R√∏d blok\" (\\(o=-1\\)). Grafisk svarer det til, at man indtegner sit \\((x_1, x_2)\\)-punkt i koordinatsystemet i figur¬†3 og ser s√• p√• om punkt ligger over eller under linjen (ligger det over skal vi stemme bl√• blok).\n\nEksempel 1 Lad os sige at en v√¶lger hverken er enig eller uenig i, at indkomstskatten skal s√¶ttes ned. Det vil sige, at \\(x_1=0\\). Samtidig er denne v√¶lger meget enig i, at danske virksomheder ikke skal p√•l√¶gges en CO2-afgift. Alts√• er \\(x_2=2\\). Vi udregner nu: \\[\n-1.5+1.2 \\cdot x_1+x_2=-1.5+1.2 \\cdot 0+2=0.5\n\\] Og da denne v√¶rdi er st√∏rre end \\(0\\), s√¶tter vi \\(o=1\\). Det vil sige, at vi vil anbefale denne v√¶lger at stemme bl√• blok.\n\nDet er da smart! Og det her er faktisk lige pr√¶cis id√©en bag perceptroner, som den amerikanske psykolog Frank Rosenblatt foreslog helt tilbage i \\(1958\\). Den klassiske perceptron er defineret ved, at perceptronen kan modtage input \\[\n\\begin{aligned}\nx_1, x_2, \\dots, x_n,\n\\end{aligned}\n\\] hvor hver enkel inputv√¶rdi i princippet kan v√¶re et vilk√•rligt reelt tal. I vores eksempel har vi dog begr√¶nset inputv√¶rdierne til \\(x_1, x_2 \\in \\{-2,-1,0,1,2 \\}\\). Vi beregner s√• en outputv√¶rdi \\(o\\) vha. v√¶gtene \\(w_1, w_2, \\dots, w_n\\) og en biasv√¶rdi, som vi her vil kalde for \\(w_0\\) p√• denne m√•de: \\[\n\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n \\geq 0 \\\\\n-1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n &lt; 0. \\\\\n\\end{cases}\n\\end{aligned}\n\\] Grafisk kan det illustreres s√•dan her:\n\n\n\n\n\n\nFigur¬†4: Grafisk illustration af en perceptron.\n\n\n\nHer illustrerer sumtegnet i cirklen, at vi tager en v√¶gtet sum af alle inputv√¶rdierne (inklusiv et input (\\(x_0\\)), som altid er \\(1\\), og som v√¶gtes med \\(w_0\\) svarende til, at vi f√•r vores bias med), mens grafen af trappefunktionen i firkanten viser, at vi diskretiserer denne v√¶gtede sum, s√•dan at outputv√¶rdien enten er \\(-1\\) eller \\(1\\).\n\n\n\nI denne video forklarer vi ovenst√•ende, men med udgangspunkt i et andet eksempel."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#sec-kandidattest",
    "href": "materialer/perceptron/perceptron.html#sec-kandidattest",
    "title": "Perceptroner",
    "section": "",
    "text": "De senere √•r er det blevet popul√¶rt, at diverse medier laver forskellige kandidattests. S√•dan nogle tests kan laves p√• mange forskellige m√•der - man kunne blandt andet bruge perceptroner! Testene fungerer som regel p√• den m√•de, at man bliver stillet en r√¶kke forskellige sp√∏rgsm√•l og s√• skal man svare p√• en skala fra meget uenig til meget enig. Disse kategorier af svar kunne f.eks. overs√¶ttes til matematik p√• denne m√•de:\n\n\n\n\n\n\n\n\n\n\n\n\nHelt enig\nOvervejende enig\nHverken/eller\nOvervejende uenig\nHelt uenig\n\n\n\n\n2\n1\n0\n-1\n-2\n\n\n\n\n\nLad os pr√∏ve at g√∏re det helt simpelt. I stedet for at komme med et bud p√• hvem man skal stemme p√•, s√• vil vi blot fors√∏ge at komme med et bud p√•, om man skal stemme p√• r√∏d eller bl√• blok (det er sikkert en h√•bl√∏s simplificering, men det m√• du tale med din samfundsfagsl√¶rer om ).\nLad os sige at vi vil basere vores bud p√• to sp√∏rgsm√•l:\n\nJeg synes, at indkomstskatten skal s√¶ttes ned.\nJeg synes ikke, at danske virksomheder skal p√•l√¶gges en CO2-afgift.\n\nVi kan sikkert hurtigt blive enige om, at hvis man er meget enig i begge sp√∏rgsm√•l, s√• h√∏rer man formentlig til i bl√• blok og modsat, hvis man er meget uenig i begge sp√∏rgsm√•l, s√• h√∏rer man nok mere hjemme i r√∏d blok. S√• at lave en perceptron, som kan hj√¶lpe os med at forudsige det, er nok ikke raketvidenskab, men det kan ikke desto mindre hj√¶lpe os med at forst√• de bagvedliggende principper og hvordan disse sidenhen kan generaliseres.\nLad os pr√∏ve at blive lidt mere specifikke og indf√∏re to variable \\(x_1\\) og \\(x_2\\), hvor\n\n\\(x_1\\): svaret p√• Jeg synes, at indkomstskatten skal s√¶ttes ned angivet p√• en skala fra -2 til 2\n\n\\(x_2\\): svaret p√• Jeg synes ikke, at danske virksomheder skal p√•l√¶gges en CO2-afgift angivet p√• en skala fra -2 til 2.\n\nVores beslutning vil vi nu ogs√• kvantificere vha. en variabel \\(t\\), som kan antage to v√¶rdier, nemlig \\(-1\\) og \\(1\\). Hvis vi h√∏rer hjemme i bl√• blok, vil vi s√¶tte \\(t=1\\), mens vi vil s√¶tte \\(t=-1\\), hvis vi vil s√¶tte vores krydset ved et r√∏dt parti. Alts√•:\n\\[\n\\begin{aligned}\nt&=-1: &\\text{R√∏d blok} \\\\\nt&=1: &\\text{Bl√• blok} \\\\\n\\end{aligned}\n\\]\nNu forestiller vi os, at vi har bedt seks personer (som godt ved, hvem de vil stemme p√• - m√•ske er det ligefrem politikere vi har spurgt) om at svare p√• de to sp√∏rgsm√•l og samtidig tilkendegive, om de vil stemme p√• bl√• eller r√∏d blok. Lad os f.eks. sige, at den f√∏rste person er meget enig i at indkomstskatten skal s√¶ttes ned (dvs. \\(x_1=2\\)), og at denne person er overvejende enig i at danske virksomheder ikke skal p√•l√¶gges en CO2-afgift (dvs. \\(x_2=1\\)). Desuden oplyser denne person, at han/hun vil stemme p√• bl√• blok (dvs. \\(t=1\\)). Det kan udtrykkes s√•dan her: \\[\n(x_1,x_2)=(2,1) \\quad \\Rightarrow \\quad  t=1\n\\tag{1}\\]\nOg s√•dan kunne man opstille andre eksempler: \\[\n\\begin{aligned}\n&(x_1,x_2)=(-1,1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(-1,-1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(1,1) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(2,2) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(-2,-1) \\quad \\Rightarrow \\quad t=-1 \\\\\n\\end{aligned}\n\\] Det f√∏rste eksempel siger for eksempel, at en person har v√¶ret overvejende uenig i at s√¶tte indkomstskatten ned (\\(x_1=-1\\)), overvejende enig i at danske virksomheder ikke skal p√•l√¶gges en CO2-afgift (\\(x_2=1\\)) og samtidig vil denne person stemme p√• r√∏d blok (\\(t=-1\\)).\nVi kan pr√∏ve at indtegne \\((x_1,x_2)\\)-punkterne i et koordinatsystem og samtidig angive den tilh√∏rende v√¶rdi af \\(t\\) med en farve. Det vil se s√•dan her ud:\n\n\n\n\n\n\n\n\nFigur¬†1: Illustration af svaret p√• sp√∏rgsm√•l 1 (\\(1.\\) aksen) og sp√∏rgsm√•l 2 (\\(2.\\) aksen) med en markering af om man vil stemme p√• r√∏d eller bl√• blok.\n\n\n\n\n\nDet kunne godt se ud som om, at det vil v√¶re muligt at indtegne en ret linje p√• en s√•dan m√•de, at alle punkter som ligger over linjen skulle farves bl√• (svarende til \"her stemmer vi p√• bl√• blok\"), mens alle punkter under linjen skulle farves r√∏de (svarende til \"her stemmer vi p√• r√∏d blok\"). En tilf√¶ldig indtegnet linje ses p√• figur¬†2.\n\n\n\n\n\n\n\n\nFigur¬†2: Illustration af svaret p√• sp√∏rgsm√•l 1 (\\(1.\\) aksen) og sp√∏rgsm√•l 2 (\\(2.\\) aksen) med en markering af om man vil stemme p√• r√∏d eller bl√• blok. En tilf√¶ldig linje er indtegnet.\n\n\n\n\n\nHerunder ser du et bud p√• en linje, som ser ud til at v√¶re god til at adskille de bl√• punkter fra de r√∏de ‚Äì faktisk er der jo uendeligt mange linjer, som vil kunne adskille de bl√• punkter fra de r√∏de:\n\n\n\n\n\n\n\n\nFigur¬†3: Illustration af svaret p√• sp√∏rgsm√•l 1 (\\(1.\\) aksen) og sp√∏rgsm√•l 2 (\\(2.\\) aksen) med en markering af om man vil stemme p√• r√∏d eller bl√• blok. Her er indtegnet en linje, som kan separere de bl√• punkter fra de r√∏de.\n\n\n\n\n\nLinjen p√• figur¬†3 har ligning \\[\\begin{aligned}\ny=-1.2 \\cdot x+1.5.\\end{aligned}\\] Men nu kaldte vi jo faktisk ikke de to variable for \\(x\\) og \\(y\\), men derimod for \\(x_1\\) og \\(x_2\\). Med denne notation f√•r vi alts√•, at \\[\n\\begin{aligned}\nx_2=-1.2 \\cdot x_1+1.5\n\\end{aligned}\n\\] Hvis vi bruger denne ligning til at skelne imellem bl√• og r√∏de punkter, s√• vil vi sige, at alle punkter, som ligger over linjen skal v√¶re bl√•. Det vil v√¶re det samme som at sige, at alle de bl√• punkter opfylder uligheden \\[\n\\begin{aligned}\nx_2&gt;-1.2 \\cdot x_1+1.5.\n\\end{aligned}\n\\] Eller skrevet p√• en anden m√•de: \\[\n\\begin{aligned}\n1.2 \\cdot x_1+ 1 \\cdot x_2&gt;1.5.\n\\end{aligned}\n\\] Her kalder man v√¶rdi \\(1.5\\) p√• h√∏jreside for threshold v√¶rdien (p√• dansk: t√¶rskelv√¶rdi), fordi det er denne v√¶rdi, som afg√∏r, om vi skal farve et punkt r√∏dt eller bl√•t. V√¶rdierne \\(1.2\\) og \\(1\\) kaldes for v√¶gte, fordi de bestemmer, hvor meget inputv√¶rdierne \\(x_1\\) og \\(x_2\\) skal v√¶gtes i forhold til hinanden.\nEn helt tredje m√•de at skrive det samme p√• vil v√¶re \\[\n\\begin{aligned}\n-1.5+1.2 \\cdot x_1+ 1 \\cdot x_2&gt;0.\n\\end{aligned}\n\\] Nu kalder man s√• bare v√¶rdien \\(-1.5\\) for en bias, men i virkeligheden er det jo bare threshold v√¶rdien med modsat fortegn1.\n1¬†Der er forskellige overvejelser i forhold til valget af denne skrivem√•de. For det f√∏rste er vi g√•et v√¶k fra \\(x\\) og \\(y\\) og over til \\(x_1\\) og \\(x_2\\). Det giver mening, fordi vi ofte t√¶nker p√• \\(y\\) som den afh√¶ngige variabel og \\(x\\) som den uafh√¶ngige variabel. Denne fortolkning af de to variable giver ikke mening i denne sammenh√¶ng. Derudover kan vi beskrive en vilk√•rlig linje i planen ved hj√¶lp af ligningen \\(ax_1+bx_2+c=0\\) ‚Äì ogs√• de lodrette linjer. Holder vi derimod fast i \\(y=ax+b\\), s√• kan vi ikke ‚Äúfange‚Äù de lodrette linjer.Vi har nu faktisk udledt en regel, som for tid og evighed kan hj√¶lpe os med at afg√∏re, om vi skal stemme p√• r√∏d eller bl√• blok. Den kan opsummeres s√•dan her:\n\n\n\n\n\n\nHvem skal jeg stemme p√•?\n\n\n\nSvar p√• en skala fra -2 til 2 p√• f√∏lgende sp√∏rgsm√•l:\n\\(x_1\\): \"Jeg synes, at indkomstskatten skal s√¶ttes ned\"\n\\(x_2\\): \"Jeg synes ikke, at danske virksomheder skal p√•l√¶gges en CO2-afgift\"\nhvor 2 svarer til \"Meget enig\" og -2 svarer til \"Meget uenig\".\nBeregn nu \\(o\\) (for outputv√¶rdi) p√• denne m√•de \\[\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 \\geq 0 \\\\\n-1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 &lt; 0. \\\\\n\\end{cases}\\end{aligned}\\] Reglen er nu: \\[\\begin{aligned}\n&\\text{Hvis } o=1: \\quad &\\text{Stem bl√• blok.}\\\\\n&\\text{Hvis } o=-1: \\quad &\\text{Stem r√∏d blok.}\\\\\\end{aligned}\\]\n\n\nMan siger ogs√•, at man p√• baggrund af inputv√¶rdierne kan lave en klassificering (eller kategorisering). Det betyder, at vi p√• baggrund af inputv√¶rdierne kan beregne, om vi er i kategorien \"Bl√• blok\" (\\(o=1\\)) eller i kategorien \"R√∏d blok\" (\\(o=-1\\)). Grafisk svarer det til, at man indtegner sit \\((x_1, x_2)\\)-punkt i koordinatsystemet i figur¬†3 og ser s√• p√• om punkt ligger over eller under linjen (ligger det over skal vi stemme bl√• blok).\n\nEksempel 1 Lad os sige at en v√¶lger hverken er enig eller uenig i, at indkomstskatten skal s√¶ttes ned. Det vil sige, at \\(x_1=0\\). Samtidig er denne v√¶lger meget enig i, at danske virksomheder ikke skal p√•l√¶gges en CO2-afgift. Alts√• er \\(x_2=2\\). Vi udregner nu: \\[\n-1.5+1.2 \\cdot x_1+x_2=-1.5+1.2 \\cdot 0+2=0.5\n\\] Og da denne v√¶rdi er st√∏rre end \\(0\\), s√¶tter vi \\(o=1\\). Det vil sige, at vi vil anbefale denne v√¶lger at stemme bl√• blok.\n\nDet er da smart! Og det her er faktisk lige pr√¶cis id√©en bag perceptroner, som den amerikanske psykolog Frank Rosenblatt foreslog helt tilbage i \\(1958\\). Den klassiske perceptron er defineret ved, at perceptronen kan modtage input \\[\n\\begin{aligned}\nx_1, x_2, \\dots, x_n,\n\\end{aligned}\n\\] hvor hver enkel inputv√¶rdi i princippet kan v√¶re et vilk√•rligt reelt tal. I vores eksempel har vi dog begr√¶nset inputv√¶rdierne til \\(x_1, x_2 \\in \\{-2,-1,0,1,2 \\}\\). Vi beregner s√• en outputv√¶rdi \\(o\\) vha. v√¶gtene \\(w_1, w_2, \\dots, w_n\\) og en biasv√¶rdi, som vi her vil kalde for \\(w_0\\) p√• denne m√•de: \\[\n\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n \\geq 0 \\\\\n-1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n &lt; 0. \\\\\n\\end{cases}\n\\end{aligned}\n\\] Grafisk kan det illustreres s√•dan her:\n\n\n\n\n\n\nFigur¬†4: Grafisk illustration af en perceptron.\n\n\n\nHer illustrerer sumtegnet i cirklen, at vi tager en v√¶gtet sum af alle inputv√¶rdierne (inklusiv et input (\\(x_0\\)), som altid er \\(1\\), og som v√¶gtes med \\(w_0\\) svarende til, at vi f√•r vores bias med), mens grafen af trappefunktionen i firkanten viser, at vi diskretiserer denne v√¶gtede sum, s√•dan at outputv√¶rdien enten er \\(-1\\) eller \\(1\\)."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-hvad-er-en-perceptron",
    "href": "materialer/perceptron/perceptron.html#video-hvad-er-en-perceptron",
    "title": "Perceptroner",
    "section": "",
    "text": "I denne video forklarer vi ovenst√•ende, men med udgangspunkt i et andet eksempel."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-perceptron-learning-algoritmen",
    "href": "materialer/perceptron/perceptron.html#video-perceptron-learning-algoritmen",
    "title": "Perceptroner",
    "section": "VIDEO: Perceptron Learning Algoritmen",
    "text": "VIDEO: Perceptron Learning Algoritmen\nI denne video forklarer vi perceptron learning algoritmen."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-perceptron-learning-versus-adaline",
    "href": "materialer/perceptron/perceptron.html#video-perceptron-learning-versus-adaline",
    "title": "Perceptroner",
    "section": "VIDEO: Perceptron Learning versus Adaline",
    "text": "VIDEO: Perceptron Learning versus Adaline\nI denne video forklarer vi id√©en bag Adaline og indf√∏rer tabsfunktionen."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#sec-ADALINE_gradientnedstigning",
    "href": "materialer/perceptron/perceptron.html#sec-ADALINE_gradientnedstigning",
    "title": "Perceptroner",
    "section": "Gradientnedstigning",
    "text": "Gradientnedstigning\nFor at g√∏re det bruges en metode, som kaldes for gradientnedstigning. For at forklare hvad det g√•r ud p√•, er det nemmest at se p√• en tabsfunktion, som kun afh√¶nger af to v√¶gte \\(w_0\\) og \\(w_1\\). I det tilf√¶lde f√•r vi \\[\n\\begin{aligned}\nE(w_0, w_1) = \\frac{1}{2} \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1}) \\right)^2.\n\\end{aligned}\n\\] Da tabsfunktionenkun afh√¶nger af to variable, kan vi tegne grafen for den. Et eksempel herp√• ses i figur¬†9.\n\n\n\n\n\n\nFigur¬†9: Grafen for en tabsfunktion som afh√¶nger af v√¶gtene \\(w_0\\) og \\(w_1\\).\n\n\n\nId√©en er nu, at vi gerne vil bestemme v√¶gtene \\(w_0\\) og \\(w_1\\), s√•dan at tabsfunktionen minimeres. T√¶nk lidt over det. Det giver god mening, at bestemme v√¶gtene s√•dan at den samlede fejl, perceptronen beg√•r p√• tr√¶ningsdata, bliver s√• lille som mulig. Vi ved faktisk godt, hvordan man bestemmer minimum for en funktion af to variable. L√∏s ligningerne \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_0} = 0 \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial w_1} = 0.\\end{aligned}\n\\] Det er en overkommelig opgave at finde de partielle afledede og s√¶tte dem lig med \\(0\\) i det tilf√¶lde, hvor tabsfunktionen kun afh√¶nger af to v√¶gte. Men vi skal senere se, at perceptroner bliver fundamentale byggesten i kunstige neurale netv√¶rk, og her viser det sig, at denne fremgangsm√•de med at s√¶tte de partielle afledede lig \\(0\\), er helt h√•bl√∏s! Derfor bruger man gradientnedstigning.\nForestil dig at grafen for tabsfunktionen i figur¬†9 er et landskab med en dal. Dit m√•l er at finde ned i dalen. Du er blevet placeret et tilf√¶ldigt sted i landskabet svarende til tilf√¶ldige v√¶rdier af \\(w_0\\) og \\(w_1\\). Hvad g√∏r du? Jo, du kommer i tanke om, at du har l√¶rt, at hvis du g√•r i gradientens \\[\n\\begin{aligned}\n\\nabla E(w_0,w_1) = \\begin{pmatrix} \\frac{\\partial E }{\\partial w_0}(w_0,w_1) \\\\ \\\\ \\frac{\\partial E }{\\partial w_1}(w_0,w_1) \\end{pmatrix}\n\\end{aligned}\n\\] retning, s√• kommer du til at g√• i den retning, hvor det g√•r allermest opad bakke! Men hov det er jo ikke det, vi vil! Vi vil g√• allermest nedad bakke, s√• vi ender i dalen. Hvad g√∏r vi s√•? Vi vender os da bare \\(180^{\\circ}\\) og g√•r i den modsatte retning - s√• ender vi nede i dalen! Det vil sige, at vi g√•r i retning af minus gradienten: \\[\n\\begin{aligned}\n- \\nabla E(w_0,w_1) = \\begin{pmatrix} - \\frac{\\partial E }{\\partial w_0}(w_0,w_1) \\\\ \\\\ - \\frac{\\partial E }{\\partial w_1}(w_0,w_1) \\end{pmatrix}\n\\end{aligned}\n\\] Fremgangsm√•den bliver derfor den, at vi starter i nogle tilf√¶ldige \\((w_0, w_1)\\)-v√¶rdier og s√• bev√¶ger vi os et lille skridt i den negative gradients retning. S√• ender vi i et nyt punkt, hvor vi igen beregner gradienten og g√•r igen et lille skridt i den negative gradients retning. S√•dan forts√¶tter vi indtil v√¶rdien af tabsfunktionen ikke rigtig √¶ndrer sig mere ‚Äì det svarer til at vi har ramt dalen. Derfor bliver vores opdateringsregler med denne metode \\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 - \\eta \\cdot \\frac{\\partial E }{\\partial w_0} \\\\\nw_1 \\leftarrow & w_1 - \\eta \\cdot \\frac{\\partial E }{\\partial w_1} \\\\\n&\\vdots  \\\\\nw_n \\leftarrow & w_n - \\eta \\cdot \\frac{\\partial E }{\\partial w_n} \\\\\n\\end{aligned}\n\\] Her er \\(\\eta\\) igen en learning rate f.eks. \\(0.05\\), som s√∏rger for, at vi hele tiden bare tager et lille skridt i den negative gradients retning. Man v√¶lger v√¶rdien af \\(\\eta\\) lille for ikke at lave alt for store justeringer af v√¶gtene ad gangen. Det svarer grafisk til, at vi lige s√• stille g√•r ned af den bakke, som tabsfunktionen giver (se figur¬†9). Hvis vi tager for store skridt, risikerer vi helt, at komme til at ‚Äútr√¶de forbi‚Äù det minimum, som vi gerne vil lande i. Omvendt vil alt for sm√• skridt f√∏re til, at vi alt for langsomt n√¶rmer os minimum. S√• v√¶rdien af \\(\\eta\\) angiver alts√•, hvor meget vi er villige til at justere v√¶gtene og dermed hvor hurtige eller hvor langsomme, vi bev√¶ger os ned mod minimum. Af den grund giver det god mening, at \\(\\eta\\) kaldes for en learning rate - fordi den afg√∏rer, hvor hurtigt vi l√¶rer af vores tr√¶ningsdata.\nNu mangler vi bare at f√• bestemt de partielle afledede. Ved at bruge sumreglen og k√¶dereglen for differentiation f√•r vi fra (3), at \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_i} &= \\frac{1}{2} \\sum_{m=1}^{M} \\frac{\\partial}{\\partial w_i}\\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2 \\\\\n&= \\frac{1}{2} \\sum_{m=1}^{M} 2 \\cdot \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\\\ & \\quad  \\quad \\quad  \\quad \\quad  \\quad \\cdot \\frac{\\partial}{\\partial w_i} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n} ) \\right) \\\\\n&= \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (-x_{m,i} \\right)\n\\end{aligned}\n\\] for \\(i \\in \\{1, 2, \\dots, n\\}\\).\nL√¶g m√¶rke til at n√•r vi differentierer den indre funktion \\[\n\\begin{aligned}\nt_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\n\\end{aligned}\n\\] med hensyn til \\(w_i\\), s√• vil alle led v√¶re at betragte som konstanter bortset fra leddet \\[\n\\begin{aligned}\n-w_i \\cdot x_{m,i}\n\\end{aligned}\n\\] og n√•r vi differentierer dette led med hensyn til \\(w_i\\) f√•r vi netop \\(- x_{m,i}\\). L√¶g ogs√• m√¶rke til at hvis vi differentierer med hensyn til \\(w_0\\), s√• f√•r vi, \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_0} = \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (-1 \\right).\n\\end{aligned}\n\\] Alts√• bliver vores opdateringsregler \\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 + \\eta \\cdot \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)  \\\\\nw_1 \\leftarrow & w_1 + \\eta \\cdot  \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (x_{m,1} \\right)\\\\\n&\\vdots  \\\\\nw_n \\leftarrow & w_n + \\eta \\cdot  \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (x_{m,n} \\right)\\\\\n\\end{aligned}\n\\] Vi kan alts√• sammenfatte gradientnedstigningsalgoritmen p√• denne m√•de:\n\nS√¶t alle v√¶gte \\(w_0, w_1, \\dots, w_n\\) til et tilf√¶ldigt tal (f.eks. \\(0.5\\)).\nV√¶lg en v√¶rdi af \\(\\eta\\) (f.eks. \\(0.05\\)).\nUdregn p√• baggrund af alle tr√¶ningsdata fejlene: \\[\n\\begin{aligned}\nerror_0 &= \\sum_{m=1}^{M} \\left (t_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\\\\nerror_1 &= \\sum_{m=1}^{M} \\left (t_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\cdot x_{m,1} \\right) \\\\\n&\\vdots \\\\\nerror_n &= \\sum_{m=1}^{M} \\left (t_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\cdot x_{m,n} \\right)\n\\end{aligned}\n\\]\nOpdat√©r alle v√¶gtene: \\[\n\\begin{aligned}\nw_0  \\leftarrow & w_0 + \\eta \\cdot error_0 \\\\\nw_1  \\leftarrow & w_1 + \\eta \\cdot error_1 \\\\\n& \\vdots \\\\\nw_n  \\leftarrow & w_n + \\eta \\cdot error_n\n\\end{aligned}\n\\]\nStart forfra indtil v√¶gtene ikke √¶ndrer sig (s√¶rlig meget)."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#eksempel-p√•-gradientnedstigning",
    "href": "materialer/perceptron/perceptron.html#eksempel-p√•-gradientnedstigning",
    "title": "Perceptroner",
    "section": "Eksempel p√• gradientnedstigning",
    "text": "Eksempel p√• gradientnedstigning\nLad os pr√∏ve at bruge gradientnedstigning p√• vores eksempel omkring kandidattest. I dette simple eksempel bliver vores opdateringsregler nu \\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 + \\eta \\cdot \\sum_{m=1}^{6} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + w_2 \\cdot x_{m,2}) \\right)  \\\\\nw_1 \\leftarrow & w_1 + \\eta \\cdot  \\sum_{m=1}^{6} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + w_2 \\cdot x_{m,2}) \\right) \\cdot \\left (x_{m,1} \\right)\\\\\nw_2 \\leftarrow & w_2 + \\eta \\cdot  \\sum_{m=1}^{6} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1}  + w_2 \\cdot x_{m,2}) \\right) \\cdot \\left (x_{m,2} \\right)\n\\end{aligned}\n\\] Hvis man bruger disse opdateringsregler p√• dataene fra tabel¬†1, s√• ender man med f√∏lgende v√¶rdier af v√¶gtene \\[\n\\begin{aligned}\nw_0 =-0.0769 , w_1=0.6410, w_2=-0.0598\n\\end{aligned}\n\\] hvor vi startede med v√¶rdierne \\(w_0=0.5, w_1=0.5\\) og \\(w_2=0.5\\) og hvor vi satte \\(\\eta=0.05\\). Dette svarer til linjen med ligning \\[\n\\begin{aligned}\n-0.0769 + 0.6410 \\cdot x_1 - 0.0598 \\cdot x_2 = 0.\n\\end{aligned}\n\\] Linjen er indtegnet sammen med tr√¶ningsdata i figur¬†10.\n\n\n\n\n\n\n\n\nFigur¬†10: Ilustration af svaret p√• sp√∏rgsm√•l 1 (\\(1.\\) aksen) og sp√∏rgsm√•l 2 (\\(2.\\) aksen) med en markering af om man vil stemme p√• r√∏d eller blok bl√•. Her er den linje indtegnet, som stammer fra gradientnedstigningsalgoritmen (med startv√¶rdier \\(w_0=0.5, w_1=0.5, w_2=0.5\\) og \\(\\eta=0.05\\)).\n\n\n\n\n\nBem√¶rk, at til forskel fra perceptron learning algoritmen s√• ender man med de samme v√¶rdier af v√¶gtene, selvom man v√¶lger andre startv√¶rdier. Det er fordi, vi finder et globalt minimum for tabsfunktionen, som er uafh√¶ngig af det punkt, hvor vi starter med at lede. Det svarer til, at ligegyldigt hvor du bliver placeret i landskabet i figur¬†9, s√• vil du til sidst ende i dalen, hvis du hele tiden g√•r sm√• skridt i den negative gradients retning.\n\nEksempel 3 Vi ser igen p√• eksempel¬†1 og eksempel¬†2, hvor \\(x_1=0\\) og \\(x_2=1\\). Baseret p√• v√¶gtene fra gradientnedstigning, f√•r vi: \\[\n-0.0769+0.6410 \\cdot x_1 -0.0598 \\cdot x_2=-0.0769+0.6410 \\cdot 0 -0.0598 \\cdot 1=-0.1367\n\\] Da denne v√¶rdi er mindre end \\(0\\), s√¶tter vi \\(o=-1\\). Alts√• er vi tilbage til at befale v√¶lgeren at stemme p√• r√∏d blok."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#forskel-p√•-perceptron-learning-og-gradientnedstigning",
    "href": "materialer/perceptron/perceptron.html#forskel-p√•-perceptron-learning-og-gradientnedstigning",
    "title": "Perceptroner",
    "section": "Forskel p√• perceptron learning og gradientnedstigning",
    "text": "Forskel p√• perceptron learning og gradientnedstigning\nDer er flere forskelle p√• perceptron learning algoritmen og gradientnedstigning. Vi har allerede v√¶ret inde p√•, at perceptron learning algoritmen g√•r i kuk, hvis data ikke er line√¶rt separable, som i figur¬†8. Mere formelt vil perceptron learning algoritmen ikke konvergere. I det tilf√¶lde vil gradientnedstigning alligevel komme med et bud p√• v√¶rdier af v√¶gtene, som kan bruges til at kategorisere tr√¶ningsdata, selvom alle tr√¶ningsdata ikke vil blive klassificeret korrekt (fordi de netop ikke er line√¶rt separable). En anden forskel ligger i hvorn√•r v√¶gtene opdateres. I perceptron learning algoritmen opdateres v√¶gtene efter hvert tr√¶ningseksempel. I gradientnedstigning bruger man alle tr√¶ningsdata for at lave en enkelt opdatering af v√¶gtene. Hvis man har mange tr√¶ningsdata, kan det godt blive lidt tungt. S√• kan man i stedet for v√¶lge at bruge et mindre, tilf√¶ldigt udvalg af data (for eksempel \\(10\\%\\)) til hver opdatering og s√• til n√¶ste opdatering bruge et nyt tilf√¶ldigt udvalg af data. Denne fremgangsm√•de kaldes for stokastisk gradientnedstigning. En anden mulighed er at lave online gradientnedstigning, hvor man opdaterer v√¶gtene for hvert tr√¶ningseksempel, som i perceptron learning algoritmen. Selvom det kun er en tiln√¶rmelse til rigtig gradientnedstigning, s√• har det alligevel en r√¶kke fordele: 1) Det er meget hurtigere at opdatere v√¶gtene. 2) I nogle anvendelser vil man have brug for l√∏bende at opdatere v√¶gtene p√• baggrund af nye tr√¶ningsdata. I stedet for at gemme alle de mange tr√¶ningsdata kan man bare opdatere v√¶gtene, hver gang man f√•r et nyt tr√¶ningseksempel til r√•dighed og s√• eventuelt slette tr√¶ningseksemplet igen, n√•r v√¶gtene er blevet opdateret. Det er b√•de hurtigere og mere pladsbesparende."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-adaline",
    "href": "materialer/perceptron/perceptron.html#video-adaline",
    "title": "Perceptroner",
    "section": "VIDEO: Adaline",
    "text": "VIDEO: Adaline\nI denne video forklarer vi, hvad gradientnedstigning g√•r ud p√•, og hvordan gradientnedstigning bruges til at opdatere v√¶gtene i Adaline."
  },
  {
    "objectID": "materialer/retningsafledede/retningsafledede.html",
    "href": "materialer/retningsafledede/retningsafledede.html",
    "title": "Retningsafledede og gradientnedstigning",
    "section": "",
    "text": "Introduktion\nI det almindelige gymnasiepensum indg√•r nogle vigtige begreber indenfor infinitesimalregningen for funktioner af √©n og to variable. For funktioner af √©n variabel siges en funktion \\(f\\) at v√¶re kontinuert i et punkt \\(x_{0}\\), hvis funktionsv√¶rdien \\(f(x)\\) n√¶rmer sig funktionsv√¶rdien \\(f(x_{0})\\) n√•r \\(x\\) n√¶rmer sig \\(x_{0}\\). Det vil sige, at\n\\[\n\\lim_{x \\rightarrow x_{0}}{f\\left( x \\right) = f(x_{0})}\n\\]\nFunktionen \\(f\\) siges at v√¶re differentiabel i \\(x_{0}\\), hvis h√¶ldningen af sekanterne gennem de to punkter \\((x_{0},f\\left( x_{0} \\right))\\) og \\((x,f\\left( x \\right))\\) p√• grafen for \\(f\\) n√¶rmer sig et fast tal \\(f'(x_{0})\\), n√•r \\(x\\) n√¶rmer sig \\(x_{0}\\):\n\\[\n\\lim_{x \\rightarrow x_{0}}{\\frac{f\\left( x \\right) - f(x_{0})}{x - x_{0}} = f'(x_{0})}\n\\]\nIntuitivt kan man t√¶nke p√• egenskaben kontinuitet i et punkt, som at grafen for funktionen ikke har et spring i punktet, og p√• egenskaben differentiabilitet i et punkt, som at grafen for funktionen hverken har spring eller kn√¶k i punktet.\n\n\n\n\nH√¶ldningen \\(f'(x_{0})\\) i punktet \\((x_{0},f\\left( x_{0} \\right))\\) p√• grafen er s√• en gr√¶nsev√¶rdi af nogle sekanth√¶ldninger, som hver for sig er gennemsnitsh√¶ldninger for et mindre og mindre stykke af grafen. Har man studeret gr√¶nsev√¶rdibegrebet lidt n√¶rmere, ved man, at den intuitive fortolkning ikke er s√¶rligt pr√¶cis og heller ikke helt rigtig, men alligevel er denne fortolkning god til at give en forst√•else af gymnasiematematikken.\nFor funktioner af to variable siges en funktion \\(f\\) at v√¶re kontinuert i et punkt \\({(x}_{0},y_{0})\\), hvis funktionsv√¶rdien \\(f(x,y)\\) n√¶rmer sig funktionsv√¶rdien \\(f(x_{0},y_{0})\\), n√•r \\((x,y)\\) n√¶rmer sig \\({(x}_{0},y_{0})\\):\n\\[\n\\lim_{(x,y) \\rightarrow (x_{0},y_{0})}{f\\left( x,y \\right) = f(x_{0},y_{0})}\n\\]\nDefinitionen er direkte overf√∏rt fra den tilsvarende definition for funktioner af √©n variabel. Det virker jo umiddelbart meget fornuftigt, men der er faktisk lidt at t√¶nke over. Det kan du l√¶se mere om her.\nDesv√¶rre kan man ikke tilsvarende genbruge differentiabilitetsbegrebet fra funktioner af √©n variabel p√• denne m√•de\n\\[\n\\lim_{(x,y) \\rightarrow (x_{0},y_{0})}{\\frac{f\\left( x,y \\right) - f\\left( x_{0},y_{0} \\right)}{\\left( x,y \\right) - {(x}_{0},y_{0})} = f'(x_{0},y_{0})}\n\\]\nHer giver br√∏ken p√• venstre side repr√¶senterende sekanth√¶ldningen ikke mening, da man ikke kan dividere med et punkt eller en vektor. I stedet tager man udgangspunkt i en alternativ definition af differentiabilitet for funktioner af √©n variabel, hvor man har kaldt skridtet fra \\(x_{0}\\) til \\(x\\) for \\(h\\):\n\\[\n\\lim_{h \\rightarrow 0}{\\frac{f\\left( x_{0} + h \\right) - f(x_{0})}{h} = f'(x_{0})}\n\\]\nMan bruger det til at definere de to f√∏rste ordens partielle afledede for en funktion \\(f\\) af to variable \\[\n\\begin{aligned}\n&\\lim_{h \\rightarrow 0} \\frac{f\\left( x_{0} + h,y_{0} \\right) - f(x_{0},y_{0})}{h} = f_{x}^{'}(x_{0},y_{0})\\\\\n&\\lim_{h \\rightarrow 0} \\frac{f\\left( x_{0},y_{0} + h \\right) - f(x_{0},y_{0})}{h} = f_{y}^{'}(x_{0},y_{0})\n\\end{aligned}\n\\] hvis gr√¶nserne eksisterer. Her tager man et skridt \\(h\\) i enten \\(x\\)-aksens eller \\(y\\)-aksens retning ud fra punktet \\((x_{0},y_{0})\\) og beregner en h√¶ldning af grafen i den retning ved hj√¶lp af en gr√¶nsev√¶rdi af sekanth√¶ldninger for snitfunktionerne \\(f(x,y_{0})\\) og \\(f(x_{0},y)\\), som hver for sig er almindelige funktioner af √©n variabel, da man kun √¶ndrer enten \\(x\\)-koordinaten eller \\(y\\)-koordinaten.\nGradientvektoren defineres som vektoren med de partielle afledede som koordinater:\n\\[\n\\nabla f\\left( x_{0},y_{0} \\right) = \\begin{pmatrix}\nf_{x}^{'}\\left( x_{0},y_{0} \\right) \\\\\nf_{y}^{'}\\left( x_{0},y_{0} \\right) \\\\\n\\end{pmatrix}\n\\]\nDet oplyses i gymnasiepensum, at denne vektor angiver den retning, man skal bev√¶ge sig v√¶k fra punktet \\((x_{0},y_{0})\\), for at funktionsv√¶rdierne \\(f(x,y)\\) vokser mest muligt. Vi vil i det f√∏lgende se n√¶rmere p√• denne egenskab og bruge den til at l√∏se optimeringsproblemer numerisk.\n\n\nRetningsafledede\nVi vil nu se p√• v√¶ksten i andre retninger end blot i aksernes retning. Vi angiver retningen med en enhedsvektor - det vil sige en vektor med l√¶ngde 1:\n\\[\n\\vec{u} = \\begin{pmatrix}\nu_{1} \\\\\nu_{2} \\\\\n\\end{pmatrix}\n\\] hvor alts√• \\(\\lvert \\vec u \\rvert = 1\\).\nVi definerer nu den retningsafledede af \\(f\\) i punktet \\((x_{0},y_{0})\\) i retningen \\(\\vec{u}\\) ved\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\lim_{h \\rightarrow 0}\\frac{f\\left( x_{0} + hu_{1},y_{0} + hu_{2} \\right) - f(x_{0},y_{0})}{h}\n\\tag{1}\\]\nhvis gr√¶nsen eksisterer.\nBem√¶rk, at hvis \\(\\vec{u}\\) peger i \\(x\\)-aksens retning, s√• bliver den retningsafledede til \\(f_{x}^{'}(x_{0},y_{0})\\), og hvis den peger i \\(y\\)-aksens retning, bliver den til \\(f_{y}^{'}(x_{0},y_{0})\\). Man udregner en sekanth√¶ldning ved at tage et skridt \\(h\\) i \\(\\vec{u}\\)‚Äôs retning og dividere den fundne funktionstilv√¶kst med \\(h\\). Derefter lader man \\(h\\) g√• mod 0. Det giver h√¶ldningen af grafen for \\(f\\) i punktet \\((x_{0},y_{0})\\) i retningen \\(\\vec{u}\\).\nId√©en med den retningsafledede er illustreret i app‚Äôen nedenfor. Til venstre ses en repr√¶sentant for \\(\\vec u\\) i \\(xy\\)-planen. Man kan √¶ndre p√• den retning, som \\(\\vec u\\) peger i, ved at tr√¶kke i skyderen. Til h√∏jre ses grafen for en funktion \\(f\\) af to variable, hvor et punkt \\(P(x_0,y_0,f(x_0,y_0))\\) p√• grafen er indtegnet. Samtidig vises den snitkurve som f√•s, hvis man p√• grafen i punktet \\(P\\) bev√¶ger sig langs en linje i retningen \\(\\vec u\\). Denne snitkurve har i punktet \\(P\\) en tangent, som ogs√• er indtegnet, og denne tangents h√¶ldning vil netop svarer til st√∏rrelsen af den retningsafledede \\(D_{\\vec{u}}f\\left( x_{0},y_{0} \\right)\\). Hvis man √¶ndrer p√• den retning, som \\(\\vec u\\) peger i, kan man se, hvordan st√∏rrelsen af den retningsafledede √¶ndrer sig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDet viser sig, at man kan udregne de retningsafledede med et prikprodukt:\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\nabla f(x_{0},y_{0}) \\cdot \\vec{u}\n\\]\nVi vil argumentere for formlen, men lad os f√∏rst se p√• konsekvenserne af den. Vi ved fra almindelig vektorregning, at\n\\[\n\\vec{a} \\cdot \\vec{b} = \\lvert \\vec{a} \\rvert \\cdot \\lvert \\vec{b} \\rvert \\cdot \\cos(v)\n\\]\nhvor \\(v\\) er vinklen mellem de to vektorer. Da \\(\\lvert \\vec{u} \\rvert = 1\\) betyder det, at\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\lvert \\nabla f(x_{0},y_{0}) \\rvert \\cdot \\cos(v)\n\\]\nhvor \\(v\\) er vinklen mellem gradientvektoren \\(\\nabla f\\left( x_{0},y_{0} \\right)\\) og den valgte retning \\(\\vec{u}\\).\nVi ved, at \\(-1 \\leq \\cos(v) \\leq 1\\) samt at \\(\\cos(0^{{^\\circ}})=1\\) og \\(\\cos(180^{{^\\circ}})=-1\\). Det f√∏lger derfor, at den retningsafledede er st√∏rst (og dermed at \\(f\\) vokser mest), n√•r \\(\\vec{u}\\) peger i \\(\\nabla f(x_{0},y_{0})\\)‚Äôs retning. Og tilsvarende at den retningsafledede er mindst (og dermed at \\(f\\) aftager mest), n√•r \\(\\vec{u}\\) peger i \\(-\\nabla f(x_{0},y_{0})\\)‚Äôs retning. Det vil sige, at den retningsaflededes st√∏rstev√¶rdi er\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\ \\ \\ \\lvert \\nabla f(x_{0},y_{0}) \\rvert\n\\]\nn√•r \\(v = 0^{{^\\circ}}\\) og retningsaflededes mindstev√¶rdi er\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = - \\lvert \\nabla f(x_{0},y_{0}) \\rvert\n\\]\nn√•r \\(v = 180^{{^\\circ}}\\). Det var netop, hvad vi gerne ville vise.\nPrincippet er illustreret i app‚Äôen herunder. Gradientvektoren \\(\\nabla f(x_{0},y_{0})\\) er indtegnet (med bl√•) og man kan se, at den retningsafledede antager den st√∏rste v√¶rdi, netop n√•r \\(\\vec u\\) peger i gradientens retning (pr√∏v at tr√¶kke i skyderen). Og omvendt antager den retningsafledede den mindste v√¶rdi, n√•r \\(\\vec u\\) peger i minus gradientens retning.\n\n\n\n\n\nMiddelv√¶rdis√¶tningen\nFor at argumentere for formlen for de retningsafledede udregnet som et prikprodukt, skal vi bruge middelv√¶rdis√¶tningen for funktioner af √©n variabel:\n\n\nS√¶tning 1 (Middelv√¶rdis√¶tningen) Hvis \\(f\\) er kontinuert p√• \\(\\left\\lbrack a;b \\right\\rbrack\\) og differentiabel i \\(\\left\\rbrack a;b \\right\\lbrack\\), s√• findes der et tal \\(c\\) mellem \\(a\\) og \\(b\\), s√• tangenth√¶ldningen i \\(c\\) er lig med middelv√¶rdien af h√¶ldningen p√• hele intervallet \\(\\left\\lbrack a;b \\right\\rbrack\\). Det vil sige, at \\[f^{'}\\left( c \\right) = \\frac{f\\left( b \\right) - f(a)}{b - a}\\]\n\n\nResultatet i middelv√¶rdis√¶tningen kan omskrives til\n\\[\nf\\left( b \\right) - f\\left( a \\right) = f^{'}\\left( c \\right) \\cdot (b - a)\n\\tag{2}\\]\nsom er det, vi f√•r brug for. Middelv√¶rdis√¶tningen virker indlysende korrekt, hvis man pr√∏ver at tegne situationen, og beviset for middelv√¶rdis√¶tningen kan findes i flere gymnasieb√∏ger.\nInden vi g√•r til argumentet for formlen for de retningsafledede, vil vi se p√• et enkelt eksempel med middelv√¶rdis√¶tningen.\n\nEksempel 1 Funktionen \\(f\\left( x \\right) = \\sqrt{x}\\) er kontinuert p√• \\(\\left\\lbrack 0;4 \\right\\rbrack\\) og differentiabel i \\(\\left\\rbrack 0;4 \\right\\lbrack\\), s√• betingelserne for at bruge middelv√¶rdis√¶tningen er opfyldt.\nDer findes s√• et tal \\(c\\) mellem 0 og 4, s√• \\(f^{'}\\left( c \\right) = \\frac{f\\left( 4 \\right) - f(0)}{4 - 0}\\).\nVi ved, at \\(f^{'}\\left( x \\right) = \\frac{1}{2\\sqrt{x}}\\) s√• ligningen ovenfor bliver \\[\n\\frac{1}{2\\sqrt{c}} = \\frac{\\sqrt{4} - \\sqrt{0}}{4 - 0}\n\\] Det vil sige, at \\[\n\\frac{1}{2\\sqrt{c}} = \\frac{1}{2}\n\\] hvilket giver \\(c = 1\\).\nTangenth√¶ldningen af grafen for \\(f\\left( x \\right) = \\sqrt{x}\\) i \\(c = 1\\) er alts√• det samme som middelv√¶rdien af h√¶ldningen af grafen p√• hele intervallet \\(\\left\\lbrack a;b \\right\\rbrack = \\left\\lbrack 0;4 \\right\\rbrack\\), det vil sige h√¶ldningen af den sekant, der forbinder startpunktet \\((0,f\\left( 0 \\right))\\) og slutpunktet \\((4,f\\left( 4 \\right))\\).\nP√• figur¬†1 illustreres dette princip.\n\n\n\n\n\n\n\nFigur¬†1: Illustration af middelv√¶rdis√¶tningen. Her har tangenten i \\((1,f(1))\\) (den gr√∏nne linje) samme h√¶ldning som sekanten gennem \\((0,f(0))\\) og \\((4,f(4))\\) (den bl√• linje).\n\n\n\nMiddelv√¶rdis√¶tningen siger alts√• bare, at hvis man forbinder start og slutpunktet ‚Äì den bl√• linje ‚Äì og udregner dens h√¶ldning, s√• kan man altid finde mindst et punkt i det indre af intervallet, hvor tangenten i punktet ‚Äì den gr√∏nne linje ‚Äì har samme h√¶ldning. I eksemplet fandt vi et bestemt \\(c\\), som vi if√∏lge middelv√¶rdis√¶tningen vidste, at vi kunne. N√•r vi i det f√∏lgende skal t√¶nke endnu mere generelt, s√• bliver middelv√¶rdis√¶tningen nyttig.\nVi vender nu tilbage til definitionen af de retningsafledede. Vi f√•r i det f√∏lgende brug for at antage, at b√•de \\(f_{x}^{'}(x,y)\\) og \\(f_{y}^{'}\\left( x,y \\right)\\) eksisterer, s√• vi kan bruge middelv√¶rdis√¶tningen. Desuden f√•r vi ogs√• brug for at antage, at \\(f_{x}^{'}(x,y)\\) og \\(f_{y}^{'}\\left( x,y \\right)\\) er kontinuerte p√• en omegn af \\((x_{0},y_{0})\\).\nVi omskriver nu t√¶lleren i (1) for at kunne bringe middelv√¶rdis√¶tningen i spil \\[\n\\begin{aligned}\nf( x_{0} + h \\cdot u_{1}, y_{0} + h &\\cdot u_{2}) - f(x_{0}, y_{0})  = \\\\\n& f\\left( x_{0} + h \\cdot u_{1},y_{0} +  h \\cdot u_{2} \\right) \\\\\n& \\color{red}- f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right)  + f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) \\color{black}\\\\\n&- f(x_{0},y_{0})\n\\end{aligned}\n\\] Bem√¶rk, at vi har lagt et led til og trukket det samme led fra (markeret med r√∏dt). Det svarer til, at vi har indskudt et punkt i \\(xy\\)-planen, som illustreret i figur¬†2.\n\n\n\n\n\n\nFigur¬†2: Et r√∏dt punkt er indskud i \\(xy\\)-planen.\n\n\n\nVi ser nu, at de to f√∏rste led kun afviger p√• \\(x\\)-koordinaten (markeret med bl√•t nedenfor), og de to sidste led afviger kun p√• \\(y\\)-koordinaten (markeret med gr√∏nt): \\[\n\\begin{aligned}\nf\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f(x_{0},y_{0})   &= \\\\\n\\color{blue} f\\left( x_{0} + h \\cdot u_{1},y_{0} +  h \\cdot u_{2} \\right)  - & \\color{blue} f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right)  \\color{black} + \\\\  \\color{green} f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) - & \\color{green} f(x_{0},y_{0})\n\\end{aligned}\n\\tag{3}\\]\nAfvigelsen p√• henholdsvis \\(x\\)- og \\(y\\)-koordinaten er vist i figur¬†3.\n\n\n\n\n\n\nFigur¬†3: Afvigelsen p√• \\(x\\)-koordinaten er markeret med bl√•t, mens afvigelsen p√• \\(y\\)-koordinaten er markeret med gr√∏nt.\n\n\n\nVed at bruge den omskrevne middelv√¶rdis√¶tning i (2) p√• de to snitfunktioner \\(f\\left( x,y_{0} + h \\cdot u_{2} \\right)\\) som en funktion af \\(x\\) og \\(f(x_{0},y)\\) som en funktion af \\(y\\), f√•r vi nu f√∏lgende:\n\\[\n\\begin{aligned}\n\\color{blue} f\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) =  \\color{blue} f_{x}^{'}(c_{1},y_{0} + h \\cdot u_{2}) \\cdot h \\cdot u_{1}\n\\end{aligned}\n\\] og \\[\n\\color{green} f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) - f\\left( x_{0},y_{0} \\right) = f_{y}^{'}(x_{0},c_{2}) \\cdot h \\cdot u_{2}\n\\]\nHer har vi brugt, at den afledede af en snitfunktion, hvor vi kun varierer \\(x\\) er \\(f_{x}^{'}\\), og den afledede af en snitfunktion, hvor vi kun varierer \\(y\\) er \\(f_{y}^{'}\\). Tallet \\(c_{1}\\) ligger mellem \\(x_{0}\\) og \\(x_{0} + h \\cdot u_{1}\\), og tallet \\(c_{2}\\) ligger mellem \\(y_{0}\\) og \\(y_{0} + h \\cdot u_{2}\\). Dette er vist i figur¬†4.\n\n\n\n\n\n\nFigur¬†4: Tallet \\(c_{1}\\) ligger mellem \\(x_{0}\\) og \\(x_{0} + h \\cdot u_{1}\\), og tallet \\(c_{2}\\) ligger mellem \\(y_{0}\\) og \\(y_{0} + h \\cdot u_{2}\\).\n\n\n\nInds√¶tter vi de to udtryk ovenfor p√• h√∏jreside i (3) f√•r vi \\[\n\\begin{multline}\nf\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f(x_{0},y_{0})  = \\color{blue} f_{x}^{'}(c_{1},y_{0} + h \\cdot u_{2}) \\cdot h \\cdot u_{1} \\color{black} + \\\\ \\color{green} f_{y}^{'}(x_{0},c_{2}) \\cdot h \\cdot u_{2} \\\\\n\\end{multline}\n\\]\nOg bruges dette i definitionen for den retningsafledede i (1) ender vi med \\[\n\\begin{aligned}\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) &= \\lim_{h \\rightarrow 0}\\frac{f\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f(x_{0},y_{0})}{h}\n\\\\\n&=\n\\lim_{h \\rightarrow 0}\\frac{f_{x}^{'}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) \\cdot h \\cdot u_{1} + f_{y}^{'}(x_{0},c_{2}) \\cdot h \\cdot u_{2}\\ }{h}\n\\end{aligned}\n\\] Vi kan nu dividere \\(h\\) op i hvert led og f√•r \\[\n\\begin{aligned}\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right)\n&= \\underset{h \\rightarrow 0}{\\text{lim}} f_{x}^{'}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) \\cdot u_{1} + f_{y}^{'}(x_{0},c_{2}) \\cdot u_{2}\\\n\\\\\n&= \\lim_{h \\rightarrow 0}\\begin{pmatrix}\nf_{x}^{'}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) \\\\\nf_{y}^{'}(x_{0},c_{2})\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\nu_{1} \\\\\nu_{2}\n\\end{pmatrix}\n\\end{aligned}\n\\tag{4}\\] hvis gr√¶nsen eksisterer.\nHusk p√•, at \\(c_1\\) ligger i intervallet \\((x_0,x_0+h \\cdot u_1)\\) og \\(c_2\\) ligger i intervallet \\((y_0,y_0+h \\cdot u_2)\\). Derfor vil\n\\[\n\\lim_{h \\rightarrow 0}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) =\n(x_{0},y_{0})\n\\] og \\[\n\\lim_{h \\rightarrow 0}\\left( x_{0},c_{2} \\right) = \\ (x_{0},y_{0})\n\\]\nVi startede med at antage, at de partielle afledede er kontinuerte. Det f√•r vi brug for nu. Det betyder nemlig, at gr√¶nsev√¶rdien i (4) eksisterer, og vi f√•r det √∏nskede resultat\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\begin{pmatrix}\nf_{x}^{'}\\left( x_{0},y_{0} \\right) \\\\\nf_{y}^{'}(x_{0},y_{0}) \\\\\n\\end{pmatrix} \\cdot \\begin{pmatrix}\nu_{1} \\\\\nu_{2} \\\\\n\\end{pmatrix} = \\nabla f(x_{0},y_{0}) \\cdot \\vec{u}\n\\] Det var netop, hvad vi √∏nskede at vise1.\n1¬†Vi startede med at antage, at de partielle afledede eksisterer og er kontinuerte p√• en omegn. Bem√¶rk, at vi ud fra den antagelse nu har vist, at alle de retningsafledede ogs√• vil eksistere.\n\nOptimering\nBetragt en funktion \\(f\\) givet ved forskriften \\[\nf\\left( x,y \\right) = \\left( \\left( x - 5 \\right)^{2} + 3 \\right) \\cdot \\left( 5 + \\left( y - 10 \\right)^{2} \\right) + 30\n\\]\nHvis man ser lidt p√• forskriften, kan man m√•ske overbevise sig selv om, at funktionen har et minimum p√• 45, som f√•s, n√•r \\(\\left( x,y \\right) = (5,10)\\).\nGrafen ses herunder.\n\n\n\nMan kan lave en iterativ metode til at finde minimumspunktet ved at udnytte egenskaben ved gradientvektoren:\n\nV√¶lg et startpunkt \\((x_0,y_0)\\) som et f√∏rste g√¶t p√• et minimumspunkt.\n\nVi udnytter nu, at \\(- \\nabla f(x_0,y_0)\\) angiver den retning, hvor funktionsv√¶rdien falder mest i punktet \\((x_0,y_0,f(x_0,y_0))\\).\n\nG√• derfor et lille skridt i retningen \\(- \\nabla f(x_0,y_0)\\). Det giver s√• det n√¶ste punkt \\((x_1,y_1)\\), som forh√•bentlig er et bedre bud p√• et minimumspunkt.\nProcessen foreg√•r i definitionsm√¶ngden, men p√• grafen svarer det til at g√• et lille stykke den stejleste vej ned ad bakken.\nProcessen itereres s√• gentagne gange indtil man forh√•bentlig n√•r minimumspunktet.\n\nV√¶lger vi med den konkrete funktion et startpunkt p√•\n\\[\n(x_0,y_0) = ( - 3,4)\n\\]\nog v√¶lger vi i hvert skridt at l√¶gge -0,001 gange den negative gradientvektor i punktet til, s√• kan nogle af de f√∏lgende \\((x,y)\\)-punkter ses til venstre i figur¬†5. L√¶g her m√¶rke til hvordan vi n√¶rmer os det globale minimumssted i \\((5,10)\\). Til h√∏jre i figur¬†5 ses det ogs√• hvordan vi ved hj√¶lp af gradientnedstigning, n√¶rmer os den globale minimumsv√¶rdi p√• \\(f(5,10)=45\\).\n\n\n\n\n\n\nFigur¬†5: Til venstre ses et udvalg af nogle af de \\((x,y)\\)-punkter, som genereres i forbindelse med gradientnedstigning. Til h√∏jre ses et udvalg af nogle af de funktionsv√¶rdier, som genereres i forbindelse med gradientnedstigning.\n\n\n\nVi ser, at den iterative gradientnedstigning faktisk n√¶rmer sig det globale minimumspunkt. S√• om ikke andet s√• virker metoden i hvert fald i dette konkrete tilf√¶lde.\n\n\nTr√¶ning af neurale netv√¶rk\nAt lede efter et globalt minimumspunkt eller i det mindste et brugbart lokalt minimumspunkt for en funktion af rigtig mange variable er et problem, man st√•r overfor, n√•r man skal tr√¶ne et neuralt netv√¶rk og have fastlagt en masse v√¶gte i netv√¶rket.\nDet kan ikke g√∏res analytisk, s√• derfor bruger man netop en iterativ proces baseret p√• gradientnedstigning som metode til at finde frem til minimumspunktet. Eksemplet ovenfor illustrerer derfor id√©en bag en central del af tr√¶ningen af et neuralt netv√¶rk.\nL√¶s mere om hvordan gradientnedstigning konkret bruges her: Perceptroner og Kunstige neurale netv√¶rk."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html",
    "href": "materialer/neurale_net/neurale_net.html",
    "title": "Kunstige neurale netv√¶rk",
    "section": "",
    "text": "Forestil dig at du gerne vil have en funktion \\(f\\), som tager et billede som input og som output fort√¶ller dig, om der er en hund p√• billedet eller ej. Det kan illustreres s√•dan her:\n\n\n\n\n\n\nFigur¬†1: Funktion, der tager et billede som input, og som returnerer \"ja\" eller \"nej\" som output.\n\n\n\nOkay, det er m√•ske ikke s√• tit, at man har brug for en funktion, som kan detektere, om der er en hund p√• et billede, men hvad s√• hvis funktionen i stedet kan afg√∏re, om der er en kr√¶ftknude p√• et r√∏ntgenbillede? Eller hvis den kan genkende h√•ndskrevet tekst? Sidstn√¶vnte bliver f.eks. flittigt brugt til sortering af breve. Vi √∏nsker os i virkeligheden at v√¶re i stand til at programmere en funktion, som kan \"t√¶nke\" som et menneske. N√•r jeg ser et billede, kan jeg p√• ingen tid afg√∏re, om der er en hund p√• billedet eller ej. St√•r jeg med et brev i h√•nden, kan jeg som regel ogs√• l√¶se navn og adresse. En l√¶ge vil ogs√• kunne kigge p√• et r√∏ntgenbillede og afg√∏re, om der er en kr√¶ftknude eller ej. Det er i bund og grund, det vi forst√•r ved kunstig intelligens. At f√• computeren til at \"t√¶nke\" som et menneske. Nu kunne man m√•ske godt indvende \"hvad skal det til for?\". Der er vel ingen grund til at f√• en computer til at finde kr√¶ftknuder p√• et r√∏ntgenbillede, hvis vi allerede kan f√• en l√¶ge til det? Men hvad nu, hvis computeren faktisk kan opdage kr√¶ftknuder tidligere end l√¶gen? Eller hvad hvis man har s√• mange r√∏ntgenbilleder, at det vil v√¶re smart, at f√• en computer til at kigge dem igennem f√∏rst?"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-1",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-1",
    "title": "Kunstige neurale netv√¶rk",
    "section": "VIDEO: Kunstige neurale netv√¶rk 1",
    "text": "VIDEO: Kunstige neurale netv√¶rk 1\nI denne video forklarer lidt om hvad et kunstigt neuralt netv√¶rk er og lidt om hvilke input- og outputv√¶rdier, man kan bruge."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-2",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-2",
    "title": "Kunstige neurale netv√¶rk",
    "section": "VIDEO: Kunstige neurale netv√¶rk 2",
    "text": "VIDEO: Kunstige neurale netv√¶rk 2\nI denne video giver vi et eksempel p√• et simpelt kunstig neuralt netv√¶rk og forklarer feedforward."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-3",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-3",
    "title": "Kunstige neurale netv√¶rk",
    "section": "VIDEO: Kunstige neurale netv√¶rk 3",
    "text": "VIDEO: Kunstige neurale netv√¶rk 3\nI videoen her forklarer vi, hvad targetv√¶rdier er, og hvordan tabsfunktionen defineres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-4",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-4",
    "title": "Kunstige neurale netv√¶rk",
    "section": "VIDEO: Kunstige neurale netv√¶rk 4",
    "text": "VIDEO: Kunstige neurale netv√¶rk 4\nI denne video bliver gradientnedstigning forklaret."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-opdatering_w",
    "href": "materialer/neurale_net/neurale_net.html#sec-opdatering_w",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Opdatering af \\(w\\)-v√¶gtene",
    "text": "Opdatering af \\(w\\)-v√¶gtene\nN√•r man bruger backpropagation, starter man med at finde de partielle afledede for de v√¶gte, som direkte p√•virker outputv√¶rdien \\(o\\). P√• figur¬†4 fremg√•r det, at det er v√¶gtene \\(w_0, w_1\\) og \\(w_2\\) (husk at vi kalder vores bias for \\(w_0\\)). Lad os starte med at finde den partielle afledede for \\(w_1\\). Ved at bruge k√¶dereglen f√•r vi: \\[\n\\frac{\\partial E}{\\partial w_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial w_1}\n\\] Vi ved fra (12), at \\(E=\\frac{1}{2}(t-o)^2\\) og derfor er: \\[\n\\frac{d E}{d o} = \\frac{1}{2} \\cdot 2 \\cdot (t-o) \\cdot (-1) = -(t-o)\n\\tag{14}\\] Fra (10) har vi, at \\(o=\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)\\) og derfor f√•r vi \\[\n\\frac{\\partial o}{\\partial w_1} =\\sigma'(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0) \\cdot z_1\n\\] Vi har tidligere vist, at \\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\) og derfor har vi \\[\n\\frac{\\partial o}{\\partial w_1} =\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)(1-\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)) \\cdot z_1\n\\] Bruger vi nu, at \\(o=\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)\\) kan vi skrive ovenst√•ende lidt mere kompakt: \\[\n\\frac{\\partial o}{\\partial w_1} =o(1-o) \\cdot z_1\n\\] Alt i alt f√•r vi alts√•, at \\[\n\\frac{\\partial E}{\\partial w_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial w_1}\n= -(t-o) \\cdot o \\cdot (1-o) \\cdot z_1\n\\tag{15}\\] Vi kan nu udlede den f√∏rste opdateringsregel for v√¶gten \\(w_1\\) ved at bruge id√©en fra (13): \\[\nw_1 \\leftarrow w_1 - \\eta  \\cdot \\frac{\\partial E}{\\partial w_1}\n\\] Inds√¶ttes udtrykket fra (15), f√•r vi \\[\nw_1 \\leftarrow w_1 - \\eta  \\cdot (-(t-o) \\cdot o \\cdot (1-o) \\cdot z_1)\n\\] Det vil sige, at \\[\nw_1 \\leftarrow w_1 + \\eta  \\cdot (t-o) \\cdot o \\cdot (1-o) \\cdot z_1\n\\] Det er v√¶rd at dv√¶le lidt ved opdateringsleddet \\(\\eta  \\cdot (t-o) \\cdot o \\cdot (1-o) \\cdot z_1\\) p√• h√∏jresiden fordi det faktisk giver intuitiv god mening. For det f√∏rste er \\(\\eta\\), det vi som sagt kalder for vores learning rate - et lille positivt tal, som s√∏rger for, at vi ikke tager for store skridt p√• vores vej ned i dalen (til det lokale minimum). Faktoren \\(t-o\\) er jo netop fejlen. Nemlig forskellen mellem det vi √∏nsker \\(t\\) (target), og det som netv√¶rket giver \\(o\\) (output). Jo st√∏rre fejl/forskel, desto mere m√• vi justere v√¶gten. Ser vi p√• faktoren \\(o\\cdot(1-o)\\), s√• vil det v√¶re s√•dan, at hvis outputv√¶rdien \\(o\\) er t√¶t p√• enten \\(0\\) eller \\(1\\) (man siger at neuronen er \"m√¶ttet\"), s√• vil \\(o\\cdot(1-o)\\) v√¶re t√¶t p√• \\(0\\). Det vil sige, at hvis outputv√¶rdien er t√¶t p√• \\(0\\) eller \\(1\\), s√• √¶ndrer vi heller ikke s√• meget p√• v√¶gten. Endelig er der faktoren \\(z_1\\), som er inputtet fra det foreg√•ende lag (se figur¬†4). Hvis v√¶rdien af denne er (numerisk) stor, s√• f√•r det ogs√• stor betydning for opdateringsleddet (eller t√¶nk p√• det omvendt: hvis \\(z_1\\) er t√¶t p√• \\(0\\), s√• har \\(z_1\\) alligevel ikke s√• stor indflydelse p√• outputv√¶rdien, og s√• giver det heller ikke mening, at justere s√• meget p√• den tilh√∏rende v√¶gt \\(w_1\\)).\nDet viser sig faktisk, at faktoren \\((t-o) \\cdot o \\cdot (1-o)\\) kommer til at g√• igen rigtige mange gange i det f√∏lgende. Det bliver i l√¶ngden lidt tungt at sl√¶be rundt p√•. Derfor v√¶lger vi at definere \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\tag{16}\\] og derfor kan opdateringsreglen for \\(w_1\\) nu ogs√• skrives: \\[\nw_1 \\leftarrow w_1 + \\eta  \\cdot \\delta \\cdot z_1\n\\tag{17}\\]\nHelt analogt med ovenst√•ende kan man udlede opdateringsregler for \\(w_2\\) og \\(w_0\\). Resultatet er samlet her.\n\n\n\n\n\n\nOpdateringsregler for \\(w\\)-v√¶gtene\n\n\n\n\\[\\begin{align*}\nw_0 &\\leftarrow w_0 + \\eta  \\cdot \\delta  \\\\\nw_1 &\\leftarrow w_1 + \\eta  \\cdot \\delta \\cdot z_1 \\\\\nw_2 &\\leftarrow w_2 + \\eta  \\cdot \\delta \\cdot z_2 \\\\\n\\end{align*}\\] hvor \\[\\delta = (t-o) \\cdot o \\cdot (1-o)\\]\n\n\nMen hvordan foreg√•r det der med de opdateringsregler s√• egentligt? Jo alts√• vi starter med at s√¶tte v√¶gtene mere eller mindre tilf√¶ldigt. S√• laver vi ved hj√¶lp af vores tr√¶ningseksempel \\((\\vec{x},t)\\) et feedforward i netv√¶rket, som det er beskrevet i afsnit¬†3. Derfor f√•r vi beregnet outputv√¶rdien \\(o\\) samt \\(z_1\\) og \\(z_2\\) (husk at \\(z_1\\) og \\(z_2\\) bruges til at beregne \\(o\\)). Desuden kender vi jo fra vores tr√¶ningsdata target-v√¶rdien \\(t\\). Og voila! Alt hvad der indg√•r p√• h√∏jresiderne i ovenst√•ende opdateringsregler har vi nu adgang til, og vi kan derfor beregne de nye \\(w\\) v√¶gte.\nS√• mangler vi bare at finde opdateringsreglerne for de restende v√¶gte!"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-5",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-5",
    "title": "Kunstige neurale netv√¶rk",
    "section": "VIDEO: Kunstige neurale netv√¶rk 5",
    "text": "VIDEO: Kunstige neurale netv√¶rk 5\nI videoen her forklarer vi hvordan \\(w\\)-v√¶gtene opdateres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-opdatering_uv",
    "href": "materialer/neurale_net/neurale_net.html#sec-opdatering_uv",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Opdatering af \\(u\\)- og \\(v\\)-v√¶gtene",
    "text": "Opdatering af \\(u\\)- og \\(v\\)-v√¶gtene\nVi g√•r nu et trin l√¶ngere tilbage i netv√¶rket - v√¶k fra outputlaget. Her kan vi se neuronerne, som fyrer v√¶rdierne \\(z_1\\) og \\(z_2\\), som bliver p√•virket af \\(u\\)- og \\(v\\)-v√¶gtene. Lad os her starte med at bestemme opdateringsreglerne for \\(v\\)-v√¶gtene. For at g√∏re det skal vi finde ud af hvordan \\(v\\)-v√¶gtene p√•virker neuronerne l√¶ngere fremme i netv√¶rket. Se igen p√• figur¬†4. Her er det tydeligt, at \\(v\\)-v√¶gtene p√•virker den m√∏rkegr√∏nne neuron, som fyrer v√¶rdien \\(z_1\\), som igen p√•virker outputv√¶rdien. Derfor kan vi bruge k√¶dereglen p√• f√∏lgende m√•de: \\[\n\\frac{\\partial E}{\\partial v_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial v_1}\n\\] Vi ved allerede fra (14), at \\[\n\\frac{d E}{d o} = -(t-o)\n\\] Den partielle afledede af \\(o\\) med hensyn til \\(z_1\\) finder vi ved at bruge definitionen af outputv√¶riden \\(o\\) i (10) \\[\n\\begin{aligned}\n\\frac{\\partial o}{\\partial z_1} &= \\sigma'(w_1 \\cdot z_1+w_2 \\cdot z_2 + w_0) \\cdot w_1  \\\\\n&= \\sigma(w_1 \\cdot z_1+w_2 \\cdot z_2 + w_0) \\cdot (1-\\sigma(w_1 \\cdot z_1+w_2 \\cdot z_2 + w_0)) \\cdot w_1  \\\\\n&= o \\cdot (1-o) \\cdot w_1\n\\end{aligned}\n\\tag{18}\\] hvor vi igen har brugt s√¶tning¬†1. Og endelig ved at udnytte definitionen af \\(z_1\\) i (8) f√•r vi, at \\[\\begin{align}\n\\frac{\\partial z_1}{\\partial v_1} &= \\sigma'(v_1 \\cdot y_1+v_2 \\cdot y_2 + v_0) \\cdot y_1 \\\\\n&= \\sigma(v_1 \\cdot y_1+v_2 \\cdot y_2 + v_0) \\cdot (1-\\sigma(v_1 \\cdot y_1+v_2 \\cdot y_2 + v_0)) \\cdot y_1 \\\\\n&= z_1 \\cdot (1-z_1) \\cdot y_1\n\\end{align}\\] S√¶tter vi det hele sammen f√•r vi, at \\[\n\\frac{\\partial E}{\\partial v_1} = \\underbrace{-(t-o)}_{\\frac{\\partial E}{\\partial o}}  \\cdot \\underbrace{o \\cdot (1-o) \\cdot w_1}_{\\frac{\\partial o}{\\partial z_1}} \\cdot \\underbrace{z_1 \\cdot (1-z_1) \\cdot y_1}_{\\frac{\\partial z_1}{\\partial v_1}}\n\\] og bruger vi definitionen af \\(\\delta\\) i (16) f√•r vi et lidt mere kompakt udtryk \\[\n\\frac{\\partial E}{\\partial v_1} = -\\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_1\n\\] Opdateringsreglen for \\(v_1\\) bliver derfor \\[\nv_1 \\leftarrow v_1 - \\eta \\cdot \\frac{\\partial E}{\\partial v_1}\n\\] og med det netop udledte udtryk for \\(\\frac{\\partial E}{\\partial v_1}\\) f√•r vi \\[\nv_1 \\leftarrow v_1 - \\eta \\cdot (-\\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1)\\cdot y_1)\n\\] Det vil sige, at \\[\nv_1 \\leftarrow v_1 + \\eta \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_1\n\\] L√¶g igen m√¶rke til, at n√•r vi har v√¶ret igennem et feedforward i netv√¶rket, s√• kender vi alle de st√∏rrelser, som indg√•r i ovenst√•ende udtryk.\nP√• helt tilsvarende vis kan man bestemme opdateringsreglerne for \\(v_0\\) og \\(v_2\\). De tre opdateringsregler for \\(v\\)-v√¶gtene ses her:\n\n\n\n\n\n\nOpdateringsregler for \\(v\\)-v√¶gtene\n\n\n\n\\[\\begin{align}\nv_0 &\\leftarrow v_0 + \\eta  \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\\\\nv_1 &\\leftarrow v_1 + \\eta  \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_1 \\\\\nv_2 &\\leftarrow v_2 + \\eta  \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_2 \\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]\n\n\nOpdateringsreglerne for \\(u\\)-v√¶gtene findes p√• pr√¶cis samme m√•de. Her skal man blot se, at \\(u\\)-v√¶gtene har indflydelse p√• outputtet via \\(z_2\\) (se figur¬†4). Derfor skal man f.eks. finde den partielle afledede af \\(E\\) med hensyn til \\(u_1\\) ved at bruge k√¶dereglen p√• denne m√•de \\[\n\\frac{\\partial E}{\\partial u_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial u_1}\n\\] Udregninger svarende til det netop gennemg√•ede giver os\n\n\n\n\n\n\nOpdateringsregler for \\(u\\)-v√¶gtene\n\n\n\n\\[\\begin{align}\nu_0 &\\leftarrow u_0 + \\eta  \\cdot \\delta \\cdot w_2 \\cdot z_2 \\cdot (1-z_2) \\\\\nu_1 &\\leftarrow u_1 + \\eta  \\cdot \\delta \\cdot w_2 \\cdot z_2 \\cdot (1-z_2) \\cdot y_1 \\\\\nu_2 &\\leftarrow u_2 + \\eta  \\cdot \\delta \\cdot w_2 \\cdot z_2 \\cdot (1-z_2) \\cdot y_2 \\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-6",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-6",
    "title": "Kunstige neurale netv√¶rk",
    "section": "VIDEO: Kunstige neurale netv√¶rk 6",
    "text": "VIDEO: Kunstige neurale netv√¶rk 6\nI videoen her forklarer vi, hvordan \\(v\\)-v√¶gtene opdateres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-opdatering_rs",
    "href": "materialer/neurale_net/neurale_net.html#sec-opdatering_rs",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Opdatering af \\(r\\)- og \\(s\\)-v√¶gtene",
    "text": "Opdatering af \\(r\\)- og \\(s\\)-v√¶gtene\nS√• er vi endelig fremme ved \\(r\\)- og \\(s\\) v√¶gtene. Start lige med at tage en dyb ind√•nding! Nu bliver det lidt mere kompliceret. Se p√• figur¬†4. Lad os starte med at finde den partielle afledede af \\(E\\) med hensyn til \\(r_1\\). N√•r man ser p√• netv√¶rket, kan man se, at \\(r_1\\) i f√∏rste omgang p√•virker \\(y_1\\), \\(y_1\\) p√•virker b√•de \\(z_1\\) og \\(z_2\\), som s√• til sidst p√•virker outputv√¶rdien \\(o\\). Det kan illustreres s√•dan her \\[\n\\begin{matrix}\n& & & & z_1 & & & \\\\\n& & & \\nearrow & & \\searrow & & \\\\\nr_1 & \\rightarrow & y_1 & & & & \\rightarrow & o \\\\\n& & & \\searrow & & \\nearrow & & \\\\\n& & & & z_2 & & & \\\\\n\\end{matrix}\n\\]\nBalladen er, at \\(y_1\\) b√•de p√•virker \\(z_1\\) og \\(z_2\\), og det g√∏r det hele lidt mere kompliceret. Lad os lige starte med at se bort fra det. If√∏lge k√¶dereglen f√•r vi s√•: \\[\n\\frac{\\partial E}{\\partial r_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial y_1}  \\cdot \\frac{\\partial y_1}{\\partial r_1}\n\\] Men s√• var det jo, at \\(o\\) i virkeligheden afh√¶nger af \\(y_1\\) b√•de via \\(z_1\\) og \\(z_2\\). Man kunne skrive det s√•dan her: \\[\no(z_1(y_1), z_2(y_1))\n\\] Bem√¶rk, at \\(z_1\\) og \\(z_2\\) jo ogs√• afh√¶nger af \\(y_2\\), men n√•r vi skal differentiere med hensyn til \\(y_1\\), s√• er \\(y_2\\) at betragte som en konstant. Og n√•r konstanter bliver differentieret, s√• giver det som bekendt \\(0\\).\nDerfor: For at finde den partielle afledede af \\(o\\) med hensyn til \\(y_1\\) m√• vi benytte k√¶dereglen for funktioner af flere variable. Den siger, at \\[\n\\frac{\\partial o}{\\partial y_1} = \\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial y_1} + \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial y_1}\n\\] Det samlede udtryk for den partielle afledede af \\(E\\) med hensyn til \\(r_1\\) bliver derfor \\[\n\\frac{\\partial E}{\\partial r_1} = \\frac{d E}{d o} \\cdot\n\\left(\n\\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial y_1} + \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial y_1}\n\\right)\n\\cdot \\frac{\\partial y_1}{\\partial r_1}\n  \\tag{19}\\] Vi finder hver af de afledede, som indg√•r i ovenst√•ende udtryk √©n ad gangen. Vi ved allerede fra (14), at \\[\n\\frac{d E}{d o} = \\frac{1}{2} \\cdot 2 \\cdot (t-o) \\cdot (-1) = -(t-o)\n\\] Vi ved ogs√• fra (18), at \\[\n\\frac{\\partial o}{\\partial z_1} = o \\cdot (1-o) \\cdot w_1\n\\] Differentieres \\(z_1\\) (se (8)) med hensyn til \\(y_1\\) f√•r vi \\[\\begin{align}\n\\frac{\\partial z_1}{\\partial y_1} &= \\sigma'(v_1 \\cdot y_1 + v_2 \\cdot y_2 + v_0)\\cdot v_1 \\\\\n&= z_1 \\cdot (1-z_1) \\cdot v_1\n\\end{align}\\] hvor vi igen har brugt s√¶tning¬†1 og definitionen af \\(z_1\\) i (8). Helt tilsvarende kan vi finde \\(\\frac{\\partial o}{\\partial z_2}\\) og \\(\\frac{\\partial z_2}{\\partial y_1}\\) (se (9)) \\[\n\\frac{\\partial o}{\\partial z_2} = o \\cdot (1-o)\\cdot w_2\n\\] og \\[\n\\frac{\\partial z_2}{\\partial y_1} = z_2 \\cdot (1-z_2)\\cdot u_1\n\\] Den sidste partielle afledede \\(\\frac{\\partial y_1}{\\partial r_1}\\) finder vi ved at differentiere udtrykket for \\(y_1\\) i (4), hvor vi endnu engang udnytter s√¶tning¬†1.\nInds√¶tter vi nu alle de udtryk, som vi netop har udledt, i (19) f√•r vi et temmelig langt udtryk for \\(\\frac{\\partial E}{\\partial r_1}\\): \\[\\begin{align}\n\\frac{\\partial E}{\\partial r_1} &= \\underbrace{-(t-o)}_{\\frac{dE}{do}}\\cdot \\\\ &\\Big( \\underbrace{o\\cdot(1-o)\\cdot w_1}_{\\frac{\\partial o}{\\partial z_1}} \\cdot \\underbrace{z_1\\cdot(1-z_1)\\cdot v_1}_{\\frac{\\partial z_1}{\\partial y_1}}+\\underbrace{o\\cdot(1-o)\\cdot w_2}_{\\frac{\\partial o}{\\partial z_2}}\\cdot \\underbrace{z_2\\cdot(1-z_2)\\cdot u_1}_{\\frac{\\partial z_2}{\\partial y_1}}\\Big)\\cdot \\\\ & \\qquad \\underbrace{y_1\\cdot(1-y_1)\\cdot x_1}_{\\frac{\\partial y_1}{\\partial r_1}}\n\\end{align}\\] Og s√¶tter vi \\(o\\cdot(1-o)\\) uden for parentesen og erstatter \\((t-o)\\cdot o\\cdot (1-o)\\) med \\(\\delta\\) f√•r vi \\[\n\\frac{\\partial E}{\\partial r_1}\n=-\\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_1\n\\] Helt i tr√•d med tidligere f√•r vi alts√• f√∏lgende opdateringsregel for \\(r_1\\) \\[r_1 \\leftarrow r_1 - \\eta \\cdot \\frac{\\partial E}{\\partial r_1} \\] Det vil sige \\[\nr_1 \\leftarrow r_1 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_1\n\\]\nUdleder man tilsvarende opdateringsregler for \\(r_2, r_3, r_4\\) og \\(r_0\\) vil man se, at det eneste som kommer til at √¶ndre sig i ovenst√•ende er den sidste faktor \\(x_1\\), som bliver erstattet med henholdsvis \\(x_2, x_3, x_4\\) og \\(1\\). Derfor f√•r vi samlet set\n\n\n\n\n\n\nOpdateringsregler for \\(r\\)-v√¶gtene\n\n\n\n\\[\\begin{align}\nr_0 &\\leftarrow r_0 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\\\\nr_1 &\\leftarrow r_1 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_1\\\\\nr_2 &\\leftarrow r_2 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_2\\\\\nr_3 &\\leftarrow r_3 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_3\\\\\nr_4 &\\leftarrow r_4 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_4\\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]\n\n\nOpdateringen af \\(s\\)-v√¶gtene foreg√•r p√• samme m√•de. Hvis du ser p√• figur¬†4, kan du se, at alle \\(s\\)-v√¶gtene p√•virker \\(y_2\\), som s√• p√•virker b√•de \\(z_1\\) og \\(z_2\\), som i sidste ende p√•virker outputtet \\(o\\). Ser vi generelt p√• v√¶gten \\(s_i\\), hvor \\(i=0, 1, 2, 3\\) eller \\(4\\), har vi alts√• \\[\n\\begin{matrix}\n& & & & z_1 & & & \\\\\n& & & \\nearrow & & \\searrow & & \\\\\ns_i & \\rightarrow & y_2 & & & & \\rightarrow & o \\\\\n& & & \\searrow & & \\nearrow & & \\\\\n& & & & z_2 & & & \\\\\n\\end{matrix}\n\\] Som tidligere kan vi starte med at skrive \\[\n\\frac{\\partial E}{\\partial s_i} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial y_2}  \\cdot \\frac{\\partial y_2}{\\partial s_i}\n\\] og bruger vi igen k√¶dreglen for funktioner af flere variable, f√•r vi \\[\n\\frac{\\partial E}{\\partial s_i} = \\frac{d E}{d o} \\cdot\n\\left(\n\\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial y_2} + \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial y_2}\n\\right)\n\\cdot \\frac{\\partial y_2}{\\partial s_i}\n\\] I ovenst√•ende udtryk bliver det klart, at opdateringsreglerne vil blive ens bortset fra den sidste faktor.\nNu udledes alle de partielle afledede, fuldst√¶ndig som for \\(r\\)-v√¶gtene og vi ender med f√∏lgende opdateringsregler for \\(s\\)-v√¶gtene:\n\n\n\n\n\n\nOpdateringsregler for \\(s\\)-v√¶gtene\n\n\n\n\\[\\begin{align}\ns_0 &\\leftarrow s_0 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2) \\\\\ns_1 &\\leftarrow s_1 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2) \\cdot x_1\\\\\ns_2 &\\leftarrow s_2 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2)  \\cdot x_2\\\\\ns_3 &\\leftarrow s_3 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2)  \\cdot x_3\\\\\ns_4 &\\leftarrow s_4 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2)  \\cdot x_4\\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]\n\n\nDet var faktisk det! Alts√• det blev jo en v√¶rre omgang bogstavgymnastik, men faktum er, at vi er i m√•l med at udlede backpropagation algoritmen for vores simple netv√¶rk i figur¬†4. Hurra for det!"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-7",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netv√¶rk-7",
    "title": "Kunstige neurale netv√¶rk",
    "section": "VIDEO: Kunstige neurale netv√¶rk 7",
    "text": "VIDEO: Kunstige neurale netv√¶rk 7\nI denne video forklares hvordan \\(r\\)-v√¶gtene opdateres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-feedforward_indekser",
    "href": "materialer/neurale_net/neurale_net.html#sec-feedforward_indekser",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Feedforward med indekser",
    "text": "Feedforward med indekser\nVi vil nu se p√•, hvordan de forskellige \\(a_j^{(k)}\\)-v√¶rdier beregnes. Det vil sige, at vi alts√• skal se p√•, hvordan de forskellige feedforward-ligninger ser ud med vores nye notation.\n\n\n\n\n\n\nFigur¬†9: Udregning af \\(a_1^{(2)}\\).\n\n\n\nLad os se p√• et konkret eksempel, s√• bliver det lidt nemmere at forholde sig til. Vi starter med at udregne outputv√¶rdien \\(a_1^{(2)}\\) for den f√∏rste neuron i det andet lag. Denne neuron f√•r input fra alle neuroner i det foreg√•ende lag (som her er inputlaget). Bruger vi den notation for v√¶gtene, som vi lige har indf√∏rt, s√• starter vi med at beregne: \\[\nz_1^{(2)} = w_{11}^{(2)} \\cdot x_1 + w_{12}^{(2)} \\cdot x_2 + w_{13}^{(2)} \\cdot x_3 + w_{14}^{(2)} \\cdot x_4 + b_1^{(2)}\n\\] Der er to ting at bem√¶rke her: 1) Vi v√¶lger, at kalde udtrykket p√• h√∏jreside for \\(z_1^{(2)}\\) og, 2) vi har kaldt biasen for \\(b_1^{(2)}\\).\nBruger vi nu de mere generelle udtryk for inputv√¶rdierne \\(a_1^{(1)}, a_2^{(1)}, \\dots, a_4^{(1)}\\) kan vi skrive: \\[\\begin{align}\nz_1^{(2)} &= w_{11}^{(2)} \\cdot a_1^{(1)} + w_{12}^{(2)} \\cdot a_2^{(1)} + w_{13}^{(2)} \\cdot a_3^{(1)} + w_{14}^{(2)} \\cdot a_4^{(1)} + b_1^{(2)} \\\\\n&= \\sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)}\n\\end{align}\\] Og endelig finder vi outputv√¶rdien \\(a_1^{(2)}\\) for den f√∏rste neuron i det andet lag ved som tidligere at anvende sigmoid-funktionen p√• ovenst√•ende udtryk: \\[\\begin{align}\na_1^{(2)} &= \\sigma(z_1^{(2)}) \\\\\n&= \\sigma \\left( \\sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)} \\right)\n\\end{align}\\] Det her er faktisk notationsm√¶ssigt selve id√©en. Folder vi det ud til hele det andet lag f√•r vi derfor:\n\n\n\n\n\n\nFeedforwardligninger til lag 2\n\n\n\nBeregn f√∏rst: \\[\\begin{align}\nz_1^{(2)} &= \\sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)} \\\\\n& \\\\\nz_2^{(2)} &=\\sum_{i=1}^{4} w_{2i}^{(2)} a_i^{(1)} +  b_2^{(2)} \\\\\n& \\\\\nz_3^{(2)} &= \\sum_{i=1}^{4} w_{3i}^{(2)} a_i^{(1)} +  b_3^{(2)} \\\\\n\\end{align}\\] Outputv√¶rdierne for neuroner i det andet lag udregnes dern√¶st p√• denne m√•de: \\[\\begin{align}\na_1^{(2)} &= \\sigma(z_1^{(2)}) \\\\\n& \\\\\na_2^{(2)} &= \\sigma(z_2^{(2)}) \\\\\n&\\\\\na_3^{(2)} &= \\sigma(z_3^{(2)}) \\\\\n\\end{align}\\]\n\n\nOg vover vi pelsen, kan vi helt generelt skrive:\n\n\n\n\n\n\nFeedforward-ligninger til lag 2\n\n\n\nBeregn f√∏rst: \\[\\begin{align}\nz_j^{(2)} = \\sum_{i=1}^{4} w_{ji}^{(2)} a_i^{(1)} +  b_j^{(2)}\n\\end{align}\\] Outputv√¶rdierne for neuroner i det andet lag udregnes dern√¶st p√• denne m√•de: \\[\\begin{align}\na_j^{(2)} &= \\sigma(z_j^{(2)})\n\\end{align}\\] for \\(j \\in \\{1, 2, 3 \\}\\).\n\n\nFordelen ved denne notation er, at det nu er utrolig nemt at opskrive feedforward-ligningerne for lag 3 og 4 - det er blot nogle indekser, som skal √¶ndres lidt. I det tredje lag er der to neuroner, hvis outputv√¶rdier beregnes p√• denne m√•de:\n\n\n\n\n\n\nFeedforward-ligninger til lag 3\n\n\n\nBeregn f√∏rst: \\[\nz_j^{(3)} = \\sum_{i=1}^{3} w_{ji}^{(3)} a_i^{(2)} +  b_j^{(3)}\n\\tag{20}\\] Outputv√¶rdierne for neuroner i det tredje lag udregnes dern√¶st p√• denne m√•de: \\[\\begin{align}\na_j^{(3)} &= \\sigma(z_j^{(3)})\n\\end{align}\\] for \\(j \\in \\{1, 2 \\}\\).\n\n\nOg endelig beregnes outputtet fra hele netv√¶rket i det fjerde lag:\n\n\n\n\n\n\nFeedforward-ligninger til lag 4\n\n\n\nUdregn f√∏rst: \\[\nz_j^{(4)} = \\sum_{i=1}^{2} w_{ji}^{(4)} a_i^{(3)} +  b_j^{(4)}\n\\tag{21}\\] Outputv√¶rdierne for neuroner i det tredje lag udregnes dern√¶st p√• denne m√•de: \\[\\begin{align}\ny_j = a_j^{(4)} &= \\sigma(z_j^{(4)})\n\\end{align}\\] for \\(j \\in \\{1, 2, 3 \\}\\).\n\n\nDet fremg√•r nu tydeligt, at feedforward-ligningerne er p√• fuldst√¶ndig samme form, og vi vil derfor helt generelt kunne skrive:\n\n\n\n\n\n\nFeedforward-ligninger generelt\n\n\n\nBeregn f√∏rst: \\[\\begin{align}\nz_j^{(k)} = \\sum_{i} w_{ji}^{(k)} a_i^{(k-1)} +  b_j^{(k)}\n\\end{align}\\] Outputv√¶rdierne for neuroner i det \\(k\\)‚Äôte lag udregnes dern√¶st p√• denne m√•de: \\[\\begin{align}\na_j^{(k)} &= \\sigma(z_j^{(k)})\n\\end{align}\\] for \\(k \\in \\{2, 3, 4 \\}\\).\n\n\nN√•r man bruger feedforward, starter man alts√• med at udregne outputv√¶rdierne for det f√∏rste skjulte lag, dern√¶st for det andet skjulte lag og s√• videre, indtil man n√•r til outputv√¶rdierne for selve netv√¶rket (deraf navnet: feedforward ). Bem√¶rk her, at det ikke giver mening at udregne \\(a_j^{(1)}\\), fordi det svarer til inputv√¶rdierne til netv√¶rket."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-backpropagation_indekser",
    "href": "materialer/neurale_net/neurale_net.html#sec-backpropagation_indekser",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Backpropagation med indekser",
    "text": "Backpropagation med indekser\nLad os nu se p√• hvordan backpropagation fungerer. Vi skal alts√• have opskrevet vores opdateringsregler med den nye notation, og vi vil gribe det an, ligesom vi gjorde det i afsnit¬†4. Nemlig ved at starte i det sidste lag (her lag \\(4\\)) og finde opdateringsreglerne for de v√¶gte og bias, som har direkte indflydelse p√• outputv√¶rdierne fra lag \\(4\\).\n\nOpdateringsregler for lag \\(4\\)\nVi er alts√• i f√∏rste omgang p√• jagt efter \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(4)}},\n\\] for \\(j \\in \\{1, 2, 3\\}\\), \\(i \\in \\{1, 2\\}\\). Tabsfunktionen \\(E\\), som h√∏rer til netv√¶rket i figur¬†7, bliver her: \\[\nE=\\frac{1}{2} \\sum_{j=1}^3 \\left( t_j-y_j \\right)^2 = \\frac{1}{2} \\sum_{j=1}^3 \\left( t_j-a_j^{(4)} \\right)^2\n\\tag{22}\\] hvor igen \\(t_j\\) er den √∏nskede target-v√¶rdi for den \\(j\\)‚Äôte outputneuron.\nLad os starte med at bestemme \\(\\frac{\\partial E}{\\partial w_{ji}^{(4)}}\\). Vi m√• derfor f√∏rst se p√•, hvordan \\(w_{ji}^{(4)}\\) p√•virker tabsfunktionen \\(E\\). Da \\(w_{ji}^{(4)}\\) kun indg√•r i udtrykket for beregningen af \\(z_j^{(4)}\\), som igen bruges til beregningen af \\(a_j^{(4)}\\), som dern√¶st direkte p√•virker tabsfunktionen, kan vi skrive: \\[\nw_{ji}^{(4)} \\rightarrow z_j^{(4)} \\rightarrow a_j^{(4)} \\rightarrow E\n\\] Bruger vi f√∏rst k√¶dereglen √©n gang, f√•r vi derfor \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} = \\frac{\\partial E}{\\partial z_j^{(4)}} \\cdot \\frac{\\partial z_j^{(4)}}{\\partial w_{ji}^{(4)}}\n\\tag{23}\\] og bruges k√¶dereglen igen, kan f√∏rste faktor udfoldes yderligere \\[\n\\frac{\\partial E}{\\partial z_j^{(4)}} = \\frac{\\partial E}{\\partial a_j^{(4)}} \\cdot  \\frac{\\partial a_j^{(4)}}{\\partial z_j^{(4)}}\n\\] Lad os starte med at udregne \\(\\frac{\\partial E}{\\partial z_j^{(4)}}\\) ved at udregne hver faktor p√• h√∏jresiden i ovenst√•ende udtryk for sig. Fra (22) f√•r vi, at \\[\nE=\\frac{1}{2} \\sum_{j=1}^3 \\left( t_j-a_j^{(4)} \\right)^2 = \\frac{1}{2} \\left( \\left( t_1-a_1^{(4)} \\right)^2 + \\left( t_2-a_2^{(4)} \\right)^2+ \\left( t_3-a_3^{(4)}\\right)^2 \\right)\n\\] Hvis vi f.eks. skal differentiere ovenst√•ende med hensyn til \\(a_2^{(4)}\\), kan vi se at alle de led, som ikke indeholder \\(a_2^{(4)}\\), vil v√¶re at betragte som konstanter, n√•r vi differentierer - og n√•r vi differentierer konstanter, f√•r vi som bekendt \\(0\\). Derfor f√•r vi, at \\[\n\\frac{\\partial E}{\\partial a_2^{(4)}} = \\frac{1}{2}\\cdot 2 \\cdot (t_2-a_2^{(4)})\\cdot (-1) = -(t_2-a_2^{(4)})\n\\] P√• tilsvarende vis har vi derfor generelt, at \\[\n\\frac{\\partial E}{\\partial a_j^{(4)}} = -(t_j-a_j^{(4)})\n\\tag{24}\\] Vi ved ogs√•, at \\[\na_j^{(4)} = \\sigma(z_j^{(4)})\n\\] Og bruger vi endnu en gang resultatet fra s√¶tning¬†1 f√•r vi, at \\[\n\\frac{\\partial a_j^{(4)}}{\\partial z_j^{(4)}}  = \\sigma'(z_j^{(4)})= \\sigma(z_j^{(4)}) \\cdot (1-\\sigma(z_j^{(4)}))= a_j^{(4)}\\cdot (1-a_j^{(4)})\n\\tag{25}\\] Indtil videre har vi alts√•, at \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(4)}} &= \\frac{\\partial E}{\\partial a_j^{(4)}} \\cdot  \\frac{\\partial a_j^{(4)}}{\\partial z_j^{(4)}} \\\\\n&=-(t_j-a_j^{(4)}) \\cdot  a_j^{(4)}\\cdot (1-a_j^{(4)})\n\\end{align}\\] I forhold til det videre arbejde viser det sig hensigtsm√¶ssigt, at lave en samlet betegnelse for \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(4)}}  \n&=-(t_j-a_j^{(4)}) \\cdot  a_j^{(4)}\\cdot (1-a_j^{(4)})\n\\end{align}\\] Vi s√¶tter derfor \\[\n\\delta_j^{(4)} = \\frac{\\partial E}{\\partial z_j^{(4)}}  = -(t_j-a_j^{(4)}) \\cdot a_j^{(4)} \\cdot (1-a_j^{(4)})\n\\] Udtrykket \\(\\delta_j^{(4)}\\) kalder man ogs√• for fejlleddet for det fjerde lag, men det kommer vi tilbage til senere.\nVi har nu fundet den f√∏rste faktor i (23), og mangler derfor kun at bestemme \\(\\frac{\\partial z_j^{(4)}}{\\partial w_{ji}^{(4)}}\\). Bruger vi (21) ser vi, at \\[\nz_j^{(4)} =  \\sum_{i=1}^{2} w_{ji}^{(4)} a_i^{(3)} +  b_j^{(4)}  =\nw_{j1}^{(4)} a_1^{(3)}+ w_{j2}^{(4)} a_2^{(3)} +  b_j^{(4)}\n\\] Skal vi f.eks. differentiere dette udtryk med hensyn til \\(w_{j1}^{(4)}\\), f√•r vi (fordi de fleste led i ovenst√•ende, vil v√¶re at betragte som konstanter) \\[\n\\frac{\\partial z_j^{(4)}}{\\partial w_{j1}^{(4)}} =  a_1^{(3)}\n\\] Og helt tilsvarende hvis vi differentierer med hensyn til \\(w_{j2}^{(4)}\\), f√•r vi \\[\n\\frac{\\partial z_j^{(4)}}{\\partial w_{j2}^{(4)}} =  a_2^{(3)}\n\\] Generelt har vi derfor, at \\[\n\\frac{\\partial z_j^{(4)}}{\\partial w_{ji}^{(4)}} = a_i^{(3)}\n\\tag{26}\\]\nSamler vi nu de tre udtryk, som vi netop har udledt og inds√¶tter i (23) f√•r vi \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} =  -(t_j-a_j^{(4)}) \\cdot a_j^{(4)} \\cdot (1-a_j^{(4)}) \\cdot a_i^{(3)}\n\\] og med den lidt kortere notation, som vi indf√∏rte ovenfor, kan vi nu skrive \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} =  \\delta_j^{(4)} \\cdot a_i^{(3)}\n\\] For at finde opdateringsreglerne for biasene, m√• vi f√∏rst bestemme de partielle afledede af \\(E\\) med hensyn til \\(b_j^{(4)}\\). P√• helt tilsvarende vis f√•r vi, at \\[\n\\frac{\\partial E}{\\partial b_j^{(4)}} = \\frac{\\partial E}{\\partial z_j^{(4)}} \\cdot \\frac{\\partial z_j^{(4)}}{\\partial b_j^{(4)}}\n\\] Vi ved allerede, at \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(4)}}  = \\delta_j^{(4)}\n\\end{align}\\] og ser man p√• ligningen i (21), ses det nemt, at \\[\n\\frac{\\partial z_j^{(4)}}{\\partial b_j^{(4)}} = 1\n\\] og derfor har vi, at \\[\n\\frac{\\partial E}{\\partial b_j^{(4)}}  =\\delta_j^{(4)}\n\\] Opdateringsreglerne for de v√¶gte og bias, som h√∏rer til outputlaget (lag \\(4\\)) er derfor \\[\nw_{ji}^{(4)} \\leftarrow w_{ji}^{(4)} - \\eta \\cdot \\frac{\\partial E}{\\partial w_{ji}^{(4)}} = w_{ji}^{(4)} - \\eta \\cdot \\delta_j^{(4)} \\cdot a_i^{(3)}\n\\] og \\[\nb_j^{(4)} \\leftarrow b_j^{(4)} - \\eta \\cdot \\frac{\\partial E}{\\partial b_j^{(4)}}  = b_j^{(4)} - \\eta \\cdot \\delta_j^{(4)}\n\\] Vi kan alts√• opsummere:\n\n\n\n\n\n\nOpdateringsregler til v√¶gte og bias i outputlaget (lag 4)\n\n\n\nV√¶gtene i outputlaget opdateres p√• denne m√•de: \\[\nw_{ji}^{(4)} \\leftarrow w_{ji}^{(4)} - \\eta \\cdot \\delta_j^{(4)} \\cdot a_i^{(3)}\n\\] Biasene i outputlaget opdateres p√• denne m√•de: \\[\nb_j^{(4)} \\leftarrow b_j^{(4)} - \\eta \\cdot \\delta_j^{(4)}\n\\] hvor \\[\n\\delta_j^{(4)} = \\frac{\\partial E}{\\partial z_j^{(4)}}= -(t_j-a_j^{(4)}) \\cdot a_j^{(4)} \\cdot (1-a_j^{(4)})\n\\tag{27}\\]\n\n\nUdtrykket \\(\\delta_j^{(4)}\\) kalder man, som n√¶vnt tidligere, ogs√• for fejlleddet i den \\(j\\)‚Äôte r√¶kke i det fjerde lag, og man kan se p√• ovenst√•ende opdateringsregler, at dette fejlled netop indg√•r i opdateringen af b√•de v√¶gtene og biasene. Faktisk kan vi pr√¶cis, som vi gjorde det tidligere, till√¶gge dette fejlled en intuitiv god mening. Det kommer vi tilbage til igen senere!\nBem√¶rk, at hvis vi i vores netv√¶rk starter med at v√¶lge mere eller mindre tilf√¶ldige v√¶gte, s√• kan vi p√• baggrund af dem bruge feedforwardligningerne til at udregne, \\(a_j^{(4)}\\)- og \\(a_i^{(3)}\\)- v√¶rdierne. Samtidig kender vi target-v√¶rdierne \\(t_j\\), og vi kan derfor ogs√• udregne fejlleddene \\(\\delta_j^{(4)}\\). Vi har alts√• alt, hvad vi skal bruge for at benytte ovenst√•ende opdateringsregler.\n\n\nOpdateringsregler for lag \\(3\\)\nVi bev√¶ger os nu et trin l√¶ngere bagud i netv√¶rket og udleder opdateringsreglerne for det n√¶stsidste lag - lag \\(3\\). Alts√• skal vi have bestemt \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(3)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(3)}},\n\\] for \\(j \\in \\{1, 2\\}\\), \\(i \\in \\{1, 2, 3\\}\\). Vi m√• igen se p√•, hvordan \\(w_{ji}^{(3)}\\) p√•virker tabsfunktionen \\(E\\). Ser vi p√• figur figur¬†7, kan vi se, at \\(w_{ji}^{(3)}\\) direkte p√•virker \\(z_j^{(3)}\\), som igen direkte p√•virker \\(a_j^{(3)}\\). Nu vil den \\(j\\)‚Äôte neuron i det tredje lag fyre v√¶rdien \\(a_j^{(3)}\\) til alle neuroner i det fjerde lag. Alts√• vil \\(a_j^{(3)}\\) p√•virke \\(z_1^{(4)}, z_2^{(4)}\\) og \\(z_3^{(4)}\\), som bruges til beregning af \\(a_1^{(4)}, a_2^{(4)}\\) og \\(a_3^{(4)}\\), som s√• igen vil p√•virke tabsfunktionen \\(E\\). Det kan illustreres p√• denne m√•de \\[\n\\begin{matrix}\n& & & & z_1^{(4)} \\rightarrow a_1^{(4)} & & \\\\\n& & & \\nearrow  & &  \\searrow & \\\\\nw_{ji}^{(3)} & \\rightarrow & z_j^{(3)} \\rightarrow a_j^{(3)} & \\rightarrow &\nz_2^{(4)} \\rightarrow a_2^{(4)} & \\rightarrow & E \\\\\n& & & \\searrow & &  \\nearrow &  \\\\\n& & & & z_3^{(4)} \\rightarrow a_3^{(4)} & & \\\\\n\\end{matrix}\n\\] I f√∏rste omgang kan vi skrive \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(3)}} = \\frac{\\partial E}{\\partial z_j^{(3)}} \\cdot \\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}}\n\\tag{28}\\] og s√• gentagne gange bruge k√¶dereglen til at udfolde dette udtryk.\nLad os starte med det nemmeste, nemlig \\(\\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}}\\). Ser vi p√• definitionen af \\(z_j^{(3)}\\) i (20), kan vi argumentere helt tilsvarende, som da vi ovenfor udledte udtrykket i (26) og f√•r \\[\n\\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}} = a_i^{(2)}\n\\]\nLad os nu kaste os over den f√∏rste faktor i (28). Vi kan starte med at udnytte denne lidt overordnede m√•de, som \\(z_j^{(3)}\\) p√•virker \\(E\\) p√• \\[\nz_j^{(3)} \\rightarrow a_j^{(3)} \\rightarrow E\n\\] K√¶dereglen giver os derfor i f√∏rste omgang \\[\n\\frac{\\partial E}{\\partial z_j^{(3)}} = \\frac{\\partial E}{\\partial a_j^{(3)}} \\cdot\n\\frac{\\partial a_j^{(3)}}{\\partial z_j^{(3)}}\n\\tag{29}\\] Igen er sidste faktor nem nok, idet \\[\na_j^{(3)} = \\sigma (z_j^{(3)})\n\\] og derfor er \\[\n\\frac{\\partial a_j^{(3)}}{\\partial z_j^{(3)}} = \\sigma' (z_j^{(3)})=\\sigma(z_j^{(3)})\\cdot (1-\\sigma(z_j^{(3)}))= a_j^{(3)} \\cdot (1-a_j^{(3)}),\n\\] hvor vi endnu en gang har benytte s√¶tning¬†1.\nN√•r vi skal bestemme \\(\\frac{\\partial E}{\\partial a_j^{(3)}}\\) kommer vi ikke udenom k√¶dereglen for funktioner af flere variable. Det bliver tydeligt, n√•r vi zoomer ind p√• hvordan \\(a_j^{(3)}\\) p√•virker \\(E\\): \\[\n\\begin{matrix}\n  & & z_1^{(4)} \\rightarrow a_1^{(4)} & & \\\\\n  & \\nearrow  & &  \\searrow & \\\\\n  a_j^{(3)} & \\rightarrow &\nz_2^{(4)} \\rightarrow a_2^{(4)} & \\rightarrow & E \\\\\n& \\searrow & &  \\nearrow &  \\\\\n& & z_3^{(4)} \\rightarrow a_3^{(4)} & & \\\\\n\\end{matrix}\n\\] For at vi senere kan udnytte nogle af de ligninger, som vi udledte i lag \\(4\\), vil vi faktisk bare n√∏jes med at se p√• det, p√• denne m√•de: \\[\n\\begin{matrix}\n  & & z_1^{(4)} & & \\\\\n  & \\nearrow  & &  \\searrow & \\\\\n  a_j^{(3)} & \\rightarrow &\nz_2^{(4)} & \\rightarrow & E \\\\\n& \\searrow & &  \\nearrow &  \\\\\n& & z_3^{(4)}  & & \\\\\n\\end{matrix}\n\\] Nu er vi endelig klar til at bruge k√¶dereglen for funktioner af flere variable: \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(3)}} &=  \\frac{\\partial E}{\\partial z_1^{(4)}} \\cdot \\frac{\\partial z_1^{(4)}}{\\partial a_j^{(3)}}  + \\frac{\\partial E}{\\partial z_2^{(4)}} \\cdot \\frac{\\partial z_2^{(4)}}{\\partial a_j^{(3)}} + \\frac{\\partial E}{\\partial z_3^{(4)}} \\cdot \\frac{\\partial z_3^{(4)}}{\\partial a_j^{(3)}} \\\\\n&= \\sum_{k=1}^{3}\\frac{\\partial E}{\\partial z_k^{(4)}} \\cdot \\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}  \n\\end{align}\\] Se nu dukker der noget op, som vi har set f√∏r! Nemlig det fejlled, som vi definerede i (27), og som vi allerede har regnet ud, da vi opdaterede v√¶gtene og biasene i lag \\(4\\). T√¶nk lige over det - det er faktisk ret fedt! Dvs. at vi kan skrive: \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial a_j^{(3)}} &= \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot \\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}  \n\\end{aligned}\n\\tag{30}\\] S√• mangler vi kun lige at finde \\(\\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}\\)! Fra (21) har vi, at \\[\nz_k^{(4)} = \\sum_i w_{ki}^{(4)} a_i^{(3)}+b_k^{(4)}\n\\] s√• \\[\n\\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}} = \\frac{\\partial}{\\partial a_j^{(3)}} \\left(\\sum_i w_{ki}^{(4)} a_i^{(3)}+b_k^{(4)} \\right)\n\\] N√•r vi skal differentierer summen i ovenst√•ende udtryk, f√•r vi kun et led med, n√•r \\(i=j\\), fordi i alle andre tilf√¶lde, vil vi med hensyn til \\(a_j^{(3)}\\) skulle differentiere en konstant. Og da \\[\n\\frac{\\partial}{\\partial a_j^{(3)}}\\left ( w_{kj}^{(4)} a_j^{(3)} \\right) = w_{kj}^{(4)}\n\\] har vi alts√•, at \\[\n\\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}} = w_{kj}^{(4)}\n\\] Inds√¶tter vi dette i (30), har vi nu \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(3)}} = \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot \\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}  = \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot w_{kj}^{(4)}\n\\end{align}\\]\nNu skal vi i f√∏rste omgang tilbage til (29) og inds√¶tte det vi netop er kommet frem til: \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(3)}}=\\underbrace{\\left ( \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot w_{kj}^{(4)}\\right )}_{\\frac{\\partial E}{\\partial a_j^{(3)}}}\n\\cdot \\underbrace{a_j^{(3)} \\cdot (1-a_j^{(3)})}_{\\frac{\\partial a_j^{(3)}}{\\partial z_j^{(3)}}}\n\\end{align}\\] Som vi gjorde i afsnit¬†5.2.1, vil vi ogs√• give dette lidt lange udtryk en s√¶rlig betegnelse, nemlig \\[\\begin{align}\n\\delta_j^{(3)} = \\frac{\\partial E}{\\partial z_j^{(3)}}=\\left ( \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot w_{kj}^{(4)}\\right )\n\\cdot a_j^{(3)} \\cdot (1-a_j^{(3)})\n\\end{align}\\] Det kan vist godt v√¶re lidt sv√¶rt at bevare overblikket her, men nu er vi faktisk i m√•l! Vi inds√¶tter i (28) \\[\\begin{align}\n\\frac{\\partial E}{\\partial w_{ji}^{(3)}} &= \\frac{\\partial E}{\\partial z_j^{(3)}} \\cdot \\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}} \\\\\n&= \\delta_j^{(3)} \\cdot a_i^{(2)}\n\\end{align}\\]\nDet er nu en smal sag at bestemme \\(\\frac{\\partial E}{\\partial b_j^{(3)}}\\), da \\[\\begin{align}\n\\frac{\\partial E}{\\partial b_j^{(3)}} &= \\frac{\\partial E}{\\partial z_j^{(3)}} \\cdot \\frac{\\partial z_j^{(3)}}{\\partial b_j^{(3)}} \\\\ &= \\delta_j^{(3)}\\cdot \\frac{\\partial z_j^{(3)}}{\\partial b_j^{(3)}}\n\\end{align}\\] Fra (20) har vi, at \\[\nz_j^{(3)} = \\sum_{i=1}^3 w_{ji}^{(3)} a_i^{(2)} + b_j^{(3)}\n\\] og derfor er \\[\n\\frac{\\partial z_j^{(3)}}{\\partial b_j^{(3)}} = 1\n\\] Alts√• f√•r vi \\[\\begin{align}\n\\frac{\\partial E}{\\partial b_j^{(3)}} = \\delta_j^{(3)}\n\\end{align}\\] Gl√¶den er stor, da vi nu har alle ingredienser til at opskrive opdateringsreglerne for det tredje lag!\n\n\n\n\n\n\nOpdateringsregler til v√¶gte og bias i lag 3\n\n\n\nV√¶gtene i outputlaget opdateres p√• denne m√•de: \\[\nw_{ji}^{(3)} \\leftarrow w_{ji}^{(3)} - \\eta \\cdot \\delta_j^{(3)} \\cdot a_i^{(2)}\n\\] Biasene i outputlaget opdateres p√• denne m√•de: \\[\nb_j^{(3)} \\leftarrow b_j^{(3)} - \\eta \\cdot \\delta_j^{(3)}\n\\] hvor \\[\n\\delta_j^{(3)} = \\frac{\\partial E}{\\partial z_j^{(3)}}=\\left( \\sum_{k=1}^{3}\\delta_k^{(4)} \\cdot w_{kj}^{(4)}   \\right) \\cdot a_j^{(3)} \\cdot (1-a_j^{(3)})\n\\tag{31}\\]\n\n\nBem√¶rk, at udgangspunktet for ovenst√•ende er, at vi f√∏rst har lavet et feedforward i netv√¶rket, s√• vi har alle \\(a_i^{(2)}\\)- og \\(a_j^{(3)}\\)-v√¶rdier. Derudover har vi allerede opdateret v√¶gtene og biasene i lag \\(4\\). Derfor kender vi ogs√• fejleddene \\(\\delta_k^{(4)}\\) fra lag \\(4\\), som indg√•r i beregningen af \\(\\delta_j^{(3)}\\) i (31). Alts√• er det muligt at foretage de beregninger, som opdateringsreglerne i lag \\(3\\) kr√¶ver.\n\n\nOpdateringsregler for lag \\(2\\)\nVi er nu fremme ved det sidste lag, hvor vi skal have opdateret v√¶gte og bias (husk p√• at \\(a_i^{(1)}\\)-v√¶rdierne jo ikke skal beregnes, men er inputv√¶rdierne til netv√¶rket). Den gode nyhed her er, at der absolut intet nyt er under solen! Vi vil derfor heller ikke g√• i drabelige deltaljer med alle udregninger her, men blot skitsere id√©en.\nVi er nu p√• jagt efter \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(2)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(2)}}\n\\] og kigger vi p√• vores netv√¶rk i figur¬†7 kan vi se f√∏lgende afh√¶ngigheder: \\[\n\\begin{matrix}\n& & & & z_1^{(3)}  & & \\\\\n& & & \\nearrow  & &  \\searrow & \\\\\nw_{ji}^{(2)} & \\rightarrow & z_j^{(2)} \\rightarrow a_j^{(2)} &  &  &  & E \\\\\n& & & \\searrow & &  \\nearrow &  \\\\\n& & & & z_2^{(3)} & & \\\\\n\\end{matrix}\n\\] Vi kan nu igen skrive \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(2)}} = \\frac{\\partial E}{\\partial z_j^{(2)}} \\cdot \\frac{\\partial z_j^{(2)}}{\\partial w_{ji}^{(2)}}\n\\tag{32}\\] Helt analogt til tidligere ses det nemt, at \\[\n\\frac{\\partial z_j^{(2)}}{\\partial w_{ji}^{(2)}} = a_i^{(1)}\n\\] og k√¶dreglen giver igen, at \\[\n\\frac{\\partial E}{\\partial z_j^{(2)}}= \\frac{\\partial E}{\\partial a_j^{(2)}} \\cdot\n\\frac{\\partial a_j^{(2)}}{\\partial z_j^{(2)}}\n\\tag{33}\\] Her f√•s ogs√• uden problemer, at den sidste faktor kan skrives som \\[\n\\frac{\\partial a_j^{(2)}}{\\partial z_j^{(2)}} = a_j^{(2)} \\cdot (1-a_j^{(2)})\n\\] og bruger man k√¶dereglen for funktioner af flere variable, kommer man frem til, at \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(2)}} &= \\sum_{k=1}^2 \\frac{\\partial E}{\\partial z_k^{(3)}}\n\\cdot \\frac{\\partial  z_k^{(3)}}{\\partial a_j^{(2)}} \\\\\n&= \\sum_{k=1}^2  \\delta_k^{(3)} w_{kj}^{(3)},\n\\end{align}\\] hvor vi allerede har udregnet \\(\\delta_k^{(3)}\\), da vi opdaterede v√¶gtene og biasene i lag \\(3\\).\nInds√¶tter vi i (33) og samtidig definerer fejlleddet \\(\\delta_j^{(2)}\\) for det andet lag, f√•r vi \\[\n\\delta_j^{(2)} = \\frac{\\partial E}{\\partial z_j^{(2)}} = \\left ( \\sum_{k=1}^2  \\delta_k^{(3)} w_{kj}^{(3)} \\right ) \\cdot a_j^{(2)} \\cdot (1-a_j^{(2)})\n\\] Alt i alt ender vi med \\[\\begin{align}\n\\frac{\\partial E}{\\partial w_{ji}^{(2)}} &= \\delta_j^{(2)} \\cdot a_i^{(1)} \\\\\n&= \\delta_j^{(2)} \\cdot x_i\n\\end{align}\\] fordi alle \\(a_i^{(1)}\\)-v√¶rdierne svarer til selve inputv√¶rdierne \\(x_i\\) til netv√¶rket.\nDet er nu ikke sv√¶rt at se, at \\[\n\\frac{\\partial E}{\\partial b_j^{(2)}} = \\delta_j^{(2)}\n\\] og vi f√•r derfor f√∏lgende opdateringsregler for lag 2:\n\n\n\n\n\n\nOpdateringsregler til v√¶gte og bias i lag 2\n\n\n\nV√¶gtene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nw_{ji}^{(2)} \\leftarrow w_{ji}^{(2)} - \\eta \\cdot \\delta_j^{(2)} \\cdot a_i^{(1)}\n\\end{align}\\] Biasene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nb_j^{(2)} \\leftarrow b_j^{(2)} - \\eta \\cdot \\delta_j^{(2)}\n\\end{align}\\] hvor \\[\n\\delta_j^{(2)} = \\frac{\\partial E}{\\partial z_j^{(2)}}= \\left ( \\sum_{k=1}^2  \\delta_k^{(3)} w_{kj}^{(3)} \\right ) \\cdot a_j^{(2)} \\cdot (1-a_j^{(2)})\n\\tag{34}\\]"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#var-det-s√•-egentlig-smart-med-alle-de-indekser",
    "href": "materialer/neurale_net/neurale_net.html#var-det-s√•-egentlig-smart-med-alle-de-indekser",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Var det s√• egentlig smart med alle de indekser?",
    "text": "Var det s√• egentlig smart med alle de indekser?\nHvis man er n√•et hertil, kan man godt f√∏lge sig en lille smule forpustet. Der har godt nok v√¶ret mange indekser at holde styr p√•! B√•de nogle der var s√¶nkede, og nogle der var h√¶vede og sat i parenteser! Alligevel kan man m√•ske godt se fidusen nu.\nHvis vi ser p√• de opdateringsregler, som vi lige har udledt, s√• kan man se, at selve opdateringsreglerne af v√¶gte og bias f√∏lger pr√¶cis samme form. Faktisk kan man, hvis man sammenligner opdateringsreglerne for de tre lag se, at opdateringsreglerne er p√• denne form:\n\n\n\n\n\n\nGenerelle opdateringsregler til v√¶gte og bias\n\n\n\nV√¶gtene opdateres generelt p√• denne m√•de: \\[\\begin{align}\nw_{ji}^{(\\text{lag})} \\leftarrow w_{ji}^{(\\text{lag})} - \\eta \\cdot \\delta_j^{(\\text{lag})} \\cdot a_i^{(\\text{lag-1})}\n\\end{align}\\] Biasene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nb_j^{(\\text{lag})} \\leftarrow b_j^{(\\text{lag})} - \\eta \\cdot \\delta_j^{(\\text{lag})}\n\\end{align}\\]\n\n\nBem√¶rk her, at da vi allerede har lavet en feedforward i netv√¶rket, s√• kender vi outputv√¶rdierne \\(a_i^{(\\text{lag})}\\) i alle lag. Det vil sige, at vi kan opdatere v√¶gtene og biasene, n√•r blot vi kan beregne fejlleddene.\nDen eneste reelle forskel p√• opdateringsreglerne er, at fejlleddene udregnes lidt forskelligt, alt efter om der er tale om outputlaget eller et skjult lag:\n\n\n\n\n\n\nBeregning af fejlleddene\n\n\n\nFejlleddene i outputlaget beregnes p√• denne m√•de: \\[\\begin{align}\n\\delta_j^{(\\text{outputlag})} = -(t_j-y_j) \\cdot y_j \\cdot (1-y_j),\n\\end{align}\\] idet outputv√¶rdierne fra netv√¶rket netop er \\(y_1, y_2, \\dots\\).\nFejlleddene i et skjult lag beregnes p√• denne m√•de: \\[\\begin{align}\n\\delta_j^{(\\text{lag})} = \\left ( \\sum_{k}  \\delta_k^{(\\text{lag+1})} w_{kj}^{(\\text{lag+1})} \\right ) \\cdot a_j^{(\\text{lag})} \\cdot (1-a_j^{(\\text{lag})})\n\\end{align}\\]\n\n\nBem√¶rk her, at fejlleddene fra outputlaget uden videre kan beregnes, da vi kender target-v√¶rdierne \\(t_j\\) og outputv√¶rdierne \\(y_j\\) fra netv√¶rket (fordi vi allerede har lavet en feedforward). Vi kan ogs√• beregne fejlleddene i alle skjulte lag, idet vi hele tiden arbejder bagud i netv√¶rket (backpropagation). Det vil sige, at vi hele tiden har adgang til fejlleddene i laget l√¶ngere fremme (lag+1), hvor (lag+1) f√∏rste gang vil svare til outputlaget. Desuden kender vi pga. feedforward alle outputv√¶rdier \\(a_j^{(\\text{(lag)})}\\) og alle v√¶gte \\(w_{kj}^{(\\text{(lag+1)})}\\). Derfor kan vi ogs√• beregne fejlleddene i alle de skjulte lag.\nDenne indsigt og den generelle overordnede struktur p√• opdateringsreglerne, var meget sv√¶r at indse med fremgangsm√•de i afsnit¬†4. Her druknede alt bare i et sandt bogstavshelvede!\nDer er et par andre interessante ting at sige om beregningen af fejlleddene. Lad os f√∏rst se p√• outputlaget: \\[\n\\delta_j^{(\\text{outputlag})} = -(t_j-y_j) \\cdot y_j \\cdot (1-y_j)\n\\] Hvis der er stor forskel p√• target-v√¶rdien \\(t_j\\) og outputv√¶rdien \\(y_j\\), s√• bliver forskellen \\(t_j-y_j\\) numerisk stor. Alts√• vil en stor forskel p√• det, vi √∏nsker, og det vi f√•r ud af netv√¶rket betyde, at fejlleddet bliver st√∏rre og i sidste ende, at de v√¶gte, som direkte p√•virker outputtet, ogs√• vil blive opdateret meget. Endelig ser vi igen, at hvis outputneuronen er m√¶ttet (dvs. at \\(y_j\\) enten er t√¶t p√• \\(0\\) eller \\(1\\)), s√• vil fejlleddet ikke blive opdateret i samme grad, som hvis outputneuronen ikke havde v√¶ret m√¶ttet (fordi hvis \\(y_j\\) enten er t√¶t p√• \\(0\\) eller \\(1\\), s√• vil \\(y_j \\cdot (1-y_j)\\) v√¶re t√¶t p√• \\(0\\)).\nVi ser alts√•, at fejlleddet fra det sidste lag direkte afh√¶nger af hvor stor forskellen er p√• target-v√¶rdi og outputv√¶rdi.\nSer vi s√• p√• fejlleddene fra de skjulte lag: \\[\n\\delta_j^{(\\text{lag})} = \\left ( \\sum_{k}  \\delta_k^{(\\text{lag+1})} w_{kj}^{(\\text{lag+1})} \\right ) \\cdot a_j^{(\\text{lag})} \\cdot (1-a_j^{(\\text{lag})})\n\\] S√• kan vi igen se, at hvis den tilh√∏rende outputneuron, som fyrer v√¶rdien \\(a_j^{(\\text{lag})}\\), er m√¶ttet, s√• vil fejlleddet v√¶re t√¶ttere p√• \\(0\\), end hvis neuronen ikke havde v√¶ret m√¶ttet. Samtidig kan vi ogs√• se, at der i fejlleddet indg√•r en v√¶gtet sum af alle fejlleddene fra laget l√¶ngere fremme: \\[\n\\sum_{k}  \\delta_k^{(\\text{lag+1})} w_{kj}^{(\\text{lag+1})}\n\\] P√• den m√•de vil store fejl i laget l√¶ngere fremme ogs√• f√• indflydelse p√• fejlleddet i det nuv√¶rende lag."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#backpropagation---generelt",
    "href": "materialer/neurale_net/neurale_net.html#backpropagation---generelt",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Backpropagation - generelt",
    "text": "Backpropagation - generelt\nLad os s√• se p√• backpropagation. Vi ved fra det foreg√•ende, at der reelt set kun er to ting, vi skal g√∏re:\n\nFinde opdateringsreglerne for v√¶gte og bias i outputlaget\nFinde opdateringsreglerne for v√¶gte og bias i et vilk√•rligt skjult lag\n\n\nOpdateringsregler i outputlaget\nVores tabsfunktion er stadig \\[\nE= \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_i - a_i^{(K)} \\right )^2,\n\\] hvor \\(t_i\\) igen er targetv√¶rdierne. Vi skal bestemme \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(K)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(K)}}\n\\] Vi g√∏r pr√¶cis som i afsnit¬†5.2.1. Vi indser f√∏rst, at vi har denne direkte afh√¶ngighed fra \\(w_{ji}^{(K)}\\) til \\(E\\): \\[\nw_{ji}^{(K)} \\rightarrow z_j^{(K)} \\rightarrow a_j^{(K)} \\rightarrow E\n\\] Derfor f√•r vi \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(K)}} = \\frac{\\partial E}{\\partial z_j^{(K)}} \\cdot \\frac{\\partial z_j^{(K)}}{\\partial w_{ji}^{(K)}}\n\\tag{37}\\] P√• grund af feedforwardligningen i (35) f√•r vi for det f√∏rste, at \\[\n\\frac{\\partial z_j^{(K)}}{\\partial w_{ji}^{(K)}} = a_i^{(K-1)}\n\\tag{38}\\] Nu bruger vi k√¶dereglen til at bestemme \\[\n\\frac{\\partial E}{\\partial z_j^{(K)}} = \\frac{\\partial E}{\\partial a_j^{(K)}} \\cdot  \\frac{\\partial a_j^{(K)}}{\\partial z_j^{(K)}}\n\\tag{39}\\] P√• grund af feedforwardligningen i (36) og s√¶tning¬†1 f√•r vi sidste faktor til \\[\n\\frac{\\partial a_j^{(K)}}{\\partial z_j^{(K)}}  =  a_j^{(K)}\\cdot (1-a_j^{(K)})\n\\] Endelig f√•r vi, ved at differentiere tabsfunktionen med hensyn til \\(a_j^{(K)}\\) \\[\n\\frac{\\partial E}{\\partial a_j^{(K)}} = -(t_j-a_j^{(K)})\n\\] Vi definerer nu igen fejlleddet for outputlaget \\(\\delta_j^{(K)}\\), som tidligere \\[\n\\delta_j^{(K)} = \\frac{\\partial E}{\\partial z_j^{(K)}}\n\\] og inds√¶tter vi det, vi netop har udledt, i (39) f√•r vi \\[\n\\delta_j^{(K)} =  -(t_j-a_j^{(K)}) \\cdot a_j^{(K)} \\cdot (1-a_j^{(K)})\n\\] Inds√¶tter vi nu det hele i (37), har vi alts√•: \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(K)}} = \\delta_j^{(K)}  \\cdot a_i^{(K-1)}\n\\] Det er ikke sv√¶rt at overbevise sig selv om, at \\[\n\\frac{\\partial E}{\\partial b_j^{(K)}} = \\delta_j^{(K)}\n\\] og derfor har vi:\n\n\n\n\n\n\nGenerelle opdateringsregler til v√¶gte og bias i outputlaget (lag \\(K\\))\n\n\n\nV√¶gtene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nw_{ji}^{(K)} \\leftarrow w_{ji}^{(K)} - \\eta \\cdot \\delta_j^{(K)} \\cdot a_i^{(K-1)}\n\\end{align}\\] Biasene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nb_j^{(K)} \\leftarrow b_j^{(K)} - \\eta \\cdot \\delta_j^{(K)}\n\\end{align}\\] hvor \\[\\begin{align}\n\\delta_j^{(K)} = \\frac{\\partial E}{\\partial z_j^{(K)}}= -(t_j-a_j^{(K)}) \\cdot a_j^{(K)} \\cdot (1-a_j^{(K)})\n\\end{align}\\]\n\n\n\n\nOpdateringsregler i et vilk√•rligt skjult lag\nVi ser nu p√• et vilk√•rligt skjult lag \\(k\\), som hverken er inputlaget eller outputlaget. Det vil sige, at \\(k \\in \\{2, 3, \\dots, K-1 \\}\\). Vi antager, at vi har k√∏rt backpropagation p√• alle lag, der ligger l√¶ngere fremme i netv√¶rket, og specielt har vi alts√• beregnet fejlleddene i lag \\(k+1:\\) \\[\n\\delta_j^{(k+1)} = \\frac{\\partial E}{\\partial z_j^{(k+1)}}\n\\] Vi indser f√∏rst, at vi har denne afh√¶ngighed fra \\(w_{ji}^{(k)}\\) til tabsfunktionen \\(E\\): \\[\n\\begin{matrix}\n& & & & z_1^{(k+1)}  & & \\\\\n& & & \\nearrow  & \\vdots &  \\searrow & \\\\\nw_{ji}^{(k)} & \\rightarrow & z_j^{(k)} \\rightarrow a_j^{(k)} & \\rightarrow  &  z_j^{(k+1)} & \\rightarrow  & E \\\\\n& & & \\searrow & \\vdots &  \\nearrow &  \\\\\n& & & & z_{n_{k+1}}^{(k+1)} & & \\\\\n\\end{matrix}\n\\tag{40}\\] Vi starter som tidligere med at bruge k√¶dereglen √©n gang: \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} = \\frac{\\partial E}{\\partial z_j^{(k)}} \\cdot \\frac{\\partial z_j^{(k)}}{\\partial w_{ji}^{(k)}}\n\\tag{41}\\] Fra feedforwardligningen i (35) f√•r vi for det f√∏rste, at \\[\n\\frac{\\partial z_j^{(k)}}{\\partial w_{ji}^{(k)}} = a_i^{(k-1)}\n\\] Endnu en anvendelse af k√¶dereglen, og hvor vi ogs√• i samme hug definerer fejlleddet \\(\\delta_j^{(k)}\\) for det \\(k\\)‚Äôte skjulte lag, giver: \\[\n\\delta_j^{(k)} = \\frac{\\partial E}{\\partial z_j^{(k)}}= \\frac{\\partial E}{\\partial a_j^{(k)}} \\cdot\n\\frac{\\partial a_j^{(k)}}{\\partial z_j^{(k)}}\n\\tag{42}\\] Den sidste partielle afledede kan vi udlede fra feedforwardligningen (36) og s√¶tning¬†1: \\[\n\\frac{\\partial a_j^{(k)}}{\\partial z_j^{(k)}} = a_j^{(k)} \\cdot (1-a_j^{(k)})\n\\] For at beregne \\(\\frac{\\partial E}{\\partial a_j^{(k)}}\\) m√• vi have fat i k√¶dereglen for funktioner af flere variable (se illustrationen i (40): \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(k)}} = \\sum_{i=1}^{n_{k+1}} \\frac{\\partial E}{\\partial z_i^{(k+1)}}\n\\cdot \\frac{\\partial  z_i^{(k+1)}}{\\partial a_j^{(k)}}\n\\end{align}\\] Vi udnytter nu, at vi allerede kender fejlleddene fra lag \\(k+1\\) og kan derfor omskrive til\n\\[\n\\frac{\\partial E}{\\partial a_j^{(k)}} = \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)}\n\\cdot \\frac{\\partial  z_i^{(k+1)}}{\\partial a_j^{(k)}}\n\\tag{43}\\]\nFra feedforwardligningen i (35) f√•r vi, at \\(z_i^{(k+1)}\\) kan skrives som \\[\nz_i^{(k+1)} = \\sum_{j} w_{ij}^{(k+1)} a_j^{(k)} +  b_i^{(k+1)}\n\\] og derfor er \\[\\begin{align}\n\\frac{\\partial  z_i^{(k+1)}}{\\partial a_j^{(k)}} = w_{ij}^{(k+1)}\n\\end{align}\\] Inds√¶tter vi i (43) f√•s \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(k)}} = \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)}\n\\cdot w_{ij}^{(k+1)}\n\\end{align}\\] og ved inds√¶ttelse i (42) f√•r vi nu fejlleddet i det \\(k\\)‚Äôte lag \\[\n\\delta_j^{(k)} = \\frac{\\partial E}{\\partial z_j^{(k)}}=  \\left ( \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)} \\cdot w_{ij}^{(k+1)} \\right) \\cdot a_j^{(k)} \\cdot (1-a_j^{(k)})\n\\] Vi bruger nu udtrykket for \\(\\frac{\\partial E}{\\partial w_{ji}^{(k)}}\\) i (41) og ender med \\[\\begin{align}\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} = \\delta_j^{(k)} \\cdot a_i^{(k-1)}\n\\end{align}\\] og tilsvarende f√•r vi ogs√•, at \\[\n\\frac{\\partial E}{\\partial b_j^{(k)}} = \\delta_j^{(k)}\n\\] Opdateringsreglerne for et vilk√•rligt skjult lag bliver s√•:\n\n\n\n\n\n\nOpdateringsregler til v√¶gte og bias i et vilk√•rligt skjult lag \\(k\\)\n\n\n\nV√¶gtene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nw_{ji}^{(k)} \\leftarrow w_{ji}^{(k)} - \\eta \\cdot \\delta_j^{(k)} \\cdot a_i^{(k-1)}\n\\end{align}\\] Biasene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nb_j^{(k)} \\leftarrow b_j^{(k)} - \\eta \\cdot \\delta_j^{(k)}\n\\end{align}\\] hvor \\[\\begin{align}\n\\delta_j^{(k)} = \\frac{\\partial E}{\\partial z_j^{(k)}}=  \\left ( \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)} \\cdot w_{ij}^{(k+1)} \\right) \\cdot a_j^{(k)} \\cdot (1-a_j^{(k)})\n\\end{align}\\]"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#stokastisk-gradientnedstigning",
    "href": "materialer/neurale_net/neurale_net.html#stokastisk-gradientnedstigning",
    "title": "Kunstige neurale netv√¶rk",
    "section": "Stokastisk gradientnedstigning",
    "text": "Stokastisk gradientnedstigning\nVi har faktisk snydt lidt‚Ä¶ Okay ‚Äì indr√∏mmet ‚Äì det er lidt tr√¶ls at komme at sige nu! Men i alt hvad vi har lavet indtil nu, har vi kun kigget p√• √©t tr√¶ningseksempel. Vi har ladet inputv√¶rdierne for det ene tr√¶ningseksempel \"k√∏rer igennem\" netv√¶rket (feedforward), beregnet tabsfunktionen og brugt resultatet herfra til at opdatere alle v√¶gtene (backpropagation). Men vi har jo ikke kun √©t tr√¶ningseksempel. Vi har faktisk rigtig mange! M√•ske ligefrem tusindvis af tr√¶ningsdata. Men hvad g√∏r man s√•?\nLad os lige genopfriske den tabsfunktion, som vi endte med i det helt generelle tilf√¶lde: \\[\nE= \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_i - a_i^{(K)} \\right )^2.\n\\tag{44}\\] Her er \\(t_i\\) target-v√¶rdien for den \\(i\\)‚Äôte outputneuron for lige pr√¶cis det tr√¶ningseksempel vi st√•r med. Husk p√• at et givet tr√¶ningseksempel best√•r af inputv√¶rdierne \\[\nx_1, x_2, \\dots, x_{n_1}\n\\] og de √∏nskede target-v√¶rdier \\[\nt_1, t_2, \\dots, t_{n_K}.\n\\] N√•r vi k√∏rer disse inputv√¶rdier igennem netv√¶rket, f√•r de selvf√∏lgelig i sidste ende direkte betydning for outputv√¶rdierne i det sidste lag (\\(K\\)): \\[\na_1^{(K)}, a_2^{(K)}, \\cdots, a_{n_K}^{(K)}.\n\\] Det vil sige, at i vores tabsfunktion i (44), s√• afh√¶nger b√•de \\(t_i\\)‚Äôerne og \\(a_i^{(K)}\\)‚Äôerne af tr√¶ningseksemplet. Hvis vi s√•dan lidt generelt ben√¶vner vores tr√¶ningseksempel med \\(x\\), s√• vil det kunne udtrykkes s√•dan her: \\[\nE_x= \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_{x,i} - a_{x,i}^{(K)} \\right )^2,\n\\] hvor s√• \\(t_{x,i}\\) er target-v√¶rdien for den \\(i\\)‚Äôte outputneuron fra tr√¶ningsdata \\(x\\) og \\(a_{x,i}^{(K)}\\) er outputv√¶rdien for den \\(i\\)‚Äôte outputneuron, som er beregnet p√• baggrund af inputv√¶rdierne fra tr√¶ningsdata \\(x\\).\nDen samlede tabsfunktion, som er den, vi i virkeligheden √∏nsker at minimere, bliver s√• gennemsnittet af tabsfunktionerne h√∏rende til de enkelte tr√¶ningsdata: \\[\nE = \\frac{1}{n} \\sum_x E_x= \\frac{1}{n} \\sum_x \\left ( \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_{x,i} - a_{x,i}^{(K)} \\right )^2 \\right ).\n\\tag{45}\\] Husk p√• at vi er ude efter \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(k)}}\n\\] for \\(k \\in \\{2, 3, \\dots, K\\}\\) og hvor \\(E\\) nu er summen i (45). Heldigvis kan vi differentiere ledvis, og der g√¶lder derfor \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} = \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial w_{ji}^{(k)}}\n\\] og tilsvarende \\[\n\\frac{\\partial E}{\\partial b_j^{(k)}} = \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial b_j^{(k)}}\n\\] Det kommer s√• til at betyde, at opdateringsreglerne nu generelt bliver p√• formen \\[\nw_{ji}^{(k)} \\leftarrow w_{ji}^{(k)}-\\eta \\cdot \\frac{\\partial E}{\\partial w_{ji}^{(k)}}  =\nw_{ji}^{(k)}-\\eta \\cdot \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial w_{ji}^{(k)}}\n\\tag{46}\\] og tilsvarende for biasene \\[\nb_j^{(k)} \\leftarrow b_j^{(k)} -\\eta \\cdot \\frac{\\partial E}{\\partial b_j^{(k)}}  =\nb_j^{(k)}-\\eta \\cdot \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial b_j^{(k)}}\n\\tag{47}\\] Alle leddene \\(\\frac{\\partial E_x}{\\partial w_{ji}^{(k)}}\\) og \\(\\frac{\\partial E_x}{\\partial b_j^{(k)}}\\), som indg√•r i opdateringsreglerne, svarer netop til hvad vi har udledt i de foreg√•ende afsnit, fordi vi jo netop her kun s√• p√• √©t tr√¶ningseksempel ad gangen. Hvis vi overf√∏rer dette til opdateringsreglerne i outputlaget, s√• vil vi f.eks. f√•\n\n\n\n\n\n\nGenerelle opdateringsregler til v√¶gte og bias i outputlaget (lag \\(K\\)) med brug af alle tr√¶ningsdata\n\n\n\nV√¶gtene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nw_{ji}^{(K)} \\leftarrow w_{ji}^{(K)} - \\eta \\cdot \\frac{1}{n} \\sum_x \\left ( \\delta_{x,j}^{(K)} \\cdot a_{x,i}^{(K-1)} \\right )\n\\end{align}\\] Biasene i outputlaget opdateres p√• denne m√•de: \\[\\begin{align}\nb_j^{(K)} \\leftarrow b_j^{(K)} - \\eta \\cdot \\frac{1}{n} \\sum_x \\left ( \\delta_{x,j}^{(K)} \\right )\n\\end{align}\\] hvor \\[\\begin{align}\n\\delta_{x,j}^{(K)} = \\frac{\\partial E_x}{\\partial z_j^{(K)}}= -(t_{x,j}-a_{x,j}^{(K)}) \\cdot a_{x,j}^{(K)} \\cdot (1-a_{x,j}^{(K)})\n\\end{align}\\]\n\n\nOg helt tilsvarende vil det se ud for opdateringsreglerne i de skjulte lag.\nLad os lige dv√¶le lidt ved, hvad det her, det egentlig betyder. Lad os sige at vi har \\(1000\\) tr√¶ningsdata. S√• skal vi lade de \\(1000\\) tr√¶ningsdata k√∏rer igennem netv√¶rket, s√• vi kan beregne de \\(1000\\) led, som indg√•r i de ovenst√•ende summer. Herefter kan vi opdatere alle v√¶gte og bias √©n gang. Det vil blot v√¶re √©t lille skridt p√• vej ned i dalen mod det lokale minimum, som vi er p√• jagt efter. Dette lille skridt skal gentages rigtig mange gange indtil v√¶rdierne af alle v√¶gte og bias ser ud til at begynde at konvergere ‚Äì svarende til at vi har ramt det lokale minimum.\nS√• selvom gradientnedstigning kan bruges til at finde et lokalt minimum for tabsfunktionen \\(E\\), s√• er det faktisk ogs√• en beregningsm√¶ssig stor og tung opgave! Derfor er der forsket meget videre i at g√∏re det endnu bedre og endnu hurtigere. I algoritmer som disse er der ofte et trade-off: Man kan g√∏re noget hurtigere ved at bruge mere hukommelse ‚Äì eller bruge mindre hukommelse ved at g√∏re det en smule langsommere. En af de teknikker, der er kommet ud af den forskning, er, at man kan bruge mindre hukommelse ved i hvert opdateringsskridt kun at bruge en tilf√¶ldigt udvalgt del af tr√¶ningsdata ‚Äì det kunne f.eks. v√¶re \\(10\\%\\) af alle tr√¶ningsdata. S√• vil man i hvert skridt stadig bruge opdateringsreglerne i (46) og (47), men hvor der nu kun summeres over de \\(10 \\%\\) af tr√¶ningsdatene. Hver gang man laver et nyt opdateringsskridt, vil man tage en ny tilf√¶ldigt udvalgt del af tr√¶ningsdata. Denne teknik kalder man stokastisk gradientnedstigning (stochastic gradient descent). Og der er endnu flere af s√•danne sm√• √¶ndringer, der enten g√∏r algoritmen hurtigere eller at den bruger mindre hukommelse. Det vil komme an p√• den enkelte anvendelse, hvad der er vigtigst her."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html",
    "href": "materialer/naivbayes/NaivBayes.html",
    "title": "Naiv Bayes klassifier",
    "section": "",
    "text": "For at introducere teorien om Bayes naive klassifikation, vil vi starte med at se p√• et eksempel for at f√• en id√© om, hvad Bayes klassifikation g√•r ud p√•.\nVi vil se p√• en person, og vi √∏nsker at give et bud p√•, om vedkommende stemmer p√• r√∏d eller bl√• blok. Vi har p√• forh√•nd oplysninger om en del andre personer og √∏nsker at bruge den viden til at give det bedste bud p√•, om personen stemmer p√• r√∏d eller bl√• blok.\nHer har vi f√∏lgende data, der viser, hvem der stemmer p√• r√∏d og bl√• blok.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlle\nM√¶nd\nKvinder\nUnge\n√Üldre\nSj√¶lland\nJylland\nAnden bop√¶l\n\n\n\n\nR√∏d\n\\(51.85 \\%\\)\n\\(48.00 \\%\\)\n\\(55.00 \\%\\)\n\\(65.00 \\%\\)\n\\(47.47 \\%\\)\n\\(54.00 \\%\\)\n\\(49.90 \\%\\)\n\\(52.78 \\%\\)\n\n\nBl√•\n\\(48.15 \\%\\)\n\\(52.00 \\%\\)\n\\(45.00 \\%\\)\n\\(35.00 \\%\\)\n\\(52.53 \\%\\)\n\\(46.00 \\%\\)\n\\(50.10 \\%\\)\n\\(47.22 \\%\\)\n\n\nAntal\n\\(10000\\)\n\\(4500\\)\n\\(5500\\)\n\\(2500\\)\n\\(7500\\)\n\\(3000\\)\n\\(4500\\)\n\\(2500\\)\n\n\n\n\n\n\n\n\n\nOpgave\n\n\n\n\n\nBrug tabellen ovenfor og giv det bedste bud p√• hvilken blok en person stemmer p√•:\n\nHvis det er en tilf√¶ldig person.\nHvis det er en mand.\n\n\n\n\nFra skemaet med oplysninger kan det v√¶re sv√¶re at give et bud p√•, hvad en √¶ldre kvinde fra Sj√¶lland vil stemme p√•, da oplysningen om k√∏n tyder p√• personen vil stemme p√• r√∏d, mens information om, at det er en √¶ldre person, tyder p√•, at personen vil stemme p√• bl√•. Endelig vil oplysningen om, at kvinden bor p√• Sj√¶lland igen f√• os til at t√¶nke, at hun stemmer p√• r√∏d blok.\nHer kunne vi selvf√∏lgelig l√∏se problemet ved at f√• information for hver kombination af k√∏n, aldersgruppe og bop√¶l. Men det viser sig ikke at v√¶re en helt gangbar fremgangsm√•de. Forklaringen f√∏lger her: Hvis vi ser p√• kombinationer af k√∏n, aldersgruppe og bop√¶l vil det i dette eksempel give \\(2\\cdot 2\\cdot 3=12\\) kombinationer, og hvis vi i stedet havde set p√•, om man svarer ja eller nej til \\(50\\) sp√∏rgsm√•l, vil man kunne f√• \\(2^{50}\\) forskellige kombinationer af svar. Hvis man ser p√• en person, der har svaret p√• de \\(50\\) sp√∏rgsm√•l, kan man her forvente, at man i ens data kun har ganske f√• eller m√•ske slet ingen personer, der har svaret p√• fuldst√¶ndig samme m√•de, og der vil ikke v√¶re meget at basere ens bud p√•.\nDerfor √∏nsker vi en metode, hvor vores bud, p√• hvad en ny person vil stemme p√•, udelukkende baseres p√• information svarende til det fra skemaet ovenfor, hvor vi ikke ser p√• alle de forskellige kombinationer. Det er det Naive Bayes klassifikation kan."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-klassifikation",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-klassifikation",
    "title": "Naiv Bayes klassifier",
    "section": "",
    "text": "For at introducere teorien om Bayes naive klassifikation, vil vi starte med at se p√• et eksempel for at f√• en id√© om, hvad Bayes klassifikation g√•r ud p√•.\nVi vil se p√• en person, og vi √∏nsker at give et bud p√•, om vedkommende stemmer p√• r√∏d eller bl√• blok. Vi har p√• forh√•nd oplysninger om en del andre personer og √∏nsker at bruge den viden til at give det bedste bud p√•, om personen stemmer p√• r√∏d eller bl√• blok.\nHer har vi f√∏lgende data, der viser, hvem der stemmer p√• r√∏d og bl√• blok.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlle\nM√¶nd\nKvinder\nUnge\n√Üldre\nSj√¶lland\nJylland\nAnden bop√¶l\n\n\n\n\nR√∏d\n\\(51.85 \\%\\)\n\\(48.00 \\%\\)\n\\(55.00 \\%\\)\n\\(65.00 \\%\\)\n\\(47.47 \\%\\)\n\\(54.00 \\%\\)\n\\(49.90 \\%\\)\n\\(52.78 \\%\\)\n\n\nBl√•\n\\(48.15 \\%\\)\n\\(52.00 \\%\\)\n\\(45.00 \\%\\)\n\\(35.00 \\%\\)\n\\(52.53 \\%\\)\n\\(46.00 \\%\\)\n\\(50.10 \\%\\)\n\\(47.22 \\%\\)\n\n\nAntal\n\\(10000\\)\n\\(4500\\)\n\\(5500\\)\n\\(2500\\)\n\\(7500\\)\n\\(3000\\)\n\\(4500\\)\n\\(2500\\)\n\n\n\n\n\n\n\n\n\nOpgave\n\n\n\n\n\nBrug tabellen ovenfor og giv det bedste bud p√• hvilken blok en person stemmer p√•:\n\nHvis det er en tilf√¶ldig person.\nHvis det er en mand.\n\n\n\n\nFra skemaet med oplysninger kan det v√¶re sv√¶re at give et bud p√•, hvad en √¶ldre kvinde fra Sj√¶lland vil stemme p√•, da oplysningen om k√∏n tyder p√• personen vil stemme p√• r√∏d, mens information om, at det er en √¶ldre person, tyder p√•, at personen vil stemme p√• bl√•. Endelig vil oplysningen om, at kvinden bor p√• Sj√¶lland igen f√• os til at t√¶nke, at hun stemmer p√• r√∏d blok.\nHer kunne vi selvf√∏lgelig l√∏se problemet ved at f√• information for hver kombination af k√∏n, aldersgruppe og bop√¶l. Men det viser sig ikke at v√¶re en helt gangbar fremgangsm√•de. Forklaringen f√∏lger her: Hvis vi ser p√• kombinationer af k√∏n, aldersgruppe og bop√¶l vil det i dette eksempel give \\(2\\cdot 2\\cdot 3=12\\) kombinationer, og hvis vi i stedet havde set p√•, om man svarer ja eller nej til \\(50\\) sp√∏rgsm√•l, vil man kunne f√• \\(2^{50}\\) forskellige kombinationer af svar. Hvis man ser p√• en person, der har svaret p√• de \\(50\\) sp√∏rgsm√•l, kan man her forvente, at man i ens data kun har ganske f√• eller m√•ske slet ingen personer, der har svaret p√• fuldst√¶ndig samme m√•de, og der vil ikke v√¶re meget at basere ens bud p√•.\nDerfor √∏nsker vi en metode, hvor vores bud, p√• hvad en ny person vil stemme p√•, udelukkende baseres p√• information svarende til det fra skemaet ovenfor, hvor vi ikke ser p√• alle de forskellige kombinationer. Det er det Naive Bayes klassifikation kan."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-klassifier",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-klassifier",
    "title": "Naiv Bayes klassifier",
    "section": "Bayes klassifier",
    "text": "Bayes klassifier\nI det f√∏lgende indf√∏rer vi det n√∏dvendige matematik og notation til Naive Bayes klassifikation. F√∏rst og fremmest indf√∏rer vi en stokastisk variabel \\(Y\\), som kan antage de v√¶rdier, der svarer til vores forskellige forudsigelser/bud. I vores eksempel vil \\[Y\\in\\{bl√•, r√∏d\\}.\\]\nLidt mere generelt siger man, at \\(Y\\) skal v√¶re en diskret stokastisk variabel med et bestemt antal mulige udfald, og der beh√∏ver alts√• ikke n√∏dvendigvis kun at v√¶re to udfald.\nDerudover indf√∏rer vi en stokastisk variabel \\(\\mathbf{X}\\), hvor de mulige udfald er alle kombinationer af informationer. Her kan vi t√¶nke \\(\\mathbf{X}\\) som en stokastisk vektor \\(\\mathbf{X} =(X_1,X_2,‚Ä¶,X_q)\\), hvor man ved eksemplet kunne sige \\(X_1\\):k√∏n, \\(X_2\\):aldersgruppe og \\(X_3\\):bop√¶l, og et udfald kunne v√¶re \\(\\mathbf{x}=(kvinde,√¶ldre,Sj√¶lland)\\).\nFor hvert udfald af \\(Y\\) √∏nsker vi, at bestemme sandsynligheden for at v√¶rdien \\(y\\) antages, n√•r vi allerede har observeret, at \\(\\mathbf{X}=\\mathbf{x}\\).\nSandsynligheden vil vi skrive som \\[P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\]\nDenne notation og betydningen deraf ser vi snart p√•.\nVi kalder \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\) en posterior sandsynlighed, fordi den udtrykker sandsynligheden for \\(Y\\) efter (post), vi har informationen \\(\\mathbf{x}\\).\nDet mest sandsynlige udfald for \\(Y\\), n√•r vi har informationen \\(\\mathbf{x}\\), betegnes \\(C(\\mathbf{x})\\) og kaldes Bayes klassifikation."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#betinget-sandsynlighed-og-uafh√¶ngighed",
    "href": "materialer/naivbayes/NaivBayes.html#betinget-sandsynlighed-og-uafh√¶ngighed",
    "title": "Naiv Bayes klassifier",
    "section": "Betinget sandsynlighed og uafh√¶ngighed",
    "text": "Betinget sandsynlighed og uafh√¶ngighed\nF√∏rst vender vi dog lige tilbage til notationen \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\), som vi kaldte en posterior sandsynlighed. I sandsynlighedsregningen kalder vi det ogs√• for en betinget sandsynlighed, hvilket er grunden til notationen \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\).\nGivet to h√¶ndelser \\(A\\) og \\(B\\) s√• benyttes notationen \\(P(A\\mid B)\\) som sandsynligheden for, at \\(A\\) sker, n√•r det er givet, at \\(B\\) er sket. Det l√¶ses derfor ogs√• som sandsynligheden for \\(A\\) givet \\(B\\).\nS√• \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\) er derved sandsynligheden for \\(Y = y\\), n√•r det er givet, at \\(\\mathbf{X} = \\mathbf{x}\\).\nEt banalt eksempel kunne v√¶re at \\(Y\\) angiver antal ben p√• et givent dyr, mens \\(\\mathbf{X}\\) angiver dyrearten. Her er det oplagt, at sandsynligheden for fire eller to ben afh√¶nger af hvilken dyreart, der er tale om.\nFormelt defineres betinget sandsynlighed for to h√¶ndelser \\(A\\) og \\(B\\) som: \\[P(A\\mid B) = \\frac{P(A \\cap B)}{P(B)} \\tag{1}\\]\nUdtrykket \\(P(A \\cap B)\\) i t√¶lleren er sandsynligheden for f√¶llesh√¶ndelsen mellem \\(A\\) og \\(B\\) ‚Äì det vil sige h√¶ndelsen, at b√•de \\(A\\) og \\(B\\) indtr√¶ffer ‚Äì og i n√¶vneren s√∏rger vi for, at man kun ser p√• de udfald, hvor \\(B\\) er givet1.\n1¬†Man siger ogs√•, at n√¶vneren normaliserer sandsynligheden i forhold til sandsynligheden for h√¶ndelsen \\(B\\).\nEksempel med betinget sandsynlighed\nLad os fokusere p√• en almindelig terning med seks sider. Lad \\(B\\) v√¶re h√¶ndelsen at antal √∏jne er mindre eller lig med \\(3\\). Det vil sige, at h√¶ndelsen \\(B\\) best√•r af udfaldene: \\(B\\) = {‚öÄ, ‚öÅ, ‚öÇ}. Lad h√¶ndelsen \\(A\\) v√¶re udfald med ulige antal √∏jne: \\(A\\) = {‚öÄ, ‚öÇ, ‚öÑ}.\nDa kan vi nemt indse, at \\[P(A) = 3/6 = 1/2\\] samt ligeledes at \\[P(B) = 1/2\\] p√• grund af det symmetriske udfaldsrum.\nSer vi imidlertid p√• den betingede sandsynlighed for at \\(A\\) indtr√¶ffer givet, at \\(B\\) allerede er indtruffet, f√•r vi \\(P(A\\mid B)\\). Det svarer til sandsynligheden for at sl√• et ulige antal √∏jne, hvis vi allerede ved at antallet af √∏jne er mindre end eller lig med \\(3\\).\nF√∏rst ser vi, at \\(A\\cap B\\) = {‚öÄ, ‚öÇ, ‚öÑ} \\(\\cap\\) {‚öÄ, ‚öÅ, ‚öÇ} = {‚öÄ, ‚öÇ}, hvilket igen p√• grund af det symmetriske sandsynlighedsfelt betyder, at \\[P(A\\cap B) = 2/6 = 1/3\\] Efter at vi normaliserer sandsynligheden ud fra betingelsen om at \\(B\\) er indtruffet f√•r vi \\[P(A\\mid B) =  \\frac{P(A \\cap B)}{P(B)} = \\frac{1/3}{1/2}  = \\frac{2}{3}.\\] At betinge med h√¶ndelsen \\(B\\) svarer i dette simple eksempel til at indskr√¶nke udfaldet for \\(A\\) fra alle ulige √∏jne til dem, som er mindre end eller lig med \\(3\\). Der er s√•ledes tre mulige udfald i vores ‚Äú\\(B\\)-verden‚Äù, hvoraf to er ulige."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#stokastisk-uafh√¶ngighed",
    "href": "materialer/naivbayes/NaivBayes.html#stokastisk-uafh√¶ngighed",
    "title": "Naiv Bayes klassifier",
    "section": "Stokastisk uafh√¶ngighed",
    "text": "Stokastisk uafh√¶ngighed\nMan siger, at to h√¶ndelser \\(A\\) og \\(B\\) er uafh√¶ngige af hinanden, hvis \\[P(A \\cap B) = P(A) \\cdot P(B)\\] Hvis vi ser p√• udtrykket for \\(P(A\\mid B)\\) i (1) og antager, at \\(A\\) og \\(B\\) er uafh√¶ngige, ser vi at \\[\nP(A\\mid B) = \\frac{P(A\\cap B)}{P(B)} \\stackrel{\\text{uafh.}}{=} \\frac{P(A) \\cdot P(B)}{P(B)} = P(A)\n\\] Med andre ord betyder det, at sandsynligheden for \\(A\\) givet \\(B\\) er den samme som sandsynligheden for \\(A\\). Det vil sige, at oplysningen om, at \\(B\\) allerede er indtruffet, ikke √¶ndrer p√• sandsynligheden for \\(A\\). Information om \\(B\\) tilf√∏rer alts√• ikke noget nyt i forhold til information om \\(A\\), og det giver derfor mening af sige, at \\(A\\) og \\(B\\) er uafh√¶ngige af hinanden."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-s√¶tning",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-s√¶tning",
    "title": "Naiv Bayes klassifier",
    "section": "Bayes‚Äô s√¶tning",
    "text": "Bayes‚Äô s√¶tning\nEn meget vigtig matematisk egenskab ved betinget sandsynlighed er muligheden for at ombytte rollerne i formlen, s√•ledes vi kan udtrykke \\(P(B\\mid A)\\) ud fra vores viden om \\(P(A\\mid B)\\). S√¶tningen kaldes Bayes‚Äô s√¶tning (eller formel) og kan let vises ved f√∏rst at bestemme \\(P(A\\cap B)\\) ved at isolere denne sandsynlighed. Fra (1) f√•r vi\n\\[P(A\\cap B)=P(A\\mid B)\\cdot P(B)\\] P√• helt tilsvarende vis m√• der ogs√• g√¶lde, at\n\\[P(B\\cap A)=P(B\\mid A)\\cdot P(A)\\] Og da \\(A\\cap B=B\\cap A\\) m√• ogs√• \\(P(A\\cap B)=P(B\\cap A)\\). De to ovenfor udledte sandsynligheder, m√• derfor v√¶re ens:\n\\[P(A\\mid B)\\cdot P(B)=P(B\\mid A)\\cdot P(A)\\] Her Kan \\(P(A\\mid B)\\) isoleres \\[\nP(A\\mid B)=  \\frac{P(B\\mid A)\\cdot P(A)}{P(B)}\n\\] Dette resultat er netop Bayes‚Äô s√¶tning:\n\n\nS√¶tning 1 (Bayes‚Äô s√¶tning) Lad \\(A\\) og \\(B\\) v√¶re h√¶ndelser, hvor \\(P(B) \\neq 0\\). Da g√¶lder, at \\[\nP(A\\mid B)=  \\frac{P(B\\mid A)\\cdot P(A)}{P(B)}\n\\]\n\n\nVi kan alts√• ved at kende \\(P(B\\mid A)\\), \\(P(B)\\) og \\(P(A)\\) udtrykke den betingede sandsynlighed \\(P(A\\mid B)\\). Vi vender lige om lidt tilbage til, hvad vi kan bruge det til.\nSom sidste bem√¶rkning er det v√¶sentligt at understrege, at \\(P(A\\mid B) \\neq P(B\\mid A)\\) med mindre \\(P(A) = P(B)\\) j√¶vnf√∏r (1) ovenfor. F.eks. er sandsynligheden for et tilf√¶ldigt dyr er en elefant, givet dyret har fire ben ikke den samme som sandsynligheden for, at dyret har fire ben givet, at dyret er en elefant!"
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bin√¶r-bayes-klassifier",
    "href": "materialer/naivbayes/NaivBayes.html#bin√¶r-bayes-klassifier",
    "title": "Naiv Bayes klassifier",
    "section": "Bin√¶r Bayes klassifier",
    "text": "Bin√¶r Bayes klassifier\nAntag nu at \\(Y\\) kun kan antage to tilstande som ved eksemplet med r√∏d eller bl√•. I dette tilf√¶lde overs√¶tter man ofte de to udfald til henholdsvis \\(0\\) og \\(1\\), eller i visse sammenh√¶nge til \\(-1\\) og \\(+1\\). Husk p√• at Bayes klassifikationen \\(C(\\mathbf{x})\\) er det mest sandsynlige udfald for \\(Y\\), n√•r vi har informationen \\(\\mathbf{x}\\). I det tilf√¶lde hvor \\(Y\\) kun kan antage to tilstande, f√•r vi derfor\n\\[ C(\\mathbf{x}) = \\begin{cases}\n0 & \\textrm{hvis } P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x}) &gt; P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x}) \\\\\n1 & \\textrm{ellers} \\\\\n\\end{cases} \\tag{2}\\]\nDette kan ogs√• udtrykkes p√• anden vis: \\[\n\\begin{aligned}\nP(Y = 0 \\mid \\mathbf{X} = \\mathbf{x}) &gt; P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x}) \\Leftrightarrow\n\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} &gt; 1.\n\\end{aligned}\n\\] Her er vi dog n√∏dt til at antage, at \\[P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})\\neq 0,\\] s√• vi ikke kommer til at dividere med \\(0\\).\nBruger vi denne omskrivning, kan vi udtrykke den bin√¶re Bayes klassifikation i (2) p√• denne m√•de:\n\\[ C(\\mathbf{x}) = \\begin{cases}\n0 & \\textrm{hvis } \\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} &gt; 1 \\\\\n1 & \\textrm{ellers} \\\\\n\\end{cases} \\tag{3}\\]\nI det f√∏lgende vil vi benytte os af Bayes‚Äô s√¶tning til at se p√•, hvordan ovenst√•ende br√∏k kan beregnes."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-s√¶tning-i-anvendelse",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-s√¶tning-i-anvendelse",
    "title": "Naiv Bayes klassifier",
    "section": "Bayes‚Äô s√¶tning i anvendelse",
    "text": "Bayes‚Äô s√¶tning i anvendelse\nVi bruger f√∏rst Bayes‚Äô s√¶tning til at udtrykke \\(P(A\\mid C)\\) og \\(P(B\\mid C)\\). \\[\nP(A\\mid C) = \\frac{P(C\\mid A)P(A)}{P(C)} \\quad\\text{og}\\quad\nP(B\\mid C) = \\frac{P(C\\mid B)P(B)}{P(C)},\n\\] N√•r vi bestemmer forholdet mellem \\(P(A\\mid C)\\) og \\(P(B\\mid C)\\) vil vi kunne slippe af med n√¶vneren, som de har til f√¶lles:\n\\[\n\\begin{aligned}\n\\frac{P(A\\mid C)}{P(B\\mid C)} &= \\frac{\\frac{P(C\\mid A)P(A)}{P(C)}}{\\frac{P(C\\mid B)P(B)}{P(C)}}  \n= \\frac{P(C\\mid A)P(A)}{P(C)} \\cdot \\frac{P(C)}{P(C\\mid B)P(B)} \\\\\n&= \\frac{P(C\\mid A)P(A)}{P(C\\mid B)P(B)},\n\\end{aligned}\n\\] hvor vi har udnyttet, at man dividerer med en br√∏k ved at gange med den omvendte br√∏k.\nI den bin√¶re Bayes klassifikation i (3) indg√•r br√∏ken \\(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\), som vi ved at benytte ovenst√•ende kan omskrive til: \\[\n\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} =\n\\frac{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)P(Y = 0)}{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 1)P(Y = 1)}.\n\\tag{4}\\]"
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#naiv-bayes-klassifier",
    "href": "materialer/naivbayes/NaivBayes.html#naiv-bayes-klassifier",
    "title": "Naiv Bayes klassifier",
    "section": "Naiv Bayes klassifier",
    "text": "Naiv Bayes klassifier\nUd fra udtrykket for forholdet mellem de to posterior sandsynligheder \\(P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})\\) og \\(P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})\\) som indg√•r i (4) kan vi se, at der indg√•r to typer af sandsynligheder:\n\\[P(\\mathbf{X} = \\mathbf{x}\\mid Y = y) \\quad \\textrm{og} \\quad P(Y = y)\\]\nDisse ben√¶vnes henholdsvis likelihood og prior sandsynlighed, idet \\(P(\\mathbf{X} = \\mathbf{x}\\mid Y = y)\\) udtrykker likelihooden (troligheden) for at observere \\(\\mathbf{X} = \\mathbf{x}\\) givet \\(Y = y\\). Omvendt er prior sandsynligheden \\(P(Y = y)\\) et udtryk for forh√•ndsandsynligheden for at \\(Y = y\\). Alts√• bruger vi disse betegnelser:\n\\[\n\\begin{aligned}\n&\\textrm{Likelihood: }  &P(\\mathbf{X} = \\mathbf{x}\\mid Y = y) \\\\\n&\\textrm{Prior sandsynlighed: } &P(Y = y)\n\\end{aligned}\n\\]\nVi kan sammenfatte udtrykket i (4) til det s√•kaldte posterior forhold: \\[\n\\underbrace{\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}}_\\text{Posterior forhold} =\n\\underbrace{\\frac{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)}{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 1)}}_\\text{Likelihood forhold} \\cdot\n\\underbrace{\\frac{P(Y = 0)}{P(Y = 1)}}_\\text{Prior forhold}\n\\]\nHvis vi vender tilbage til vores sp√∏rgsm√•l om at stemme p√• bl√• eller r√∏d blok og s√• kan vi sige, at \\[ Y=\n\\begin{cases}\n0 & \\textrm {hvis der stemmes p√• r√∏d blok} \\\\\n1 & \\textrm {hvis der stemmes p√• bl√• blok} \\\\\n\\end{cases}\n\\] Hvis \\(x=(kvinde, ung, Sj√¶lland)\\), s√• udtrykker \\(P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)\\) s√•ledes sandsynligheden for, at en person er en ung kvinde fra Sj√¶lland givet, at personen stemmer p√• r√∏d blok. N√•r man skal bestemme den sandsynlighed skal vi huske, at det er sandsynligheden for det samlede udsagn med k√∏n, alder og bop√¶l.\nFor at kunne beregne ovenst√•ende sandsynligheder bliver vi n√∏dt til at antage et eller andet, der g√∏r det muligt. Man siger, at vi opstiller en model.\n√ân af de simpleste modeller er at antage at k√∏n, alder og bop√¶l er uafh√¶ngige af hinanden givet \\(Y = y\\). Denne forsimplende antagelse har medvirket til metodens navn: Naiv Bayes eller Uafh√¶ngig Bayes klassifikation.\nDet betyder if√∏lge vores tidligere definition af uafh√¶ngighed at \\[\n\\begin{aligned}\nP(\\mathbf{X} = \\mathbf{x} \\mid Y = y) &=\nP(X_1 = x_1, X_2 = x_2, \\dots, X_q = x_q \\mid Y = y)\\\\\n&= P(X_1 = x_1\\mid Y = y)P(X_2 = x_2\\mid Y = y)\\cdots P(X_q = x_q \\mid Y = y)\\\\\n&= \\prod_{i=1}^q P(X_i = x_i\\mid Y = y),\n\\end{aligned}\n\\] hvor \\(\\prod\\)-symbolet i sidste linje betyder, at vi tager produktet af alle faktorerne p√• formen \\(P(X_i = x_i\\mid Y = y)\\) fra \\(i=1\\) op til \\(q\\) ‚Äì alts√• pr√¶cist det, som st√•r i linjen over. Det minder s√•ledes om sum-tegnet \\[\\sum_{i=1}^n x_i = x_1 + x_2 + \\cdots + x_n,\\] men blot for multiplikation i stedet for addition."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#posterior-forholdet-score-og-v√¶gte",
    "href": "materialer/naivbayes/NaivBayes.html#posterior-forholdet-score-og-v√¶gte",
    "title": "Naiv Bayes klassifier",
    "section": "Posterior forholdet, score og v√¶gte",
    "text": "Posterior forholdet, score og v√¶gte\nSamler vi nu udtrykkene, som indg√•r i vores posterior forhold i (4), samtidig med at vi antager, at \\(X_1, X_2, \\cdots, X_q\\) er uafh√¶ngige af hinanden givet \\(Y\\), f√•r vi nedenst√•ende:\n\\[\n\\begin{aligned}\n\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} &=\n\\frac{P(Y=0)}{P(Y=1)}\\cdot \\frac{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)}{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 1)} \\\\\n&= \\frac{P(Y = 0)}{P(Y = 1)}\n\\prod_{i=1}^q\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)},\n\\end{aligned}\n\\]\nhvor hver faktor p√• h√∏jre siden bidrager ligeligt til, om observationen \\(\\mathbf{x}\\) skal klassificeres som \\(Y=0\\) eller \\(Y=1\\).\nN√•r vi skal lave beregninger p√• computer baseret p√• data, er det ofte v√¶sentligt at tage h√∏jde for numerisk pr√¶cision. Alle tal p√• en computer skal repr√¶senteres af et endelig antal bits. Det betyder, at visse tal (f.eks. \\(1/3\\)) bliver afrundet efter et vist antal decimaler. Derfor kan der opst√• problemer, n√•r man enten ganger eller adderer meget sm√• (eller store) tal sammen. For at undg√• dette i udtrykket ovenfor, er det derfor tit en god id√© at benytte sig af (den naturlige) logaritme p√• begge sider af lighedstegnet:\n\\[\n\\begin{aligned}\n\\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right) &=\n\\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right) +\n\\sum_{i=1}^q \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right),\n\\end{aligned}\n\\tag{5}\\]\nhvor vi har brugt logaritmeregnereglen \\[\\ln(a\\cdot b) = \\ln(a) + \\ln(b)\\] gentagende gange.\nVi minder om, at forholdet mellem \\(P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})\\) og \\(P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})\\) er af s√¶rlig interesse omkring v√¶rdien 1 ‚Äì se (3). N√•r forholdet er \\(1\\) betyder det, at de to klasser er lige sandsynlige givet \\(\\mathbf{x}\\). Endvidere, n√•r forholdet er over \\(1\\), er \\(Y=0\\) mere sandsynlig end \\(Y=1\\), og n√•r det er under \\(1\\), er det omvendte tilf√¶ldet.\nLad os nu se p√• hvilken effekt det f√•r, at vi ikke l√¶ngere ser direkte p√• det posterior forhold \\[ \\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\] men i stedet p√• logaritmen af det posterior forhold \\[ \\ln \\left ( \\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right )\\]\nI figur¬†1 nedenfor er grafen for logaritmefunktionen tegnet for \\(x\\in ]0, 10].\\)\n\n\n\n\n\n\nFigur¬†1: Grafen for \\(f(x)=\\ln (x)\\).\n\n\n\nVi ved, at \\(\\ln(1) = 0\\) (hvilket ogs√• kan ses p√• grafen i figur¬†1), samt at for \\(x&lt;1\\) er \\(\\ln(x)&lt;0\\), mens for \\(x&gt;1\\) er \\(\\ln(x)&gt;0\\).\nS√• n√•r vi ser p√• \\[\\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right)\\] bliver det vigtige nu, om denne st√∏rrelse er positiv eller negativ.\nLad os derfor indf√∏re \\(S\\) som en score, der er lig med logaritmen til posterior forholdet:\n\\[\nS = \\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right)\n\\tag{6}\\]\nDen bin√¶re Bayes klassifikation i (3) kan derfor i stedet skrives ved hj√¶lp af scoren \\(S\\) p√• denne m√•de:\n\\[ C(\\mathbf{x}) = \\begin{cases}\n0 & \\textrm{hvis } \\ S&gt;0  \\\\\n1 & \\textrm{ellers} \\\\\n\\end{cases} \\tag{7}\\]\nVi ved s√•ledes, at hvis \\(S&gt;0\\), s√• klassificerer vi \\(\\mathbf{x}\\), som \\(C(x)=0\\) og ellers \\(C(x)=1\\).\nSammenholder vi definitionen af \\(S\\) i (6) med udtrykket i (5), ser vi, at \\(S\\) ogs√• kan skrives som\n\\[\n\\begin{aligned}\nS &= \\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right) \\\\&=\n\\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right) +\n\\sum_{i=1}^q \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right),\n\\end{aligned}\n\\tag{8}\\]\nIndf√∏rer vi nu bidragene til \\(S\\) som \\(w_0\\) og \\(w_i(x_i)\\) s√•ledes \\[\nw_0 = \\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right)\n\\quad\\text{og}\\quad\nw_i(x_i) = \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right)\n\\] kan vi skrive udtrykket for \\(S\\) i (8) som \\[\nS = w_0 + \\sum_{i = 1}^q w_i(x_i),\n\\tag{9}\\] hvor det tydeligg√∏res, at hvis \\(w_i(x_i)&gt;0\\), s√• underst√∏tter bidraget fra den \\(i\\)‚Äôte oplysning \\(x_i\\), at \\(Y=0\\) og ellers hvis \\(w_i(x_i)&lt;0\\) at \\(Y=1\\). Denne egenskab g√∏r, at man ogs√• kan omtale \\(w_i(x_i)\\) som en slags ‚Äúbevism√¶ssig‚Äù v√¶gt.\n\nV√¶gten \\(w_0\\)\nVi har alts√• set i (5) og (9), hvordan vi kan omskrive forholdet mellem posterior sandsynlighederne for de to klasser \\(Y=0\\) og \\(Y=1\\) til en sum af bidrag.\nDet f√∏rste led \\(w_0\\) afh√¶nger ikke af nogen information \\(x_i\\), og vi har tidligere omtalt disse sandsynligheder som prior sandsynligheder. Man kan sige, at det er vores umiddelbare bud p√• hvad \\(Y\\) er, uden at vi kender noget som helst til informationerne i \\(\\mathbf{x}\\).\nN√•r vi g√•r fra vores model, som vi har udledt i det foreg√•ende, til at skulle implementere den i en specifik anvendelse, bliver vi derfor n√∏dt til at estimere de parametre, som indg√•r i modellen. For \\(w_0\\) betyder det, at vi skal estimere b√•de \\(P(Y=0)\\) og \\(P(Y=1)\\). Her vil det v√¶re oplagt blot at estimere \\(P(Y=0)\\) og \\(P(Y=1)\\) ud fra tr√¶ningsdata ved helt simpelt at bestemme andelen, som stemmer p√• henholdsvis r√∏d og bl√•, hvorefter vi kan beregne \\[\nw_0 = \\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right)\\]\nHvis du vil se en mere teoretisk begrundelse for dette valg, kan du folde boksen nedenfor ud.\n\n\n\n\n\n\nTeoretisk begrundelse for hvordan \\(P(Y=0)\\) kan estimeres\n\n\n\n\n\nVi √∏nsker at bestemme det bedst mulige estimat for \\(p=P(Y=0)\\) ud fra vores tr√¶ningsdata. Vi vil her t√¶nke p√• resultaterne fra datas√¶ttet som kommende fra et binomialfors√∏g med sandsynligheds-parameter \\(p\\) og antalsparameter \\(n\\). I eksemplet kan vi derfor lade \\(Z\\) v√¶re en stokastisk variabel, der betegner antallet, som stemmer p√• r√∏d blok. Det vil sige, at\n\\[ Z \\sim bin(n,p) \\]\nog fra binomialfordelingen ved vi, at\n\\[\nP(Z = r) = {n \\choose r}p^r(1-p)^{n-r} = \\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}\n\\] N√•r vi skal estimere \\(p\\) (alts√• sandsynligheden for at stemme p√• r√∏d blok ‚Äì det vil sige \\(P(Y=0)\\)) ud fra data benyttes en metode, som kaldes for maksimum likelihood estimation. Den g√•r i al sin enkelhed ud p√• at bestemme den v√¶rdi af \\(p\\), som g√∏r de data, vi har set, mest sandsynlige. Alts√• vil vi maksimere udtrykket ovenfor med hensyn til \\(p\\). Dette kan vi g√∏re ved at differentiere udtrykket og s√¶tte det lig med \\(0\\). I stedet for at arbejde direkte med \\(P(Z = r)\\) er det nemmere at arbejde med udtrykket for \\(\\ln\\bigl(P(Z = r)\\bigr)\\), idet der g√¶lder, at \\(f(p)=P(Z = r)\\) og \\(\\ln\\bigl(f(p) \\bigr)\\) har maksimum ved samme \\(p\\) (fordi logaritmefunktionen er voksende).\nVi finder derfor f√∏rst et udtryk for \\(\\ln\\bigl(P(Z = r)\\bigr)\\):\n\\[ \\ln\\bigl(P(Z = r)\\bigr) = \\ln {n \\choose r} + r \\cdot \\ln(p) + (n-r) \\cdot \\ln(1-p)\\] hvor vi har brugt logaritmeregnereglerne \\[\\ln(a\\cdot b) = \\ln(a) + \\ln(b) \\quad \\textrm{og} \\quad \\ln(a^x)=x \\cdot \\ln(a)\\] Derfor bliver den afledede med hensyn til \\(p\\) \\[\n\\frac{d}{dp} \\left ( \\ln\\bigl(P(Z = r)\\bigr) \\right ) = \\frac{r}{p} - \\frac{n-r}{1-p}\n\\] S√¶tter vi ovenst√•ende lig \\(0\\) og isolerer for \\(p\\), f√•r vi, at\n\\[\\hat{p} = \\frac{r}{n}\\]\nhvilket svarer til den andel af de \\(n\\) observationer, som har \\(Y=0\\). Vi s√¶tter en ‚Äúhat‚Äù p√• \\(p\\) for at tydeligg√∏re, at det er et estimat af \\(p\\) ‚Äì og alts√• ikke den ukendte, sande v√¶rdi af \\(p\\).\nBem√¶rk, at man ogs√• kan vise, at denne v√¶rdi af \\(p\\) rent faktisk svarer til et maksimumssted.\n\n\n\n\n\nV√¶gtene \\(w_i\\)\nDe √∏vrige bidrag til \\(S\\) afh√¶nger af den specifikke v√¶rdi af \\(x_i\\). Det er alts√• her data for observationen, vi √∏nsker at klassificere, kommer ind i billedet. Her vil v√¶gten \\(w_i(x_i)\\), som bidrager til den samlede score \\(S\\), afh√¶nge af informationen2 \\(x_i\\).\n2¬†Hvis \\(w_i(x_i)\\) er mere eller mindre konstant for forskellige v√¶rdier af \\(X_i\\), betyder det, at den \\(i\\)‚Äôte information ikke er s√¶rlig informativ (og m√•ske b√∏r udelades fra modellen).Ved hver information \\(X_i\\) estimeres \\(P(X_i = x_i \\mid Y = y)\\) ved at se p√• andelen af \\(x_i\\) blandt alle tr√¶ningsdata med \\(Y = y\\). V√¶gtene \\(w_i\\) estimeres s√•ledes p√• tilsvarende m√•de som for \\(w_0\\).\nN√•r disse estimater er fundet, kan man bestemme \\[w_i(x_i) = \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right)\n\\]"
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#eksempel-med-r√∏dbl√•-blok",
    "href": "materialer/naivbayes/NaivBayes.html#eksempel-med-r√∏dbl√•-blok",
    "title": "Naiv Bayes klassifier",
    "section": "Eksempel med r√∏d/bl√• blok",
    "text": "Eksempel med r√∏d/bl√• blok\nLad os se p√• eksemplet fra tidligere med at stemme p√• r√∏d eller bl√• blok, hvor vi t√¶nker p√• \\(Y=0\\) som en stemme p√• r√∏d blok og \\(Y=1\\) som en stemme p√• bl√• blok.\nVi havde allerede f√∏lgende information fra tr√¶ningsdata:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlle\nM√¶nd\nKvinder\nUnge\n√Üldre\nSj√¶lland\nJylland\nAnden bop√¶l\n\n\n\n\nR√∏d\n\\(51.85 \\%\\)\n\\(48.00 \\%\\)\n\\(55.00 \\%\\)\n\\(65.00 \\%\\)\n\\(47.47 \\%\\)\n\\(54.00 \\%\\)\n\\(49.90 \\%\\)\n\\(52.78 \\%\\)\n\n\nBl√•\n\\(48.15 \\%\\)\n\\(52.00 \\%\\)\n\\(45.00 \\%\\)\n\\(35.00 \\%\\)\n\\(52.53 \\%\\)\n\\(46.00 \\%\\)\n\\(50.10 \\%\\)\n\\(47.22 \\%\\)\n\n\nAntal\n\\(10000\\)\n\\(4500\\)\n\\(5500\\)\n\\(2500\\)\n\\(7500\\)\n\\(3000\\)\n\\(4500\\)\n\\(2500\\)\n\n\n\nFra dette bestemmes f√∏rst v√¶gten \\(w_0\\) ved \\[w_0 = \\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right)=\\ln\\left(\\frac{51.85\\%}{48.15\\%}\\right)=0.074\\]\nFor at kunne beregne vores v√¶gte \\[w_i(x_i) = \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right)\n\\]\nskal vi for at beregne t√¶lleren i ovenst√•ende br√∏k have fat p√• hvor stor en andel af de stemmer, der g√•r til r√∏d blok, som kommer fra henholdsvis m√¶nd, kvinder, unge, √¶ldre og s√• videre.\nVi tager beregningen for kvinder og starter med at finde \\(P(X_1 = kvinde \\mid Y = 0)\\). Her ved vi at \\(55 \\%\\) af de i alt \\(5500\\) kvinder stemte p√• r√∏d blok. Samtidig ved vi, at der var \\(51.85 \\%\\) ud af i alt \\(10000\\) adspurgte (det vil sige \\(5185\\) pesroner), som stemte p√• r√∏d blok. Derfor f√•r vi \\[P(X_1 = kvinde \\mid Y = 0)=\\frac{55\\% \\cdot 5500}{5185}=58.34 \\%\\] Da \\(X_1\\) kun kan antage v√¶rdierne kvinde og mand, ved vi ogs√•, at\n\\[P(X_1 = mand \\mid Y = 0)=100\\%-58.34 \\%=41.66\\%.\\] P√• tilsvarende m√•de kan vi finde \\(P(X_1 = kvinde \\mid Y = 1)\\). Her ved vi, at \\(45 \\%\\) af de \\(5500\\) kvinder stemte p√• bl√• blok, og samtidig ved vi, at der var \\(4815\\) stemmer p√• bl√• blok i alt (igen \\(48.15 \\%\\) af \\(10000\\)). Derfor f√•r vi \\[P(X_1 = kvinde \\mid Y = 1)=\\frac{45\\%\\cdot 5500}{4815}= 51.40 \\%\\] og \\[P(X_1 = mand \\mid Y = 1)=100\\%-51.40 \\%=48.60\\%\\] Alle tilsvarende sandsynligheder kan beregnes, s√• man kan se hvor stor en andel af stemmerne p√• de to blokke, der kommer fra hver gruppe. Resultatet ses i nedenst√•ende tabel:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM√¶nd\nKvinder\nUnge\n√Üldre\nSj√¶lland\nJylland\nAnden bop√¶l\n\n\n\n\nR√∏d\n\\(41.66 \\%\\)\n\\(58.34 \\%\\)\n\\(31.34 \\%\\)\n\\(68.66 \\%\\)\n\\(31.24 \\%\\)\n\\(43.31 \\%\\)\n\\(25.45 \\%\\)\n\n\nBl√•\n\\(48.60 \\%\\)\n\\(51.40 \\%\\)\n\\(18.17 \\%\\)\n\\(81.83 \\%\\)\n\\(28.66 \\%\\)\n\\(46.82 \\%\\)\n\\(24.52 \\%\\)\n\n\n\nNu kan vi bestemme v√¶gtene \\(w_i(x_i)\\). Her findes \\(w_1(kvinde)\\) ved\n\\[w_1(kvinde) = \\ln \\left(\\frac{P(X_1 = kvinde \\mid Y = 0)}{P(X_1 = kvinde \\mid Y = 1)}\\right)=\\ln \\left(\\frac{58.34\\%}{51.40\\%}\\right)=0.1267\\] Herunder ses v√¶gtene for alle grupper:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(w_0\\)\n\\(w_1\\)\n\n\\(w_2\\)\n\n\\(w_3\\)\n\n\n\n\n\n\n\nM√¶nd\nKvinder\nUnge\n√Üldre\nSj√¶lland\nJylland\nAnden bop√¶l\n\n\n\\(0.074\\)\n\\(-0.154\\)\n\\(0.127\\)\n\\(0.545\\)\n\\(-0.175\\)\n\\(0.086\\)\n\\(-0.078\\)\n\\(0.037\\)\n\n\n\nTidligere havde vi indset, at n√•r \\(w_i(x_i)&gt;0\\), s√• underst√∏tter bidraget fra den \\(i\\)‚Äôte oplysning \\(x_i\\), at \\(Y=0\\) (alts√• at stemme p√• r√∏d blok). I tabellen ovenfor ses det derfor, at oplysningerne kvinde, ung, Sj√¶lland og anden bop√¶l g√∏r det mere sandsynligt med en stemme p√• r√∏d blok, mens oplysningerne mand, √¶ldre og Jylland g√∏r det mere sandsynligt med en stemme p√• bl√• blok.\nVi kan nu beregne scoren \\(S\\) for en kvinde, som er √¶ldre og fra Sj√¶lland, alts√• hvor \\(x=(kvinde,√¶ldre,Sj√¶lland)\\). \\[\\begin{equation}\n\\begin{split}\nS &  = w_0 + \\sum_{i = 1}^q w_i(x_i)=w_0+w_1(kvinde)+w_2(√¶ldre)+w_3(Sj√¶lland) \\\\\n& =0.074+0.127+(-0.175)+0.086=0.112\n\\end{split}\n\\end{equation}\\]\nDerved bliver forudsigelsen ud fra Bayes Naive metode, at en √¶ldre kvinde, der bor p√• Sj√¶lland, med st√∏rst sandsynlighed stemmer p√• r√∏d blok."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#opgave-ham-or-spam",
    "href": "materialer/naivbayes/NaivBayes.html#opgave-ham-or-spam",
    "title": "Naiv Bayes klassifier",
    "section": "Opgave: Ham or Spam?",
    "text": "Opgave: Ham or Spam?\nI denne opgave skal vi se p√•, hvordan man kan lave et simpelt spamfilter. Vi har et datas√¶t med \\(35000\\) mails med oplysninger om emailens oprindelse (Danmark, Europa uden Danmark, USA og anden oprindelse), afsenders mailadresse (firma, Google, Hotmail og anden) samt indhold (dating, spil og andet). M√•let er, at man gerne ud fra disse oplysninger gerne automatisk vil kunne afg√∏re, om det er spam og derved at mailen ikke skal vises i mail-boxen, eller om det er Ham (non-spam). For hver af de \\(35000\\) mails er det ogs√• noteret om, der er tale om spam eller ham.\nTr√¶ningsdata best√•r af\n\n\n\n\nOprindelse\n\n\n\n\n\n\n\n\nDK\nEuropa\nUSA\nAndet\n\n\nSpam\n\\(20 \\%\\)\n\\(30 \\%\\)\n\\(35 \\%\\)\n\\(55 \\%\\)\n\n\nHam\n\\(80 \\%\\)\n\\(70 \\%\\)\n\\(65 \\%\\)\n\\(45\\%\\)\n\n\nAntal\n\\(10000\\)\n\\(12000\\)\n\\(8000\\)\n\\(5000\\)\n\n\n\nog\n\n\n\n\nMail\n\n\n\nIndhold\n\n\n\n\n\n\n\nFirma\nGoogle\nHotmail\nAndet\nDating\nSpil\nAndet\n\n\nSpam\n\\(10 \\%\\)\n\\(20 \\%\\)\n\\(60 \\%\\)\n\\(80 \\%\\)\n\\(80 \\%\\)\n\\(90 \\%\\)\n\\(12.5 \\%\\)\n\n\nHam\n\\(90 \\%\\)\n\\(80 \\%\\)\n\\(40 \\%\\)\n\\(20 \\%\\)\n\\(20\\%\\)\n\\(10\\%\\)\n\\(87.5\\%\\)\n\n\nAntal\n\\(17000\\)\n\\(6450\\)\n\\(5400\\)\n\\(6150\\)\n\\(4325\\)\n\\(4975\\)\n\\(25700\\)\n\n\n\n\n\n\n\n\n\nOpgave 1\n\n\n\n\n\n\nHvis man blot modtager en mail fra en Hotmail-konto, vil man da t√¶nke, at det er spam eller ham?\nHvis man blot modtager en tilf√¶ldig mail, vil man da t√¶nke, at det er spam eller ham?\nForklar hvorfor det kan v√¶re sv√¶rt at afg√∏re, om det er spam, hvis man modtager en mail fra Danmark, som er sendt fra en firma-mail og hvor indhold er relateret til dating.\n\n\n\n\n\n\n\n\n\n\nOpgave 2\n\n\n\n\n\nIndf√∏r selv stokastiske variable \\(X1\\), \\(X2\\), \\(X3\\) og \\(Y\\) og angiv udfaldsrum for hver af dem.\n\n\n\n\n\n\n\n\n\nOpgave 3\n\n\n\n\n\n\nOpstil posterior forholdet\n\n\\[ \\frac{P(Y = 0 \\mid X_1=x_1, X_2=x_2, X_3=x_3)}{P(Y = 1 \\mid X_1=x_1, X_2=x_2, X_3=x_3)}\\]\nog forklar det smarte, der er sket ved at bruge Bayes formel.\n\nRedeg√∏r for betydningen af forholdet og forklar hvorfor man ser p√•, om det er over eller under \\(1\\).\n\n\n\n\n\n\n\n\n\n\nOpgave 4\n\n\n\n\n\nAngiv den antagelse (modelforuds√¶tning), som anvendes ved Bayes Naive klassifikation, og forklar hvad det g√∏r for beregningen af sandsynlighederne fra opgave 3.\n\n\n\n\n\n\n\n\n\nOpgave 5\n\n\n\n\n\nBestem prior forholdet\n\\[\\frac{P(Y=0)}{P(Y=1)}\\]\nog beregn p√• den baggrund v√¶gten\n\\[w_0 = \\ln \\left ( \\frac{P(Y=0)}{P(Y=1)} \\right )\\]\n\n\n\n\n\n\n\n\n\nOpgave 6\n\n\n\n\n\nBestem alle betingede sandsynligheder\n\\[P(X_i=x_i \\mid Y=0) \\quad \\textrm{og} \\quad P(X_i=x_i \\mid Y=1)\\]\nfor hver enkelt information, givet at det er henholdsvis spam og ham (i alt 22 sandsynligheder) og forklar id√©en bag √©n af disse udregninger.\n\n\n\n\n\n\n\n\n\nOpgave 7\n\n\n\n\n\nFor hver af de 11 informationer bestemmes forholdet mellem sandsynlighederne\n\\[ \\frac{P(X_i=x_i \\mid Y=0)}{P(X_i=x_i \\mid Y=1)}\\]\n\n\n\n\n\n\n\n\n\nOpgave 8\n\n\n\n\n\nForklar hvordan man kommer fra resultaterne i opgave 7 til v√¶gte og udregn v√¶gtene h√∏rende til hver af informationerne (i alt 11).\n\n\n\n\n\n\n\n\n\nOpgave 9\n\n\n\n\n\nForklar hvad der sker ved at benytte logaritmen p√• udtrykket fra opgave 3 og 4, hvor man ellers ganger faktorer sammen.\n\n\n\n\n\n\n\n\n\nOpgave 10\n\n\n\n\n\nAfg√∏r ud fra de v√¶gte, som du har beregnet i opgave 8, hvilke informationer der taler for spam, og hvilke der taler for ham.\n\n\n\n\n\n\n\n\n\nOpgave 11\n\n\n\n\n\nAfg√∏r ved at beregne scoren \\(S\\), om man vil t√¶nke at en mail er ham eller spam i f√∏lgende to situationer:\n\nMailens oprindelse er andet, den er sendt fra en hotmail-konto og omhandler ikke dating eller spil.\nMailen er en firma-mail fra Danmark med indhold relateret til dating."
  },
  {
    "objectID": "materialer/afstande/afstand.html",
    "href": "materialer/afstande/afstand.html",
    "title": "Afstande, n√¶rmest, st√∏rst, mindst",
    "section": "",
    "text": "N√•r vi adskiller eller samler data bygger vi p√• en form for afstand. De \\(k\\) n√¶rmeste naboer er dem, der ligger t√¶ttest p√• i √©n eller anden forstand. Hvis det drejer sig om dem, hvis h√∏jder er t√¶tte p√• hinanden eller m√•ske dem, der vejer nogenlunde det samme, er det klart, hvad man mener. Der er tal, man umiddelbart kan sammenligne. Men hvad med at sammenligne b√•de v√¶gt og h√∏jde? Hvad betyder s√• mest? Er der lige langt mellem en person A, der vejer 80 kg og er 1,80 m h√∏j og en anden, B, der vejer 90 kg og er 2,00 m eller mellem A og C, der vejer 70 kg og er 1,60 m? Det er ikke klart, selvom vi da kan plotte de tre punkter i et (v√¶gt, h√∏jde) koordinatsystem og endda bruge Pythagoras og f√• den samme afstand.1 Udregner man BMI, er \\(A\\) t√¶ttere p√• \\(B\\) end p√• \\(C\\). Det kommer nok ogs√• an p√•, hvad vi gerne vil udtale os om: Er de nogenlunde lige gode til at l√∏be langt? Eller hurtigt? Mere kompliceret bliver det, hvis vi ogs√• vil inddrage √∏jenfarve, skost√∏rrelse eller m√•ske, om de k√∏ber rigtig meget m√¶lk. Der er mange eksempler p√• afstande, som ikke umiddelbart er fysisk afstand. For eksempel mellem ord (LINK) eller mellem DNA (Link)"
  },
  {
    "objectID": "materialer/afstande/afstand.html#hierarkisk-clustering",
    "href": "materialer/afstande/afstand.html#hierarkisk-clustering",
    "title": "Afstande, n√¶rmest, st√∏rst, mindst",
    "section": "Hierarkisk clustering",
    "text": "Hierarkisk clustering\nHer kender vi alle parvise afstande. Og ikke andet.\nUdfra den information laver vi et dendogram, hvor i f√∏rste omgang par af datapunkter \"m√∏des\" i den h√∏jde, der svarer til deres afstand. Men der er mere: Hvorn√•r skal datapunktet \\(p\\) m√∏des med \\(qr\\), som m√∏dtes tidligere? Hvorn√•r skal \\(pqr\\) m√∏des med \\(ab\\)? Det er linkage-reglerne.\n\nSingle linkage: \\(pqr\\) m√∏des med \\(ab\\) i den h√∏jde, hvor minimumsafstanden mellem de to grupper af punkter n√•s:\nMinimum af \\(d(a,p),d(a,q), d(a,r), d(b,p), d(b,q), d(p,r)\\)\nComplete linkage: \\(pqr\\) m√∏des med \\(ab\\), n√•r den maksimale afstand mellem punkter i de to grupper er n√•et.\nMaksimum af \\(d(a,p),d(a,q), d(a,r), d(b,p), d(b,q), d(p,r)\\)\nMiddelafstand- average linkage: N√•r den gennemsnitlige afstand er n√•et. \\(\\frac{1}{2\\cdot 3}(d(a,p)+d(a,q)+ d(a,r)+ d(b,p)+ d(b,q)+ d(p,r))\\)\n\n(OBS: Her skal v√¶re tegninger og diagrammer -dendrogrammer. Og eksempler p√•, hvad forskellen er p√• de forskellige linkagekrav)\nKlyngeanalyse af DNA eller for eksempel mRNA giver anledning til dendrogrammer, som kaldes de phylogenetiske tr√¶er for de arter/sygdomme,... der svarer til den analyserede DNA.\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2859286/\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6130602/"
  },
  {
    "objectID": "materialer/afstande/afstand.html#k-means-clustering",
    "href": "materialer/afstande/afstand.html#k-means-clustering",
    "title": "Afstande, n√¶rmest, st√∏rst, mindst",
    "section": "k-means clustering",
    "text": "k-means clustering\nVores data er punkter med \\(d\\) koordinater. Afstanden er Euklidisk. Vi v√¶lger \\(k\\), det antal clusters, det skal ende med. M√•let er at opdele data i \\(k\\) dele, \\(S_1, S_2,\\ldots , S_k\\) s√• den samlede gennemsnitlige kvadratiske afstand \\[\\Sigma_{i=1}^{k}\\Sigma_{p,q\\in S_i}\\frac{1}{2|S_i|}\\|p-q\\|^2\\] indenfor de \\(k\\) clusters er mindst mulig."
  },
  {
    "objectID": "materialer/afstande/afstand.html#footnotes",
    "href": "materialer/afstande/afstand.html#footnotes",
    "title": "Afstande, n√¶rmest, st√∏rst, mindst",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAfstanden bliver \\(10^2+0,2^2\\). Bem√¶rk, at det er udregnet udfra v√¶gt i kg og h√∏jde i meter. Med h√∏jde i cm ville det v√¶re \\(10^2+20^2\\), men stadig samme afstand fra A til B som fra A til C. Se Afstand udfra Data for mere info om effekten af at skifte enheder. Det kan godt lave om p√•, hvilke punkter, der ligger n√¶rmest.‚Ü©Ô∏é"
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html",
    "href": "materialer/afstande/feature_scaling.html",
    "title": "Feature-skalering",
    "section": "",
    "text": "Det er ikke altid helt klart, hvordan man skal bestemme afstanden mellem to datapunkter, hvis koordinaterne i hvert datapunkter beskriver vidt forskellige ting. Vi vil her se n√¶rmere p√•, hvilke problemer, der kan opst√•, hvis man ikke t√¶nker sig om ‚Äì og hvad man kan g√∏re for at l√∏se dem.\nLad os forestille os, at data best√•r af v√¶gt og h√∏jde for nogle personer, s√• hvert datapunkt er p√• formen \\[(v,h)\\] hvor \\(v\\) er den p√•g√¶ldende persons v√¶gt og \\(h\\) er h√∏jden. Her er det faktisk ikke klart, hvad afstanden mellem to punkter \\((v_1,h_1)\\) og \\((v_2,h_2)\\) skal v√¶re. Alts√•, hvorn√•r to punkter ligger t√¶t p√• hinanden.\nEt f√∏rste bud kunne v√¶re at bestemme den euklidiske afstand mellem de to punkter ‚Äì det der bare svarer til at bruge Pythagoras. G√∏r vi det f√•r vi f√∏lgende afstandsm√•l mellem punkterne \\((v_1,h_1)\\) og \\((v_2,h_2)\\):\n\\[\\sqrt{(v_2-v_1)^2+(h_2-h_1)^2}\\]\nVi pr√∏ver at regne lidt p√• det, og forestiller os, at tre personer er givet som datapunkter i nedenst√•ende tabel.\n\n\n\nPerson\n(v√¶gt, h√∏jde)\n\n\n\n\nA\n\\((70 \\ kg, 165 \\ cm)\\)\n\n\nB\n\\((90 \\ kg, 180 \\ cm)\\)\n\n\nC\n\\((80 \\ kg, 190 \\ cm)\\)\n\n\n\nBruger vi Pythagoras p√• tallene, der st√•r her, er:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+15^2}=25 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+25^2}\\simeq 27 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+10^2}\\simeq 14\n\\end{align*}\\]\nMed dette afstandsm√•l er der alts√• l√¶ngst fra \\(A\\) til \\(C\\).\nSkifter vi nu enhed og udtrykker h√∏jden i meter f√•r vi f√∏lgende datapunkter:\n\n\n\nPerson\n(v√¶gt, h√∏jde)\n\n\n\n\nA\n\\((70 \\ kg, 1.65 \\ m)\\)\n\n\nB\n\\((90 \\ kg, 1.80 \\ m)\\)\n\n\nC\n\\((80 \\ kg, 1.90 \\ m)\\)\n\n\n\nNu er\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+0.15^2} \\simeq 20 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+0.25^2} \\simeq 10 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+0.10^2} \\simeq 10\n\\end{align*}\\]\nDer er nu l√¶ngst fra \\(A\\) til \\(B\\).\nDet er ikke ret smart. Skal man finde de datapunkter, som ligger t√¶ttest p√• hinanden, er svaret tilsyneladende som vinden bl√¶ser og afh√¶ngig af hvilken enhed, vi har valgt at m√•le i.\nMen selv hvis begge variable er i samme enhed, kan Pythagoras brugt med hovedet under armen v√¶re uheldigt, som nedenst√•ende eksempel illustrerer.\nVi forestiller os, at vi har data for, hvor meget familierne \\(A\\), \\(B\\) og \\(C\\) bruger p√• bolig og p√• m√¶lk om m√•neden. Begge variable kan v√¶re i kroner. I tabellen ses et eksempel:\n\n\n\nFamilie\n(bolig, m√¶lk)\n\n\n\n\nA\n\\((7500 \\ kr, 200 \\ kr)\\)\n\n\nB\n\\((7500 \\ kr, 1700 \\ kr)\\)\n\n\nC\n\\((6000 \\ kr, 200 \\ kr)\\)\n\n\n\nDet vil alts√• for eksempel sige, at familie \\(A\\) bruger \\(7500\\) kr p√• bolig og \\(200\\) kr p√• m√¶lk. Her er afstanden udregnet med Pythagoras:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{0^2+1500^2} = 1500 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{1500^2+0^2} = 1500 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{1500^2+1500^2} \\simeq 2121\n\\end{align*}\\]\nAlts√• er der samme afstand fra \\(A\\) til \\(B\\) som fra \\(A\\) til \\(C\\), men vi vil nok mene, at \\(B\\) afviger mere fra \\(A\\) end \\(C\\) g√∏r, fordi m√¶lkeforbruget i familie \\(B\\) er us√¶dvanligt.\nHar vi data for mange familier, kan vi kvantificere id√©en om, hvad der er us√¶dvanligt og bruge det til at lave en mere passende afstand."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#afstand-med-hovedet-under-armen",
    "href": "materialer/afstande/feature_scaling.html#afstand-med-hovedet-under-armen",
    "title": "Feature-skalering",
    "section": "",
    "text": "Det er ikke altid helt klart, hvordan man skal bestemme afstanden mellem to datapunkter, hvis koordinaterne i hvert datapunkter beskriver vidt forskellige ting. Vi vil her se n√¶rmere p√•, hvilke problemer, der kan opst√•, hvis man ikke t√¶nker sig om ‚Äì og hvad man kan g√∏re for at l√∏se dem.\nLad os forestille os, at data best√•r af v√¶gt og h√∏jde for nogle personer, s√• hvert datapunkt er p√• formen \\[(v,h)\\] hvor \\(v\\) er den p√•g√¶ldende persons v√¶gt og \\(h\\) er h√∏jden. Her er det faktisk ikke klart, hvad afstanden mellem to punkter \\((v_1,h_1)\\) og \\((v_2,h_2)\\) skal v√¶re. Alts√•, hvorn√•r to punkter ligger t√¶t p√• hinanden.\nEt f√∏rste bud kunne v√¶re at bestemme den euklidiske afstand mellem de to punkter ‚Äì det der bare svarer til at bruge Pythagoras. G√∏r vi det f√•r vi f√∏lgende afstandsm√•l mellem punkterne \\((v_1,h_1)\\) og \\((v_2,h_2)\\):\n\\[\\sqrt{(v_2-v_1)^2+(h_2-h_1)^2}\\]\nVi pr√∏ver at regne lidt p√• det, og forestiller os, at tre personer er givet som datapunkter i nedenst√•ende tabel.\n\n\n\nPerson\n(v√¶gt, h√∏jde)\n\n\n\n\nA\n\\((70 \\ kg, 165 \\ cm)\\)\n\n\nB\n\\((90 \\ kg, 180 \\ cm)\\)\n\n\nC\n\\((80 \\ kg, 190 \\ cm)\\)\n\n\n\nBruger vi Pythagoras p√• tallene, der st√•r her, er:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+15^2}=25 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+25^2}\\simeq 27 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+10^2}\\simeq 14\n\\end{align*}\\]\nMed dette afstandsm√•l er der alts√• l√¶ngst fra \\(A\\) til \\(C\\).\nSkifter vi nu enhed og udtrykker h√∏jden i meter f√•r vi f√∏lgende datapunkter:\n\n\n\nPerson\n(v√¶gt, h√∏jde)\n\n\n\n\nA\n\\((70 \\ kg, 1.65 \\ m)\\)\n\n\nB\n\\((90 \\ kg, 1.80 \\ m)\\)\n\n\nC\n\\((80 \\ kg, 1.90 \\ m)\\)\n\n\n\nNu er\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+0.15^2} \\simeq 20 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+0.25^2} \\simeq 10 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+0.10^2} \\simeq 10\n\\end{align*}\\]\nDer er nu l√¶ngst fra \\(A\\) til \\(B\\).\nDet er ikke ret smart. Skal man finde de datapunkter, som ligger t√¶ttest p√• hinanden, er svaret tilsyneladende som vinden bl√¶ser og afh√¶ngig af hvilken enhed, vi har valgt at m√•le i.\nMen selv hvis begge variable er i samme enhed, kan Pythagoras brugt med hovedet under armen v√¶re uheldigt, som nedenst√•ende eksempel illustrerer.\nVi forestiller os, at vi har data for, hvor meget familierne \\(A\\), \\(B\\) og \\(C\\) bruger p√• bolig og p√• m√¶lk om m√•neden. Begge variable kan v√¶re i kroner. I tabellen ses et eksempel:\n\n\n\nFamilie\n(bolig, m√¶lk)\n\n\n\n\nA\n\\((7500 \\ kr, 200 \\ kr)\\)\n\n\nB\n\\((7500 \\ kr, 1700 \\ kr)\\)\n\n\nC\n\\((6000 \\ kr, 200 \\ kr)\\)\n\n\n\nDet vil alts√• for eksempel sige, at familie \\(A\\) bruger \\(7500\\) kr p√• bolig og \\(200\\) kr p√• m√¶lk. Her er afstanden udregnet med Pythagoras:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{0^2+1500^2} = 1500 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{1500^2+0^2} = 1500 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{1500^2+1500^2} \\simeq 2121\n\\end{align*}\\]\nAlts√• er der samme afstand fra \\(A\\) til \\(B\\) som fra \\(A\\) til \\(C\\), men vi vil nok mene, at \\(B\\) afviger mere fra \\(A\\) end \\(C\\) g√∏r, fordi m√¶lkeforbruget i familie \\(B\\) er us√¶dvanligt.\nHar vi data for mange familier, kan vi kvantificere id√©en om, hvad der er us√¶dvanligt og bruge det til at lave en mere passende afstand."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#f√∏rste-naive-tilgang-min-max-skalering",
    "href": "materialer/afstande/feature_scaling.html#f√∏rste-naive-tilgang-min-max-skalering",
    "title": "Feature-skalering",
    "section": "F√∏rste naive tilgang: Min-Max skalering",
    "text": "F√∏rste naive tilgang: Min-Max skalering\nEksemplet ovenfor illustrerer, at det nok vil v√¶re smart at pr√∏ve at inddrage i hvilket interval \\(x\\)- og \\(y\\)-v√¶rdierne varierer. For eksempel er et udsving p√• \\(500\\) kr i boligudgifter ikke lige s√• voldsomt, som et udsving p√• \\(500\\) kr i m√¶lkeudgifter. Problemet er, at den absolutte forskel p√• henholdsvis \\(x\\)- og \\(y\\)-v√¶rdierne ikke er sammenlignelige her.\nLad os sige, at vi betragter datapunkter \\((x_i,y_i)\\) i planen, hvor \\(x\\)-v√¶rdierne ligger mellem \\(a\\) og \\(b\\), mens \\(y\\)-v√¶rdierne ligger mellem \\(c\\) og \\(d\\). Situationen er illustreret i figur¬†1.\n\n\n\n\n\n\nFigur¬†1: Datapunkter i planen, hvor \\(x\\)-v√¶rdierne ligger mellem \\(a\\) og \\(b\\), mens \\(y\\)-v√¶rdierne ligger mellem \\(c\\) og \\(d\\).\n\n\n\nId√©en er nu, at vi skalerer, s√• afstandene langs \\(x\\)-aksen f√•r samme v√¶gt som afstandene langs \\(y\\)-aksen.\nDet vil sige, at vi definerer afstanden fra \\((x_1,y_1)\\) til \\((x_2,y_2)\\) som\n\\[\\sqrt{\\left ( \\frac{x_2-x_1}{b-a} \\right )^2+ \\left ( \\frac{y_2-y_1}{d-c} \\right )^2} \\tag{1}\\]\nDet f√•r den betydning, at hvis \\(x\\)-v√¶rdierne f.eks. kan varierer i et langt bredere interval end \\(y\\)-v√¶rdierne (dvs. at \\(b-a&gt;d-c\\)), s√• bliver forskellen p√• \\(x\\)-v√¶rdierne i ovenst√•ende afstandsm√•l skaleret mere ned (fordi vi kommer til at dividere med et st√∏rre tal).\nEn anden m√•de at forst√• dette afstandsm√•l p√•, er ved at erstatte hvert punkt \\((x_i,y_i)\\) med et nyt skaleret punkt:\n\\[(x_i,y_i)_{Norm}=\\left(\\frac{x_i-a}{b-a}, \\frac{y_i-c}{d-c}\\right) \\tag{2}\\]\nResultatet af at lave denne skalering af punkterne fra figur¬†1 ses i figur¬†2.\n\n\n\n\n\n\nFigur¬†2: Datapunkterne fra figur¬†1, men hvor alle punkter er blevet min-max skaleret ved at bruge formlen i (2).\n\n\n\nBem√¶rk, at datapunkterne nu er skaleret p√• en s√•dan m√•de, at alle \\(x\\)- og \\(y\\)-v√¶rdier ligger mellem \\(0\\) og \\(1\\). Derfor giver det mening, at bruge Pythagoras p√• disse skalerede punkter ‚Äì og g√∏r vi det, f√•r vi\n\\[\\sqrt{\\left(\\frac{x_2-a}{b-a}-\\frac{x_1-a}{b-a}\\right)^2 +\\left(\\frac{y_2-c}{d-c}-\\frac{y_1-c}{d-c}\\right)^2}=\\sqrt{\\left(\\frac{x_2-x_1}{b-a}\\right)^2+\\left(\\frac{y_2-y_1}{d-c}\\right)^2}\\]\nBem√¶rk, at det pr√¶cis er afstandsm√•let i (1), som vi startede ud med."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#mindre-naivt-mere-b√∏vlet-feature-skaling",
    "href": "materialer/afstande/feature_scaling.html#mindre-naivt-mere-b√∏vlet-feature-skaling",
    "title": "Feature-skalering",
    "section": "Mindre naivt, mere b√∏vlet: Feature-skaling",
    "text": "Mindre naivt, mere b√∏vlet: Feature-skaling\nDen skalering vi har pr√¶senteret i (2) benytter ikke som s√•dan information om data, men kun om den mindste og st√∏rste v√¶rdi, som henholdsvis \\(x\\)- og \\(y\\)-v√¶rdierne ligger imellem (nemlig \\(a\\) og \\(b\\) for \\(x\\)-v√¶rdierne og \\(c\\) og \\(d\\) for \\(y\\)-v√¶rdierne).\nEt alternativ til dette er at bruge alle data til at bestemme skaleringen (og ikke kun den st√∏rste og den mindste v√¶rdi). Dette kaldes for feature-skaling, n√•r vi arbejder med perceptroner eller neurale netv√¶rk.\nHvis de data, der skal l√¶res fra ‚Äì det vil sige tr√¶ningsdata ‚Äì er \\[(x_1,y_1), (x_2,y_2),\\ldots, (x_n,y_n)\\] s√• skalerer vi langs f√∏rsteaksen ved, at\n\nudregne et estimat for middelv√¶rdien af \\(x\\): \\[\\bar{x}=\\frac{x_1 + x_2 + \\cdots + x_n}{n}=\\frac{\\Sigma_{i=1}^nx_i}{n} \\tag{3}\\]\nog et estimat for denne variabels spredning: \\[s_x=\\sqrt{\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}{n-1}} \\tag{4}\\]\n\nFeature-skaling af \\(x_i\\) er da \\[\\hat{x}_i=\\frac{x_i-\\bar{x}}{s_x} \\tag{5}\\]\nTilsvarende estimeres middelv√¶rdi og spredning for \\(y\\) og feature-skaling udregnes: \\[\\hat{y}_i= \\frac{y_i-\\bar{y}}{s_y} \\tag{6}\\]\nResultatet af at lave denne feature skalering af punkterne fra figur¬†1 ses i figur¬†3.\n\n\n\n\n\n\nFigur¬†3: Datapunkterne fra figur¬†1, men hvor alle punkter er blevet feature skaleret ved at bruge formlerne i (5) og (6).\n\n\n\nBem√¶rk, hvordan de feature skalerede datapunkter har \\(x\\)- og \\(y\\)-v√¶rdier, som alle1 ligger mellem \\(-2\\) og \\(2\\).\n1¬†I virkelighedens verden kan der godt v√¶re v√¶rdier, som er mindre end \\(-2\\) eller st√∏rre end \\(2\\), men som oftest vil det v√¶re s√•dan, at omkring \\(95 \\%\\) af v√¶rdierne vil ligge mellem \\(-2\\) og \\(2\\) efter feature skalering.Da alle \\(x\\)- og \\(y\\)-v√¶rdier nu er p√• samme skala, giver det igen mening at beregne den euklidiske afstand mellem disse nye punkter. Betragter vi de skalerede punkter \\((\\hat{x}_1,\\hat{y}_1)\\) og \\((\\hat{x}_2,\\hat{y}_2)\\) s√• bliver den euklidiske afstand mellem dem\n\\[\\begin{align*}\n\\sqrt{(\\hat{x}_2-\\hat{x}_1)^2+(\\hat{y}_2-\\hat{y}_1)^2} &= \\sqrt{\\left(\\frac{x_2-\\bar{x}}{s_x}-\\frac{x_1-\\bar{x}}{s_x}\\right)^2 +\\left(\\frac{y_2-\\bar{y}}{s_y}-\\frac{y_1-\\bar{y}}{s_y}\\right)^2}\\\\\n& =\\sqrt{\\left(\\frac{x_2-x_1}{s_x}\\right)^2+\\left(\\frac{y_2-y_1}{s_y}\\right)^2}\n\\end{align*}\\]\nHvis vi sammenligner med den naive tilgang i (1), er den ikke helt sk√¶v. Der skal bare skaleres med \\(s_x\\) i stedet for \\(b-a\\) og med \\(s_y\\) i stedet for \\(c-d\\).\nBem√¶rk, at den feature skalering, som foretages i (5) og (6), svarer til at standardisere en normalfordelt stokastisk variabel \\(X \\sim N(\\mu, \\sigma)\\):\n\\[ Z = \\frac{X-\\mu}{\\sigma}\\] Derfor vil det ogs√• v√¶re s√•dan, at hvis de oprindelige data er normalfordelte, s√• vil de nye feature skalerede data v√¶re standard normalfordelte (dvs. normalfordelte med middelv√¶rdi \\(0\\) og spredning \\(1\\)). Heraf f√∏lger ogs√•, at cirka \\(95 \\%\\) af de feature skalerede data vil ligge mellem \\(-2\\) og \\(2\\), som bem√¶rket ovenfor."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#eksempel-min-max-og-feature-skalering",
    "href": "materialer/afstande/feature_scaling.html#eksempel-min-max-og-feature-skalering",
    "title": "Feature-skalering",
    "section": "Eksempel: Min-max og feature skalering",
    "text": "Eksempel: Min-max og feature skalering\nVi vil pr√∏ve at se p√• et udvidet eksempel om udgifter til bolig og m√¶lk. Se nedenst√•ende tabel:\n\n\n\n\n\n\nFamilie\n(bolig, m√¶lk)\n\n\n\n\nA\n\\((7500 \\ kr, 200 \\ kr)\\)\n\n\nB\n\\((7500 \\ kr, 1700 \\ kr)\\)\n\n\nC\n\\((6000 \\ kr, 200 \\ kr)\\)\n\n\nD\n\\((5200 \\ kr, 300 \\ kr)\\)\n\n\nE\n\\((8100 \\ kr, 250 \\ kr)\\)\n\n\nF\n\\((6200 \\ kr, 350  \\ kr)\\)\n\n\nG\n\\((7700 \\ kr, 400 \\ kr)\\)\n\n\nH\n\\((5800 \\ kr, 350 \\ kr)\\)\n\n\nI\n\\((7200 \\ kr, 250 \\ kr)\\)\n\n\nJ\n\\((6800 \\ kr, 400  \\ kr)\\)\n\n\n\n\n\nTabel¬†1: Udvidet eksempel om udgifter til bolig og m√¶lk.\n\n\n\nDatapunkterne fra tabellen ses indtegnet i figur¬†4. De tre f√∏rste familier \\(A\\), \\(B\\) og \\(C\\) fra det tidligere eksempel er markeret. Bem√¶rk, at vi tidligere har beregnet, at afstanden mellem \\(A\\) og \\(B\\) er den samme som afstanden mellem \\(A\\) og \\(C\\), hvilket ogs√• fremg√•r af figur¬†4.\n\n\n\n\n\n\nFigur¬†4: Datapunkterne fra eksemplet i tabel¬†1 omkring udgifter til bolig og m√¶lk.\n\n\n\nVi vil nu lave b√•de min-max skalering samt feature skalering af punkterne i tabel¬†1. For at lave min-max skalering f√•r vi brug for mindste- og st√∏rstev√¶rdi for b√•de \\(x\\)- (bolig) og \\(y\\)-v√¶rdierne (m√¶lk). De er: \\[\\begin{align}\na&=5200 \\quad  &\\text{og} \\quad \\quad &b=8100 \\\\\nc&=200 \\quad  &\\text{og} \\quad \\quad &d=1700 \\\\\n\\end{align}\\] Bruges disse v√¶rdier samt formlerne i (2) f√•s punkterne som ses i figur¬†5.\n\n\n\n\n\n\nFigur¬†5: Datapunkterne fra eksemplet omkring udgifter til bolig og m√¶lk efter min-max skalering.\n\n\n\nL√¶g m√¶rke til hvordan afstanden mellem \\(A\\) og \\(C\\) med denne skalering er blevet mindre end afstanden mellem \\(A\\) og \\(B\\), som √∏nsket.\nVi vil nu pr√∏ve at lave en feature skalering af punkterne i tabel¬†1. Vi f√•r derfor brug for et estimat for middelv√¶rdien af henholdvis \\(x\\) og \\(y\\). De kan beregnes ved hj√¶lp af (3) til:\n\\[\\begin{align}\n\\bar{x} &= 6800 \\\\\n\\bar{y} &= 440\n\\end{align}\\]\nBruges (4) f√•s et estimat for spredningen for henholdsvis \\(x\\) og \\(y\\):\n\\[\\begin{align}\n\\bar{s_x} &= 954.52 \\\\\n\\bar{s_y} &= 448.95\n\\end{align}\\]\nAnvendes formlerne i (5) og (6) til feature skalering af punkterne i tabel¬†1 f√•s punkterne, som ses i figur¬†6.\n\n\n\n\n\n\nFigur¬†6: Datapunkterne fra eksemplet omkring udgifter til bolig og m√¶lk efter feature skalering.\n\n\n\nL√¶g her m√¶rke til at afstanden mellem \\(A\\) og \\(B\\) nu er blevet endnu st√∏rre end afstanden mellem \\(A\\) og \\(C\\). Det ses ogs√•, at \\(y\\)-v√¶rdien for det skalerede punkt for familie \\(B\\) er cirka \\(2.8\\). Sammenlignes dette med standard normalfordelingen, kan vi se, at der er tale om en forholdsvis ekstrem v√¶rdi2. Det vil sige, at vi ud fra de skalerede v√¶rdier kan se, at familien \\(B\\)‚Äôs m√¶lkeforbrug p√• \\(1700\\) kroner rent faktisk er us√¶dvanligt sammenlignet med de andre families m√¶lkeforbrug.\n\n\n2¬†Husk p√• at for en standard normalfordelt stokastisk variabel vil omkring \\(95 \\%\\) af v√¶rdierne ligge mellem \\(-2\\) og \\(2\\)."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemStrenge.html",
    "href": "materialer/afstande/AfstandeMellemStrenge.html",
    "title": "Afstande mellem ord",
    "section": "",
    "text": "Et ord er en f√∏lge eller en streng af bogstaver eller tal. Det kunne for eksempel v√¶re 12DvbdN34fdg eller hnaikgoh (nej, det beh√∏ver ikke give mening). Det kunne ogs√• v√¶re en DNA-sekvens, et ord i en tekst eller noget helt andet1. Man siger, at l√¶ngden af en streng er antallet af bogstaver i strengen.\n1¬†Ofte g√∏r man det desuden bin√¶rt, s√• det er en streng af \\(0\\) og \\(1\\) s√•som \\(00110110.\\) Det er fornuftigt nok, eftersom computere opererer med den slags strenge.Vi vil i det f√∏lgende se p√• s√•kaldte edit-afstande, som basalt set t√¶ller, hvor mange √¶ndringer, man skal lave, for at komme fra den ene streng til den anden. Det kommer naturligvis til at afh√¶nge af, hvilke typer √¶ndringer, man tillader. Lad os her se p√• nogle af dem.\n\nHammingafstanden\nHammingafstanden mellem to lige lange strenge er antallet af pladser, hvor de to strenge er forskellige. Afstanden fra sne til sno er derfor \\(1\\). Afstanden fra sne til neg er \\(3\\), fordi de to strenge er forskellige p√• alle pladser. Det svarer til, at man m√• √¶ndre et bogstav ad gangen:\n\\[ sne \\rightarrow nne \\rightarrow nee \\rightarrow neg\\] Dette er illustreret i figur¬†1 ved de tre gr√∏nne kanter fra sne til neg.\n\n\n\n\n\n\nFigur¬†1: Hver knude i figuren svarer til et ord. En kant imellem to knuder svarer til, at der findes et \"move\" mellem de to ord enten ved hj√¶lp af Hamming-, Levenshtein- eller Damerau-Levenshteinafstanden (angivet med henholdsvis gr√∏n, lilla og pink).\n\n\n\n\n\nLevenshteinafstanden\nLevenshteinafstanden har flere tilladte √¶ndringer: Man m√• √¶ndre bogstaver, som i Hamming, men man m√• ogs√• inds√¶tte og fjerne bogstaver. Levenshteinafstanden er det mindste antal s√•danne √¶ndringer, man skal lave for at n√• fra det ene ord til det andet. Ordene/strengene beh√∏ver ikke have samme l√¶ngde - man kan jo inds√¶tte og fjerne bogstaver.\nSe p√• figur¬†1:\n\nAfstanden fra sne til see er \\(1\\), ligesom Hammingafstanden.\nAfstanden fra sne til sneg er ogs√• \\(1\\), fordi vi blot har tilf√∏jet et g ‚Äì og her er Hammingafstanden slet ikke meningsfuld. Den er simpelthen ikke defineret.\nAfstanden fra sne til neg er \\(2\\) ‚Äì via disse √¶ndringer:\n\\[sne \\rightarrow sneg \\rightarrow neg\\]\nHammingafstanden, som vi fandt ovenfor, er \\(3\\).\n\nBem√¶rk, at vi i ovenst√•ende eksempel ogs√• kunne have valgt\n\\[sne \\rightarrow ne \\rightarrow neg\\] som ogs√• har \\(2\\) \"moves\".\nJo flere tilladte √¶ndringer, jo kortere afstand. Der er algoritmer, der finder den mindste vej mellem to ord ‚Äì det er dog ikke helt s√• klart, hvordan man regner den ud, som det er for Hammingafstanden.\n\n\nDamerau-Levenshteinafstanden\nDamerau-Levenshteinafstanden er som Levenshtein, men man tillader nu ogs√• ombytning af to bogstaver, som st√•r ved siden af hinanden. Hvis man skriver teskt p√• en telefon eller pc, er det let at bytte om p√• den m√•de. Hvis man s√• har en liste over ord, der giver mening, kan man opdage, at teskt ikke giver mening, men at ordet tekst ligger meget t√¶t p√• - afstand \\(1\\) i Damerau-Levenshteinafstand ‚Äì og \\(2\\) i Hamming- eller Levenshteinafstand. Ordet teske har ogs√• Hammingafstand \\(1\\) til teskt, s√• man kan ikke v√¶re sikker p√•, hvad det oprindelige var.\nI figur¬†2 ses et eksempel p√• hvilke \"moves\", der er tilladt mellem forskellige ord ved hj√¶lp af Hamming-, Levenshtein- eller Damerau-Levenshteinafstanden.\n\n\n\n\n\n\nFigur¬†2: Hver knude i figuren svarer til et ord. En kant imellem to knuder svarer til, at der findes et \"move\" mellem de to ord enten ved hj√¶lp af Hamming-, Levenshtein- eller Damerau-Levenshteinafstanden (angivet med henholdsvis gr√∏n, lilla og pink).\n\n\n\n\n\nAfstande mellem navne\nNavne som Peter, Pieter, Pietro, Petrus, Peder, Per, Pelle, Pekka, Peer, Petur, Pedro, Pierre, Pjotr, Pyotr, Petar eller m√•ske Katarina, Katharina, Katrina, Katrine, Katrin, Cathryn, Kathryn, Catherine har samme oprindelse. Der er stor forskel p√•, hvor hyppigt, de optr√¶der i forskellige lande. Overvej, om edit-afstandene ovenfor kan bruges til for eksempel at afsl√∏re, hvor t√¶t p√• hinanden lande med Peter som hyppigst, er p√• lande med Pyotr."
  },
  {
    "objectID": "materialer/logistisk/log-reg.html",
    "href": "materialer/logistisk/log-reg.html",
    "title": "Logistisk regression",
    "section": "",
    "text": "Logistisk regression, i den betydning vi betragter i denne note, bruges, n√•r man gerne vil modellere, hvordan en sandsynlighed afh√¶nger af en anden variabel. Som et eksempel forestiller vi os, at vi vil unders√∏ge, hvordan sandsynligheden for at lide af hjerte-kar-sygdom afh√¶nger af det systoliske blodtryk. Vi kigger derfor p√• et datas√¶t best√•ende af 2000 mennesker, som har f√•et m√•lt deres blodtryk. Desuden har de f√•et unders√∏gt, om de lider af hjerte-kar-sygdom. Datas√¶ttet er fiktivt, men det er lavet til at ligne virkelige data1. Vi kalder blodtrykket for \\(x\\), mens vi lader \\(y\\) v√¶re en variabel, der er \\(1\\) hvis personen lider af hjertekarsygdom og \\(0\\) ellers. P√• figur¬†1 har vi tegnet samh√∏rende \\(x\\)- og \\(y\\)-v√¶rdier ind i et koordinatsystem for de f√∏rste 200 personer i datas√¶ttet.\nFigur¬†1: Her ses et plot af data med blodtryk p√• \\(x\\)-aksen og sygdomsstatus p√• \\(y\\)-aksen.\nDet ses p√• figur¬†1, at der er flest personer med \\(y\\)-v√¶rdien 0, alts√• raske personer, blandt folk med lavt blodtryk, mens der er flest med \\(y\\)-v√¶rdien 1, svarende til syge, blandt folk med h√∏jt blodtryk. Ved de fleste blodtryksv√¶rdier er der dog b√•de syge og raske, og det er sv√¶rt at f√• et pr√¶cist overblik ud fra figuren.\nS√• hvordan kan man beskrive sammenh√¶ngen mellem \\(x\\) og \\(y\\)? I stedet for at se direkte p√• sammenh√¶ngen mellem \\(x\\) og \\(y\\), vil vi se p√• hvordan sandsynligheden for hjerte-kar-sygdom afh√¶nger af blodtrykkket. Vi vil betragte denne sandsynlighed som en funktion \\(p(x)\\) af blodtrykket \\(x\\). Vi vil nu se p√•, hvordan man kan modellere denne funktion.\nFor at f√• en id√© om, hvordan \\(p(x)\\) kunne se ud, kigger vi p√• datas√¶ttet fra f√∏r. Vi inddeler blodtrykket i intervaller af l√¶ngde 25 og t√¶ller op, hvor mange syge og raske der er inden for hvert interval.\nDesuden har vi beregnet, hvor stor en andel af patienterne inden for hvert interval, der lider af hjerte-kar-sygdom. Dette bruges som et estimat for sandsynligheden for hjerte-kar-sygdom i den gruppe. For eksempel er der 194 raske og 42 syge personer med et blodtryk i intervallet \\(]100,125]\\). Sammenlagt er der \\(194+42=236\\) personer i dette interval. Andelen af syge i denne gruppe er derfor \\[\n\\frac{42}{236} \\approx 0.178.\n\\] P√• figur¬†2 har vi tegnet disse andele ind i et koordinatsystem. For hvert blodtryksinterval er midtpunktet for intervallet indtegnet som \\(x\\)-v√¶rdien, og andelen af syge er indtegnet som den tilh√∏rende \\(y\\)-v√¶rdi.\nFigur¬†2: Andel syge inden for hvert blodtryksinterval.\nUmiddelbart kunne det v√¶re fristende at lave line√¶r regression. Vi forestiller os alts√• en forskrift \\[\n    p(x) = ax + b.\n\\] P√• figur¬†3 har vi indtegnet den bedste rette linje i figur¬†2.\nFigur¬†3: Grafen for \\(p(x)\\) tiln√¶rmet med en ret linje.\nDer er et problem her: En sandsynlighed ligger altid mellem 0 og 1, men regressionslinjen ovenfor sk√¶rer \\(x\\)-aksen ved blodtryksv√¶rdier omkring 70. Det betyder, at sandsynligheden er negativ for blodtryk under 70. Tilsvarende f√•r vi sandsynligheder, der er st√∏rre end 1 ved blodtryk over 300. Det giver selvf√∏lgelig ikke mening.\nHvis vi kigger p√• figur¬†3 igen, ser sammenh√¶ngen da heller ikke line√¶r ud, men snarere S-formet. I stedet for en ret linje, ville det give mening at lade \\(p\\) v√¶re en funktion med en S-formet graf som indtegnet i figur¬†4.\nFigur¬†4: Graf for \\(p(x)\\) tiln√¶rmet med en S-formet kurve."
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#odds",
    "href": "materialer/logistisk/log-reg.html#odds",
    "title": "Logistisk regression",
    "section": "Odds",
    "text": "Odds\nFor at komme n√¶rmere hvordan funktionsforskriften for \\(p\\) skal se ud, kigger vi p√• oddsene for sygdom i stedet for sandsynligheden2. Oddsene \\(O\\) for en h√¶ndelse er defineret som sandsynligheden \\(p\\) for h√¶ndelsen divideret med sandsynligheden for, at h√¶ndelsen ikke indtr√¶ffer (det kaldes ogs√• for komplement√¶rh√¶ndelsen), som er \\(1-p\\). Alts√• er \\[\n    O=\\frac{p}{1-p}.\n\\] Odds m√•ler s√•ledes, hvor mange gange mere sandsynlig en h√¶ndelsen er i forhold til komplement√¶rh√¶ndelsen. Hvis for eksempel sandsynligheden for hjerte-kar-sygdom er \\(p=\\frac{4}{5}\\), s√• er odds for sygdom \\[\n    O=\\frac{\\frac{4}{5}}{\\frac{1}{5}} = 4.\n\\] Det er alts√• fire gange s√• sandsynligt at v√¶re syg som at v√¶re rask.\n2¬†Du kender m√•ske begrebet odds fra sportsgambling. Det er dog en anden betydning af ordet, end det vi bruger her. Inden for gambling angiver odds, hvor mange gange man f√•r pengene igen, hvis en bestemt h√¶ndelse indtr√¶ffer(for eksempel at et bestemt hold vinder). Gambling odds er naturligvis ogs√• udregnet ud fra sandsynligheden for h√¶ndelsen, men de er altid justeret for at sikre, at bookmakeren vinder i det lange l√∏b.For at f√• en lidt bedre fornemmelse for, hvordan odds fungerer, kan vi lave en tabel, der viser odds svarende til forskellige v√¶rdier af \\(p\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)\n\\(\\frac{1}{5}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{2}{3}\\)\n\\(\\frac{3}{4}\\)\n\\(\\frac{4}{5}\\)\n\n\n\n\n\\(O\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{2}\\)\n1\n2\n3\n4\n\n\n\nL√¶g her m√¶rke til at jo st√∏rre \\(p\\) bliver, desto st√∏rre bliver odds \\(O\\) ogs√•.\nFunktionen, der omdanner sandsynligheder til odds, har forskriften \\[\nO(p) = \\frac{p}{1-p}.\n\\] Definitionsm√¶ngden for \\(O\\) er \\(]0,1[\\). Grafen for \\(O\\) er vist p√• figur¬†5.\n\n\n\n\n\n\n\n\nFigur¬†5: Grafen for odds-funktionen.\n\n\n\n\n\nVi kan se p√• figur¬†5, at odds-funktionen \\(O\\) er voksende. Det kan ogs√• let bevises ved at vise, at \\(O'(p)&gt;0\\) for alle v√¶rdier af \\(p\\). Vi ser desuden, at \\(O(p)\\) altid er positiv, da b√•de t√¶ller og n√¶vner er positive. N√•r \\(p\\) n√¶rmer sig \\(0\\), n√¶rmer t√¶lleren sig \\(0\\), mens n√¶vneren n√¶rmer sig \\(1\\), s√• \\(O(p)\\) g√•r mod \\(0\\). N√•r \\(p\\) n√¶rmer sig \\(1\\), n√¶rmer t√¶lleren sig \\(1\\), og n√¶vneren n√¶rmer sig \\(0\\), s√• hele br√∏ken \\(O(p)\\) g√•r mod uendelig. V√¶rdim√¶ngden for \\(O\\) best√•r derfor af alle de positive reelle tal.\n\n\n\n\n\n\nOpgave 1\n\n\n\n\n\n\nLad \\(p=4/7\\). Hvad er de tilh√∏rende odds?\nAntag at oddsene er \\(O=3/2\\). Hvad er den tilsvarende sandsynlighed?\nVis at oddsfunktionen \\(O(p)\\) er voksende ved at differentiere og indse at \\(O'(p)&gt;0\\).\n\nFodboldholdene AFC og BFC spiller mod hinanden. Der spilles med forl√¶nget spilletid og straffesparkskonkurrence indtil, der er fundet en vinder. Det er dobbelt s√• sandsynligt, at AFC vinder som, at BFC vinder.\n\nHvad er (de matematiske) odds for at AFC vinder?\nHvad er sandsynligheden for at AFC vinder?"
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#den-logistiske-regressionsmodel",
    "href": "materialer/logistisk/log-reg.html#den-logistiske-regressionsmodel",
    "title": "Logistisk regression",
    "section": "Den logistiske regressionsmodel",
    "text": "Den logistiske regressionsmodel\nI vores dataeksempel, hvor sandsynligheden for hjerte-kar-sygdom er en funktion \\(p(x)\\), bliver oddsene for hjerte-kar-sygdom ogs√• en funktion af \\(x\\) \\[\nO(p(x)) = \\frac{p(x)}{1-p(x)}.\n\\] P√• figur¬†6 vises dataeksemplet fra f√∏r, men nu har vi oddsene \\(O(p(x))\\) p√• \\(y\\)-aksen.\n\n\n\n\n\n\n\n\nFigur¬†6: Odds for hjerte-kar-sygdom inden for de forskellige blodtryksintervaller.\n\n\n\n\n\nVi ser, at oddsene for sygdom stiger med blodtrykket. Det betyder derfor ogs√•, at sandsynligheden for sygdom stiger med blodtrykket. Kigger vi p√• grafen, ser tendensen ikke line√¶r ud. Det kunne derimod ligne en eksponentiel v√¶kst. For at bekr√¶fte dette, laver vi samme plot p√• figur¬†7, men nu med den naturlige logaritme til oddsene \\(\\ln (O(p(x)))\\) p√• \\(y\\)-aksen3.\n3¬†Man kan nemlig vise, at hvis \\(y\\) afh√¶nger eksponentielt af \\(x\\), s√• vil \\(\\ln(y)\\) afh√¶nge line√¶rt af \\(x\\).\n\n\n\n\n\n\n\nFigur¬†7: Den naturlige logaritme til odds for hjerte-kar-sygdom inden for de forskellige blodtryksintervaller.\n\n\n\n\n\nDer ser nu ud til at v√¶re en line√¶r sammenh√¶ng! Det kunne alts√• tyde p√•, at ln-oddsene afh√¶nger line√¶rt af \\(x\\). Det leder os frem til f√∏lgende model for ln-oddsene: \\[\n\\ln (O(p(x))) = ax  + b.\n\\tag{1}\\] Denne model kaldes den logistiske regressionsmodel. Virkelige data f√∏lger ofte en logistisk regressionsmodel.\n\n\n\n\n\n\nOpgave 2\n\n\n\n\n\nI denne opgave ser vi p√• sandsynligheden \\(p(x)\\) for at lide af forh√∏jet blodtryk4 som funktion af kolestreroltallet \\(x\\). Vi kigger derfor p√• datas√¶ttet nedenfor, som er en udvalgt del af et virkeligt datas√¶t. I tabellen angiver \\(y=1\\) forh√∏jet blodtryk, mens \\(y=0\\) angiver normalt blodtryk.\n\nLav en tabel, hvor du beregner sandsynligheden for forh√∏jet blodtryk, odds og ln(odds) inden for hvert interval.\nIndtegn punkter i et koordinatsystem, hvor \\(x\\)-v√¶rdien er midtpunkterne for intervallerne, og \\(y\\)-v√¶rdien er de tilh√∏rende odds.\nSer sammenh√¶ngen line√¶r ud?\nVil det give mening at bruge en logistisk regression?\n\n\n\n\n\\(x\\)\n\\(y=0\\)\n\\(y=1\\)\n\n\n\n\n]100,150]\n27\n6\n\n\n]150,200]\n693\n202\n\n\n]200,250]\n1354\n571\n\n\n]250,300]\n716\n471\n\n\n]300,350]\n156\n132\n\n\n]350,400]\n20\n23\n\n\n]400,450]\n2\n5\n\n\n\n\n\n\n4¬†Forh√∏jet blodtryk er defineret som systolisk blodtryk h√∏jere end 140mmHg eller diastolisk blodtryk h√∏jere end 90mmHg."
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#logit-funktionen-og-den-logistiske-funktion",
    "href": "materialer/logistisk/log-reg.html#logit-funktionen-og-den-logistiske-funktion",
    "title": "Logistisk regression",
    "section": "Logit-funktionen og den logistiske funktion",
    "text": "Logit-funktionen og den logistiske funktion\nN√•r vi tager den naturlige logaritme til oddsene, f√•r vi \\[\n\\ln (O(p)) = \\ln\\left(\\frac{p}{1-p}\\right).\n\\] Funktionen p√• h√∏jresiden kaldes \\(\\text{logit}\\) og er alts√• givet ved \\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right).\n\\] Definitionsm√¶ngden for \\(\\text{logit}\\)-funktionen er ligesom for oddsene \\(]0,1[\\). Vi fandt tidligere, at v√¶rdim√¶ngden for oddsene best√•r af alle de positive reelle tal. Dette er netop definitionsm√¶ngden for \\(\\ln\\). V√¶rdim√¶ngden for \\(\\text{logit}\\) bliver derfor den samme som for \\(\\ln\\), nemlig alle de reelle tal. Grafen for \\(\\text{logit}\\) er vist p√• figur¬†8.\n\n\n\n\n\n\n\n\nFigur¬†8: Grafen for logit-funktionen.\n\n\n\n\n\nDen logistiske regressionsmodel i (1) kan skrives ved hj√¶lp at \\(\\text{logit}\\)-funtionen som \\[\n\\text{logit}(p(x)) = \\ln (O(p(x))) = ax  + b.\n\\tag{2}\\] Egentlig var vi jo ude p√• at finde et udtryk for sandsynligheden \\(p(x)\\) som funktion af \\(x\\). Vi pr√∏ver derfor at isolere \\(p(x)\\) i (2). For at g√∏re det, finder vi f√∏rst den omvendte (eller inverse) funktion til \\(\\text{logit}\\). Vi antager derfor, at \\[\ny = \\text{logit(p)} = \\ln\\left( \\frac{p}{1-p} \\right).\n\\] Vi skal s√• isolere \\(p\\) for at udtrykke \\(p\\) som funktion af \\(y\\). Vi tager f√∏rst eksponentialfunktionen p√• begge sider af udtrykket og f√•r \\[\ne^y =  \\frac{p}{1-p}.\n\\] S√• ganger vi med \\((1-p)\\) p√• begge sider. Det giver \\[\ne^y(1-p) =p.\n\\] Hvis parentesen oph√¶ves, f√•r vi \\[\ne^y - p\\cdot e^y =p.\n\\]\nVi kan s√• l√¶gge \\(p\\cdot e^y\\) til p√• begge sider og s√¶tte \\(p\\) uden for parantes. Det giver \\[\\begin{align*}\ne^y &= p \\cdot e^y+p  \\\\\ne^y &= p\\cdot (e^y+1).\n\\end{align*}\\] Endelig kan vi isolere \\(p\\) og f√• \\[\n\\frac{e^y}{e^y+1} =p.\n\\] Her er \\(p\\) egentlig isoleret, men vi kan v√¶lge at forkorte br√∏ken med \\(e^y\\) for at f√• et andet udtryk for \\(p\\) \\[\np=\\frac{\\frac{e^y}{e^y}}{\\frac{e^y+1}{e^y}}=\\frac{\\frac{e^y}{e^y}}{\\frac{e^y}{e^y}+\\frac{1}{e^y}}    =\\frac{1}{1+e^{-y}} .\n\\]\nSammenlagt har vi vist, at den inverse funktion til logit er den standard logistiske funktion (ogs√• nogle gange kaldet sigmoid-funktionen) \\[\nf(y) = \\frac{1}{1+e^{-y}}.\n\\] Grafen for den standard logistiske funktion er indtegnet i figur¬†9. Vi ser, at grafen har en karakteristisk S-form, som vokser fra \\(0\\) mod \\(1\\).\n\n\n\n\n\n\n\n\nFigur¬†9: Grafen for den standard logistiske funktion\n\n\n\n\n\nBruger vi den inverse til \\(\\text{logit}\\) p√• begge sider af lighedstegnet i den logistiske regressionsmodel i (2), f√•r vi isoleret \\(p(x)\\) \\[\np(x) = \\frac{1}{1+e^{-(ax+b)}}.\n\\tag{3}\\] Det ligner alts√• den standard logistiske funktion, men med \\((ax+b)\\) indsat p√• \\(y\\)‚Äôs plads. Denne funktion kaldes den generelle logistiske funktion."
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#fortolkning-af-parametrene-i-den-logistiske-regressionsmodel",
    "href": "materialer/logistisk/log-reg.html#fortolkning-af-parametrene-i-den-logistiske-regressionsmodel",
    "title": "Logistisk regression",
    "section": "Fortolkning af parametrene i den logistiske regressionsmodel",
    "text": "Fortolkning af parametrene i den logistiske regressionsmodel\nHvordan skal vi forst√• betydningen af konstanterne \\(a\\) og \\(b\\) i den logistiske regressionsmodel? Hvis man har en funktion \\(f\\), s√• svarer funktionen \\(f(ax)\\) til, at v√¶ksthastigheden er blevet sat op med en faktor \\(a\\). Alternativt kan man forestille sig, at \\(x\\)-aksen er blevet skaleret med en faktor \\(1/a\\). Grafen for \\(f(x-k)\\) svarer til, at man har forskudt grafen med \\(k\\) enheder i \\(x\\)-aksens retning. Hvis man kombinerer disse, kan man indse, at \\(f(ax+b)\\) svarer til f√∏rst at √∏ge v√¶ksthastigheden med en faktor \\(a\\) og derefter forskyde grafen med \\(k=\\frac{-b}{a}\\) i \\(x\\)-aksens retning, idet \\[\nf(ax+b)=f\\Big(a\\cdot \\Big(x-\\Big(\\frac{-b}{a}\\Big)\\Big)\\Big).\n\\]\nHvis man g√∏r dette for den standard logistiske funktion, f√•r man netop den generelle logistiske funktion \\[\nf(ax+b)= \\frac{1}{1+e^{-(ax+b)}}.\n\\] Sammenlignet med den standard logistiske funktion f√•r man alts√• en \\(S\\)-formet kurve, der vokser \\(a\\) gange s√• hurtigt og er forskudt med \\(\\frac{-b}{a}\\).\n\n\n\nI app‚Äôen herunder ser du grafen for \\(f(x)=\\frac{1}{1+e^{-(ax+b)}}\\). Hvis du tr√¶kker i skyderne for \\(a\\) og \\(b\\), kan du se, hvordan kurven √¶ndrer form. Den stiplede linje har ligning \\(x=\\frac{-b}{a}\\) og svarer alts√• til den vandrette forskydning af grafen for den standard logistiske funktion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI den logistiske regressionsmodel (3) er \\(p(x)\\) givet ved en generel logistisk funktion. Det var en generel logistisk funktion, der blev brugt til at lave den S-formede kurve p√• figur¬†4.\nEn anden m√•de at fortolke konstanterne \\(a\\) og \\(b\\) p√• er ved at g√• tilbage til at se p√• oddsene. For at f√• et udtryk for oddsene i den logistiske regressionsmodel, kan vi anvende eksponentialfunktionen p√• begge sider i (1) og f√• \\[\nO(x) = e^{ax + b} = e^b \\cdot e^{ax}=e^b \\cdot (e^{a})^x.\n\\] Hvis \\(e^b\\) kaldes \\(b_{ny}\\), og \\(e^{a}\\) kaldes \\(a_{ny}\\), ses at \\[\nO(x)=b_{ny}\\cdot a_{ny}^x\n\\] er en eksponentiel udvikling med fremskrivningsfaktor \\(a_{ny}=e^a\\). Derved vil odds for sygdom stige med \\(r=e^a-1\\) procent, hver gang blodtrykket stiger med \\(1\\)mmHg. Hvis \\(a\\) er positiv, er \\(e^a&gt;1\\), og oddsene vokser alts√• eksponentielt. Hvis derimod \\(a\\) er negativ, er \\(e^a&lt;1\\), og dermed aftager oddsene eksponentielt.\nI forbindelse med logistisk regression kaldes \\(e^a\\) ogs√• for odds-ratioen. For at forst√• hvorfor, kan vi forestille os to patienter, en med blodtryk \\(x_1\\) og en med blodtryk \\(x_2\\). De har dermed oddsene \\[\\begin{align*}\nO(x_1) &= e^b \\cdot e^{ax_1}\\\\\nO(x_2) &= e^b \\cdot e^{ax_2}.\n\\end{align*}\\] Lad os se p√• forholdet (ratioen) mellem de to personers odds: \\[\n\\frac{O(x_1)}{O(x_2)} = \\frac{e^b \\cdot e^{ax_1}}{ e^b \\cdot e^{ax_2}} = \\frac{e^{ax_1}}{e^{ax_2}} = e^{ax_1 - ax_2} = e^{a(x_1 - x_2)} = (e^{a} )^{x_1-x_2}.\n\\] Forholdet mellem oddsene afh√¶nger alts√• kun af forskellen \\(x_1 - x_2\\) mellem de to personers blodtryk. Hvis person 1 er har et blodtryk, der er 1mmHg h√∏jere end person 2, bliver forholdet (ratioen) mellem oddsene lige pr√¶cis \\(e^a\\).\n\n\n\n\n\n\nOpgave 3\n\n\n\n\n\nGrafen for den generelle logitiske funktion med forskrift \\[\nf(x)=\\frac{1}{1+e^{-(ax+b)}}\n\\] er stejlest, n√•r funktionsv√¶rdien er \\(f(x)=1/2\\).\n\nHvilken v√¶rdi af \\(x\\) svarer til en funktionsv√¶rdi p√• \\(1/2\\) (\\(x\\) kan udtrykkes ved hj√¶lp af \\(a\\) og \\(b\\)).\n\nI et (fiktivt) dataeksempel ser vi p√• sandsynligheden \\(p(x)\\) for, at en kunde i et supermarked v√¶lger at k√∏be den √∏kologiske m√¶lk frem for den konventionelle som funktion af kundens √•rlige indt√¶gt \\(x\\) (i 100.000 kr). Vi kommer frem til f√∏lgende logistiske regressionsmodel \\[\n\\text{logit}(p(x))= -1.3+0.5x.\n\\]\n\nTegn grafen for \\(p(x)\\).\nHvor mange procent stiger odds for at v√¶lge √∏kologisk, n√•r √•rsindt√¶gten stiger med 100.000kr?"
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#maksimum-likelihood-estimation",
    "href": "materialer/logistisk/log-reg.html#maksimum-likelihood-estimation",
    "title": "Logistisk regression",
    "section": "Maksimum likelihood estimation",
    "text": "Maksimum likelihood estimation\nI den logistiske regressionsmodel \\[\n\\ln (O(x)) = \\text{logit}(p(x)) = ax  + b.\n\\] indg√•r to ukendte konstanter \\(a\\) og \\(b\\). Hvis vi har et datas√¶t, hvordan finder vi s√• de v√¶rdier af \\(a\\) og \\(b\\), der passer bedst til vores data?\nVed at se p√• figur¬†7 kunne man fristes til at benytte line√¶r regression til at finde \\(a\\) og \\(b\\). Bem√¶rk dog, at hvert punkt egentlig er beregnet ud fra fra flere observationer, som ikke har samme \\(x\\)-v√¶rdi. Med et lille datas√¶t ville det slet ikke v√¶re muligt at lave en intervalinddeling som i tabel¬†1, uden at der kommer meget f√• personer i nogle grupper. Begge dele g√∏r, at de beregnede v√¶rdier af \\(\\ln(O(x))\\) bliver meget upr√¶cise, og det samme g√∏r estimaterne for \\(a\\) og \\(b\\) derfor.\nI stedet benytter man som regel maksimum likelihood metoden, som er en teknik, der stammer fra statistikken. Kort fortalt er id√©en at v√¶lge de v√¶rdier af \\(a\\) og \\(b\\), der g√∏r vores data s√• sandsynligt som muligt.\nLad os kalde punkterne i vores datas√¶t \\((x_i,y_i)\\), hvor \\(i=1,\\ldots,n\\) er en nummerering af datapunkterne. Her angiver \\(x_i\\) alts√• blodtrykket hos den \\(i\\)‚Äôte person, og \\(y_i\\) er en variabel, der antager v√¶rdien \\(1\\) hvis \\(i\\)‚Äôte person har hjerte-kar-sygdom og er \\(0\\) ellers. For hvert par \\((x_i,y_i)\\) kan vi nu fors√∏ge at beregne sandsynligheden \\(p_i\\) for at \\(i\\)‚Äôte person faktisk har den sygsomsstatus \\(y_i\\), som vi observerer. Hvis den \\(i\\)‚Äôte person er syg, dvs. \\(y_i=1\\), er \\(p_i\\) alts√• sandsynligheden for at v√¶re syg, n√•r blodtrykket er \\(x_i\\), s√• \\[\np_i= p(x_i)  = \\frac{ 1}{1 + e^{-(ax_i  + b)}}.\n\\tag{4}\\] Hvis patienten er rask, alts√• \\(y_i=0\\), er \\(p_i\\) sandsynligheden for at v√¶re rask, n√•r blodtrykket er \\(x_i\\), det vil sige \\[\np_i=1-  p(x_i)  = 1- \\frac{ 1}{1 + e^{-(ax_i  + b)}}.\n\\tag{5}\\] Vi kan opskrive et samlet udtryk for \\(p_i\\) uden at opdele efter v√¶rdien af \\(y_i\\), nemlig \\[\np_i= p_i(x_i)^{y_i}(1-p(x_i))^{1-y_i}.\n\\tag{6}\\] For at indse dette, ser vi f√∏rst p√• tilf√¶ldet \\(y_i=1\\), hvor (6) giver \\[\np_i = p(x_i)^{1} (1-p(x_i))^{0} = p(x_i).\n\\] For \\(y_i=0\\) giver (6)\n\\[\np_i = p(x_i)^{0} (1-p(x_i))^{1} = 1-p(x_i).\n\\] Det passer alts√• med formlerne i henholdsvis (4) og (5). Bem√¶rk at \\(p_i\\) afh√¶nger af de ukendte v√¶rdier \\(a\\) og \\(b\\). Vi kan alts√• opfatte \\(p_i\\) som en funktion af to variable \\(p_i(a,b)\\).\nNu kigger vi p√• den samlede sandsynlighed for at observere netop de v√¶rdier \\(y_1,\\ldots,y_n\\), som vi faktisk har observeret, n√•r vi ved at patienternes blodtryk er givet ved \\(x_1,\\ldots,x_n\\). Til det form√•l antager vi, at personerne i datas√¶ttet er uafh√¶ngige af hinanden5.\n5¬†Afh√¶ngigheder kan for eksempel opst√•, hvis mange af personerne er i familie med hinanden, g√•r i klasse sammen eller bor i samme by. I s√• fald kan de have noget til f√¶lles, der g√∏r at deres \\(y\\)-v√¶rdier er mere ens end ellers. Familiemedlemmer kan fx have samme arvelige tendens til hjerte-kar-sygdom. Som regel fors√∏ger man at undg√• s√•danne afh√¶ngigheder, n√•r man indsamler data.For at komme videre, er vi n√∏dt til at vide lidt om uafh√¶ngighed af h√¶ndelser: Husk p√• at to h√¶ndelser \\(A\\) og \\(B\\) er uafh√¶ngige, hvis man kan finde sandsynligheden for f√¶llesh√¶ndelsen \\(A\\cap B\\) (at \\(A\\) og \\(B\\) indtr√¶ffer p√• en gang) ved at gange de enkelte sandsynligheder sammen: \\[\n    P(A\\cap B) = P(A)\\cdot P(B).\n\\]\nUafh√¶ngighed af \\(n\\) h√¶ndelser \\(A_1,\\ldots,A_n\\) betyder tilsvarende, at sandsynligheden for, at alle \\(n\\) h√¶ndelser indtr√¶ffer p√• samme tid \\(A_1 \\cap A_2 \\cap \\dotsm \\cap A_n\\), kan findes som et produkt af sandsynlighederne for de enkelte h√¶ndelser6 \\[\n    P(A_1 \\cap A_2 \\cap \\dotsm \\cap A_n) = P(A_1)\\cdot P(A_2) \\cdot \\dotsm \\cdot P(A_n).\n\\]\n6¬†Desuden skal der g√¶lde, at hver gang vi udtager \\(m\\) ud af de \\(n\\) h√¶ndelser, skal sandsynligheden for, at de \\(m\\) h√¶ndelser indtr√¶ffer samtidig, kunne findes ved en tilsvarende produktformel, men dette skal vi ikke bruge i det f√∏lgende.Vi vender nu tilbage til vores data og lader \\(A_1\\) v√¶re h√¶ndelsen at f√∏rste patient har sygdomsstatus \\(y_1\\), \\(A_2\\) v√¶re h√¶ndelsen at anden patient har sygdomsstatus \\(y_2\\) og s√• videre. Bem√¶rk, at \\(P(A_i)\\) er det samme, som det vi tidligere kaldte \\(p_i(a,b)\\). H√¶ndelsen at vi observerer \\(y_1,\\ldots,y_n\\) p√• samme tid, er f√¶llesh√¶ndelsen \\(A_1 \\cap A_2 \\cap \\dotsm \\cap A_n\\). Da vi antog, at de \\(n\\) personer er udvalgt uafh√¶ngigt af hinanden, kan vi bruge produktformlen: \\[\nP(A_1 \\cap A_2 \\cap \\dotsm \\cap A_n) = P(A_1) \\cdot \\dotsm \\cdot P(A_n) = p_1(a,b) \\cdot p_2(a,b)\\cdot \\dotsm \\cdot p_n(a,b).\n\\] Bem√¶rk, at sandsynligheden for vores udfald \\(y_1,\\ldots,y_n\\) afh√¶nger af \\(a\\) og \\(b\\). Den kan derfor betragtes som en funktion af to variable \\[\nL(a,b) = p_1(a,b) \\cdot p_2(a,b)\\cdot \\dotsm \\cdot p_n(a,b).\n\\] Denne funktion kaldes likelihoodfunktionen. Id√©en med maksimum likelihood metoden er at v√¶lge de v√¶rdier af \\(a\\) og \\(b\\), der g√∏r sandsynligheden \\(L(a,b)\\) for det, vi har observeret s√• stor som mulig. Vi s√∏ger alts√• de \\(a\\) og \\(b\\), der maksimerer funktionen \\(L(a,b)\\). I praksis v√¶lger man som regel at maksimere \\(\\log(L(a,b))\\), som kaldes for log-likelihoodfunktionen. Det kommer vi mere ind p√• i det f√∏lgende afsnit. P√• figur¬†10 er det vist, hvordan grafen for en log-likelihoodfunktion kunne se ud. Den viser alts√• logaritmen af sandsynligheden for vores observationer som funktion af \\(a\\) og \\(b\\). Det sorte punkt angiver, hvor funktionsv√¶rdien er st√∏rst. De tilh√∏rende \\(a\\)- og \\(b\\)-koordinater er alts√• dem, der g√∏r vores observationer mest sandsynlige.\n\n\n\n\n\n\n\n\nFigur¬†10: Graf for en log-likelihoodfunktion. Det sorte punkt angiver, hvor funktionsv√¶rdien er st√∏rst.\n\n\n\n\nMaksimum for \\(L(a,b)\\) kan ikke beregnes eksakt. I stedet kan man bruge numeriske metoder, for eksempel gradient nedstigning, som du kan l√¶se mere om her. Man kan ogs√• fors√∏ge at finde kritiske punkter, alts√• punkter, hvor de partielt afledte er nul, ved hj√¶lp af numeriske metoder. Det kan du l√¶se mere om her. I praksis kan optimeringen foretages ved hj√¶lp af Excel som forklaret nedenfor.\nFinder man \\(a\\) og \\(b\\) ved hj√¶lp af maksimum likelihood metoden i vores dataeksempel, f√•s f√∏lgende funktionsudtryk for sandsynlighederne og oddsene \\[\n    p(x) = \\frac{1}{1+e^{-0.022 x +3.9}}, \\qquad O(x)=e^{0.022 x - 3.9}.\n\\]\nGrafen for \\(p\\) er vist p√• figur¬†4. Vi f√•r en odds-ratio p√• \\(e^{0.022} \\approx 1.023\\). Odds for hjerte-kar-sygdom stiger derfor med en faktor 1.023 (alts√• 2.3%), for hver gang blodtrykket stiger med \\(1\\) mmHg.\n\nYderligere omskrivning af likelihoodfunktionen\nVi ser nu lidt n√¶rmere p√•, hvordan man selv kan finde \\(a\\) og \\(b\\), der maksimerer v√¶rdien af likelihoodfunktionen \\(L(a,b)\\). Til det form√•l omskriver vi f√∏rst likelihoodfunktionen til noget, der er lidt nemmere at regne p√•. Vi havde \\[\nL(a,b) = p_1(a,b) \\cdot p_2(a,b)\\cdot \\dotsm \\cdot p_n(a,b).\n\\tag{7}\\] Da \\(\\ln(x)\\) er en voksende funktion, vil \\(L(a,b)\\) have maksimum for de samme v√¶rdier af \\(a\\) og \\(b\\) som den sammensatte funktion \\(l(a,b)=\\ln(L(a,b))\\). Det er alts√• nok at finde de v√¶rdier af \\(a\\) og \\(b\\), der maksimerer \\(l(a,b)\\).\nTager vi logaritmen i (7) og bruger logaritmeregnereglen \\(\\ln(a\\cdot b) = \\ln(a) + \\ln(b)\\), f√•r vi \\[\nl(a,b)=\\sum_{i=1}^n \\ln(p_i(a,b)).\n\\tag{8}\\] Vi fandt i (6), at \\[\np_i(a,b)=p(x_i)^{y_i}\\cdot (1-p(x_i))^{1-y_i}.\n\\] Vi kan nu finde \\(\\ln(p_i(a,b))\\) ved f√∏rst at benytte regnereglen \\(\\ln(a\\cdot b) = \\ln(a) + \\ln(b)\\) og dern√¶st regnereglen \\(\\ln(a^k)=k\\cdot \\ln(a)\\). Det giver \\[\\begin{align*}\n\\ln(p_i(a,b)) &=  \\ln\\big(p(x_i)^{y_i}\\big) + \\ln\\big((1-p(x_i))^{1-y_i}\\big) \\\\\n&= {y_i}\\cdot \\ln(p(x_i))+(1-y_i)\\cdot \\ln(1-p(x_i))\n\\end{align*}\\] Samlet set f√•r vi \\[\n\\begin{aligned}\nl(a,b) &=\\sum_{i=1}^n \\ln(p_i(a,b)) \\\\  &= \\sum_{i=1}^n\\big( {y_i}\\cdot \\ln(p(x_i))+(1-y_i)\\cdot \\ln(1-p(x_i)) \\big).\n\\end{aligned}\n\\tag{9}\\] Dette udtryk kan man nemt selv udregne og maksimere ved hj√¶lp af Excel.\n\n\nBestemmelse af \\(a\\) og \\(b\\) med Excels probleml√∏ser-v√¶rkt√∏j\nFor at finde estimater for \\(a\\) og \\(b\\) ved hj√¶lp af Excel, skal man f√∏rst og fremmest s√∏rge for, at man har aktiveret probleml√∏ser-v√¶rkt√∏jet. Det g√∏res p√• f√∏lgende m√•de: G√• op under filer og v√¶lg indstillinger. Derefter v√¶lges tilf√∏jelsesprogrammer. Nederst kan man v√¶lge Excel-tilf√∏jelsesprogrammer og trykke udf√∏r. Til sidst kan man v√¶lge tilf√∏jelsesprogrammet probleml√∏ser fra en liste.\n\n\n\nIllustration af Excel ark til bestemmelse af a og b samt brug af probleml√∏ser.\n\n\nP√• billedet ses, hvordan man kan lave et lille regneark til at beregne de relevante st√∏rrelser. Der er lavet et par celler til de ukendte parametre \\(a\\) og \\(b\\), som bare kan s√¶ttes til 0 fra starten. Det oprindelige datas√¶t inds√¶ttes i s√∏jlerne \\(x_i\\) og \\(y_i\\). I de n√¶ste s√∏jler beregnes odds, \\(p(x_i)\\) og \\(\\ln(p_i)\\) med formlerne7 \\[\\begin{align*}\nodds &= e^{ax_i + b}\\\\\np(x_i) &= \\frac{e^{ax_i + b}}{1+e^{ax_i +b}} = \\frac{odds}{1+odds}\\\\\n\\ln(p_i)&= {y_i}\\cdot \\ln(p(x_i))+(1-y_i)\\cdot \\ln(1-p(x_i)).\n\\end{align*}\\]\n7¬†I Excel p√• dansk f√•s eksponentialfunktionen ved at skrive EKSP (og EXP i den engelske version) og for at f√• den naturlige logaritmen skriver man LN i begge tilf√¶lde.Her er det vigtigt, at cellerne, der indeholder v√¶rdien af \\(a\\) og \\(b\\), benyttes n√•r oddsene beregnes (det vil v√¶re smart med fastl√•sning af referencerne, hvor man har $ foran b√•de tal og bogstav ved reference). Til sidst finder man \\(l(a,b)\\) i det bl√• felt ved at beregne summen af alle \\(\\ln(p_i)\\), som i formlen (9).\nNu mangler man bare at benytte probleml√∏seren til at finde de v√¶rdier af \\(a\\) og \\(b\\), der g√∏r v√¶rdien i det bl√• felt maksimal. P√• billedet er der vist med r√∏d, hvor man finder probleml√∏seren, og hvad der skal justeres. M√•ls√¶tningen er den bl√• celle, der indeholder summen. Variabelcellerne er de to, der indeholder \\(a\\) og \\(b\\). S√∏rg for ikke at s√¶tte flueben i boksen ‚ÄúG√∏r variabler uden begr√¶nsninger ikke-negative‚Äù. Tryk p√• l√∏s.\n\n\n\n\n\n\nOpgave 4\n\n\n\n\n\nAntag, at vi har tre observationer nedenfor.\n\nOpskriv et udtryk for likelihoodfunktionen \\(L(a,b)\\).\n\n\n\n\n\\(x\\)\n1\n2\n3\n\n\n\n\n\\(y\\)\n1\n1\n0\n\n\n\nEn nyhedshjemmeside √∏nsker at m√•lrette en biografreklame til brugerne. De har derfor registreret om 10 af hjemmesidens brugere har klikket p√• reklamen (\\(y=1\\) hvis de har klikket, \\(y=0\\) ellers) og hvor mange gange \\(x\\), de har l√¶st kulturnyheder den sidste m√•ned. Datas√¶ttet er givet i nedenst√•ende tabel. Firmaet bag hjemmesiden √∏nsker at modellere sandsynligheden \\(p(x)\\) for at klikke p√• reklamen som funktion af \\(x\\), s√• de kan m√•lrette reklamen mod de brugere, der har st√∏rst sandsynlighed for at klikke p√• den.\n\nBrug Excel til at finde \\(a\\) og \\(b\\).\nTegn grafen for \\(p(x)\\).\nSkal firmaet v√¶lge at vise reklamen til brugere, der ofte eller sj√¶ldent l√¶ser kulturnyheder?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\ny\n0\n0\n1\n0\n0\n1\n1\n1\n1\n1"
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#multipel-logistisk-regression",
    "href": "materialer/logistisk/log-reg.html#multipel-logistisk-regression",
    "title": "Logistisk regression",
    "section": "Multipel logistisk regression",
    "text": "Multipel logistisk regression\nI praksis er der selvf√∏lgelig flere faktorer end blodtryk, der afg√∏r ens risiko for hjerte-kar-sygdom. For eksempel stiger risikoen med alderen, ligesom rygning √∏ger risikoen. Vi kan opstille en model, der inddrager alle tre variable p√• en gang. Lader vi \\(x_1\\) betegne blodtryk, \\(x_2\\) betegne alder, og \\(x_3\\) betegne antal cigaretter, man ryger pr. dag, kan vi lave en model, hvor logaritmen til oddsene afh√¶nger af alle tre variable: \\[\n    \\ln \\left ( O(x_1,x_2,x_3)  \\right) = a_1x_1 + a_2x_2 +a_3x_3 + b.\n\\] Nu er der fire ukendte konstanter \\(a_1\\), \\(a_2\\), \\(a_3\\) og \\(b\\) i modellen, som skal bestemmes ud fra data. Dette kaldes den multiple regressionsmodel. Ved at tage eksponentialfunktionen f√•r vi en formel for oddsene \\[\n     O(x_1,x_2,x_3) = e^{a_1x_1 + a_2x_2 +a_3x_3 + b}.\n\\] Man kan ogs√• bruge den standard logistiske funktion og f√• en formel for sandsynligheden \\[\n    p(x_1,x_2,x_3) = \\frac{1}{1+e^{-(a_1x_1 + a_2x_2 +a_3x_3 + b)}}.\n\\]\nHvordan skal vi forst√• denne model? Jo, lad os forestille os en patient med alder \\(x_1\\) og blodtryk \\(x_2\\), som ryger \\(x_3\\) cigaretter om dagen. Hvis vedkommende begynder at ryge \\(1\\) cigaret mere om dagen (og vi forestiller os at alder og blodtryk er u√¶ndret) s√• vil odds-ratioen v√¶re \\[\n    \\frac{O(x_1,x_2,x_3+1)}{O(x_1,x_2,x_3)} = \\frac{ e^{a_1x_1 + a_2x_2 +a_3(x_3+1) + b}}{e^{a_1x_1 + a_2x_2 +a_3x_3 + b}} =e^{a_3}.\n\\] Den ekstra daglige cigaret vil alts√• √∏ge odds for sygdom med en faktor \\(e^{a_3}\\). Tilsvarende har \\(e^{a_1}\\) og \\(e^{a_2}\\) fortolkninger som odds-ratioer, n√•r henholdsvis blodtryk og alder stiger med 1, mens alle andre variable fastholdes. Selv om modellen tager alle tre variable i betragtning, giver odds-ratioerne et m√•l for den individuelle effekt af hver af de tre variable.\nMaximum likelihood metoden kan igen benyttes til at estimere parametrene \\(a_1,a_2,a_3\\) og \\(b\\). Likelihoodfunktionen, som skal maksimeres, bliver nu til en funktion af fire variable. Vi vil ikke g√• i detaljer med, hvordan denne maksimering finder sted.\nFramingham datas√¶ttet er et rigtigt datas√¶t, der indeholder data for hjerte-kar-sygdom og de tre risikofaktorer \\(x_1\\), \\(x_2\\) og \\(x_3\\). Estimerer man \\(a_1,a_2,a_3\\) og \\(b\\) p√• en udvalgt del af dette datas√¶t, f√•r man \\[\n    O(x_1,x_2,x_3) = e^{0.06x_1 + 0.02x_2 + 0.02x_3 -6.77 }.\n\\] Odds for hjerte-kar-sygdom stiger s√•ledes med en faktor \\(e^{0.02}\\approx 1.02\\) (alts√• med 2%), for hver cigaret man ryger om dagen. Tilsvarende stiger odds for sygdom med en faktor \\(e^{0.06}\\approx 1.06\\), for hvert √•r √¶ldre man bliver, og med en faktor \\(e^{0.02}\\approx 1.02\\), for hver gang blodtrykket stiger med 1 mmHg.\n\n\n\n\n\n\nOpgave 5\n\n\n\n\n\nI en multipel regression har man fundet f√∏lgende model for odds \\(O(x_1,x_2)\\) for, at en bruger af en hjemmeside klikker p√• en given reklame, hvor \\(x_1\\) og \\(x_2\\) er antal gange kunden har l√¶st henholdsvis kulturnyheder og sportsnyheder inden for den sidste m√•ned \\[\nO(x_1,x_2) = e^{-2+0.5x_1-0.1x_2 }.\n\\]\n\nEn bruger har l√¶st kulturnyheder 4 gange og sportsnyheder 7 gange inden for den sidste m√•ned. Hvad er odds for, at brugeren klikker p√• reklamen?\nHvad er odds ratioen for kulturnyheder?\nEr sandsynligheden for at klikke p√• reklamen h√∏jere eller lavere blandt brugere, der l√¶ser mange kulturnyheder?"
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#pr√¶diktion",
    "href": "materialer/logistisk/log-reg.html#pr√¶diktion",
    "title": "Logistisk regression",
    "section": "Pr√¶diktion",
    "text": "Pr√¶diktion\nN√•r vi har fundet en god model for sammenh√¶ngen mellem sygdom og forskellige risikofaktorer, kan vi bruge den til at forudsige (pr√¶diktere), om en ny patient er syg. Som eksempel kan vi se p√• den multiple regressionsmodel, hvor risikoen for hjerte-kar-sygdom var givet ved \\[\n    p(x_1,x_2,x_3) = \\frac{1}{1+e^{-(0.06x_1 + 0.02x_2 +0.02x_3 -6.77)}},\n\\] hvor \\(x_1\\) var alderen, \\(x_2\\) var blodtrykket, og \\(x_3\\) var antal cigaretter.\nForestil dig nu, at vi f√•r en ny patient med alderen \\(x_1\\) og blodtrykket \\(x_2\\), som ryger \\(x_3\\) cigaretter om dagen. Vi kan beregne sandsynligheden \\(p(x_1,x_2,x_3)\\) for, at patienten er syg ud fra vores model. Den mest oplagte pr√¶diktionsregel er at pr√¶diktere det mest sandsynlige:\n\nHvis \\(p(x_1,x_2,x_3)&gt;1/2\\): Patienten er syg.\nHvis \\(p(x_1,x_2,x_3)\\leq 1/2\\): Patienten er rask.\n\nLad os for eksempel sige, at vores patient er 30 √•r gammel, har et blodtryk p√• \\(145\\) mmHg og ryger 7 cigaretter om dagen. If√∏lge vores model vil hans risiko for hjerte-kar-sygdom v√¶re \\[\n    p(30,145,7) = \\frac{1}{1+e^{-(0.06\\cdot 30 + 0.02\\cdot 145 +0.02\\cdot 7 -6.77)}}\\approx 0.127.\n\\] Hans risiko er p√• 12.7%. Hvis vi skal lave en pr√¶diktion, vil vi sige, at han er rask, da dette vil v√¶re det mest sandsynlige.\nI praksis kan der v√¶re et problem med altid at v√¶lge det mest sandsynlige. Hvis man gerne vil kunne forudsige en meget sj√¶lden sygdom, vil det ofte v√¶re s√•dan, at \\(p(x)\\leq 1/2\\) for alle patienter. Ingen ville blive diagnosticeret med sygdommen p√• denne m√•de - og s√• er pr√¶diktionsalgoritmen jo ikke meget v√¶rd. Derfor v√¶lger man ofte et lavere delepunkt end \\(p(x)=1/2\\). Dermed kommer man til at fejldiagnosticere en hel del patienter. Til geng√¶ld f√•r man fanget flere af dem, der faktisk er syge.\nHer p√• siden har vi flere eksempler p√• algoritmer, som vil kunne bruges til at pr√¶diktere om patienter er syge eller raske, fx neurale netv√¶rk8 og Bayes klassifikation. Fordelen ved at bruge logistisk regression er, at man ikke bare f√•r en pr√¶diktion, men ogs√• en model for, hvordan sandsynligheden \\(p(x)\\) afh√¶nger af variablen \\(x\\). Dermed opn√•r man en indsigt i, hvordan sammenh√¶ngen mellem for eksempel blodtryk og hjerte-kar-sygdom er. Ved hj√¶lp af odds-ratioer kan vi endda s√¶tte tal p√•, hvordan odds for sygdom √¶ndrer sig, n√•r blodtrykket vokser. Dette er i mods√¶tning til mange andre pr√¶diktionsalgoritmer, der blot giver en pr√¶diktion, uden at brugeren af algoritmen ved, hvor den kommer fra. Inden for medicin er det ofte vigtigt at kende baggrunden for en given pr√¶diktion, s√• man kan forholde sig kritisk til resultatet og r√•dgive patienten om, hvordan man s√¶nker risikoen for sygdom (for eksempel med blodtrykss√¶nkende medicin). Til geng√¶ld har de mere avancerede algoritmer, s√• som neurale netv√¶rk, mulighed for at give en mere pr√¶cis pr√¶diktion.\n8¬†Logistisk regression er i √∏vrigt et meget simpelt eksempel p√• et neuralt netv√¶rk, hvis man v√¶lger af bruge cross-entropy funktionen som tabsfunktion.\nAndre eksempler p√• anvendelser\nLogistisk regression kan bruges til at modellere meget andet end sygdom. Forestil dig for eksempel en nyhedshjemmeside, der benytter cookies til at m√•lrette reklamer. Hjemmesiden registrerer, hvor mange gange du har l√¶st kulturnyheder, \\(x_1\\), og hvor mange gange du har l√¶st sportsnyheder, \\(x_2\\), inden for den sidste m√•ned. Desuden registrerer den, om du har klikket p√• en bestemt reklame for en ny biograffilm. Man kan bruge disse data til at finde en logistisk regressionsmodel for sandsynligheden \\(p(x_1,x_2)\\) for, at en ny bruger klikker p√• reklamen. Med s√•dan en model kan man s√• pr√¶diktere, om en ny bruger vil klikke p√• reklamen ud fra indsamlet data om brugerens forbrug af sports- og kulturnyheder. Reklamen vil s√• kun blive vist til brugeren, hvis det pr√¶dikteres, at brugeren rent faktisk vil klikke p√• reklamen.\nEt andet eksempel kunne v√¶re en meningsm√•ling. Et mindre antal v√¶lgere sp√∏rges, om de har t√¶nkt sig at stemme p√• r√∏d eller bl√• blok. Desuden noteres deres alder \\(x_1\\) og √•rsindt√¶gt \\(x_2\\). Ud fra dette datas√¶t laves en model for sandsynligheden \\(p(x_1,x_2)\\) for at stemme p√• r√∏d blok som funktion af alder og √•rsindt√¶gt. Ud fra modellen kan man s√• pr√¶diktere, hvad resten af befolkningen har t√¶nkt sig at stemme.\n\n\n\n\n\n\nOpgave 6\n\n\n\n\n\nVi kigger igen p√• en multipel regressionsmodel for odds \\(O(x_1,x_2)\\) for, at en bruger af en hjemmeside klikker p√• en given reklame, hvor \\(x_1\\) og \\(x_2\\) er antal gange kunden har l√¶st henholdsvis kulturnyheder og sportsnyheder inden for den sidste m√•ned. Modellen for odds er fundet til \\[\nO(x_1,x_2) = e^{-2+0.5x_1-0.1x_2 }.\n\\] Vi vil gerne pr√¶diktere, om en bruger klikker p√• reklamen, s√• vi kan beslutte, om det er relevant at vise ham den.\n\nEn bruger har l√¶st kulturnyheder 5 gange og sportsnyheder 8 gange inden for den sidste m√•ned. Vil du pr√¶diktere, at brugeren klikker p√• reklamen?"
  },
  {
    "objectID": "materialer/logistisk/log-reg.html#sammenh√¶ng-mellem-logistisk-regression-og-logistisk-v√¶kst",
    "href": "materialer/logistisk/log-reg.html#sammenh√¶ng-mellem-logistisk-regression-og-logistisk-v√¶kst",
    "title": "Logistisk regression",
    "section": "Sammenh√¶ng mellem logistisk regression og logistisk v√¶kst",
    "text": "Sammenh√¶ng mellem logistisk regression og logistisk v√¶kst\nHvis du har h√∏rt om logistisk v√¶kst og logistisk regression, spekulerer du m√•ske over, om der er en sammenh√¶ng mellem de to begreber. Vi skal nu se, at der i nogle anvendelser faktisk er en sammenh√¶ng.\nLad os se p√• et eksempel med en smitsom sygdom, hvor infektionen aldrig forlader kroppen igen, og man kan forts√¶tte med at smitte andre resten af livet, n√•r f√∏rst man er blevet smittet. HIV og herpes er eksempler p√• s√•danne sygdomme. Lad \\(I(x)\\) betegne antallet af smittede efter \\(x\\) dage (\\(I\\) st√•r for inficeret). If√∏lge den klassiske SI-model, er v√¶ksthastigheden for \\(I\\) proportional med b√•de antallet af smittede \\(I(x)\\) og antallet af raske \\(M-I(x)\\), hvor \\(M\\) er det samlede befolkningstal. Det vil sige \\[\nI'(x) = k I(x)(M-I(x)),\n\\] hvor \\(k&gt;0\\) er en konstant. Denne ligning kaldes den logistiske differentialligning, og l√∏sningen er givet ved \\[\nI(x)=\\frac{M}{1+c\\cdot e^{-M\\cdot k\\cdot x}},\n\\] hvor \\(c&gt;0\\) igen er en konstant, som kan bestemmes, hvis man kender antallet af smittede \\(I(0)\\) til tiden \\(x=0\\). S√¶tter vi \\(c=\\exp(-b)\\) og \\(a=Mk\\), f√•r vi \\[\nI(x)=\\frac{M}{1+e^{-b}\\cdot e^{-a\\cdot x}} = \\frac{M}{1+e^{-(a\\cdot x+b)}}.\n\\] P√• dag \\(x\\) vil en tilf√¶ldigt udvalgt person have en sandsynlighed p√• \\(p(x)=I(x)/M\\) for at v√¶re smittet. Denne sandsynlighed vil v√¶re beskrevet af en logistisk funktion \\[\np(x)=\\frac{I(x)}{M} = \\frac{1}{1+e^{-(a\\cdot x+b)}}.\n\\] Dette genkender vi som en logistisk regressionsmodel for sandsynligheden \\(p(x)\\).\nFor at bestemme \\(a\\) og \\(b\\), kunne man derfor lave et datas√¶t, hvor vi hver dag tager en test af en tilf√¶ldig person og ser, om personen er smittet eller rask. Derved f√•r vi et datas√¶t med punkter \\((x,y)\\), hvor \\(x\\) er antal dage, og \\(y\\) er \\(0\\) hvis personen er rask, eller \\(1\\) hvis personen er smittet. Vi kan nu finde \\(a\\) og \\(b\\) ved at lave logistisk regression p√• dette datas√¶t og finde et udtryk for \\(p(x)\\). Hvis vi gerne vil vide, hvor mange der faktisk er syge efter \\(x\\) dage, ganger vi bare sandsynligheden for at v√¶re syg op med befolkningstallet \\[\nI(x)=M\\cdot p(x)=\\frac{M}{1+e^{-(a\\cdot x+b)}}.\n\\] Det er dog ikke ved alle eksempler, det er muligt at lave denne kobling mellem de to emner. Logistisk v√¶kst vedr√∏rer en udvikling i tid, dvs. \\(x\\)-variablen skal angive tid. Desuden skal udviklingen foreg√• i en population af fast st√∏rrelse \\(M\\)."
  },
  {
    "objectID": "materialer/logistisk/max_likelihood.html",
    "href": "materialer/logistisk/max_likelihood.html",
    "title": "Maksimering af log-likelihoodfunktionen ved brug af partielt afledte",
    "section": "",
    "text": "N√•r \\(l(a,b)\\) skal maksimeres, kan det g√∏res ved hj√¶lp af partielt afledede. Husk p√•, at i et maksimumspunkt, vil begge de partielt afledte v√¶re lig 0, dvs. \\[\\begin{align*}\n\\frac{\\partial l(a,b)}{\\partial a} &=0 \\\\\n\\frac{\\partial l(a,b)}{\\partial b} &=0.\n\\end{align*}\\] Vi finder derfor f√∏rst et mere eksplicit udtryk for \\(l(a,b)\\) som funktion af \\(a\\) og \\(b\\).\n\nEksplicit udtryk for \\(l(a,b)\\)\nVi ved, at log-likelihoodfunktionen er givet ved \\[\n\\begin{aligned}\nl(a,b) &=\\sum_{i=1}^n \\ln(p_i(a,b)) \\\\  \n&= \\sum_{i=1}^n\\big( {y_i}\\cdot \\ln(p(x_i))+(1-y_i)\\cdot \\ln(1-p(x_i)) \\big).\n\\end{aligned}\n\\tag{1}\\]\nVed at oph√¶ve parentesen \\((1-y_i)\\) i (1) f√•s \\[ l(a,b)=\\sum_{i=1}^n \\big({y_i}\\cdot \\ln(p(x_i))+\\ln(1-p(x_i))- y_i\\cdot \\ln(1-p(x_i))\\big).\\] I to af leddene inden for sumtegnet har vi \\(y_i\\) som en faktor. Vi kan derfor s√¶tte \\(y_i\\) uden for parentes \\[ l(a,b)=\\sum_{i=1}^n \\big(\\ln(1-p(x_i))+{y_i}\\cdot (\\ln(p(x_i))-\\ln(1-p(x_i)))\\big).\\] Ved hj√¶lp af logaritmeregnereglen \\(ln(a/b)=ln(a)-ln(b)\\) f√•r vi \\[ l(a,b)=\\sum_{i=1}^n \\left(\\ln(1-p(x_i))+y_i\\cdot \\ln\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)\\right).\\] Her opsplitter vi til to summer, hvor den ene ikke afh√¶nger af \\(y_i\\). \\[ l(a,b)=\\sum_{i=1}^n \\ln(1-p(x_i))+\\sum_{i=1}^n y_i\\cdot \\ln\\left(\\frac{p(x_i)}{1-p(x_i)}\\right). \\tag{2}\\] Nu har vi f√•et styr p√• udtrykket for \\(l(a,b)\\), som dog afh√¶nger af \\(p(x_i)\\). Vi udnytter nu, at vi havde udtrykket \\[p(x_i) = \\frac{1}{1+e^{-(a\\cdot x_i+b)}}\\] og \\[\\ln\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)=ax_i + b.\\] Inds√¶ttes dette i (2), f√•r vi \\[l(a,b)=\\sum_{i=1}^n \\ln\\left(1-\\frac{1}{1+e^{-(a\\cdot x_i+b)}}\\right)+\\sum_{i=1}^n y_i\\cdot (ax_i+b).\\] Udtrykket i logaritmen s√¶ttes p√• f√¶lles br√∏kstreg, og br√∏ken forl√¶nges med \\(e^{a\\cdot x_i+b}\\)\n\\[\n\\begin{aligned}\nl(a,b)&=\\sum_{i=1}^n \\ln\\left(\\frac{e^{-(a\\cdot x_i+b)}}{1+e^{-(a\\cdot x_i+b)}}\\right)+\\sum_{i=1}^n y_i\\cdot (ax_i+b) \\\\ &=\\sum_{i=1}^n \\ln\\left(\\frac{1}{1+e^{a\\cdot x_i+b}}\\right)+\\sum_{i=1}^n y_i\\cdot (ax_i+b)\n\\end{aligned}\n\\] Her benytter vi igen regnereglen \\(\\ln(a/b)=\\ln(a)-\\ln(b)\\). \\[ l(a,b)=\\sum_{i=1}^n\\big(\\ln(1)-\\ln(1+e^{a\\cdot x_i+b})\\big)+\\sum_{i=1}^n y_i\\cdot (ax_i+b).\\] Da \\(\\ln(1)=0\\), har vi endelig \\[\nl(a,b)=\\sum_{i=1}^n \\big(-\\ln(1+e^{a\\cdot x_i+b})\\big)+\\sum_{i=1}^n y_i\\cdot (ax_i+b).\n\\tag{3}\\]\n\n\nPartielt afledede\nVi finder nu de partielt afledte af \\(l(a,b)\\) ved at differentiere (3). Lad os f√∏rst se p√• \\(\\frac{\\partial l(a,b)}{\\partial b}\\). I den f√∏rste sum i (3) skal vi se hvert led som en sammensat funktion, hvor den indre funktion har et led, som ogs√• er en sammensat funktion. S√• f√•r vi \\[\\frac{\\partial l(a,b)}{\\partial b}= \\sum_{i=1}^n -\\frac{1}{1+e^{a\\cdot x_i+b}}\\cdot (0+e^{a\\cdot x_i+b})\\cdot(0+1) +\\sum_{i=1}^n y_i\\cdot (0+1).\\] Ved at reducere f√•s \\[\\frac{\\partial l(a,b)}{\\partial b}= \\sum_{i=1}^n -\\frac{e^{a\\cdot x_i+b}}{1+e^{a\\cdot x_i+b}} +\\sum_{i=1}^n y_i\\] Ved at bruge at \\[\np(x) = \\frac{1}{1+e^{-(ax+b)}}= \\frac{e^{(ax+b)}}{1+e^{(ax+b)}}\n\\tag{4}\\] i forbindelse med den f√∏rste sum og efterf√∏lgende samle leddene i en sum, f√•s \\[\\frac{\\partial l(a,b)}{\\partial b}= \\sum_{i=1}^n -p(x_i) +\\sum_{i=1}^n y_i=\\sum_{i=1}^n (y_i-p(x_i)).\\]\nNu ser vi p√• \\(\\frac{\\partial l(a,b)}{\\partial a}\\) p√• tilsvarende m√•de. \\[\\frac{\\partial l(a,b)}{\\partial a}= \\sum_{i=1}^n -\\frac{1}{1+e^{a\\cdot x_i+b}}\\cdot (0+e^{a\\cdot x_i+b})\\cdot(1\\cdot x_i+0) +\\sum_{i=1}^n y_i\\cdot (1\\cdot x_i+0)\\] Der reduceres \\[\\frac{\\partial l(a,b)}{\\partial a}= \\sum_{i=1}^n -\\frac{e^{a\\cdot x_i+b}}{1+e^{a\\cdot x_i+b}}\\cdot x_i +\\sum_{i=1}^n y_i\\cdot x_i.\\] Igen bruges (4) til at f√• \\[\\frac{\\partial l(a,b)}{\\partial a}= \\sum_{i=1}^n -p(x_i)\\cdot x_i +\\sum_{i=1}^n y_i\\cdot x_i=\\sum_{i=1}^n (y_i\\cdot x_i-p(x_i)\\cdot x_i).\\]\nEndelig kan \\(x_i\\) s√¶ttes udenfor parentes, hvorved vi har \\[\\frac{\\partial l(a,b)}{\\partial a}=\\sum_{i=1}^n (y_i-p(x_i))\\cdot x_i.\\]\nFor at lave optimering og finde maksimum, skal vi unders√∏ger, hvorn√•r de partielt afledte er nul. Vi skal s√•ledes l√∏se f√∏lgende to ligninger med to ubekendte \\[\\begin{align*}\n0=\\frac{\\partial l(a,b)}{\\partial a}=\\sum_{i=0}^n (y_i-p(x_i))\\cdot x_i \\quad \\text{og} \\quad 0=\\frac{\\partial l(a,b)}{\\partial b}=\\sum_{i=1}^n (y_i-p(x_i)) .\n\\end{align*}\\] Dette ligningssystem er dog ikke bare lige til at l√∏se, s√• her bliver man n√∏dt til at benytte sig af numeriske metoder til l√∏sning af ligningssytemer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Om os",
    "section": "",
    "text": "Projektet Aalborg Intelligence, som er finansieret af Novo Nordisk Fonden, er forankret p√• Institut for Matematiske Fag p√• Aalborg Universitet (AAU), og inkluderer en repr√¶sentant fra de fem gymnasier i Aalborg."
  },
  {
    "objectID": "about.html#gymnasiel√¶rere",
    "href": "about.html#gymnasiel√¶rere",
    "title": "Om os",
    "section": "Gymnasiel√¶rere",
    "text": "Gymnasiel√¶rere\n\nMalene Cramer Engebjerg (Aalborghus Gymnasium)\nAllan Frendrup (N√∏rresundby Gymnasium)\nNikolaj Hess-Nielsen (Aalborg Katedralskole)\nMette Kristensen (Hasseris Gymnasium)\nJan B. S√∏rensen (Aalborg City Gymnasium)"
  },
  {
    "objectID": "srp.html",
    "href": "srp.html",
    "title": "SRP",
    "section": "",
    "text": "I arbejdet med studieretningsprojektet kan matematik og AI indg√• i et samarbejde med en lang r√¶kke andre fag. Id√©er til s√•danne samarbejder findes herunder. Under nogle af emnerne er der ogs√• indsat konkrete forslag til problemformuleringer.\nHvis man √∏nsker, at inddrage kunstige neurale netv√¶rk kan noten om kunstige neurale netv√¶rk benyttes. En mulig fremgangsm√•de er at bede eleven udlede opdateringsreglerne for et konkret, lille netv√¶rk med f.eks. √©t skjult lag.\nEn anden mulighed er at bruge noten om perceptroner - eventuelt kombineret med noten om retningsafledede og gradientnedstigning."
  },
  {
    "objectID": "srp.html#samfundsfag-og-matematik",
    "href": "srp.html#samfundsfag-og-matematik",
    "title": "SRP",
    "section": "Samfundsfag og matematik",
    "text": "Samfundsfag og matematik\n\n\n\n\n\n\nKandidattest\n\n\n\n\n\nUdarbejdelse af kandidattest i forbindelse med valg. [samfundsfag A]\n\nMaterialer\nNoten om perceptroner.\n\n\n\n\n\n\n\n\n\n\nOverv√•gning\n\n\n\n\n\nBrugen af kunstig intelligens i forbindelse med ansigtsgenkendelse. Herunder kan emner som persondataloven, retssikkerhed og/eller partiernes holdning til overv√•gning behandles. [samfundsfag A]\n\nMaterialer\nNoten om kunstige neurale netv√¶rk."
  },
  {
    "objectID": "srp.html#dansk-og-matematik",
    "href": "srp.html#dansk-og-matematik",
    "title": "SRP",
    "section": "Dansk og matematik",
    "text": "Dansk og matematik\n\n\n\n\n\n\nAI og anvendelser\n\n\n\n\n\nFormidlingsopgave hvor AI metoder behandles og derefter formidles f.eks. som en popul√¶rvidenskabelig artikel. Eleverne skal skrive en danskfaglig meta-del, hvor de redeg√∏r for deres overvejelser og valg med hensyn til m√•lgruppe, virkemidler med videre.\n\nMaterialer\nNoten om kunstige neurale netv√¶rk.\nNoten om perceptroner.\nNoten om naiv Bayes klassifier."
  },
  {
    "objectID": "srp.html#engelsk-og-matematik",
    "href": "srp.html#engelsk-og-matematik",
    "title": "SRP",
    "section": "Engelsk og matematik",
    "text": "Engelsk og matematik\n\n\n\n\n\n\nMachines like me\n\n\n\n\n\nRedeg√∏relse for hvad et kunstigt neuralt netv√¶rk er. I engelsk perspektiveres der til Ian McEwans bog ‚ÄúMachines like me‚Äù. [engelsk A]"
  },
  {
    "objectID": "srp.html#idr√¶t-og-matematik",
    "href": "srp.html#idr√¶t-og-matematik",
    "title": "SRP",
    "section": "Idr√¶t og matematik",
    "text": "Idr√¶t og matematik\n\n\n\n\n\n\nBaseball og machine learning\n\n\n\n\n\nImplementering af et kunstig neuralt netv√¶rk, som kan forudsige baseball tegn (app til implementering af netv√¶rk er under udarbejdelse). [idr√¶t C, evt. innovativ]\n\nMaterialer\nStealing Baseball Signs with a Phone (Machine Learning)."
  },
  {
    "objectID": "srp.html#biologi-og-matematik",
    "href": "srp.html#biologi-og-matematik",
    "title": "SRP",
    "section": "Biologi og matematik",
    "text": "Biologi og matematik\n\n\n\n\n\n\nDiagnosticering af sygdomme\n\n\n\n\n\nRedeg√∏relse for hvordan et kunstigt neuralt netv√¶rk kan tr√¶nes, s√• det kan anvendes i forbindelse med diagnosticering af sygdomme - herunder kan opdateringsreglerne for et lille, simpelt netv√¶rk udledes. [biologi A]\n\nMaterialer\nMeet the computer diagnosing cancer."
  },
  {
    "objectID": "srp.html#informatik-og-matematik",
    "href": "srp.html#informatik-og-matematik",
    "title": "SRP",
    "section": "Informatik og matematik",
    "text": "Informatik og matematik\n\n\n\n\n\n\nGenkendelse af h√•ndskrevne tal\n\n\n\n\n\nImplementering af et kunstig neuralt netv√¶rk med √©t skjult lag, som kan kende forskel p√• f.eks. h√•ndskrevne 2- og 9-taller. [informatik B, innovativ opgave]\n\nProblemformulering\nUdarbejd et l√∏sningsforslag til hvordan man overs√¶tter h√•ndskrevne tal, s√• de kan genkendes af en computer. I den forbindelse skal du:\n\nRedeg√∏r for hvad der forst√•s ved et kunstigt neuralt netv√¶rk, hvor du tager udgangspunkt i et netv√¶rk med √©t skjult lag. Kom herunder ind p√• feedforward og backpropagation.\nImplementer et kunstig neuralt netv√¶rk med √©t skjult lag, som kan bruges til at kende forskel p√• 2- og 9-taller (brug en passende delm√¶ngde af MNIST train-datas√¶ttet).\nVurder dit l√∏sningsforslag i forhold til styrker og svagheder samt graden af innovation. Inddrag i den forbindelse en passende delm√¶ngde af MNIST test-datas√¶ttet.\n\n\n\nMaterialer\nNetv√¶rket kan tr√¶nes p√• en passende delm√¶ngde af MNIST datas√¶ttet.\n\n\n\n\n\n\n\n\n\n\nKunstig intelligens - muligheder og begr√¶nsninger\n\n\n\n\n\nRedeg√∏relse for hvordan et kunstigt neuralt netv√¶rk tr√¶nes. Diskussion af de etiske problemstillinger, som kan opst√• i forbindelse med anvendelsen af kunstig intelligens og/eller diskussion af de muligheder og begr√¶nsninger, der er ved brugen kunstig intelligens. [informatik C]\n\nProblemformulering 1\n\nRedeg√∏r kort for begrebet ‚Äùkunstig intelligens‚Äù - herunder ‚Äùdeep learning‚Äù.\nForklar hvordan et kunstig neuralt netv√¶rk virker. Herunder √∏nskes en redeg√∏relse for hvordan et kunstigt neuralt netv√¶rk l√¶rer vha. backpropagation og hvordan k√¶dereglen benyttes i den forbindelse.\nDiskuter de etiske problemstillinger som kan opst√• i anvendelsen af kunstig intelligens.\n\n\n\nProblemformulering 2\n\nRedeg√∏r for udviklingen inden for kunstig intelligens. Inddrag begreberne machine learning, deep learning samt supervised og unsupervised learning.\nRedeg√∏r for teorien bag kunstige neurale netv√¶rk herunder hvordan kunstige neurale netv√¶rk l√¶rer vha. backpropagation og costfunktionen. Forklar ogs√• hvordan k√¶dereglen benyttes i den forbindelse.\nDiskuter hvilke muligheder og begr√¶nsninger der er ved brugen af machine learning. Inddrag bilag 1.\n\nBilag 1"
  },
  {
    "objectID": "srp.html#psykologi-og-matematik",
    "href": "srp.html#psykologi-og-matematik",
    "title": "SRP",
    "section": "Psykologi og matematik",
    "text": "Psykologi og matematik\n\n\n\n\n\n\nPr√¶diktion af psykisk sygdom ved hj√¶lp af deep learning\n\n\n\n\n\nForklare hvordan kunstige neurale netv√¶rk kan bruges til at pr√¶diktere psykisk sygdom baseret p√• register og genetiske data.\n\nMaterialer\nNoten om kunstige neurale netv√¶rk.\nDeep Learning for Cross-Diagnostic Prediction of Mental Disorder Diagnosis and Prognosis Using Danish Nationwide Register and Genetic Data."
  },
  {
    "objectID": "undervisningsforloeb/klimaudfordring_innovation.html",
    "href": "undervisningsforloeb/klimaudfordring_innovation.html",
    "title": "Milj√∏- og klimaudfodringer",
    "section": "",
    "text": "Foruds√¶tninger og tidsforbrug\n\n\n\n\n\nForl√∏bet er et tv√¶rfagligt samarbejde med dansk og kr√¶ver kendskab til:\n\nPopul√¶rvidenskabelige artikler som genre\n\nTidsforbrug: ca. 4 x 90 minutter i hvert fag."
  },
  {
    "objectID": "undervisningsforloeb/klimaudfordring_innovation.html#opgaveformulering",
    "href": "undervisningsforloeb/klimaudfordring_innovation.html#opgaveformulering",
    "title": "Milj√∏- og klimaudfodringer",
    "section": "Opgaveformulering",
    "text": "Opgaveformulering\n\nRedeg√∏r for dele af matematikken bag kunstig intelligens og pr√¶senter et forslag til en mulig innovativ anvendelse af kunstig intelligens indenfor emnet ‚Äùklimaforandringer‚Äù.\nRedeg√∏r i den forbindelse for hvilket tr√¶nings- og testdata, der skal indsamles, og for fordelene ved den efterf√∏lgende potentielle anvendelse af kunstig intelligens.\nSkriv en popul√¶rvidenskabelig artikel om jeres emne (omfang: 3-4 sider).\nBegrund jeres valg i forbindelse med layout, sprogbrug samt nyheds- og relevanskriterier og vurd√©r hvordan artiklen kan bidrage til nye indsigter og handlemuligheder.\n\nMateriale i dansk: Changemaker-modellen.\nMateriale i matematik: Forskellige AI metoder."
  },
  {
    "objectID": "undervisningsforloeb/opklar_et_mord.html",
    "href": "undervisningsforloeb/opklar_et_mord.html",
    "title": "Opklar et mord!",
    "section": "",
    "text": "SCREENCAST om neurale netv√¶rk - uden at det bliver for teknisk."
  },
  {
    "objectID": "undervisningsforloeb/opklar_et_mord.html#hvad-er-et-kunstigt-neuralt-netv√¶rk",
    "href": "undervisningsforloeb/opklar_et_mord.html#hvad-er-et-kunstigt-neuralt-netv√¶rk",
    "title": "Opklar et mord!",
    "section": "",
    "text": "SCREENCAST om neurale netv√¶rk - uden at det bliver for teknisk."
  },
  {
    "objectID": "undervisningsforloeb/opklar_et_mord.html#hvem-har-afsat-fingeraftryk-i-de-forskellige-lokaler",
    "href": "undervisningsforloeb/opklar_et_mord.html#hvem-har-afsat-fingeraftryk-i-de-forskellige-lokaler",
    "title": "Opklar et mord!",
    "section": "Hvem har afsat fingeraftryk i de forskellige lokaler?",
    "text": "Hvem har afsat fingeraftryk i de forskellige lokaler?\nI alt 10 elever er under mistanke. Det drejer sig om:\n\n\nAlexander\nBent\nCecilie\nHugo\nKaroline\nMette\nSigne\nSigurd\nValdemar\nVictoria\n\nPolitiet har taget syv forskellige fingeraftryk fra hver elev. P√• skolen har man i forskellige lokaler ogs√• fundet fingeraftryk - man ved bare ikke, hvem fingeraftrykkene stammer fra.\nAlle fingeraftryk, som er fundet i lokalerne, er nummeret fra 101-110. Fingeraftrykkene er fundet i f√∏lgende lokaler:\n\n\n\nFysik\nKemi\nBiotek\nMatematik\nBiologi\n\n\n\n\n107\n102\n101\n104\n103\n\n\n109\n106\n105\n108\n110\n\n\n\nI skal nu ud fra fingeraftrykkene hj√¶lpe politiet med at afg√∏re, hvem der har befundet sig i de forskellige lokaler.\n\n\n\n\n\n\nOpgave 1: Tr√¶n forskellige neurale netv√¶rk\n\n\n\n\n\nStart med at se denne SCREENCAST, som handler om hvordan man tr√¶ner et kunstigt neuralt netv√¶rk i Orange.\n\nVed hj√¶lp af tr√¶ningsdata skal I tr√¶ne forskellige neurale netv√¶rk, som kan pr√¶diktere hvem et givent fingeraftryk tilh√∏rer. Pr√∏v med forskellige neurale netv√¶rk af forskellig dybde (dvs. et varierende antal skjulte lag) og forskellig antal neuroner i hvert skjult lag.\nSammenlign jeres forskellige modeller vha. ‚ÄùTest and Score‚Äù (brug krydsvalidering - og husk at CA skal v√¶re t√¶t p√• 1). ‚ÄùTest and Score‚Äù skal som input have billederne fra tr√¶ningsdata og de forskellige modeller (dvs. de forskellige neurale netv√¶rk).\n\n\n\n\n\n\n\n\n\n\nOpgave 2: Lav en pr√¶diktion p√• baggrund af den valgte model\n\n\n\n\n\nSe denne SCREENCAST som viser hvordan man kan pr√¶diktere i Orange ud fra en valgt model.\n\nV√¶lg den bedste model og brug den til at pr√¶diktere hvem de ti forskellige fingeraftryk (101-110) stammer fra. Brug ‚ÄùPredictions‚Äù - ‚ÄùPredictions‚Äù skal som input have den valgte model (og den valgte model skal have billederne som input) samt billederne i testdatas√¶ttet.\n\n\n\n\n\n\n\n\n\n\nOpgave 3: Hvem har v√¶ret hvor?\n\n\n\n\n\n\nNoter her hvem der er hvem:\n\n\n\n\nLokale\nNummer\nNavn\n\n\n\n\nFysik\n107\n\n\n\nFysik\n109\n\n\n\nKemi\n102\n\n\n\nKemi\n106\n\n\n\nBiotek\n101\n\n\n\nBiotek\n105\n\n\n\nMatematik\n104\n\n\n\nMatematik\n108\n\n\n\nBiologi\n103\n\n\n\nBiologi\n110\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpgave 4: Hvem er morderen?\n\n\n\n\n\nPolitiet kan nu oplyse, at mordet blev beg√•et i matematik‚Ä¶ üò±\nTager du nummeret for hver af de to personer, som befandt sig i matematik, tr√¶kker 100 fra og dividerer med 2, s√• har du antallet af bogstaver i morderens navn.\n\nHvem er morderen?"
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html",
    "href": "undervisningsforloeb/test_for_sygdomme.html",
    "title": "Test for sygdomme",
    "section": "",
    "text": "Foruds√¶tninger og tidsforbrug\n\n\n\n\n\nForl√∏bet kr√¶ver kendskab til:\n\nSandsynlighedsregning\nBetingede sandsynligheder\n\nTidsforbrug: Ca. 2 x 90 minutter."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#sensitivitet-og-specificitet",
    "href": "undervisningsforloeb/test_for_sygdomme.html#sensitivitet-og-specificitet",
    "title": "Test for sygdomme",
    "section": "Sensitivitet og specificitet",
    "text": "Sensitivitet og specificitet\nN√•r man tester for en sygdom, s√• vil man m√•ske umiddelbart t√¶nke, at hvis testen er positiv, s√• er man syg, og hvis testen er negativ, s√• er man rask. Men det beh√∏ver faktisk ikke at v√¶re tilf√¶ldet. Man kan godt v√¶re rask, selvom testen er positiv (det kalder man for en falsk positiv), og man kan godt v√¶re syg, som testen er negativ (det kalder man for en falsk negativ). Det er fordi, at der ikke findes nogen test, som er helt perfekt!\nDet vil sige, at resultatet af en test vil falde i √©n af f√∏lgende fire kategorier:\n\n\n\n\nSyg\nRask\n\n\n\n\nPositiv test\nSand positiv (SP)\nFalsk positiv (FP)\n\n\nNegativ test\nFalsk negativ (FN)\nSand negativ (SN)\n\n\n\nDet er klart, at man selvf√∏lgelig helst vil have en test, hvor flest mulige lander i diagonalen med sande positiver og sande negativer.\nEn god test skal derfor have f√∏lgende egenskaber:\n\nHvis testen anvendes p√• en syg person, s√• skal sandsynligheden for at testen bliver positiv v√¶re h√∏j.\nHvis testen anvendes p√• en rask person, s√• skal sandsynligheden for at testen bliver negativ v√¶re h√∏j.\n\nDisse to betingede sandsynligheder kaldes for henholdsvis sensitivitet og specificitet og kan skrives matematisk s√•dan her:\n\\[\n\\mathrm{sensitivitet } = P(\\textrm{positiv test } | \\textrm{ syg})\n\\] og\n\\[\n\\mathrm{specificitet } = P(\\textrm{negativ test } | \\textrm{ rask})\n\\]\n\n\n\n\n\n\nOpgave 1: Sensitivitet og specificitet\n\n\n\n\n\nV√¶lg en sygdom som du vil arbejde med (eller som din l√¶rer har bestemt, at du skal arbejde med!). Det kan for eksempel v√¶re corona, klamydia, RS virus eller influenza.\n\nUnders√∏g sensitivitet og specificitet for forskellige tests for den sygdom, som du har valgt.\nKan du lave en test hvor sensitiviteten er 100% (du beh√∏ver ikke at bekymre dig om specificiteten)?\nKan du lave en test hvor specificiteten er 100% (du beh√∏ver ikke at bekymre dig om sensitiviteten)?\n\n\n\n\nDet er klart, at hvis en test skal v√¶re god, s√• √∏nsker vi, at b√•de sensitiviteten og specificiteten er t√¶t p√• 100%."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#pr√¶valens",
    "href": "undervisningsforloeb/test_for_sygdomme.html#pr√¶valens",
    "title": "Test for sygdomme",
    "section": "Pr√¶valens",
    "text": "Pr√¶valens\nBlandt alle dem, vi tester, vil en vis andel i virkeligheden v√¶re syge. Det kaldes for sygdommens pr√¶valens. Det vil sige:\n\\[\n\\mathrm{pr√¶valens } = P(\\textrm{syg})\n\\]\n\n\n\n\n\n\nOpgave 2: Pr√¶valens\n\n\n\n\n\n\nUnders√∏g pr√¶valensen for den sygdom, som du arbejder med.\n\n\n\n\n\n\n\n\n\n\nOpgave 3: Sensitivitet, specificitet og pr√¶valens\n\n\n\n\n\nVi forestiller os nu, at du tester 10000 personer for den sygdom, som du arbejder med og lad os sige, at f√∏lgende er oplyst (du m√• ogs√• gerne bruge de tal, som du har fundet i de foreg√•ende opgaver):\nPr√¶valens: \\(P(\\textrm{syg})= 5 \\%\\)\nSensitivitet: \\(P(\\textrm{positiv test } | \\textrm{ syg}) = 86 \\%\\)\nSpecificitet: \\(P(\\textrm{negativ test } | \\textrm{ rask}) = 92 \\%\\)\n\nUdfyld nedenst√•ende tabel (start med at bestemme det samlede antal syge og raske):\n\n\n\n\n\nSyg\nRask\nI alt\n\n\n\n\nPositiv test\n\n\n\n\n\nNegativ test\n\n\n\n\n\nI alt"
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#positiv-og-negativ-pr√¶diktiv-v√¶rdi",
    "href": "undervisningsforloeb/test_for_sygdomme.html#positiv-og-negativ-pr√¶diktiv-v√¶rdi",
    "title": "Test for sygdomme",
    "section": "Positiv og negativ pr√¶diktiv v√¶rdi",
    "text": "Positiv og negativ pr√¶diktiv v√¶rdi\nHvis du bliver testet for en sygdom, s√• vil du enten st√• med en positiv eller en negativ test, og du er dybest set slet ikke interesseret i ovenst√•ende sandsynligheder (sensitivitet, specificitet og pr√¶valens)! Du vil i stedet stille dig selv √©t af f√∏lgende to sp√∏rgsm√•l:\n\nMin test er positiv - hvad er sandsygligheden for, at jeg rent faktisk er syg?\n\neller\n\nMin test er negativ - hvad er sandsygligheden for, at jeg rent faktisk er rask?\n\nDu vil jo gerne undg√•, at din test enten er falsk positiv eller falsk negativ.\nOvenst√•ende sandsynligheder kaldes for den positive pr√¶diktive v√¶rdi og den negative pr√¶diktive v√¶rdi. Skrevet som en betinget sandsynlighed bliver det:\n\\[\n\\text{positiv pr√¶diktiv v√¶rdi } = P(\\textrm{syg } | \\textrm{ positiv test})\n\\] og\n\\[\n\\text{negativ pr√¶diktiv v√¶rdi } = P(\\textrm{rask } | \\textrm{ negativ test})\n\\]\n\n\n\n\n\n\nOpgave 4: Positiv og negativ pr√¶diktiv v√¶rdi\n\n\n\n\n\nHvis du har brugt oplysningerne fra den forrige opgave, skulle du gerne have f√•et f√∏lgende tabel:\n\n\n\n\nSyg\nRask\nI alt\n\n\n\n\nPositiv test\n\\(430\\)\n\\(760\\)\n\\(1190\\)\n\n\nNegativ test\n\\(70\\)\n\\(8740\\)\n\\(8810\\)\n\n\nI alt\n\\(500\\)\n\\(9500\\)\n\\(10000\\)\n\n\n\n\nBenyt ovenst√•ende tabel til at udregne den positive og negative pr√¶diktive v√¶rdi.\n\n\n\n\nDu undrer dig m√•ske over, at den positive pr√¶diktive v√¶rdi er s√• forholdsvis lav (36.1%), mens den negative pr√¶diktive v√¶rdi er s√• t√¶t p√• 100% (99.2%). Men det er fordi, at den positive og negative pr√¶diktive v√¶rdi ikke kun afh√¶nger af testens sensitivitet og specificitet, men ogs√• af pr√¶valensen af sygdommen (i den gruppe vi tester iblandt). Hvis vi ser p√•, hvad vi ved, inden vi overhovedet begynder at teste (det kaldes for prior sandsynligheder), s√• er det f√∏lgende:\n\\[P(\\textrm{syg})= 5 \\%\\]\nog dermed ogs√• at\n\\[P(\\textrm{rask})= 95 \\%\\] Det vil sige, at inden vi har taget testen, er vi ret sikre p√•, at vi er raske. F√•r vi s√• (som forventet) en negativ test, s√• bliver vi bare endnu mere sikre p√•, at vi er raske (svarende til en negativ pr√¶diktiv v√¶rdi p√• 99.2%). F√•r vi derimod en positiv test, s√• bliver vi lidt mere sikre p√•, at vi er syge. Vi opjusterer alts√• fra en prior sandsynlighed p√• 5% til en positiv pr√¶diktiv v√¶rdi p√• 36.1%. Men fordi at sandsynligheden for at v√¶re syg p√• forh√•nden er s√• lille, s√• vil en positiv test stadig efterlade en vis chance for, at vi rent faktisk ikke er syge alligevel!\nDet virker m√•ske underligt, men forestil dig, at vi laver graviditetstest blandt m√¶nd. Da ingen test er perfekt (sensitivitet og specificitet vil altid v√¶re under 100%), s√• vil der f√∏r eller siden ske det, at en af m√¶ndene tester positiv. Men her er det ret tydeligt, at pr√¶valensen (det vil sige sandsynligheden for at v√¶re gravid) blandt dem vi tester (det vil sige m√¶nd) er 0%. Derfor bliver den positive pr√¶diktive v√¶rdi ogs√• 0%, selvom testen er positiv! Men det er selvf√∏lgelig ogs√• lidt √•ndsvagt at lave graviditetstest blandt m√¶nd‚Ä¶!\nHvis vi tester en hel befolkning for eksempelvis corona, s√• vil pr√¶valensen v√¶re forholdsvis lav. Tester vi derimod kun blandt personer, som har symptomer p√• corona, s√• vil pr√¶valens straks v√¶re h√∏jere. Vi skal nu unders√∏ge, hvilken betydning det har p√• den positive og negative pr√¶diktive v√¶rdi.\n\n\n\n\n\n\nOpgave 5: Positiv og negativ pr√¶diktiv v√¶rdi og forskellige pr√¶valenser\n\n\n\n\n\nVi forestiller os igen, at vi tester 10000 personer for den sygdom, som vi arbejder med og lad os sige, at sensitivitet og specificitet er som f√∏r, men at pr√¶valensen varierer, som angivet nedenfor:\nPr√¶valens: \\(P(\\textrm{syg})\\) p√• henholdsvis \\(1 \\%\\), \\(5 \\%\\), \\(20 \\%\\) og \\(40 \\%\\).\nSensitivitet: \\(P(\\textrm{positiv test } | \\textrm{ syg}) = 86 \\%\\)\nSpecificitet: \\(P(\\textrm{negativ test } | \\textrm{ rask}) = 92 \\%\\)\n\nUdfyld tabeller som nedenst√•ende for hver af de fire forskellige pr√¶valenser (husk at du allerede har tabellen for pr√¶valensen p√• 5% fra opgave 3!):\n\n\n\n\n\nSyg\nRask\nI alt\n\n\n\n\nPositiv test\n\n\n\n\n\nNegativ test\n\n\n\n\n\nI alt\n\n\n\n\n\n\n\nBeregn positiv pr√¶diktiv v√¶rdi (\\(P(\\textrm{syg } | \\textrm{ positiv test})\\)) og negativ pr√¶diktiv v√¶rdi (\\(P(\\textrm{rask } | \\textrm{ negativ test})\\)) for de fire forskellige pr√¶valenser og udfyld denne tabel:\n\n\n\n\nPr√¶valens\nPositiv pr√¶diktiv v√¶rdi\nNegativ pr√¶diktiv v√¶rdi\n\n\n\n\n\\(1 \\%\\)\n\n\n\n\n\\(5 \\%\\)\n\n\n\n\n\\(20 \\%\\)\n\n\n\n\n\\(40 \\%\\)\n\n\n\n\n\n\nHvad sker der med henholdsvis den positive og den negative pr√¶diktive v√¶rdi, n√•r pr√¶valensen stiger? Hvordan giver det mening?\n\n\n\n\n\n\n\n\n\n\nOpgave 6: Hurtigtest for corona\n\n\n\n\n\nL√¶s artiklen Antigentest gav 47% falsk negative svar.\n\nUdfyld p√• baggrund af artiklen tabellen med antal raske/syge og positive/negative.\nUdregn testens sensitivitet og specificitet.\nUdregn pr√¶valensen.\nUdregn den positive og negative pr√¶diktive v√¶rdi."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#bayes-formel-mest-for-a-niveau",
    "href": "undervisningsforloeb/test_for_sygdomme.html#bayes-formel-mest-for-a-niveau",
    "title": "Test for sygdomme",
    "section": "Bayes formel (mest for A-niveau)",
    "text": "Bayes formel (mest for A-niveau)\nHvis du har l√¶st med her (link til Allans note om Bayes klassificer - eller m√•ske vi skal have splittet de generelle afsnit om betingede sandsynligheder og Bayes formel ud i en note for sig selv?) s√• ved du, at en betinget sandsynlighed er defineret p√• f√∏lgende m√•de:\n\\[\nP(A | B) = \\frac{P(A,B)}{P(B)}\n\\]\n(er det ok at skrive \\(A,B\\) i stedet for f√¶llesm√¶ngden - der er alligevel ingen m√¶ngdel√¶re tilbage i gymnasiet?). Og du har l√¶rt, at hvis man bruger det lidt smart, s√• kan man bevise Bayes‚Äô s√¶tning, som siger, at\n\\[\nP(A | B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\nDet vil vi nu udnytte til at opskrive et udtryk for den positive pr√¶diktive v√¶rdi:\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{P(\\text{positiv test } | \\text{ syg}) \\cdot P(\\text{syg})}{P(\\text{positiv test})}\n\\]\nUdnytter vi definitionen af sensitivitet og pr√¶valens, s√• kan vi omskrive t√¶lleren til\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{\\text{sensitivitet} \\cdot \\text{pr√¶valens}}{P(\\text{positiv test})}\n\\tag{1}\\]\nNu mangler vi at finde et udtryk for n√¶vneren. Der m√• g√¶lde, at\n\\[\nP(\\text{positiv test}) = P(\\text{positiv test, syg}) + P(\\text{positiv test, rask})\n\\] Bruger vi definitionen p√• betingede sandsynligheder, kan vi skrive ovenst√•ende som\n\\[\nP(\\text{positiv test}) = P(\\text{positiv test } | \\text{ syg} ) \\cdot P(\\text{syg}) + P(\\text{positiv test } | \\text{ rask}) \\cdot P(\\text{rask})\n\\]\nVi udnytter nu, at \\[P(\\text{rask})+P(\\text{syg})=1\\] og dermed at \\[P(\\text{rask}) = 1- P(\\text{syg})\\] Tilsvarende er ogs√• \\[P(\\text{positiv test } | \\text{rask}) = 1-P(\\text{syg test } | \\text{rask})\\] Derfor er\n\\[\nP(\\text{positiv test}) = P(\\text{positiv test } | \\text{ syg} ) \\cdot P(\\text{syg}) + \\left ( 1 - P(\\text{negativ test } | \\text{ rask}) \\right )  \\cdot \\left ( 1- P(\\text{syg}) \\right )\n\\]\nMen nu er sandsynligheden for at teste positiv alene udtrykt ved hj√¶lp af sensitiviteten, specificiteten og pr√¶valensen:\n\\[\nP(\\text{positiv test}) = \\text{sensitivitet} \\cdot \\text{pr√¶valens} + \\left ( 1 - \\text{specificitet} \\right )  \\cdot \\left ( 1- \\text{pr√¶valens} \\right )\n\\] Inds√¶tter vi dette i (1), f√•r vi\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{\\text{sensitivitet} \\cdot \\text{pr√¶valens}}{\\text{sensitivitet} \\cdot \\text{pr√¶valens} + \\left ( 1 - \\text{specificitet} \\right )  \\cdot \\left ( 1- \\text{pr√¶valens} \\right )}\n\\tag{2}\\]\nBruger vi denne formel til at udregne den positive pr√¶diktive v√¶rdi i det tilf√¶lde, hvor pr√¶valensen er 5%, sensitiviteten er 86% og specificiteten er 92%, f√•r vi\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{0.86 \\cdot 0.05}{0.86 \\cdot 0.05 + (1-0.92) \\cdot (1-0.05)} = 0.361=36.1 \\%\n\\]\nDet skulle meget gerne stemme med det, du har f√•et i opgave 4 (men hvor den positive pr√¶diktive v√¶rdi blev beregner p√• baggrund af tabelv√¶rdier).\n\n\n\n\n\n\nOpgave 7: Beregning af positiv pr√¶diktiv v√¶rdi for forskellige pr√¶valenser\n\n\n\n\n\nAntag, at vi bruger den samme sensitivitet, specificitet og pr√¶valenser som tidligere:\nPr√¶valens: \\(P(\\textrm{syg})\\) p√• henholdsvis \\(1 \\%\\), \\(5 \\%\\), \\(20 \\%\\) og \\(40 \\%\\).\nSensitivitet: \\(P(\\textrm{positiv test } | \\textrm{ syg}) = 86 \\%\\)\nSpecificitet: \\(P(\\textrm{negativ test } | \\textrm{ rask}) = 92 \\%\\)\n\nBrug nu formlen i (2) til at beregn den positive pr√¶diktiv v√¶rdi (\\(P(\\textrm{syg } | \\textrm{ positiv test})\\)) for de fire forskellige pr√¶valenser og udfyld denne tabel:\n\n\n\n\nPr√¶valens\nPositiv pr√¶diktiv v√¶rdi\n\n\n\n\n\\(1 \\%\\)\n\n\n\n\\(5 \\%\\)\n\n\n\n\\(20 \\%\\)\n\n\n\n\\(40 \\%\\)\n\n\n\n\n\nKontroller at dit resultat stemmer med det, du fik i opgave 5.\n\n\n\n\n\n\n\n\n\n\nOpgave 8: Formel for negative pr√¶diktiv v√¶rdi (sv√¶r)\n\n\n\n\n\n\nOpstil en formel for udregning af den negative pr√¶diktive v√¶rdi ved at f√∏lge udledningen af formlen for den positive pr√¶diktive v√¶rdi ovenfor.\nBrug din formel til at udregne negativ pr√¶diktiv v√¶rdi for pr√¶valenserne fra opgave 7.\nKontroller at dit resultat stemmer med det du fik i opgave 5."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#videre-l√¶sning",
    "href": "undervisningsforloeb/test_for_sygdomme.html#videre-l√¶sning",
    "title": "Test for sygdomme",
    "section": "Videre l√¶sning",
    "text": "Videre l√¶sning\nEpidemimatematik: Test for smitte og sygdomme"
  },
  {
    "objectID": "undervisningsforloeb/polynomium_old.html",
    "href": "undervisningsforloeb/polynomium_old.html",
    "title": "Perceptroner og r√∏dder",
    "section": "",
    "text": "Foruds√¶tninger og tidsforbrug\n\n\n\n\n\nForl√∏bet kr√¶ver kendskab til:\n\nAndengradspolynomier og r√∏dder\n\nTidsforbrug: Ca. 90 minutter."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_old.html#hvad-er-en-perceptron",
    "href": "undervisningsforloeb/polynomium_old.html#hvad-er-en-perceptron",
    "title": "Perceptroner og r√∏dder",
    "section": "Hvad er en perceptron?",
    "text": "Hvad er en perceptron?\nI dette forl√∏b skal vi arbejde med perceptoner, og det har du nok aldrig h√∏rt om f√∏r! Start derfor med at se videoen herunder, hvor vi kort forklarer, hvad en perceptron er.\n\nDu kan ogs√• l√¶se meget mere om perceptroner her."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_old.html#andengradspolynomier-og-r√∏dder",
    "href": "undervisningsforloeb/polynomium_old.html#andengradspolynomier-og-r√∏dder",
    "title": "Perceptroner og r√∏dder",
    "section": "Andengradspolynomier og r√∏dder",
    "text": "Andengradspolynomier og r√∏dder\nNu tilbage til vores eksempel om andengradspolynomier og r√∏dder! Lad os for en god ordens skyld minde om, at et andengradspolynomium er en funktion med en forskrift p√• formen \\[\nf(x)=ax^2 + bx + c, \\quad a \\neq 0\n\\] Grafen for et andengradspolynomium kaldes som bekendt for en parabel. I figur¬†1 ses tre eksempler p√• s√•danne parabler.\n\n\n\n\n\n\nFigur¬†1: Graferne for tre forskellige andengradspolynomier.\n\n\n\nHvis vi l√∏ser andengradsligningen \\[\nf(x)=ax^2 + bx + c=0\n\\] finder vi andengradspolynomiets r√∏dder. Men at l√∏se \\(f(x)=0\\), svarer netop til at bestemme, hvor den tilh√∏rende parabel sk√¶rer \\(x\\)-aksen. I figur¬†1 kan vi se, at den gr√∏nne parabel sk√¶rer \\(x\\)-aksen to steder. Det vil sige, at det tilh√∏rende andengradspolynomium har to r√∏dder. Den r√∏de parabel sk√¶rer \\(x\\)-aksen √©t sted ‚Äì det tilh√∏rende andengradspolynomium har alts√• √©n rod. Endelig kan vi se, at den bl√• parabel slet ikke sk√¶rer \\(x\\)-aksen, og det tilh√∏rende andengradspolynomium har derfor ingen r√∏dder.\nDu husker nok, hvordan man bestemmer antallet af r√∏dder i et andengradspolynomium. Vi har brug for diskriminanten \\(d\\):\n\\[\nd = b^2-4ac\n\\tag{1}\\]\nOg der g√¶lder s√•, at \\[\n\\begin{aligned}\n&d&lt;0: \\quad f \\textrm{ har ingen r√∏dder} \\\\\n&d=0: \\quad f \\textrm{ har √©n rod} \\\\\n&d&gt;0: \\quad f \\textrm{ har to r√∏dder} \\\\\n\\end{aligned}\n\\]\nId√©en er nu at unders√∏ge, om det er muligt at f√• en perceptron til at l√¶re1, om et andengradspolynomium overhovedet har nogle r√∏dder alene ude fra de tre koefficienter \\(a\\), \\(b\\) og \\(c\\) ‚Äì og helt uden at kende noget til diskriminantformlen i (1)!\n1¬†Det er klart, at der er intet nyt under solen her. Vi kan jo bare selv beregne diskriminanten og svare p√• sp√∏rgsm√•let. Men form√•let er her at l√¶re lidt om, hvad det vil sige at tr√¶ne en perceptron i et tilf√¶lde, hvor vi allerede selv kender svaret. Desuden findes der ingen lukkede l√∏sningsformler for at bestemme r√∏dder i et polynomium, s√• snart graden af polynomiet er \\(5\\) eller derover. S√• id√©en kan generaliseres, og s√• er den m√•ske slet ikke s√• tosset endda!Inden vi g√•r i gang, vil vi starte med at indse, at i stedet for at l√∏se ligningen\n\\[\na x^2 + bx +c = 0\n\\tag{2}\\]\nS√• kan vi lige s√• godt l√∏se en ligning p√• formen\n\\[\nx^2 + bx +c =0\n\\] hvor alts√• \\(a=1\\). Det virker m√•ske som en forsimpling, men da vi har antaget, at \\(a \\neq 0,\\) s√• kan vi i ligningen i (2) dividere igennem med \\(a\\) og f√•\n\\[\n\\begin{aligned}\n\\frac{a}{a} x^2 + \\frac{b}{a} x + \\frac{c}{a} &= \\frac{0}{a} \\quad \\Leftrightarrow \\\\\nx^2 + \\frac{b}{a} x + \\frac{c}{a} &= 0\n\\end{aligned}\n\\]\nDet betyder, at n√•r vi skal bestemme r√∏dder i andengradspolynomier, s√• er det tilstr√¶kkeligt, at betragte andengradspolynomier med en forskrift p√• formen\n\\[\nf(x)=x^2+bx+c\n\\] fordi man simpelthen bare tager sit oprindelige andengradspolynomium og dividerer igennem med \\(a\\). Lad os illustrere det med et eksempel.\n\nBetragt andengradspolynomiet med forskriften\n\\[\nf(x)=-4x^2+8x+12\n\\] Her har vi \\(a=-4, b=8\\) og \\(c=12\\). L√∏ser vi ligningen \\(f(x)=0\\), finder vi ud af, at \\(f\\) har to r√∏dder nemlig \\(-1\\) og \\(3\\). Dividerer vi forskriften for \\(f\\) igennem med \\(a=-4\\) f√•s et nyt andengradspolynomium \\(g\\) med forskrift\n\\[\ng(x)=x^2-2x-3\n\\] Her er koefficienterne \\(a=1, b=-2\\) og \\(c=-3\\). Men \\(g\\) har pr√¶cis samme r√∏dder som \\(f\\) ‚Äì nemlig \\(-1\\) og \\(3\\). Dette ses ogs√• illustreret i figur¬†2, hvor grafen for \\(f\\) og \\(g\\) begge sk√¶rer \\(x\\)-aksen i \\(-1\\) og \\(3\\).\n\n\n\n\n\n\nFigur¬†2: Grafen for \\(f(x)=-4x^2+8x+12\\) (den bl√•) og \\(g(x)=x^2-2x-3\\) (den gr√∏nne), som begge sk√¶rer \\(x\\)-aksen samme sted. Det vil sige, at \\(f\\) og \\(g\\) har de samme r√∏dder. I dette tilf√¶lde \\(-1\\) og \\(3\\)."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_old.html#tr√¶ningsdata",
    "href": "undervisningsforloeb/polynomium_old.html#tr√¶ningsdata",
    "title": "Perceptroner og r√∏dder",
    "section": "Tr√¶ningsdata",
    "text": "Tr√¶ningsdata\nI dette eksempel vil vi n√∏jes med at se p√•, hvordan man kan tr√¶ne en perceptron, s√• den forh√•bentlig kan fort√¶lle os, om et givent andengradspolynomium enten har ingen eller en eller to r√∏dder. Det svarer til, at vi √∏nsker en perceptron, som for en given parabel kan svare p√•, om parablen sk√¶rer \\(x\\)-aksen eller ej (og alts√• ikke hvor mange gange den eventuelt sk√¶rer \\(x\\)-aksen).\n\n\n\n\n\n\nOpgave 1: R√∏dder eller ej?\n\n\n\n\n\nOvervej f√∏lgende:\n\nHvordan laver man et andengradspolynomium, der har √©n eller to r√∏dder?\nHvordan laver man et andengradspolynomium, som ingen r√∏dder har?\n\n\n\n\nFor at tr√¶ne en perceptron, skal perceptronen se en masse eksempler p√• forskellige andengradspolynomier (det vil her sige med forskellige v√¶rdier af \\(b\\) og \\(c\\)) samtidig med, at vi fort√¶ller perceptronen, om det tilh√∏rende andengradspolynomium har r√∏dder eller ej. At angive om et polynomium har r√∏dder eller ej kalder man for en targetv√¶rdi. T√¶nk p√• det som en lille label du s√¶tter p√• hvert eksempel, hvor du fort√¶ller perceptronen, hvad det rigtige svar er ‚Äì ‚Äúdet er alts√• det her, jeg gerne vil have, at du l√¶rer!‚Äù. Samlet set kalder man de forskellige eksempler inklusiv targetv√¶rdien for tr√¶ningsdata.\n\n\n\n\n\n\nOpgave 2: Tr√¶ningsdata\n\n\n\n\n\n\nFind selv p√• forskellige v√¶rdier af \\(b\\) og \\(c\\) og find ud af om det tilh√∏rende andengradspolynomium har r√∏dder eller ej. Du skal finde p√• mindst to andengradspolynomier, der har r√∏dder og to, der ikke har, men gerne et par stykker mere.Skriv dine v√¶rdier ned (enten bare p√• papir eller i f.eks. et regneark).\nIndtegn dine v√¶rdier \\(b\\) og \\(c\\) i et koordinatsystem, hvor v√¶rdien af \\(b\\) er p√• \\(x\\)-aksen, og v√¶rdien af \\(c\\) er p√• \\(y\\)-aksen. Herunder er lavet et eksempel med \\(b=0\\) og \\(c=-1\\), som svarer til et andengradspolynomium med to r√∏dder samt \\(b=2\\) og \\(c=4\\), som svarer til et andengradspolynomium uden r√∏dder."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_old.html#tr√¶ning-af-perceptron",
    "href": "undervisningsforloeb/polynomium_old.html#tr√¶ning-af-perceptron",
    "title": "Perceptroner og r√∏dder",
    "section": "Tr√¶ning af perceptron",
    "text": "Tr√¶ning af perceptron\nVi skal nu overveje, hvordan perceptronen kan tr√¶nes. Perceptronen g√∏r dybest set det, at den pr√∏ver at bestemme en ret linje, som kan bruges til at adskille de r√∏de punkter fra de bl√• punkter i punktplottet ovenfor. En ret linje i et 2-dimensionalt koordinatsystem har helt generelt en ligning p√• formen2\n2¬†Du er nok vant til at m√∏de linjens ligning p√• denne form: \\(a \\cdot x+b \\cdot y+c=0\\). Skrivem√•den, vi bruger her, er \\(w_0+w_1 \\cdot x + w_2 \\cdot y=0\\). Det vil sige i forhold til den skrivem√•de, som du kender, s√• er \\(w_0=c, w_1=a\\) og \\(w_2=b\\).\\[\nw_0 + w_1 \\cdot x + w_2 \\cdot y = 0\n\\] Og for alle punkter p√• den ene side af linjen g√¶lder, at\n\\[\nw_0 + w_1 \\cdot x + w_2 \\cdot y &gt; 0\n\\] og for alle punkter p√• den anden side, at\n\\[\nw_0 + w_1 \\cdot x + w_2 \\cdot y &lt; 0\n\\]\nI vores tilf√¶lde har vi \\(b\\)-v√¶rdier ud af \\(x\\)-aksen og \\(c\\)-v√¶rdier op af \\(y\\)-aksen. Med de betegnelser bliver ligningen for en ret linje\n\\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c = 0\n\\]\nHer t√¶nker vi alts√• p√• \\(b\\) og \\(c\\) som de variable.\nN√•r man tr√¶ner en perceptron, g√∏r man det ved hj√¶lp af en algoritme, som l√∏bende opdaterer v√¶gtene \\(w_0, w_1\\) og \\(w_2\\), s√• den linje, v√¶gtene giver, bliver bedre og bedre til at adskille de r√∏de punkter fra de bl√•. Hver gang man opdaterer v√¶gtene, siger man, at algoritmen har foretaget √©n iteration3.\n3¬†En iteration betyder en gentagelse.Du kan godt l√∏se resten af opgaverne uden at forst√•, hvorfor v√¶gtene opdateres, som de g√∏r. Men hvis du gerne vil have en forklaring s√• se videoen herunder.\n\n\n\n\n\n\n\nOpgave 3: Tr√¶ning af perceptron\n\n\n\n\n\nLad os bruge startv√¶gtene \\(w_0=1\\), \\(w_1=-3\\) og \\(w_2=2\\).\n\nHvilken linje svarer det til? Indtegn linjen i et koordinatsystemet.\nAdskiller denne linje de to grupper af punkter (med og uden r√∏dder)? Hvis grupperne allerede er adskilt, skal du tilf√∏je punktet med \\(b=2\\) og \\(c=1\\), som svarer til et andengradspolynomium, der har √©n rod.\n\nTr√¶ningsdata der svarer til polynomier med r√∏dder, giver vi targetv√¶rdien \\(t=-1\\) og dem uden r√∏dder f√•r targetv√¶rdien \\(t=1\\).\nAlle punkter, der ligger over startlinjen, opfylder uligheden \\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c &gt; 0\n\\] og f√•r outputv√¶rdien \\(o=1\\), mens dem, der ligger under linjen, opfylder den omvendte ulighed og f√•r outputv√¶rdien \\(o=-1\\).\n\nUdv√¶lg et punkt der bliver fejlklassificeret. Det vil sige som enten ligger under linjen (\\(o=-1\\)), men har target \\(t=1\\) svarende til ingen r√∏dder eller omvendt.\nUdregn fejlen \\(error=t-o\\) som enten er -2 eller 2.\nOpdater nu alle tre v√¶gte ved brug af opdateringsreglen (hvor du selv v√¶lger \\(\\eta\\), f.eks. \\(\\eta=1\\)): \\[\n\\begin{aligned}\n  w_0 \\leftarrow w_0 + & \\,\\eta \\cdot error \\\\\n  w_1 \\leftarrow w_1 + & \\,\\eta \\cdot error \\cdot x_1 \\\\\n  w_2 \\leftarrow w_2 + & \\,\\eta \\cdot error \\cdot x_2 \\\\\n\\end{aligned}\n\\] Husk at \\(x_1\\) er \\(b\\)-v√¶rdien og \\(x_2\\) er \\(c\\)-v√¶rdien!\nFik du efter opdateringen en linje, der adskiller de to grupper?\nHvis ikke, kan du s√• selv lave en ret linje ‚Äúp√• √∏jem√•l‚Äù, der adskiller dem?\n\n\n\n\n\n\n\n\n\n\nOpgave 4: Flere tr√¶ningsdata\n\n\n\n\n\n\nAfg√∏r om f√∏lgende andengradspolynomier har r√∏dder og tilf√∏j dem til dit tr√¶ningsdata:\n\n\\[\n\\begin{aligned}\nf_1(x) &= x^2 + 10x + 26 \\\\\nf_2(x) &= x^2 + 10x + 24\\\\\nf_3(x) &= x^2 + 5x + 6\\\\\nf_4(x) &= x^2 + 5x + 7 \\\\\nf_5(x) &= x^2 + 2x + 1\\\\\nf_6(x) &= x^2 + 2x + 2 \\\\\n\\end{aligned}\n\\]\n\nKan det lade sig g√∏re at adskille de to grupper med en ret linje nu?\n\n\n\n\nSom du netop har opdaget, er det en umulig opgave, vi har givet perceptronen! Vi kan ikke finde en ret linje, som i alle tilf√¶lde kan bruges til at adskille de to slags punkter. Lad os se p√• hvorfor. Som tidligere n√¶vnt har vores linje en ligning p√• formen\n\\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c = 0\n\\tag{3}\\]\nVi husker nu p√• formlen for diskriminanten \\(d=b^2-4ac=b^2-4c\\), da \\(a=1\\) i vores eksempel. Skillelinjen for om andengradspolynomiet har ingen eller flere r√∏dder, g√•r netop ved \\(d=0\\). Det vil sige\n\\[\nb^2-4c =0\n\\tag{4}\\]\nMen vi kan ikke finde nogle v√¶rdier af \\(w_0, w_1\\) og \\(w_2\\), s√• udtrykket i (3) kommer til at svare til udtrykket i (4). Det er fordi, at i (3) indg√•r der kun et \\(b\\), mens der i (4) indg√•r et \\(b^2\\). Denne observation giver os imidlertid ogs√• en l√∏sning p√• vores problem. I stedet for at fodre perceptroner med forskellige v√¶rdier af \\(b\\) og \\(c\\), s√• giver vi den i stedet v√¶rdier af \\(b^2\\) og \\(c\\)!\n\n\n\n\n\n\nOpgave 5: Nye tr√¶ningsdata\n\n\n\n\n\n\nLav et nyt koordinatsystem og indtegn dine tr√¶ningsdata med v√¶rdien af \\(b^2\\) p√• \\(x\\)-aksen og v√¶rdien af \\(c\\) p√• \\(y\\)-aksen.\nHvilken linje kan du v√¶lge til at adskille de to grupper?"
  }
]