[
  {
    "objectID": "materialer.html",
    "href": "materialer.html",
    "title": "Materialer",
    "section": "",
    "text": "Perceptroner\n\n\nForløberen til kunstige neurale netværk er perceptroner, som du kan læse mere om her.\n\n\n\n\n\n\n\n\n\n\n\n\n\nKunstige neurale netværk\n\n\nEn grundig gennemgang af matematikken bag kunstige neurale netværk.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetningsafledede og gradientnedstigning\n\n\nEn introduktion til retningsafledede og gradientnedstigning.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaiv Bayes klassifier\n\n\nBeskrivelse af naiv Bayes klassifikation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering med K-means\n\n\nClustering med K-means er en metode, som kan bruges til at opdele observationer i et antal grupper.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverfitting, modeludvælgelse og krydsvalidering\n\n\nHvordan vælger man den bedste model til beskrivelse af data? Skal man bare vælge den mest komplicerede? Eller kan der mon gå noget galt? Det handler overfitting og…\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstande og feature-skalering\n\n\nLidt om afstande mellem punkter, ord, DNA-strenge og feature-skalering.\n\n\n\n\n\n\n\nNo matching items\n\n\n\nVideoer\nTil en del af materialerne findes en række videoer, hvor teorien forklares. Der er linket til de relevante videoer under de respektive materialer, men en samlet liste findes også her."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html",
    "href": "undervisningsforloeb/polynomium.html",
    "title": "Perceptroner og rødder",
    "section": "",
    "text": "Forudsætninger og tidsforbrug\n\n\n\n\n\nForløbet kræver kendskab til:\n\nAndengradspolynomier og rødder\n\nTidsforbrug: Ca. 90 minutter."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html#hvad-er-en-perceptron",
    "href": "undervisningsforloeb/polynomium.html#hvad-er-en-perceptron",
    "title": "Perceptroner og rødder",
    "section": "Hvad er en perceptron?",
    "text": "Hvad er en perceptron?\nI dette forløb skal vi arbejde med perceptoner, og det har du nok aldrig hørt om før! Start derfor med at se videoen herunder, hvor vi kort forklarer, hvad en perceptron er.\n\nDu kan også læse meget mere om perceptroner her."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html#andengradspolynomier-og-rødder",
    "href": "undervisningsforloeb/polynomium.html#andengradspolynomier-og-rødder",
    "title": "Perceptroner og rødder",
    "section": "Andengradspolynomier og rødder",
    "text": "Andengradspolynomier og rødder\nNu tilbage til vores eksempel om andengradspolynomier og rødder! Lad os for en god ordens skyld minde om, at et andengradspolynomium er en funktion med en forskrift på formen \\[\nf(x)=ax^2 + bx + c, \\quad a \\neq 0\n\\] Grafen for et andengradspolynomium kaldes som bekendt for en parabel. I figur 1 ses tre eksempler på sådanne parabler.\n\n\n\n\n\n\nFigur 1: Graferne for tre forskellige andengradspolynomier.\n\n\n\nHvis vi løser andengradsligningen \\[\nf(x)=ax^2 + bx + c=0\n\\] finder vi andengradspolynomiets rødder. Men at løse \\(f(x)=0\\), svarer netop til at bestemme, hvor den tilhørende parabel skærer \\(x\\)-aksen. I figur 1 kan vi se, at den grønne parabel skærer \\(x\\)-aksen to steder. Det vil sige, at det tilhørende andengradspolynomium har to rødder. Den røde parabel skærer \\(x\\)-aksen ét sted – det tilhørende andengradspolynomium har altså én rod. Endelig kan vi se, at den blå parabel slet ikke skærer \\(x\\)-aksen, og det tilhørende andengradspolynomium har derfor ingen rødder.\nDu husker nok, hvordan man bestemmer antallet af rødder i et andengradspolynomium. Vi har brug for diskriminanten \\(d\\):\n\\[\nd = b^2-4ac\n\\tag{1}\\]\nOg der gælder så, at \\[\n\\begin{aligned}\n&d&lt;0: \\quad f \\textrm{ har ingen rødder} \\\\\n&d=0: \\quad f \\textrm{ har én rod} \\\\\n&d&gt;0: \\quad f \\textrm{ har to rødder} \\\\\n\\end{aligned}\n\\]\nIdéen er nu at undersøge, om det er muligt at få en perceptron til at lære1, om et andengradspolynomium overhovedet har nogle rødder alene ude fra de tre koefficienter \\(a\\), \\(b\\) og \\(c\\) – og helt uden at kende noget til diskriminantformlen i (1)!\n1 Det er klart, at der er intet nyt under solen her. Vi kan jo bare selv beregne diskriminanten og svare på spørgsmålet. Men formålet er her at lære lidt om, hvad det vil sige at træne en perceptron i et tilfælde, hvor vi allerede selv kender svaret. Desuden findes der ingen lukkede løsningsformler for at bestemme rødder i et polynomium, så snart graden af polynomiet er \\(5\\) eller derover. Så idéen kan generaliseres, og så er den måske slet ikke så tosset endda!Inden vi går i gang, vil vi starte med at indse, at i stedet for at løse ligningen\n\\[\na x^2 + bx +c = 0\n\\tag{2}\\]\nSå kan vi lige så godt løse en ligning på formen\n\\[\nx^2 + bx +c =0\n\\] hvor altså \\(a=1\\). Det virker måske som en forsimpling, men da vi har antaget, at \\(a \\neq 0,\\) så kan vi i ligningen i (2) dividere igennem med \\(a\\) og få\n\\[\n\\begin{aligned}\n\\frac{a}{a} x^2 + \\frac{b}{a} x + \\frac{c}{a} &= \\frac{0}{a} \\quad \\Leftrightarrow \\\\\nx^2 + \\frac{b}{a} x + \\frac{c}{a} &= 0\n\\end{aligned}\n\\]\nDet betyder, at når vi skal bestemme rødder i andengradspolynomier, så er det tilstrækkeligt, at betragte andengradspolynomier med en forskrift på formen\n\\[\nf(x)=x^2+bx+c\n\\] fordi man simpelthen bare tager sit oprindelige andengradspolynomium og dividerer igennem med \\(a\\). Lad os illustrere det med et eksempel.\n\nBetragt andengradspolynomiet med forskriften\n\\[\nf(x)=-4x^2+8x+12\n\\] Her har vi \\(a=-4, b=8\\) og \\(c=12\\). Løser vi ligningen \\(f(x)=0\\), finder vi ud af, at \\(f\\) har to rødder nemlig \\(-1\\) og \\(3\\). Dividerer vi forskriften for \\(f\\) igennem med \\(a=-4\\) fås et nyt andengradspolynomium \\(g\\) med forskrift\n\\[\ng(x)=x^2-2x-3\n\\] Her er koefficienterne \\(a=1, b=-2\\) og \\(c=-3\\). Men \\(g\\) har præcis samme rødder som \\(f\\) – nemlig \\(-1\\) og \\(3\\). Dette ses også illustreret i figur 2, hvor grafen for \\(f\\) og \\(g\\) begge skærer \\(x\\)-aksen i \\(-1\\) og \\(3\\).\n\n\n\n\n\n\nFigur 2: Grafen for \\(f(x)=-4x^2+8x+12\\) (den blå) og \\(g(x)=x^2-2x-3\\) (den grønne), som begge skærer \\(x\\)-aksen samme sted. Det vil sige, at \\(f\\) og \\(g\\) har de samme rødder. I dette tilfælde \\(-1\\) og \\(3\\)."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html#træningsdata",
    "href": "undervisningsforloeb/polynomium.html#træningsdata",
    "title": "Perceptroner og rødder",
    "section": "Træningsdata",
    "text": "Træningsdata\nI dette eksempel vil vi nøjes med at se på, hvordan man kan træne en perceptron, så den forhåbentlig kan fortælle os, om et givent andengradspolynomium enten har ingen eller en eller to rødder. Det svarer til, at vi ønsker en perceptron, som for en given parabel kan svare på, om parablen skærer \\(x\\)-aksen eller ej (og altså ikke hvor mange gange den eventuelt skærer \\(x\\)-aksen).\n\n\n\n\n\n\nOpgave 1: Rødder eller ej?\n\n\n\n\n\nOvervej følgende:\n\nHvordan laver man et andengradspolynomium, der har én eller to rødder?\nHvordan laver man et andengradspolynomium, som ingen rødder har?\n\n\n\n\nFor at træne en perceptron, skal perceptronen se en masse eksempler på forskellige andengradspolynomier (det vil her sige med forskellige værdier af \\(b\\) og \\(c\\)) samtidig med, at vi fortæller perceptronen, om det tilhørende andengradspolynomium har rødder eller ej. At angive om et polynomium har rødder eller ej kalder man for en targetværdi. Tænk på det som en lille label du sætter på hvert eksempel, hvor du fortæller perceptronen, hvad det rigtige svar er – “det er altså det her, jeg gerne vil have, at du lærer!”. Samlet set kalder man de forskellige eksempler inklusiv targetværdien for træningsdata.\n\n\n\n\n\n\nOpgave 2: Træningsdata\n\n\n\n\n\n\nFind selv på forskellige værdier af \\(b\\) og \\(c\\) og find ud af om det tilhørende andengradspolynomium har rødder eller ej. Du skal finde på mindst to andengradspolynomier, der har rødder og to, der ikke har, men gerne et par stykker mere.Skriv dine værdier ned (enten bare på papir eller i f.eks. et regneark).\nIndtegn dine værdier \\(b\\) og \\(c\\) i et koordinatsystem, hvor værdien af \\(b\\) er på \\(x\\)-aksen, og værdien af \\(c\\) er på \\(y\\)-aksen. Herunder er lavet et eksempel med \\(b=0\\) og \\(c=-1\\), som svarer til et andengradspolynomium med to rødder samt \\(b=2\\) og \\(c=4\\), som svarer til et andengradspolynomium uden rødder."
  },
  {
    "objectID": "undervisningsforloeb/polynomium.html#træning-af-perceptron",
    "href": "undervisningsforloeb/polynomium.html#træning-af-perceptron",
    "title": "Perceptroner og rødder",
    "section": "Træning af perceptron",
    "text": "Træning af perceptron\nVi skal nu overveje, hvordan perceptronen kan trænes. Perceptronen gør dybest set det, at den prøver at bestemme en ret linje, som kan bruges til at adskille de røde punkter fra de blå punkter i punktplottet ovenfor. En ret linje i et 2-dimensionalt koordinatsystem har helt generelt en ligning på formen2\n2 Du er nok vant til at møde linjens ligning på denne form: \\(a \\cdot x+b \\cdot y+c=0\\). Skrivemåden, vi bruger her, er \\(w_0+w_1 \\cdot x + w_2 \\cdot y=0\\). Det vil sige i forhold til den skrivemåde, som du kender, så er \\(w_0=c, w_1=a\\) og \\(w_2=b\\).\\[\nw_0 + w_1 \\cdot x + w_2 \\cdot y = 0\n\\] Og for alle punkter på den ene side af linjen gælder, at\n\\[\nw_0 + w_1 \\cdot x + w_2 \\cdot y &gt; 0\n\\] og for alle punkter på den anden side, at\n\\[\nw_0 + w_1 \\cdot x + w_2 \\cdot y &lt; 0\n\\]\nI vores tilfælde har vi \\(b\\)-værdier ud af \\(x\\)-aksen og \\(c\\)-værdier op af \\(y\\)-aksen. Med de betegnelser bliver ligningen for en ret linje\n\\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c = 0\n\\]\nHer tænker vi altså på \\(b\\) og \\(c\\) som de variable.\nNår man træner en perceptron, gør man det ved hjælp af en algoritme, som løbende opdaterer vægtene \\(w_0, w_1\\) og \\(w_2\\), så den linje, vægtene giver, bliver bedre og bedre til at adskille de røde punkter fra de blå. Hver gang man opdaterer vægtene, siger man, at algoritmen har foretaget én iteration3.\n3 En iteration betyder en gentagelse.Du kan godt løse resten af opgaverne uden at forstå, hvorfor vægtene opdateres, som de gør. Men hvis du gerne vil have en forklaring så se videoen herunder.\n\n\n\n\n\n\n\nOpgave 3: Træning af perceptron\n\n\n\n\n\nLad os bruge startvægtene \\(w_0=1\\), \\(w_1=-3\\) og \\(w_2=2\\).\n\nHvilken linje svarer det til? Indtegn linjen i et koordinatsystemet.\nAdskiller denne linje de to grupper af punkter (med og uden rødder)? Hvis grupperne allerede er adskilt, skal du tilføje punktet med \\(b=2\\) og \\(c=1\\), som svarer til et andengradspolynomium, der har én rod.\n\nTræningsdata der svarer til polynomier med rødder, giver vi targetværdien \\(t=-1\\) og dem uden rødder får targetværdien \\(t=1\\).\nAlle punkter, der ligger over startlinjen, opfylder uligheden \\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c &gt; 0\n\\] og får outputværdien \\(o=1\\), mens dem, der ligger under linjen, opfylder den omvendte ulighed og får outputværdien \\(o=-1\\).\n\nUdvælg et punkt der bliver fejlklassificeret. Det vil sige som enten ligger under linjen (\\(o=-1\\)), men har target \\(t=1\\) svarende til ingen rødder eller omvendt.\nUdregn fejlen \\(error=t-o\\) som enten er -2 eller 2.\nOpdater nu alle tre vægte ved brug af opdateringsreglen (hvor du selv vælger \\(\\eta\\), f.eks. \\(\\eta=1\\)): \\[\n\\begin{aligned}\n  w_0 \\leftarrow w_0 + & \\,\\eta \\cdot error \\\\\n  w_1 \\leftarrow w_1 + & \\,\\eta \\cdot error \\cdot x_1 \\\\\n  w_2 \\leftarrow w_2 + & \\,\\eta \\cdot error \\cdot x_2 \\\\\n\\end{aligned}\n\\] Husk at \\(x_1\\) er \\(b\\)-værdien og \\(x_2\\) er \\(c\\)-værdien!\nFik du efter opdateringen en linje, der adskiller de to grupper?\nHvis ikke, kan du så selv lave en ret linje “på øjemål”, der adskiller dem?\n\n\n\n\n\n\n\n\n\n\nOpgave 4: Flere træningsdata\n\n\n\n\n\n\nAfgør om følgende andengradspolynomier har rødder og tilføj dem til dit træningsdata:\n\n\\[\n\\begin{aligned}\nf_1(x) &= x^2 + 10x + 26 \\\\\nf_2(x) &= x^2 + 10x + 24\\\\\nf_3(x) &= x^2 + 5x + 6\\\\\nf_4(x) &= x^2 + 5x + 7 \\\\\nf_5(x) &= x^2 + 2x + 1\\\\\nf_6(x) &= x^2 + 2x + 2 \\\\\n\\end{aligned}\n\\]\n\nKan det lade sig gøre at adskille de to grupper med en ret linje nu?\n\n\n\n\nSom du netop har opdaget, er det en umulig opgave, vi har givet perceptronen! Vi kan ikke finde en ret linje, som i alle tilfælde kan bruges til at adskille de to slags punkter. Lad os se på hvorfor. Som tidligere nævnt har vores linje en ligning på formen\n\\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c = 0\n\\tag{3}\\]\nVi husker nu på formlen for diskriminanten \\(d=b^2-4ac=b^2-4c\\), da \\(a=1\\) i vores eksempel. Skillelinjen for om andengradspolynomiet har ingen eller flere rødder, går netop ved \\(d=0\\). Det vil sige\n\\[\nb^2-4c =0\n\\tag{4}\\]\nMen vi kan ikke finde nogle værdier af \\(w_0, w_1\\) og \\(w_2\\), så udtrykket i (3) kommer til at svare til udtrykket i (4). Det er fordi, at i (3) indgår der kun et \\(b\\), mens der i (4) indgår et \\(b^2\\). Denne observation giver os imidlertid også en løsning på vores problem. I stedet for at fodre perceptroner med forskellige værdier af \\(b\\) og \\(c\\), så giver vi den i stedet værdier af \\(b^2\\) og \\(c\\)!\n\n\n\n\n\n\nOpgave 5: Nye træningsdata\n\n\n\n\n\n\nLav et nyt koordinatsystem og indtegn dine træningsdata med værdien af \\(b^2\\) på \\(x\\)-aksen og værdien af \\(c\\) på \\(y\\)-aksen.\nHvilken linje kan du vælge til at adskille de to grupper?"
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html",
    "title": "Hvem ligner du mest?",
    "section": "",
    "text": "Forudsætninger og tidsforbrug\n\n\n\n\n\nForløbet kræver kendskab til:\n\nAfstand mellem to punkter\n\nTidsforbrug: Ca. 2 x 90 minutter."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#introduktion",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#introduktion",
    "title": "Hvem ligner du mest?",
    "section": "Introduktion",
    "text": "Introduktion\nVi har apps på vores telefoner eller computere, som ud fra et billede kan genkende personer eller fra nogle få strofer kan genkende en sang. Der er scannere i lufthavne og andre steder, som kan genkende farlige ting, og biler har autopiloter, der selv holder afstanden til forankørende. Vi har også apps, som går den anden vej, og forvrænger et billede af en person, så personen bliver svær at genkende, men ofte alligevel kan genkendes, selvom ansigtet er fordrejet.\nSå alle steder og hele tiden foregår der bevist eller ubevist en skelnen mellem forskellige kategorier, men hvordan foregår denne skelnen i grunden? Hvis vi skulle svare fyldestgørende på dette spørgsmål, om overhovedet muligt, ville det nok betyde et langt studie på universitetet og sikkert mere end dette, men lad os starte med et meget simpelt eksempel, og tage den derfra.\nVi får brug for din viden om afstande mellem punkter, men kommer også til senere at se på andre former for afstande. Det er en fordel af lave opgaverne i grupper, da der kan blive en del af diskutere."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#video-hvem-ligner-du-mest",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#video-hvem-ligner-du-mest",
    "title": "Hvem ligner du mest?",
    "section": "VIDEO: Hvem ligner du mest?",
    "text": "VIDEO: Hvem ligner du mest?\nI denne video gives en kort introduktion til forløbet."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-1-afstand-mellem-to-punkter",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-1-afstand-mellem-to-punkter",
    "title": "Hvem ligner du mest?",
    "section": "Case 1 – Afstand mellem to punkter",
    "text": "Case 1 – Afstand mellem to punkter\n\n\n\n\n\n\nOpgave 1\n\n\n\n\n\nPå figur 1 nedenfor er der \\(10\\) røde punkter, \\(10\\) blå punkter og \\(10\\) grønne punkter – samt et enkelt gråt punkt \\(P\\) i midten. Et af de blå punkter er særligt markeret, men da det ligger ret langt fra \\(P\\) bør det nok ikke betyde så meget for, hvilket farve \\(P\\) skal have, som de punkter, der ligger tættere på \\(P\\)\nVurdér ud fra de øvrige punkter i nærheden af \\(P\\), om du synes, at \\(P\\) bør være rødt, blåt eller grønt så det mest ligner sine naboer.\n\n\n\n\n\n\n\n\n\nFigur 1: Koordinatsystem med \\(10\\) røde punkter, \\(10\\) blå punkter og \\(10\\) grønne punkter – samt et enkelt gråt punkt \\(P\\).\n\n\n\n\n\n\n\n\n\nOpgave 2\n\n\n\n\n\nPå figur 2 er der andre \\(31\\) punkter, samt en cirkel med radius \\(10\\) omkring det grå punkt \\(P(15,15)\\).\nTæl hvor mange røde, blå og grønne punkter, der ligger inde i cirklen. Kan det bruges til at beslutte, hvilken farve det grå punkt bør have?\n\n\n\n\n\n\n\n\n\nFigur 2: Koordinatsystem med \\(31\\) punkter, samt en cirkel med radius \\(10\\) omkring det grå punkt \\(P(15,15)\\)\n\n\n\n\n\n\n\n\n\nOpgave 2, fortsat\n\n\n\n\n\nHvilket resultat giver det, hvis cirklens radius kun var halvt så stor? Brug app’en nedenfor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpgave 3\n\n\n\n\n\nDet går nok ikke i længden blot at ville vurdere på øjemål, så du må til at regne lidt. I tabellen herunder er koordinater og farver på de \\(10\\) punkter, der ligger tættest på det grå punkt i opgave 2.\nUdregn afstanden fra det grå punkt \\(P(15,15)\\) til hver af disse \\(10\\) punkter.\nAfgør så, hvor mange af hver farve, der ligger indenfor en cirkel med radius \\(5\\) omkring det grå punkt. Det er nok specielt det blå punkt lige på kanten af cirklen, hvor beregningen er vigtig, men hvis en computer skal lave arbejdet automatisk, vil den jo beregne alle afstandene, da den ikke bare kan “kigge på figuren”, som et menneske kan.\nHvilken farve tyder det på, at det grå punkt bør have?\nTabellens data er punkter i kommatal. F.eks. er det første blå punkt \\((13,8 ; 19,9)\\), så det har x-koordinaten \\(13,8\\) og y-koordinaten \\(19,9\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFarve\n1\n2\n3\n4\n\n\n\n\nBlå\n\\((13,8 ; 19,9)\\)\n\\((8,2 ; 14,9)\\)\n\\((16,4 ; 14,1)\\)\n\\((15,5 ; 13,1)\\)\n\n\nRød\n\\((10,6 ; 16,0)\\)\n\\((16,3 ; 15,2)\\)\n\\((15,6 ; 11,3)\\)\n\n\n\nGrøn\n\\((11,1 ; 18,6)\\)\n\\((16,4 ; 17,5)\\)\n\\((21,7 ; 13,4)\\)\n\n\n\n\n\n\n\n\n\n\nOpgave 4\n\n\n\n\n\nOvervej, hvad der sker, hvis cirklens radius vælges som en meget lille værdi eller som en meget stor værdi i forhold til at bruge metoden til at bestemme, hvilken farve det grå punkt bør have. Kan det give problemer?\n\n\n\n\n\n\n\n\n\nOpgave 5\n\n\n\n\n\n\nI app’en herunder er \\(27\\) andre punkter indsat i et koordinatsystem, og der er tegnet en cirkel med centrum i det grå punkt \\(P(15,15)\\). Ændr på radius af cirklen og se, om det gør en forskel.\nDiskutér, hvilken radius, der er bedst.\n\n\n\n\n\n\n\nAfslutning på case 1\nDet bliver nok klart, at der er brug for en metode til at beslutte, hvor stor radius skal være for at få det bedste resultat. Kort og lidt simpelt forklaret, involverer det noget, som man kalder træningsdata og testdata. Man har f.eks. 1000 punkter, som man kender farven på. Man lader så som om, at man ikke kender farven på f.eks. 200 af punkterne (som man så kalder testdata). Så bruger man de øvrige 800 punkter (træningsdata) til at forudsige farven af hver af de 200 punkter i testdata. Dette gør man for forskellige værdier af radius, hvorefter man vælger den radius, der forudsiger flest af de 200 punkters farve korrekt. Hvis f.eks. radius 5 forudsiger 121 punkters farve korrekt, radius 10 furudsiger 135 punkters farve korrekt, og radius 20 kun forudsiger 87 punkters farve korrekt, så har radius 10 jo klaret sig bedst."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-2-manhattan-afstand",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-2-manhattan-afstand",
    "title": "Hvem ligner du mest?",
    "section": "Case 2 – Manhattan afstand",
    "text": "Case 2 – Manhattan afstand\nI nogle situationer giver den almindelige afstand mellem punkter ikke så god mening. F.eks. er de fleste veje på Manhattan i New York enten nord-syd eller øst-vest, så man kan ikke bare gå eller køre “på skrå”, men kun lodret eller vandret.\nHvis vi ser på punkterne \\(P(2,3)\\) og \\(Q(5,7)\\), så er den almindelige afstand \\(5\\) vha. Pythagoras, mens Manhattan afstanden er \\(3+4=7\\). Dette er illustreret på figur 3.\n\n\n\n\n\n\nFigur 3: Den almindelige afstand mellem punkterne \\(P(2,3)\\) og \\(Q(5,7)\\) er \\(5\\), mens Manhattan afstanden er \\(7\\).\n\n\n\n\n\n\n\n\n\nOpgave 6\n\n\n\n\n\nIndtegn punkterne \\(A(1,7)\\), \\(B(3,4)\\) og \\(C(5,6)\\) i et koordinatsystem.\nUdregn både almindelig afstand og Manhattan afstand mellem hvert par af punkter.\n\n\n\n\n\n\n\n\n\nOpgave 7\n\n\n\n\n\nI eksemplet og i opgave 6 var Manhattan afstanden større end den almindelige afstand, men kan Manhattan afstanden være mindre end den almindelige afstand eller kan de to afstande være ens?\n\n\n\n\n\n\n\n\n\nOpgave 8\n\n\n\n\n\nSe på figur 4 fra opgave 2 igen.\nPunkterne inde i cirklen har en almindelig afstand på under \\(10\\) til det grå punkt, men hvilke af punkterne har en Manhattan afstand på under \\(10\\) til det grå punkt?\nDen almindelige afstand giver en cirkel med det grå punkt i centrum, men hvilken figur giver Manhatten afstand omkring det grå punkt?\nHvilken farve bør det grå punkt derfor have, hvis Manhattan afstanden benyttes? Giver det samme resultatet, som du fik i opgave 2?\n\n\n\n\n\n\n\n\n\nFigur 4: Koordinatsystem med \\(31\\) punkter, samt en cirkel med radius \\(10\\) omkring det grå punkt \\(P(15,15)\\)\n\n\n\n\n\n\n\n\n\nOpgave 9\n\n\n\n\n\nOpstil en formel for den almindelige afstand mellem to punkter med koordinaterne \\((x_1, y_1)\\) og \\((x_2, y_2)\\). Det er en formel, du kender i forvejen.\nOpstil tilsvarende en formel for Manhattan afstanden. Det er nok ikke en formel, du har set før.\nDu kan læse mere om Manhattan afstanden her."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-3-hvor-ens-er-to-tekster",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-3-hvor-ens-er-to-tekster",
    "title": "Hvem ligner du mest?",
    "section": "Case 3 – Hvor ens er to tekster?",
    "text": "Case 3 – Hvor ens er to tekster?\nI forbindelse med at undersøge om en tekst, f.eks. en dansk stil, er plagiat, bliver det relevant at sammenligne, hvor ens to tekster er. Helt så avanceret bliver det dog ikke her.\nVi vil kun se meget simpelt på ord med \\(5\\) bogstaver, og hvordan man f.eks. kan måle afstande mellem forskellige ord. Vi vil se på alle kombinationer af \\(5\\) bogstaver, også f.eks. xtmsp, selvom de ikke er normale ord.\n\n\n\n\n\n\nOpgave 10\n\n\n\n\n\nI tabellen herunder ses ordet “nedes” sammen med ordene “model”, “metal” og “nudts”.\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\ne\nd\ne\ns\n\n\n\n\n\n\nm\no\nd\ne\nl\n\n\n\n\nm\ne\nt\na\nl\n\n\n\n\nn\nu\nd\nt\ns\n\n\n\n\nHvilket af ordene “model”, “metal” og “nudts” synes du, at ordet “nedes” ligner mest. Begrund dit svar.\n\n\n\n\n\n\n\n\n\nOpgave 11\n\n\n\n\n\nHvis vi vælger, at afstanden mellem to ord er antallet af bogstaver, som er forskellige incl. placering, så er afstanden mellem “xtmsp” og “xmtsq” \\(3\\), da kun \\(2\\) af de \\(5\\) bogstaver matcher incl. placering i de to ord, nemlig x og s.\nUdregn med den metode afstanden mellem “nedes” og hver af de \\(3\\) ord i opgave 10. Var det sådan du allerede havde gjort det i opgave 10, eller gav dette et andet resultat?\n\n\n\n\n\n\n\n\n\nOpgave 12\n\n\n\n\n\nOvervej og diskuter andre måder at regne afstand mellem to ord på hver \\(5\\) bogstaver. Det kunne f.eks. være noget, hvor ombytning af to nabobogstaver giver mindre afstand end helt tilfældige andre bogstaver, så f.eks. “kolon” og “kloon” er tættere på hinanden end “kolon” og “kston”."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-4-dna-strenge-og-alignment",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-4-dna-strenge-og-alignment",
    "title": "Hvem ligner du mest?",
    "section": "Case 4 – DNA-strenge og alignment",
    "text": "Case 4 – DNA-strenge og alignment\nUden i øvrigt at komme ind på biologien repræsenteres DNA som meget lange tekststrenge. Når mennesker og chimpanser er meget ens, kommer det til udtryk ved, at DNA-strengen for et menneske ligner den for en chimpanse meget, der er altså en kort afstand mellem DNA for et menneske og DNA for en chimpanse. Indenfor biologien kaldes dette for alignment. I stedet for at sammenligne på DNA niveau, sammenlignes også nogle gange på aminosyre niveau, hvilket vi vil bruge her,\nFølgende eksempel, der viser et meget lille udsnit af sådanne koder fra mus, rotter, mennesker og gær er taget fra Tema12-Link5.pdf (nucleus.dk), der kan anbefales, hvis man ønsker at arbejde mere med alignment.\n\n\n\nDyr\nKode\n\n\n\n\nMus\nS W A W A E G W T R Y G P\n\n\nRotte\nK W V W A E G W T R Y G P\n\n\nMenneske\nA W A W A E G W T R Y G P\n\n\nGær\nE W L R K P G W V K Y V P\n\n\n\nHvis afstanden her regnes som antal bogstaver, der er forskellige, ses det at afstanden mellem mus og rotte er på \\(2\\), som vist nedenfor.\n\n\n\nDyr\nKode\n\n\n\n\nMus\nS W A W A E G W T R Y G P\n\n\nRotte\nK W V W A E G W T R Y G P\n\n\n\n\n\n\n\n\n\nOpgave 13\n\n\n\n\n\nUdregn på tilsvarende vis afstandene mellem mus-menneske, mus-gær, rotte-menneske, rotte-gær og menneske-gær.\n\n\n\nNår resultatet sikkert virker overraskende, skyldes det, at vi kun har set på et meget lille udsnit af DNA for de fire. I figur 5 har man set på hele det protein, som udsnittet stammer fra, og her bliver resultatet mere som forventet.\n\n\n\n\n\n\nFigur 5: De faktiske afstande mellem mus-menneske, mus-gær, rotte-menneske, rotte-gær og menneske-gær."
  },
  {
    "objectID": "undervisningsforloeb/hvem_ligner_du_mest.html#case-5-hvilken-politiker-er-du-mest-enig-med",
    "href": "undervisningsforloeb/hvem_ligner_du_mest.html#case-5-hvilken-politiker-er-du-mest-enig-med",
    "title": "Hvem ligner du mest?",
    "section": "Case 5 – Hvilken politiker er du mest enig med?",
    "text": "Case 5 – Hvilken politiker er du mest enig med?\nOp til både folketingsvalg og kommunal- og regionalvalg kan man svare på en række spørgsmål, hvorefter ens svar bliver sammenlignet med politikernes svar på de samme spørgsmål. Herefter kan man så se, hvem man er mest enig med.\nHer er et eksempel fra kommunal- og regionalvalget i 2021.\nTag kandidattesten Kommunalvalg 2021 - Altinget - Alt om politik: altinget.dk\n\n\n\n\n\n\nOpgave 14\n\n\n\n\n\nKlik på linket ovenfor, vælg din egen kommune i testen og besvar spørgsmålene. Se derefter, hvem dine svar er mest enige med, og hvor mange procent enige I er.\n\n\n\nMen hvordan virker det mon? Hvordan vurderes, hvilken kandidat du er mest enig med, og hvordan udregnes, hvor mange procent enige I er?\nTil hvert spørgsmål kan der svares “helt uenig”, “uenig”, “enig” eller “helt enig”, men desuden er der en skjult “neutral” mulighed i midten, som man ikke kan vælge.\n\n\n\nHelt uenig\nUenig\nNeutral\nEnig\nHelt enig\n\n\n\nAfstanden mellem to svar regnes som antal “felter” i tabellen, så afstanden mellem Uenig og Enig er \\(2\\), mens afstanden mellem Enig og Helt enig er \\(1\\), og den største afstand er \\(4\\).\nI figur 6 ses en persons svar og et partis svar på \\(23\\) spørgsmål til kommunalvalget i 2021. Ved et partis svar forstås det svar, som flest af kandidaterne fra partiet har givet (ved lighed afgjort ud af, hvilken kandidat, der står først på listen). Så det er typetallet (typesvaret), der er anvendt for partierne, ikke gennemsnittet af svarene fra partiets kandidater. Figuren er fra testen på Altinget. Bemærk, at antallet af spørgsmål kan variere fra kommune til kommune, så du har måske færre eller flere spørgsmål.\n\n\n\n\n\n\nFigur 6: En persons svar (sort) og et partis svar (rød) på \\(23\\) spørgsmål til kommunalvalget i 2021.\n\n\n\nAfstanden i det første spørgsmål er \\(1\\), afstanden i det andet spørgsmål også \\(1\\) og afstanden i det tredje spørgsmål er \\(2\\) pga. den skjulte “neutral” mulighed i midten.\n\n\n\n\n\n\nOpgave 15\n\n\n\n\n\nDe 3 første afstand er altså 1, 1 og 2. Udregn afstanden for hver af de øvrige \\(20\\) spørgsmål.\nLæg så afstandene sammen, hvilket svarer til en form for Manhattan afstand, da afstanden regnes for hvert enkelt spørgsmål for sig. Hvilken samlet afstand giver det?\n\n\n\n\n\n\n\n\n\nOpgave 16\n\n\n\n\n\nHvad er den mindst mulig samlede afstand for de \\(23\\) spørgsmål? Hvordan skal svarene for partiet og for vælgeren se ud fra at få denne afstand?\nHvad er den størst mulige afstand, og hvordan skal svarene så se ud?\n\n\n\n\n\n\n\n\n\nOpgave 17\n\n\n\n\n\nPå figur 7 ses det, at siden angiver, at enigheden med Socialdemokratiet er på \\(79 \\%\\).\nOvervej, hvordan den afstand du udregnede i opgave 15 og den største mulige afstand, som du fandt i opgave 16, kan betyde, at enigheden er \\(79 \\%\\).\n\n\n\n\n\n\n\n\n\nFigur 7: På siden angives det, at enigheden med Socialdemokratiet er på \\(79 \\%\\).\n\n\n\n\n\n\n\n\n\nOpgave 18\n\n\n\n\n\nVend tilbage til dine egne svar på testen.\nBeregn afstand og procent i forhold til den kandidat, du var mest enig med, og den kandidat, du var mest uenig med.\nBeregn desuden afstand og procent til det parti, du var mest enig med, og til det parti, du var mest uenig med.\nPasser dine udregninger med sidens procenter?\n\n\n\n\n\n\n\n\n\nOpgave 19 (svær)\n\n\n\n\n\nDen kandidat, som svarene i opgave 15 var mest enig med, giver en procent på \\(76 \\%\\), så procenten for samtlige kandidater fra Socialdemokratiet er altså lavere end procenten for selve partiet.\nDet virker måske umiddelbart underligt. Overvej, hvorfor det faktisk er korrekt ud fra den metode, som siden anvender til beregningerne.\nDiskutér derefter, om procenten for partiet kunne være beregnet anderledes."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_v2.html",
    "href": "undervisningsforloeb/polynomium_v2.html",
    "title": "AI og rødder i andengradspolynomier",
    "section": "",
    "text": "Forudsætninger og tidsforbrug\n\n\n\n\n\nForløbet kræver kendskab til:\n\nRette linjer.\nAndengradspolynomier og rødder.\n\nTidsforbrug: Ca. 90 minutter.\nVi anbefaler, at I i dette forløb arbejder i grupper på 3-4 elever."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_v2.html#andengradspolynomier-og-rødder",
    "href": "undervisningsforloeb/polynomium_v2.html#andengradspolynomier-og-rødder",
    "title": "AI og rødder i andengradspolynomier",
    "section": "Andengradspolynomier og rødder",
    "text": "Andengradspolynomier og rødder\nLad os lige starte med at minde om, at et andengradspolynomium er en funktion med en forskrift på formen \\[\nf(x)=ax^2 + bx + c, \\quad a \\neq 0\n\\] Grafen for et andengradspolynomium kaldes som bekendt for en parabel. I figur 1 ses tre eksempler på sådanne parabler.\n\n\n\n\n\n\nFigur 1: Graferne for tre forskellige andengradspolynomier.\n\n\n\nHvis vi løser andengradsligningen \\[\nf(x)=ax^2 + bx + c=0\n\\] finder vi andengradspolynomiets rødder. Men at løse \\(f(x)=0\\), svarer netop til at bestemme, hvor den tilhørende parabel skærer \\(x\\)-aksen. I figur 1 kan vi se, at den grønne parabel skærer \\(x\\)-aksen to steder. Det vil sige, at det tilhørende andengradspolynomium har to rødder. Den røde parabel skærer \\(x\\)-aksen ét sted – det tilhørende andengradspolynomium har altså én rod. Endelig kan vi se, at den blå parabel slet ikke skærer \\(x\\)-aksen, og det tilhørende andengradspolynomium har derfor ingen rødder.\nDu husker nok, hvordan man bestemmer antallet af rødder i et andengradspolynomium. Vi har brug for diskriminanten \\(d\\):\n\\[\nd = b^2-4ac\n\\tag{1}\\]\nOg der gælder så, at \\[\n\\begin{aligned}\n&d&lt;0: \\quad f \\text{ har ingen rødder} \\\\\n&d=0: \\quad f \\text{ har én rod} \\\\\n&d&gt;0: \\quad f \\text{ har to rødder} \\\\\n\\end{aligned}\n\\]\nIdéen er nu at undersøge, om vi kan bruge kunstig intelligens til at afgøre1, om et andengradspolynomium overhovedet har nogle rødder alene ude fra de tre koefficienter \\(a\\), \\(b\\) og \\(c\\) – og helt uden at kende noget til diskriminantformlen i (1)!\n1 Det er klart, at der er intet nyt under solen her. Vi kan jo bare selv beregne diskriminanten og svare på spørgsmålet. Men formålet er her at lære lidt om, hvad det vil sige at træne en perceptron i et tilfælde, hvor vi allerede selv kender svaret. Desuden findes der ingen lukkede løsningsformler for at bestemme rødder i et polynomium, så snart graden af polynomiet er \\(5\\) eller derover. Så idéen kan generaliseres, og så er den måske slet ikke så tosset endda!Inden vi går i gang, vil vi starte med at indse, at i stedet for at løse ligningen\n\\[\na x^2 + bx +c = 0\n\\tag{2}\\]\nSå kan vi lige så godt løse en ligning på formen\n\\[\nx^2 + bx +c =0\n\\] hvor altså \\(a=1\\). Det virker måske som en forsimpling, men da vi har antaget, at \\(a \\neq 0,\\) så kan vi i ligningen i (2) dividere igennem med \\(a\\) og få\n\\[\n\\begin{aligned}\n\\frac{a}{a} x^2 + \\frac{b}{a} x + \\frac{c}{a} &= \\frac{0}{a} \\quad \\Leftrightarrow \\\\\nx^2 + \\frac{b}{a} x + \\frac{c}{a} &= 0\n\\end{aligned}\n\\]\nDet betyder, at når vi skal bestemme rødder i andengradspolynomier, så er det tilstrækkeligt, at betragte andengradspolynomier med en forskrift på formen\n\\[\nf(x)=x^2+bx+c\n\\] fordi man simpelthen bare tager sit oprindelige andengradspolynomium og dividerer igennem med \\(a\\). Lad os illustrere det med et eksempel.\n\nBetragt andengradspolynomiet med forskriften\n\\[\nf(x)=-4x^2+8x+12\n\\] Her har vi \\(a=-4, b=8\\) og \\(c=12\\). Løser vi ligningen \\(f(x)=0\\), finder vi ud af, at \\(f\\) har to rødder nemlig \\(-1\\) og \\(3\\). Dividerer vi forskriften for \\(f\\) igennem med \\(a=-4\\) fås et nyt andengradspolynomium \\(g\\) med forskrift\n\\[\ng(x)=x^2-2x-3\n\\] Her er koefficienterne \\(a=1, b=-2\\) og \\(c=-3\\). Men \\(g\\) har præcis samme rødder som \\(f\\) – nemlig \\(-1\\) og \\(3\\). Dette ses også illustreret i figur 2, hvor grafen for \\(f\\) og \\(g\\) begge skærer \\(x\\)-aksen i \\(-1\\) og \\(3\\).\n\n\n\n\n\n\nFigur 2: Grafen for \\(f(x)=-4x^2+8x+12\\) (den blå) og \\(g(x)=x^2-2x-3\\) (den grønne), som begge skærer \\(x\\)-aksen samme sted. Det vil sige, at \\(f\\) og \\(g\\) har de samme rødder. I dette tilfælde \\(-1\\) og \\(3\\)."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_v2.html#træningsdata",
    "href": "undervisningsforloeb/polynomium_v2.html#træningsdata",
    "title": "AI og rødder i andengradspolynomier",
    "section": "Træningsdata",
    "text": "Træningsdata\nI dette eksempel vil vi nøjes med at se på, om vi kan bruge en metode fra kunstig intelligens, så vi forhåbentlig kan få svar på, om et givent andengradspolynomium enten har ingen eller en eller to rødder. Vi vil altså gerne finde en metode, som for en given parabel kan svare på, om parablen skærer \\(x\\)-aksen eller ej (og altså ikke hvor mange gange den eventuelt skærer \\(x\\)-aksen).\n\n\n\n\n\n\nOpgave 1: Rødder eller ej?\n\n\n\n\n\nOvervej følgende:\n\nHvordan laver man et andengradspolynomium, der har én eller to rødder?\nHvordan laver man et andengradspolynomium, som ingen rødder har?\n\n\n\n\nFor at bruge kunstig intelligens skal vi have lavet en masse eksempler på forskellige andengradspolynomier (det vil her sige med forskellige værdier af \\(b\\) og \\(c\\)) samtidig med, at vi også finder ud af, om det tilhørende andengradspolynomium har rødder eller ej. At angive om et polynomium har rødder eller ej kalder man for en targetværdi. Tænk på det som en lille label du sætter på hvert eksempel, hvor du fortæller, hvad det rigtige svar er – “det er altså det her, jeg gerne vil have, at du lærer!”. Samlet set kalder man de forskellige eksempler inklusiv targetværdien for træningsdata.\n\n\n\n\n\n\nOpgave 2: Træningsdata\n\n\n\n\n\nAlle i gruppen skal nu:\n\nFinde et andengradspolynomium som ikke har nogle rødder (husk at \\(a=1\\)). Notér din værdi af \\(b\\) og \\(c\\) og sæt her targetværdien \\(t\\) til \\(t=-1\\).\nFinde et andengradspolynomium som har én eller to rødder (husk at \\(a=1\\)). Notér din værdi af \\(b\\) og \\(c\\) og sæt her targetværdien \\(t\\) til \\(t=1\\).\n\nI skal nu indsætte jeres forskellige værdier for \\(b, c\\) og \\(t\\) i et regneark. I videoen herunder er det vist, hvordan man gør i GeoGebra:\nVIDEO MED SCREENCAST HER.\nDisse data er nu præcis det, man kalder for træningsdata.\n\nIndtast jeres værdier i et regnark, som det er vist i videoen.\nIndtegn dine værdier af \\(b\\) og \\(c\\) i et koordinatsystem, hvor værdien af \\(b\\) er på \\(x\\)-aksen, og værdien af \\(c\\) er på \\(y\\)-aksen. Hvis \\((b,c)\\)-punktet svarer til et andegradspolynomium, som har rødder, farves punktet rødt og ellers farves det blåt. Dette er også vist i videoen ovenfor."
  },
  {
    "objectID": "undervisningsforloeb/polynomium_v2.html#træning-af-kunstig-intelligens",
    "href": "undervisningsforloeb/polynomium_v2.html#træning-af-kunstig-intelligens",
    "title": "AI og rødder i andengradspolynomier",
    "section": "Træning af kunstig intelligens",
    "text": "Træning af kunstig intelligens\nEn simpel metode inden for kunstig intelligens er, at prøve at bestemme en ret linje, som kan bruges til at adskille de røde punkter fra de blå punkter i det punktplot, som I har lavet i opgave 2.\nHvis man skal have en computer til at gøre det, så vil man typisk starte med en hel tilfældig ret linje med en ligning på formen \\(y=a \\cdot x+b\\), og så prøve at opdatere hældningen \\(a\\) og skæring med \\(y\\)-aksen \\(b\\), sådan at linjen bliver bedre og bedre til at adskille de røde punkter fra de blå.\nSTART HER!\nNår man træner en perceptron, gør man det ved hjælp af en algoritme, som løbende opdaterer vægtene \\(w_0, w_1\\) og \\(w_2\\), så den linje, vægtene giver, bliver bedre og bedre til at adskille de røde punkter fra de blå. Hver gang man opdaterer vægtene, siger man, at algoritmen har foretaget én iteration2.\n2 En iteration betyder en gentagelse.Du kan godt løse resten af opgaverne uden at forstå, hvorfor vægtene opdateres, som de gør. Men hvis du gerne vil have en forklaring så se videoen herunder.\n\n\n\n\n\n\n\nOpgave 3: Træning af perceptron\n\n\n\n\n\nLad os bruge startvægtene \\(w_0=1\\), \\(w_1=-3\\) og \\(w_2=2\\).\n\nHvilken linje svarer det til? Indtegn linjen i et koordinatsystemet.\nAdskiller denne linje de to grupper af punkter (med og uden rødder)? Hvis grupperne allerede er adskilt, skal du tilføje punktet med \\(b=2\\) og \\(c=1\\), som svarer til et andengradspolynomium, der har én rod.\n\nTræningsdata der svarer til polynomier med rødder, giver vi targetværdien \\(t=-1\\) og dem uden rødder får targetværdien \\(t=1\\).\nAlle punkter, der ligger over startlinjen, opfylder uligheden \\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c &gt; 0\n\\] og får outputværdien \\(o=1\\), mens dem, der ligger under linjen, opfylder den omvendte ulighed og får outputværdien \\(o=-1\\).\n\nUdvælg et punkt der bliver fejlklassificeret. Det vil sige som enten ligger under linjen (\\(o=-1\\)), men har target \\(t=1\\) svarende til ingen rødder eller omvendt.\nUdregn fejlen \\(error=t-o\\) som enten er -2 eller 2.\nOpdater nu alle tre vægte ved brug af opdateringsreglen (hvor du selv vælger \\(\\eta\\), f.eks. \\(\\eta=1\\)): \\[\n\\begin{aligned}\n  w_0 \\leftarrow w_0 + & \\,\\eta \\cdot error \\\\\n  w_1 \\leftarrow w_1 + & \\,\\eta \\cdot error \\cdot x_1 \\\\\n  w_2 \\leftarrow w_2 + & \\,\\eta \\cdot error \\cdot x_2 \\\\\n\\end{aligned}\n\\] Husk at \\(x_1\\) er \\(b\\)-værdien og \\(x_2\\) er \\(c\\)-værdien!\nFik du efter opdateringen en linje, der adskiller de to grupper?\nHvis ikke, kan du så selv lave en ret linje “på øjemål”, der adskiller dem?\n\n\n\n\n\n\n\n\n\n\nOpgave 4: Flere træningsdata\n\n\n\n\n\n\nAfgør om følgende andengradspolynomier har rødder og tilføj dem til dit træningsdata:\n\n\\[\n\\begin{aligned}\nf_1(x) &= x^2 + 10x + 26 \\\\\nf_2(x) &= x^2 + 10x + 24\\\\\nf_3(x) &= x^2 + 5x + 6\\\\\nf_4(x) &= x^2 + 5x + 7 \\\\\nf_5(x) &= x^2 + 2x + 1\\\\\nf_6(x) &= x^2 + 2x + 2 \\\\\n\\end{aligned}\n\\]\n\nKan det lade sig gøre at adskille de to grupper med en ret linje nu?\n\n\n\n\nSom du netop har opdaget, er det en umulig opgave, vi har givet perceptronen! Vi kan ikke finde en ret linje, som i alle tilfælde kan bruges til at adskille de to slags punkter. Lad os se på hvorfor. Som tidligere nævnt har vores linje en ligning på formen\n\\[\nw_0 + w_1 \\cdot b + w_2 \\cdot c = 0\n\\tag{3}\\]\nVi husker nu på formlen for diskriminanten \\(d=b^2-4ac=b^2-4c\\), da \\(a=1\\) i vores eksempel. Skillelinjen for om andengradspolynomiet har ingen eller flere rødder, går netop ved \\(d=0\\). Det vil sige\n\\[\nb^2-4c =0\n\\tag{4}\\]\nMen vi kan ikke finde nogle værdier af \\(w_0, w_1\\) og \\(w_2\\), så udtrykket i (3) kommer til at svare til udtrykket i (4). Det er fordi, at i (3) indgår der kun et \\(b\\), mens der i (4) indgår et \\(b^2\\). Denne observation giver os imidlertid også en løsning på vores problem. I stedet for at fodre perceptroner med forskellige værdier af \\(b\\) og \\(c\\), så giver vi den i stedet værdier af \\(b^2\\) og \\(c\\)!\n\n\n\n\n\n\nOpgave 5: Nye træningsdata\n\n\n\n\n\n\nLav et nyt koordinatsystem og indtegn dine træningsdata med værdien af \\(b^2\\) på \\(x\\)-aksen og værdien af \\(c\\) på \\(y\\)-aksen.\nHvilken linje kan du vælge til at adskille de to grupper?"
  },
  {
    "objectID": "materialer/test2.html",
    "href": "materialer/test2.html",
    "title": "Kunstige neurale netværk",
    "section": "",
    "text": "\\[\n\\textrm{prædiktion}=\n\\begin{cases}\n\\textrm{ja} & \\textrm{hvis } o \\geq 0,5 \\\\\n\\textrm{nej} & \\textrm{hvis } o &lt; 0,5 \\\\\n\\end{cases}\n\\tag{1}\\]\nse (1)"
  },
  {
    "objectID": "materialer/kmeans/kmeans.html",
    "href": "materialer/kmeans/kmeans.html",
    "title": "Clustering med K-means",
    "section": "",
    "text": "K-means\nNår \\(K\\)-means metoden bruges, er målet at inddele nogle observationer i grupper, så observationerne i hver gruppe minder meget om hinanden.\n\n\n\n\n\n\nFigur 1: Til venstre ses en række observationer, som ønskes inddelt i \\(3\\) grupper. Til højre ses et bud på en sådan inddeling.\n\n\n\nPå figur 1 til venstre ses en række punkter, hvor vi ønsker at inddele punkterne i 3 grupper. Man kan nok godt få en idé om, hvordan grupperne kan laves alene ved at se på billedet til venstre. På figur 1 til højre ses et bud på en løsning, som ser fornuftig ud, men ved nogle punkter tænker man nok alligevel lidt, om de nu skulle have været i den orange eller blå gruppe. Når vi arbejder med \\(K\\)-means, så er idéen, at vi ikke på forhånd har nogle observationer, hvor vi ved hvilken gruppe, de tilhører. Med andre ord har vi altså ikke et træningsdatasæt at gå ud fra her. Derfor taler man også om unsupervised learning. Det eneste, vi ved om vores punkter i figur 1, er deres \\(x\\)- og \\(y\\)-koordinat og ud fra det, skal vi så prøve at danne nogle grupper. Antallet af grupper ved man faktisk heller ikke nødvendigvis noget om – så her er det et valg, at vi har besluttet at prøve at inddele data i 3 grupper. Det kunne i princippet lige så godt have været 2 eller 4 grupper eller noget helt andet!\nObservationerne vil vi her kalde for \\(\\vec{x_1}, \\vec{x_2},....,\\vec{x_n}\\), så der ialt er \\(n\\) observationer. Hver observation er et punkt med \\(d\\) koordinater (som dog behandles, som var det vektorer/stedvektorer), og som udgangspunkt benyttes euklidisk afstand til at bestemme afstand mellem punkter. I eksemplet i figur 1 er \\(d=2\\), fordi alle punkter i planen har 2 koordinater.\nGivet et heltal \\(k\\), så ønsker vi at opdele de \\(n\\) observationer \\(\\vec{x_1}, \\vec{x_2},....,\\vec{x_n}\\) i \\(k\\) grupper, som vi kalder for \\(S_1,S_2,....,S_k\\). Antallet af observationer i gruppen1 \\(S_i\\) betegnes med \\(|S_i|\\).\n1 En gruppe \\(S_i\\) er egentlig en mængde, og \\(|S_i|\\) er kardinaliteten af denne mængde – altså antallet af elementer i mængden.Hele idéen i \\(K\\)-means metoden er, at det skal være sådan, at observationerne i samme gruppe ligger tæt på hinanden. Det er også sådan, at vi har farvet punkterne til højre i figur 1.\nHvis man skal oversætte det til matematik, så betyder det, at vi ønsker at minimere følgende sum (som vi kalder for \\(SUMPAR\\))\n\\[SUMPAR=\\sum_{i=1}^{k}\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2\\] Det ser måske lidt voldsomt ud, men lad os prøve at nedbryde ovenstående lidt. Vi forestiller os, at vi har de \\(k\\) grupper \\(S_1, S_2, \\dots , S_k\\). Vi ser først på et punkt \\(\\vec p \\in S_i\\). Den kvadrerede afstand til et andet punkt \\(\\vec q\\) i samme gruppe er givet ved udtrykket\n\\[\n\\|\\vec p- \\vec q\\|^2\n\\]\nDet vil sige, den euklidiske afstand mellem \\(\\vec p\\) og \\(\\vec q\\) opløftet i anden. Den gennemsnitlige kvadrerede afstand til alle punkter i samme gruppe \\(S_i\\) vil derfor være\n\\[\\frac{1}{|S_i|} \\sum_{\\vec q \\in S_i} \\|\\vec p- \\vec q\\|^2 \\]\nDet er altså den gennemsnitlige kvadrerede afstand fra ét punkt \\(\\vec p\\) til alle andre punkter i samme gruppe – inklusiv punktet selv. Vi vil nu lægge alle disse gennemsnitlige kvadrerede afstande sammen for alle punkter i \\(S_i\\). Gør vi det, får vi:\n\\[ \\sum_{\\vec p\\in S_i}\\frac{1}{|S_i|}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2 \\]\nDa størrelsen \\(\\frac{1}{|S_i|}\\) indgår i alle led i den yderste sum, kan vi sætte \\(\\frac{1}{|S_i|}\\) uden for det yderste sumtegn2. Derfor kan vi omskrive ovenstående til\n2 Det svarer bare til at sætte uden for en parentes.\\[\n\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2\n\\]\nDet her vil vi gerne gøre for alle grupper, og derfor ender vi samlet set med\n\\[\nSUMPAR=\\sum_{i=1}^{k}\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2\n\\tag{1}\\]\nAlt i alt får vi altså, at \\(SUMPAR\\) giver summen af hvert punkts gennemsnitlige kvadrerede afstand til alle punkter i samme gruppe som sig selv (inklusiv sig selv).\nIdéen er så nu, at vi vil prøve at bestemme grupperne \\(S_1, S_2, \\dots, S_k\\) sådan, at denne sum bliver så lille så muligt. Det vil nemlig svare til, at de punkter, der ligger tæt på hinanden, kommer i samme gruppe, og punkter, som ligger langt væk fra hinanden, kommer i forskellige grupper.\nDet er desværre ikke lige til at finde den optimale løsning på dette problem, men her angives en metode/algoritme, som forhåbentlig finder en god løsning.\n\n\nAlgoritme\nVi vil nu se på en metode til at finde en god løsning til \\(K\\)-means problemet. Vi får her brug for “midterpunktet” for hver gruppe, som vi vil kalde for \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\). Ved midterpunktet vil vi simpelthen bare forstå gennemsnittet af alle punkter i den pågældende gruppe.\nI algoritmen vil vi prøve at minimere følgende sum\n\\[\nSUMMIDT=\\sum_{i=1}^{k}\\sum_{\\vec p\\in S_i}\\|\\vec p-\\vec{\\mu_i}\\|^2\n\\tag{2}\\]\nHer summeres altså den kvadrerede afstand fra hvert punkt til midterpunktet for gruppen, som punktet er i. Og det gør man så for alle grupper og lægger alle de kvadredede afstande sammen. Senere vil vi se på sammenhængen mellem summen \\(SUMPAR\\) og summen \\(SUMMIDT\\).\nSpørgsmålet er nu, hvordan man kommer igang med at fastlægge grupper og midterpunkter, for vi kender ikke mængderne \\(S_1,S_2,....,S_k\\) og dermed heller ikke midterpunkterne \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\).\nFor at løse det problem vil vi bruge følgende fremgangsmåde/algoritme:\n\nStart med at tage hver eneste observation og tilføj den til en tilfældig gruppe (der skal mindst være én observation i hver gruppe).\nMidterpunkterne bestemmes ved at lade\n\n\\[\n\\vec{\\mu_i}=\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\vec p\n\\]\n\nFor hver af de \\(n\\) observationer findes det midterpunkt, der har den mindste afstand til punktet. Hvis det for en observation \\(\\vec{x_i}\\) er midterpunktet \\(\\vec{\\mu_a}\\), der er nærmest, skal \\(\\vec{x_i}\\) være i mængden \\(S_a\\).\nGentag trin 2 og 3 indtil vi kommer til et tidspunkt, hvor ingen punkter kommer til at skifte til en anden gruppe.\n\nDet virker jo meget rimeligt. Så er spørgsmålet bare, om denne fremgangsmåde virkelig fungerer! Det vil vi se nærmere på i afsnittet om sammenhængen mellem SUMPAR og SUMMIDT. Men lad os starte med at se på et par eksempler.\n\n\nEksempel på beregning af midterpunkter\nFørst kunne det måske være rart at få en fornemmelse af, hvorfor \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\) betegnes som midterpunkter.\n\nEksempel 1 Vi forestiller os, at vi har to grupper med følgende punkter:\n\nGruppe 1 med punkterne \\((3,9)\\) og \\((7,11)\\).\nGruppe 2 med punkterne \\((10,30)\\), \\((17,34)\\), \\((12,27)\\) og \\((11,32)\\).\n\nSom nævnt tidligere kan vi tænke på hvert punkt som stedvektoren til punktet.3 Vi kan nu finde midterpunktet for den første gruppe:\n3 Husk at et punkt og stedvektoren til punktet har samme koordinater.\\[\\vec{\\mu_1}=\\frac{1}{|S_1|}\\sum_{\\vec p\\in S_1}\\vec p = \\frac{\\begin{pmatrix}\n3 \\\\ 9\\end{pmatrix} + \\begin{pmatrix}\n7 \\\\ 11\\end{pmatrix}}{2}= \\frac{\\begin{pmatrix}\n10 \\\\ 20\\end{pmatrix} }{2} = \\begin{pmatrix}\n5 \\\\ 10\\end{pmatrix}\\]\nMidterpunktet for den første gruppe har altså koordinatsæt \\((5,10)\\).\nMidterpunktet for den anden gruppe bliver tilsvarende\n\\[\\vec{\\mu_2}=\\frac{1}{|S_2|}\\sum_{\\vec p\\in S_2}\\vec p = \\frac{\\begin{pmatrix}\n10 \\\\ 30\\end{pmatrix} + \\begin{pmatrix}\n17 \\\\ 34\\end{pmatrix} + \\begin{pmatrix}\n12 \\\\ 27\\end{pmatrix} + \\begin{pmatrix}\n11 \\\\ 32\\end{pmatrix}}{4}= \\frac{\\begin{pmatrix}\n50 \\\\ 123 \\end{pmatrix} }{4} = \\begin{pmatrix}\n12.5 \\\\ 30.75 \\end{pmatrix}\\]\nMidterpunktet for den anden gruppe har så koordinatsæt \\((12.5, 30.75)\\).\nDette er illustreret i figur 2.\n\n\n\n\n\n\nFigur 2: To grupper af punkter (orange og blå) sammen med de tilhørende midterpunkter \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\).\n\n\n\nPå figur 2 bliver det tydeligt, hvorfor det er fornuftigt at vælge midterpunkterne, som det sker i trin 2 i algoritmen – midterpunkterne ligger simpelthen i “midten” af hver gruppe.\n\n\n\nEksempel på algoritmen\nLad os nu prøve at bruge algoritmen på punkterne fra eksempel 1. I figur 3 ses punkterne indtegnet, men uden angivelse af hvilken gruppe hvert enkelt punkt tilhører.\n\n\n\n\n\n\n\n\nFigur 3: Illustration af punkter som ønskes inddelt i 2 grupper.\n\n\n\n\n\nI trin 1 skal vi tilføje hver observation i en tilfældig gruppe. Et sådant valg ses i figur 4.\n\n\n\n\n\n\n\n\nFigur 4: Tilfældig inddeling af punkterne i 2 grupper.\n\n\n\n\n\nVi skal nu have beregnet midtpunkterne i hver af de to grupper. Gør man det fås:\n\\[\n\\vec{\\mu_1} = \\begin{pmatrix} 8.33 \\\\ 22.0 \\end{pmatrix} \\quad \\textrm{og} \\quad \\vec{\\mu_2} = \\begin{pmatrix} 11.7 \\\\ 25.7 \\end{pmatrix}\n\\] Disse to midtpunkter er indtegnet i figur 5 og markeret med et plus.\n\n\n\n\n\n\n\n\nFigur 5: Tilfældig inddeling af punkterne i 2 grupper og med tilhørende midtpunkter, som her er markeret med et plus.\n\n\n\n\n\nI trin 3 skal vi have beregnet afstand fra hver af de 6 punkter til hver af de to midtpunkter. For eksempel bliver afstanden \\(d\\) fra punktet \\((3,9)\\) til punktet med stedvektor \\(\\vec{\\mu_1}\\) være:\n\\[\nd=\\sqrt{(3-8.33)^2+(9-22.0)^2}=14.05\n\\] Resultatet af at beregne alle afstande på denne måde ses i tabel 1.\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand til \\(\\vec{\\mu_1}\\)\nAfstand til \\(\\vec{\\mu_2}\\)\n\n\n\n\n14.05\n18.79\n\n\n11.08\n15.39\n\n\n8.172\n4.643\n\n\n14.8\n9.894\n\n\n6.2\n1.374\n\n\n10.35\n6.368\n\n\n\n\n\n\nTabel 1: Afstanden fra de 6 datapunkter til hvert af midtpunkterne \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\).\n\n\n\n\nVi skal nu afgøre hvilken gruppe, de enkelte punkter skal tilhøre, ved at se på hvilket midtpunkt som hvert enkelt punkt ligger tættest på. For eksempel kan vi i tabel 1 se, at det første punkt \\((3,9)\\) ligger tættest på \\(\\vec{\\mu_1}\\), og det punkt skal derfor (fortsat) hører til gruppe 1.\nI tabel 2 ses den oprindelige gruppe samt den nye gruppe for hvert punkt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand til \\(\\vec{\\mu_1}\\)\nAfstand til \\(\\vec{\\mu_2}\\)\nOpr. gruppe\nNy gruppe\n\n\n\n\n14.05\n18.79\n1\n1\n\n\n11.08\n15.39\n2\n1\n\n\n8.172\n4.643\n1\n2\n\n\n14.8\n9.894\n2\n2\n\n\n6.2\n1.374\n1\n2\n\n\n10.35\n6.368\n2\n2\n\n\n\n\n\n\nTabel 2: Afstanden fra de 6 datapunkter til hvert af midtpunkterne \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\) samt en angivelse af, hvilken gruppe punktet oprindeligt tilhørte, og hvilken gruppe punktet tilhører efter trin 3.\n\n\n\n\nI figur 6 ses punkterne indtegnet med en angivelse af den nye inddeling (men stadig med de først beregnede midterpunkter).\n\n\n\n\n\n\n\n\nFigur 6: Inddeling af punkterne i 2 grupper efter første gennemløb af algoritmen.\n\n\n\n\n\nVi skal nu i gang med det næste gennemløb af algoritmen, og vi bestemmer derfor først de nye midtpunkter. Gør man det fås:\n\\[\n\\vec{\\mu_1} = \\begin{pmatrix} 5.00 \\\\ 10.0 \\end{pmatrix} \\quad \\textrm{og} \\quad \\vec{\\mu_2} = \\begin{pmatrix} 12.5 \\\\ 30.8 \\end{pmatrix}\n\\]\nDe to nye midtpunkter ses indtegnet i figur 7 – igen markeret med et plus.\n\n\n\n\n\n\n\n\nFigur 7: Inddeling af punkterne i 2 grupper efter første gennemløb af algoritmen og med de nye tilhørende midtpunkter indtegnet.\n\n\n\n\n\nVi kan nu igen udregne afstande fra alle punkter til de to nye midtpunkter og finde ud af om nogle af punkterne eventuelt skal skifte gruppe. Resultatet ses i tabel 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand til \\(\\vec{\\mu_1}\\)\nAfstand til \\(\\vec{\\mu_2}\\)\nOpr. gruppe\nNy gruppe\n\n\n\n\n2.236\n23.73\n1\n1\n\n\n2.236\n20.5\n1\n1\n\n\n20.62\n2.61\n2\n2\n\n\n26.83\n5.551\n2\n2\n\n\n18.38\n3.783\n2\n2\n\n\n22.8\n1.953\n2\n2\n\n\n\n\n\n\nTabel 3: Afstanden fra de 6 datapunkter til hvert af de nye midtpunkterne \\(\\vec{\\mu_1}\\) og \\(\\vec{\\mu_2}\\) samt en angivelse af, hvilken gruppe punktet oprindeligt tilhørte, og hvilken gruppe punktet tilhører efter trin 3 (andet gennemløb af algoritmen).\n\n\n\n\nVi kan nu se, at ingen af punkterne har skiftet gruppe, og algoritmen stopper derfor. Den endelige inddeling i grupper bliver derfor som vist i figur 7, hvilket nok også er den inddelingen, som vi ville have valgt bare ved at kigge på punkterne med det blotte øje.\n\n\nFornuftigt valg af midterpunkter og grupper\nVi vil nu argumentere for, hvorfor algoritmen virker. Vi starter med at se på, hvorfor det er fornuftigt at vælge midterpunkterne, som vi gør i trin 2 i algoritmen.\nDa vi med algoritmen ønsker, at summen i (2) kaldet \\(SUMMIDT\\) skal minimeres, vil vi se, at valget af \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\) netop minimerer denne sum, når vi tænker, at grupperne er fastlagt.\nVed summen \\(SUMMIDT\\) har \\(\\vec{\\mu_i}\\) kun en effekt på delen hørende til gruppen \\(S_i\\), altså \\[\\sum_{\\vec p\\in S_i}\\|\\vec p-\\vec{\\mu_i}\\|^2\\]\nFor en vektor \\(\\vec v\\), har vi følgende sammenhæng mellem længde og skalarprodukt/prikprodukt:\n\\[\\|\\vec v\\|^2=\\vec v\\cdot \\vec v \\tag{3}\\]\nDette gør, at vi kan omskrive vores sum for gruppen \\(S_i\\) til\n\\[\\sum_{\\vec p\\in S_i}{(\\vec  p-\\vec{\\mu_i})\\cdot (\\vec p-\\vec{\\mu_i})}\\] Skalarproduktet udregnes ved at tage summen af produktet af tilsvarende koordinater for vektorerne. Hvis vi lader \\(p_m\\) og \\(\\mu_{i,m}\\) betegne det \\(m\\)’te koordinat af henholdsvis \\(\\vec p\\) og \\(\\vec{\\mu_i}\\), så kan ovenstående sum skrives som\n\\[\\sum_{\\vec p\\in S_{i}}\\sum_{m=1}^{d}{(p_m-\\mu_{i,m})\\cdot (p_m-\\mu_{i,m})} = \\sum_{m=1}^{d} \\sum_{\\vec p\\in S_{i}} {(p_m-\\mu_{i,m})\\cdot (p_m-\\mu_{i,m})}\\] Her vil valget af \\(\\mu_{i,m}\\) kun have effekt på \\[\\sum_{\\vec  p\\in S_{i}}{(p_m-\\mu_{i,m})\\cdot (p_m-\\mu_{i,m})}\\] I denne sum har vi ikke længere vektorer, og vi kan derfor benytte anden kvadratsætning til at få\n\\[\\sum_{\\vec p\\in S_i}{(p_m^2-2\\cdot p_m\\cdot \\mu_{i,m}+\\mu_{i,m}^2)}\\] For at finde ud af hvordan \\(\\mu_{i,m}\\) skal vælges for at lave summen mindst mulig, differentieres ovenstående udtryk med hensyn til \\(\\mu_{i,m}\\) og udtrykket sættes lig med \\(0\\):\n\\[\\begin{align}\n\\frac{\\partial}{\\partial \\mu_{i,m}} \\sum_{\\vec p\\in S_i}{(p_m^2-2\\cdot p_m\\cdot \\mu_{i,m}+\\mu_{i,m}^2)}\n&= \\sum_{\\vec p\\in S_i} \\frac{\\partial}{\\partial \\mu_{i,m}} {(p_m^2-2\\cdot p_m\\cdot \\mu_{i,m}+\\mu_{i,m}^2)}\\\\\n&= \\sum_{\\vec p\\in S_i}{(-2\\cdot p_m+2\\cdot \\mu_{i,m})}=0\n\\end{align}\\]\nDen sidste ligning kan omskrives til \\[ \\sum_{\\vec p\\in S_i}{2\\cdot \\mu_{i,m}} = \\sum_{\\vec p\\in S_i}{2\\cdot p_m}\\] Ved division med \\(2\\) fås \\[ \\sum_{\\vec p\\in S_i}{\\mu_{i,m}} = \\sum_{\\vec p\\in S_i}{p_m}\\]\nVi kan nu udnytte at hvert led i den første sum slet ikke afhænger af \\(\\vec p\\), og da summen består af\\(|S_i|\\) led fås \\[|S_i| \\cdot  \\mu_{i,m}= \\sum_{\\vec p\\in S_i}{p_m}\\]\nAltså er \\[\\mu_{i,m}=\\frac{1}{|S_i|} \\sum_{\\vec p\\in S_i}p_m\\] Hvis dette valg tages for alle koordinater for \\(\\vec{\\mu_i}\\) svarer det til \\[\\vec{\\mu_{i}}=\\frac{1}{|S_i|} \\sum_{\\vec p\\in S_i} \\vec p\\] som netop er den måde \\(\\vec{\\mu_i}\\) vælges på ved trin \\(2\\) i algoritmen.\nHer glemte vi at argumentere for, at valget af \\(\\mu_{i,m}\\) rent faktisk gav et lokalt minimum, men lidt løst kan man sige, at hvis \\(\\mu_{i,m}\\) enten vælges alt for lille eller stor, vil afstanden og dermed også den kvadrerede afstand til punkterne i gruppen \\(S_i\\) blive store. Det kan selvfølgelig også bevises helt formelt.\nLad os nu se på valget af grupper ved trin \\(3\\) i algoritmen. For en punkt \\(\\vec p\\) vælges den gruppe \\(S_i\\), hvor midtpunktet \\(\\vec{\\mu_i}\\) er tættest på \\(\\vec p\\). Derved er det oplagt, at denne proces minimerer summen \\(SUMMIDT\\) i (2), når vi har fastholdt midterpunkterne \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\).\nNår et punkt skifter gruppe vil \\(SUMMIDT\\) ikke længere være optimal i forhold til \\(\\vec{\\mu_1},\\vec{\\mu_2},...,\\vec{\\mu_k}\\) før de bliver opdateret igen. I algoritmen bliver disse to trin netop gentaget, indtil ingen punkter skifter gruppe, hvorved \\(SUMMIDT\\) har ramt et lokalt minimum i forhold til valg af gruppe for det enkelt punkt og valg af midterpunkt for hver gruppe.\n\n\nSammenhængen mellem SUMPAR og SUMMIDT\nNu vil vi endelig se på sammenhængen mellem de to summer \\(SUMMIDT\\) i (2) og \\(SUMPAR\\) i (1). Vi vil vise, at \\[SUMPAR=2\\cdot SUMMIDT\\] når midterpunkterne er valgt på denne måde\n\\[\n\\vec{\\mu_{i}}=\\frac{1}{|S_i|} \\sum_{\\vec p\\in S_i}\\vec p \\quad \\quad \\textrm{og derved} \\quad \\quad |S_i|\\cdot \\vec{\\mu_{i}}=\\sum_{\\vec p\\in S_i} \\vec p\n\\tag{4}\\]\nDet betyder, at hvis vi minimerer summen \\(SUMMIDT\\), så har vi også minimeret summen \\(SUMPAR\\), som var det vi oprindeligt ønskede.\nVi starter med at se på summen \\(SUMPAR\\) i (1) dog kun for en af grupperne \\(S_a\\) og uden faktoren \\(\\frac{1}{|S_i|}\\). Altså ser vi på summen \\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p-\\vec q\\|^2\n\\tag{5}\\]\nVed at bruge sammenhængen mellem længde af vektor og skalarprodukt får vi\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}(\\vec p-\\vec q)\\cdot(\\vec p-\\vec q)\n\\]\nHer bruger vi nu, hvad der svarer til anden kvadratsætning for vektorer og vi omskriver tilbage til længder ved at bruge (3)\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}(\\|\\vec p\\|^2+\\|\\vec q\\|^2-2\\cdot \\vec p\\cdot \\vec q)\n\\]\nDenne dobbeltsum opdeles nu i tre dobbeltsummer og \\(-2\\) kan trækkes ud af den ene\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2+\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec q\\|^2-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a} \\vec p\\cdot \\vec q\n\\]\nDe to første dobbeltsummer er faktisk ens og derfor får vi\n\\[\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q\n\\tag{6}\\]\nFor at komme videre med ovenstående vælger vi at se på dobbeltsummen\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q\n\\] Den inderste sum afhænger ikke af \\(\\vec p\\) og derfor kan \\(\\vec p\\) sættes uden for sumtegnet4:\n4 Husk på at den distributive regel også gælder for vektorer: \\(\\vec a \\cdot \\vec b + \\vec a \\cdot \\vec c = \\vec a \\cdot \\left (\\vec b + \\vec c \\right)\\)\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\sum_{\\vec p\\in S_a}\\vec p\\cdot \\left (\\sum_{\\vec q\\in S_a} \\vec q \\right )\n\\]\nFra valget af \\(\\vec{\\mu_a}\\) ved vi fra (4), at \\(|S_a|\\cdot \\vec{\\mu_{a}}=\\sum_{\\vec q \\in S_a} \\vec q\\). Bruger vi det får vi\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\sum_{\\vec p\\in S_a}\\vec p\\cdot |S_a|\\cdot \\vec{\\mu_{a}}\n\\] Sætter vi \\(|S_a|\\cdot \\vec{\\mu_{a}}\\) uden for summmen5 og udnytter ovenstående én gang til, får vi:\n5 Bemærk, at vi igen her benytter den distributive regel for vektorer.\\[\\begin{align}\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a} \\vec p\\cdot \\vec q &= |S_a|\\cdot \\vec{\\mu_{a}} \\cdot \\left ( \\sum_{\\vec p\\in S_a}\\vec p \\right ) \\\\\n&= \\left ( |S_a|\\cdot \\vec{\\mu_{a}} \\right ) \\cdot \\left ( |S_a|\\cdot \\vec{\\mu_{a}} \\right )\n\\end{align}\\]\nVi har nu et prikprodukt mellem to vektorer, som hver især er ganget med en skalar (her \\(|S_a|\\)). Bruger vi den kommutative lov6 for at gange med en skalar, får vi\n6 Den kommutative lov siger, at \\(k \\cdot (\\vec a \\cdot \\vec b) =  (k \\cdot\\vec a) \\cdot (\\vec b) =  (\\vec a ) \\cdot (k \\cdot\\vec b)\\)\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a} \\vec p\\cdot \\vec q = |S_a|^2 \\cdot \\| \\vec{\\mu_{a}} \\|^2\n\\]\nDet må derfor betyde, at \\[\n|S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q=0 \\quad \\Leftrightarrow  \\quad\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q=0\n\\]\nDa dette giver \\(0\\), kan det tilføjes til udtrykket i (6):\n\\[\\begin{align}\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2&-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\\\\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2-2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q &+\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q = \\\\\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2 +\n2 \\cdot |S_a|^2 &\\cdot \\|\\vec{\\mu_a}\\|^2-4\\cdot\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\vec p\\cdot \\vec q\n\\end{align}\\]\nI den sidste dobbeltsum kan \\(\\vec p\\) igen tages ud af den inderste sum og vi kan igen udnytte at \\(|S_a|\\cdot \\vec{\\mu_{a}}=\\sum_{\\vec q \\in S_a} \\vec q\\). Derved får vi\n\\[\n2\\cdot \\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2 +\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-4\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot |S_a|\\cdot \\vec{\\mu_{a}}\n\\]\nVed den første dobbeltsum ses det, at leddene ikke afhænger af \\(\\vec q\\) og derfor er \\(\\sum_{\\vec q\\in S_a}\\|\\vec p\\|^2 = |S_a| \\cdot \\|\\vec p\\|^2\\) (fordi der er \\(|S_a|\\) led i summen). Det vil sige, at vi kan omskrive til\n\\[\n2\\cdot \\sum_{\\vec p\\in S_a}|S_a| \\cdot \\|\\vec p\\|^2 +\n2 \\cdot |S_a|^2\\cdot \\|\\vec{\\mu_a}\\|^2-4\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot |S_a|\\cdot \\vec{\\mu_{a}}\n\\]\nVi kan nu se, at \\(2 \\cdot |S_a|\\) indgår i alle led og vi kan derfor skrive:\n\\[\n2\\cdot |S_a| \\cdot \\left ( \\sum_{\\vec p\\in S_a} \\|\\vec p\\|^2  +\n|S_a|\\cdot \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot \\vec{\\mu_{a}} \\right )\n\\]\nHer kan \\(|S_a|\\cdot \\|\\mu_a\\|^2\\) laves om til en sum, hvor alle led er \\(\\|\\mu_a\\|^2\\). Det vil sige\n\\[\n2\\cdot |S_a| \\cdot \\left ( \\sum_{\\vec p\\in S_a} \\|\\vec p\\|^2  +\n\\sum_{\\vec p\\in S_a} \\|\\vec{\\mu_a}\\|^2-2\\cdot\\sum_{\\vec p\\in S_a}\\vec p\\cdot \\vec{\\mu_{a}} \\right )\n\\]\nHele udtrykket kan nu samles i én sum:\n\\[\n2\\cdot |S_a| \\sum_{\\vec p\\in S_a} \\left (  \\|\\vec p\\|^2  +\n\\|\\vec{\\mu_a}\\|^2-2\\cdot\\vec p\\cdot \\vec{\\mu_{a}} \\right )\n\\] Ved brug af anden kvadratsætning for vektorer kan dette omskrives til\n\\[\n2\\cdot |S_a| \\sum_{\\vec p\\in S_a}   (\\vec p - \\vec{\\mu_a}) \\cdot (\\vec p - \\vec{\\mu_a}) = 2\\cdot |S_a| \\sum_{\\vec p\\in S_a} \\| \\vec p - \\vec{\\mu_a} \\|^2\n\\] Nu kan man jo godt have glemt, hvad det overhovedet var, vi var igang med at regne på! Men vi minder om, at det var udtrykket i (5). Det vil sige, at vi er kommet frem til, at\n\\[\n\\sum_{\\vec p\\in S_a}\\sum_{\\vec q\\in S_a}\\|\\vec p-\\vec q\\|^2 = 2\\cdot |S_a| \\sum_{\\vec p\\in S_a} \\| \\vec p - \\vec{\\mu_a} \\|^2\n\\] Eller skrevet på en anden måde:\n\\[\n\\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2 = 2 \\sum_{\\vec p\\in S_i} \\| \\vec p - \\vec{\\mu_i} \\|^2\n\\] Summerer vi over alle \\(k\\) grupper får vi: \\[\n\\sum_{i=1}^k \\frac{1}{|S_i|}\\sum_{\\vec p\\in S_i}\\sum_{\\vec q\\in S_i}\\|\\vec p-\\vec q\\|^2 = 2 \\sum_{i=1}^k \\sum_{\\vec p\\in S_i} \\| \\vec p - \\vec{\\mu_i} \\|^2\n\\] Sammenligner vi med (1) og (2) har vi netop vist, at\n\\[\nSUMPAR = 2 \\cdot SUMMIDT\n\\]\nDet vil altså sige, at hvis vi minimerer summen \\(SUMMIDT\\), så har vi også minimeret summen \\(SUMPAR\\), hvilket præcis var, hvad vi oprindeligt ønskede.\n\n\nOpsummering/optimal løsning\nNu har vi set på selve algoritmen og fundet ud af, at den finder et lokalt minimum for summen \\(SUMPAR\\), som man ønsker minimeret. Der er dog ingen garanti for, at man opnår et globalt minimum, eller hvor lang tid algoritmen er om at finde en løsning.\nDet er egentlig heller ikke noget problem at få lavet en algoritme, der finder en optimal løsning, problemet er blot, at den vil køre alt for langsomt. En sådan optimal algoritme kan laves ved blot at undersøge hver mulig inddeling i grupper og så finde den inddeling, der giver den mindste værdi af \\(SUMPAR\\). Dog vil det være sådan, at selv ved blot \\(2\\) grupper og \\(100\\) punkter vil der være \\(2^{99}\\) muligheder7, der skal tjekkes. At undersøge så mange muligheder er ikke praktisk muligt – selv ikke på en computer!\n7 Fordi for hvert punkt kan punktet enten være i den ene eller den anden gruppe. Det giver i første omgang \\(2^{100}\\) grupper. Nu vil en inddeling hvor for eksempel punktet \\(A\\) og \\(B\\) er i gruppe \\(1\\), mens \\(C\\) er i gruppe \\(2\\) være den samme inddeling, som hvis \\(C\\) er i gruppe \\(1\\) og \\(A\\) og \\(B\\) er i gruppe \\(2\\). På grund af denne symmetri ender vi derfor samlet set med \\(2^{100}/2=2^{99}\\) grupper.\n\nK-means ikke blot med punkter\nIndtil videre har vi udelukkende set på data som værende punkter, hvor vi kan anvende euklidisk afstand for at måle afstanden mellem punkterne. Det kunne dog være langt mere interessant f.eks. at arbejde med mennesker og information om dem (f.eks. alder, køn, forbrug og så videre) og stadigvæk med et ønske om at inddele disse mennesker i et bestemt antal grupper, hvor der er stor ligmed mellem dem indenfor samme gruppe. Her skal man selvfølgelig have tænkt lidt over, hvordan man kommer fra mennesker til punkter og efterfølgende får noget, der svarer til euklidisk afstand. Det kan man læse meget mere om under feature-skalering."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html",
    "href": "materialer/naivbayes/NaivBayes.html",
    "title": "Naiv Bayes klassifier",
    "section": "",
    "text": "For at introducere teorien om Bayes naive klassifikation, vil vi starte med at se på et eksempel for at få en idé om, hvad Bayes klassifikation går ud på.\nVi vil se på en person, og vi ønsker at give et bud på, om vedkommende stemmer på rød eller blå blok. Vi har på forhånd oplysninger om en del andre personer og ønsker at bruge den viden til at give det bedste bud på, om personen stemmer på rød eller blå blok.\nHer har vi følgende data, der viser, hvem der stemmer på rød og blå blok.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlle\nMænd\nKvinder\nUnge\nÆldre\nSjælland\nJylland\nAnden bopæl\n\n\n\n\nRød\n\\(51.85 \\%\\)\n\\(48.00 \\%\\)\n\\(55.00 \\%\\)\n\\(65.00 \\%\\)\n\\(47.47 \\%\\)\n\\(54.00 \\%\\)\n\\(49.90 \\%\\)\n\\(52.78 \\%\\)\n\n\nBlå\n\\(48.15 \\%\\)\n\\(52.00 \\%\\)\n\\(45.00 \\%\\)\n\\(35.00 \\%\\)\n\\(52.53 \\%\\)\n\\(46.00 \\%\\)\n\\(50.10 \\%\\)\n\\(47.22 \\%\\)\n\n\nAntal\n\\(10000\\)\n\\(4500\\)\n\\(5500\\)\n\\(2500\\)\n\\(7500\\)\n\\(3000\\)\n\\(4500\\)\n\\(2500\\)\n\n\n\n\n\n\n\n\n\nOpgave\n\n\n\n\n\nBrug tabellen ovenfor og giv det bedste bud på hvilken blok en person stemmer på:\n\nHvis det er en tilfældig person.\nHvis det er en mand.\n\n\n\n\nFra skemaet med oplysninger kan det være svære at give et bud på, hvad en ældre kvinde fra Sjælland vil stemme på, da oplysningen om køn tyder på personen vil stemme på rød, mens information om, at det er en ældre person, tyder på, at personen vil stemme på blå. Endelig vil oplysningen om, at kvinden bor på Sjælland igen få os til at tænke, at hun stemmer på rød blok.\nHer kunne vi selvfølgelig løse problemet ved at få information for hver kombination af køn, aldersgruppe og bopæl. Men det viser sig ikke at være en helt gangbar fremgangsmåde. Forklaringen følger her: Hvis vi ser på kombinationer af køn, aldersgruppe og bopæl vil det i dette eksempel give \\(2\\cdot 2\\cdot 3=12\\) kombinationer, og hvis vi i stedet havde set på, om man svarer ja eller nej til \\(50\\) spørgsmål, vil man kunne få \\(2^{50}\\) forskellige kombinationer af svar. Hvis man ser på en person, der har svaret på de \\(50\\) spørgsmål, kan man her forvente, at man i ens data kun har ganske få eller måske slet ingen personer, der har svaret på fuldstændig samme måde, og der vil ikke være meget at basere ens bud på.\nDerfor ønsker vi en metode, hvor vores bud, på hvad en ny person vil stemme på, udelukkende baseres på information svarende til det fra skemaet ovenfor, hvor vi ikke ser på alle de forskellige kombinationer. Det er det Naive Bayes klassifikation kan."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-klassifikation",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-klassifikation",
    "title": "Naiv Bayes klassifier",
    "section": "",
    "text": "For at introducere teorien om Bayes naive klassifikation, vil vi starte med at se på et eksempel for at få en idé om, hvad Bayes klassifikation går ud på.\nVi vil se på en person, og vi ønsker at give et bud på, om vedkommende stemmer på rød eller blå blok. Vi har på forhånd oplysninger om en del andre personer og ønsker at bruge den viden til at give det bedste bud på, om personen stemmer på rød eller blå blok.\nHer har vi følgende data, der viser, hvem der stemmer på rød og blå blok.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlle\nMænd\nKvinder\nUnge\nÆldre\nSjælland\nJylland\nAnden bopæl\n\n\n\n\nRød\n\\(51.85 \\%\\)\n\\(48.00 \\%\\)\n\\(55.00 \\%\\)\n\\(65.00 \\%\\)\n\\(47.47 \\%\\)\n\\(54.00 \\%\\)\n\\(49.90 \\%\\)\n\\(52.78 \\%\\)\n\n\nBlå\n\\(48.15 \\%\\)\n\\(52.00 \\%\\)\n\\(45.00 \\%\\)\n\\(35.00 \\%\\)\n\\(52.53 \\%\\)\n\\(46.00 \\%\\)\n\\(50.10 \\%\\)\n\\(47.22 \\%\\)\n\n\nAntal\n\\(10000\\)\n\\(4500\\)\n\\(5500\\)\n\\(2500\\)\n\\(7500\\)\n\\(3000\\)\n\\(4500\\)\n\\(2500\\)\n\n\n\n\n\n\n\n\n\nOpgave\n\n\n\n\n\nBrug tabellen ovenfor og giv det bedste bud på hvilken blok en person stemmer på:\n\nHvis det er en tilfældig person.\nHvis det er en mand.\n\n\n\n\nFra skemaet med oplysninger kan det være svære at give et bud på, hvad en ældre kvinde fra Sjælland vil stemme på, da oplysningen om køn tyder på personen vil stemme på rød, mens information om, at det er en ældre person, tyder på, at personen vil stemme på blå. Endelig vil oplysningen om, at kvinden bor på Sjælland igen få os til at tænke, at hun stemmer på rød blok.\nHer kunne vi selvfølgelig løse problemet ved at få information for hver kombination af køn, aldersgruppe og bopæl. Men det viser sig ikke at være en helt gangbar fremgangsmåde. Forklaringen følger her: Hvis vi ser på kombinationer af køn, aldersgruppe og bopæl vil det i dette eksempel give \\(2\\cdot 2\\cdot 3=12\\) kombinationer, og hvis vi i stedet havde set på, om man svarer ja eller nej til \\(50\\) spørgsmål, vil man kunne få \\(2^{50}\\) forskellige kombinationer af svar. Hvis man ser på en person, der har svaret på de \\(50\\) spørgsmål, kan man her forvente, at man i ens data kun har ganske få eller måske slet ingen personer, der har svaret på fuldstændig samme måde, og der vil ikke være meget at basere ens bud på.\nDerfor ønsker vi en metode, hvor vores bud, på hvad en ny person vil stemme på, udelukkende baseres på information svarende til det fra skemaet ovenfor, hvor vi ikke ser på alle de forskellige kombinationer. Det er det Naive Bayes klassifikation kan."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-klassifier",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-klassifier",
    "title": "Naiv Bayes klassifier",
    "section": "Bayes klassifier",
    "text": "Bayes klassifier\nI det følgende indfører vi det nødvendige matematik og notation til Naive Bayes klassifikation. Først og fremmest indfører vi en stokastisk variabel \\(Y\\), som kan antage de værdier, der svarer til vores forskellige forudsigelser/bud. I vores eksempel vil \\[Y\\in\\{blå, rød\\}.\\]\nLidt mere generelt siger man, at \\(Y\\) skal være en diskret stokastisk variabel med et bestemt antal mulige udfald, og der behøver altså ikke nødvendigvis kun at være to udfald.\nDerudover indfører vi en stokastisk variabel \\(\\mathbf{X}\\), hvor de mulige udfald er alle kombinationer af informationer. Her kan vi tænke \\(\\mathbf{X}\\) som en stokastisk vektor \\(\\mathbf{X} =(X_1,X_2,…,X_q)\\), hvor man ved eksemplet kunne sige \\(X_1\\):køn, \\(X_2\\):aldersgruppe og \\(X_3\\):bopæl, og et udfald kunne være \\(\\mathbf{x}=(kvinde,ældre,Sjælland)\\).\nFor hvert udfald af \\(Y\\) ønsker vi, at bestemme sandsynligheden for at værdien \\(y\\) antages, når vi allerede har observeret, at \\(\\mathbf{X}=\\mathbf{x}\\).\nSandsynligheden vil vi skrive som \\[P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\]\nDenne notation og betydningen deraf ser vi snart på.\nVi kalder \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\) en posterior sandsynlighed, fordi den udtrykker sandsynligheden for \\(Y\\) efter (post), vi har informationen \\(\\mathbf{x}\\).\nDet mest sandsynlige udfald for \\(Y\\), når vi har informationen \\(\\mathbf{x}\\), betegnes \\(C(\\mathbf{x})\\) og kaldes Bayes klassifikation."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#betinget-sandsynlighed-og-uafhængighed",
    "href": "materialer/naivbayes/NaivBayes.html#betinget-sandsynlighed-og-uafhængighed",
    "title": "Naiv Bayes klassifier",
    "section": "Betinget sandsynlighed og uafhængighed",
    "text": "Betinget sandsynlighed og uafhængighed\nFørst vender vi dog lige tilbage til notationen \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\), som vi kaldte en posterior sandsynlighed. I sandsynlighedsregningen kalder vi det også for en betinget sandsynlighed, hvilket er grunden til notationen \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\).\nGivet to hændelser \\(A\\) og \\(B\\) så benyttes notationen \\(P(A\\mid B)\\) som sandsynligheden for, at \\(A\\) sker, når det er givet, at \\(B\\) er sket. Det læses derfor også som sandsynligheden for \\(A\\) givet \\(B\\).\nSå \\(P(Y = y \\mid \\mathbf{X} = \\mathbf{x})\\) er derved sandsynligheden for \\(Y = y\\), når det er givet, at \\(\\mathbf{X} = \\mathbf{x}\\).\nEt banalt eksempel kunne være at \\(Y\\) angiver antal ben på et givent dyr, mens \\(\\mathbf{X}\\) angiver dyrearten. Her er det oplagt, at sandsynligheden for fire eller to ben afhænger af hvilken dyreart, der er tale om.\nFormelt defineres betinget sandsynlighed for to hændelser \\(A\\) og \\(B\\) som: \\[P(A\\mid B) = \\frac{P(A \\cap B)}{P(B)} \\tag{1}\\]\nUdtrykket \\(P(A \\cap B)\\) i tælleren er sandsynligheden for fælleshændelsen mellem \\(A\\) og \\(B\\) – det vil sige hændelsen, at både \\(A\\) og \\(B\\) indtræffer – og i nævneren sørger vi for, at man kun ser på de udfald, hvor \\(B\\) er givet1.\n1 Man siger også, at nævneren normaliserer sandsynligheden i forhold til sandsynligheden for hændelsen \\(B\\).\nEksempel med betinget sandsynlighed\nLad os fokusere på en almindelig terning med seks sider. Lad \\(B\\) være hændelsen at antal øjne er mindre eller lig med \\(3\\). Det vil sige, at hændelsen \\(B\\) består af udfaldene: \\(B\\) = {⚀, ⚁, ⚂}. Lad hændelsen \\(A\\) være udfald med ulige antal øjne: \\(A\\) = {⚀, ⚂, ⚄}.\nDa kan vi nemt indse, at \\[P(A) = 3/6 = 1/2\\] samt ligeledes at \\[P(B) = 1/2\\] på grund af det symmetriske udfaldsrum.\nSer vi imidlertid på den betingede sandsynlighed for at \\(A\\) indtræffer givet, at \\(B\\) allerede er indtruffet, får vi \\(P(A\\mid B)\\). Det svarer til sandsynligheden for at slå et ulige antal øjne, hvis vi allerede ved at antallet af øjne er mindre end eller lig med \\(3\\).\nFørst ser vi, at \\(A\\cap B\\) = {⚀, ⚂, ⚄} \\(\\cap\\) {⚀, ⚁, ⚂} = {⚀, ⚂}, hvilket igen på grund af det symmetriske sandsynlighedsfelt betyder, at \\[P(A\\cap B) = 2/6 = 1/3\\] Efter at vi normaliserer sandsynligheden ud fra betingelsen om at \\(B\\) er indtruffet får vi \\[P(A\\mid B) =  \\frac{P(A \\cap B)}{P(B)} = \\frac{1/3}{1/2}  = \\frac{2}{3}.\\] At betinge med hændelsen \\(B\\) svarer i dette simple eksempel til at indskrænke udfaldet for \\(A\\) fra alle ulige øjne til dem, som er mindre end eller lig med \\(3\\). Der er således tre mulige udfald i vores “\\(B\\)-verden”, hvoraf to er ulige."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#stokastisk-uafhængighed",
    "href": "materialer/naivbayes/NaivBayes.html#stokastisk-uafhængighed",
    "title": "Naiv Bayes klassifier",
    "section": "Stokastisk uafhængighed",
    "text": "Stokastisk uafhængighed\nMan siger, at to hændelser \\(A\\) og \\(B\\) er uafhængige af hinanden, hvis \\[P(A \\cap B) = P(A) \\cdot P(B)\\] Hvis vi ser på udtrykket for \\(P(A\\mid B)\\) i (1) og antager, at \\(A\\) og \\(B\\) er uafhængige, ser vi at \\[\nP(A\\mid B) = \\frac{P(A\\cap B)}{P(B)} \\stackrel{\\text{uafh.}}{=} \\frac{P(A) \\cdot P(B)}{P(B)} = P(A)\n\\] Med andre ord betyder det, at sandsynligheden for \\(A\\) givet \\(B\\) er den samme som sandsynligheden for \\(A\\). Det vil sige, at oplysningen om, at \\(B\\) allerede er indtruffet, ikke ændrer på sandsynligheden for \\(A\\). Information om \\(B\\) tilfører altså ikke noget nyt i forhold til information om \\(A\\), og det giver derfor mening af sige, at \\(A\\) og \\(B\\) er uafhængige af hinanden."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-sætning",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-sætning",
    "title": "Naiv Bayes klassifier",
    "section": "Bayes’ sætning",
    "text": "Bayes’ sætning\nEn meget vigtig matematisk egenskab ved betinget sandsynlighed er muligheden for at ombytte rollerne i formlen, således vi kan udtrykke \\(P(B\\mid A)\\) ud fra vores viden om \\(P(A\\mid B)\\). Sætningen kaldes Bayes’ sætning (eller formel) og kan let vises ved først at bestemme \\(P(A\\cap B)\\) ved at isolere denne sandsynlighed. Fra (1) får vi\n\\[P(A\\cap B)=P(A\\mid B)\\cdot P(B)\\] På helt tilsvarende vis må der også gælde, at\n\\[P(B\\cap A)=P(B\\mid A)\\cdot P(A)\\] Og da \\(A\\cap B=B\\cap A\\) må også \\(P(A\\cap B)=P(B\\cap A)\\). De to ovenfor udledte sandsynligheder, må derfor være ens:\n\\[P(A\\mid B)\\cdot P(B)=P(B\\mid A)\\cdot P(A)\\] Her Kan \\(P(A\\mid B)\\) isoleres \\[\nP(A\\mid B)=  \\frac{P(B\\mid A)\\cdot P(A)}{P(B)}\n\\] Dette resultat er netop Bayes’ sætning:\n\n\nSætning 1 (Bayes’ sætning) Lad \\(A\\) og \\(B\\) være hændelser, hvor \\(P(B) \\neq 0\\). Da gælder, at \\[\nP(A\\mid B)=  \\frac{P(B\\mid A)\\cdot P(A)}{P(B)}\n\\]\n\n\nVi kan altså ved at kende \\(P(B\\mid A)\\), \\(P(B)\\) og \\(P(A)\\) udtrykke den betingede sandsynlighed \\(P(A\\mid B)\\). Vi vender lige om lidt tilbage til, hvad vi kan bruge det til.\nSom sidste bemærkning er det væsentligt at understrege, at \\(P(A\\mid B) \\neq P(B\\mid A)\\) med mindre \\(P(A) = P(B)\\) jævnfør (1) ovenfor. F.eks. er sandsynligheden for et tilfældigt dyr er en elefant, givet dyret har fire ben ikke den samme som sandsynligheden for, at dyret har fire ben givet, at dyret er en elefant!"
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#binær-bayes-klassifier",
    "href": "materialer/naivbayes/NaivBayes.html#binær-bayes-klassifier",
    "title": "Naiv Bayes klassifier",
    "section": "Binær Bayes klassifier",
    "text": "Binær Bayes klassifier\nAntag nu at \\(Y\\) kun kan antage to tilstande som ved eksemplet med rød eller blå. I dette tilfælde oversætter man ofte de to udfald til henholdsvis \\(0\\) og \\(1\\), eller i visse sammenhænge til \\(-1\\) og \\(+1\\). Husk på at Bayes klassifikationen \\(C(\\mathbf{x})\\) er det mest sandsynlige udfald for \\(Y\\), når vi har informationen \\(\\mathbf{x}\\). I det tilfælde hvor \\(Y\\) kun kan antage to tilstande, får vi derfor\n\\[ C(\\mathbf{x}) = \\begin{cases}\n0 & \\textrm{hvis } P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x}) &gt; P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x}) \\\\\n1 & \\textrm{ellers} \\\\\n\\end{cases} \\tag{2}\\]\nDette kan også udtrykkes på anden vis: \\[\n\\begin{aligned}\nP(Y = 0 \\mid \\mathbf{X} = \\mathbf{x}) &gt; P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x}) \\Leftrightarrow\n\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} &gt; 1.\n\\end{aligned}\n\\] Her er vi dog nødt til at antage, at \\[P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})\\neq 0,\\] så vi ikke kommer til at dividere med \\(0\\).\nBruger vi denne omskrivning, kan vi udtrykke den binære Bayes klassifikation i (2) på denne måde:\n\\[ C(\\mathbf{x}) = \\begin{cases}\n0 & \\textrm{hvis } \\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} &gt; 1 \\\\\n1 & \\textrm{ellers} \\\\\n\\end{cases} \\tag{3}\\]\nI det følgende vil vi benytte os af Bayes’ sætning til at se på, hvordan ovenstående brøk kan beregnes."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#bayes-sætning-i-anvendelse",
    "href": "materialer/naivbayes/NaivBayes.html#bayes-sætning-i-anvendelse",
    "title": "Naiv Bayes klassifier",
    "section": "Bayes’ sætning i anvendelse",
    "text": "Bayes’ sætning i anvendelse\nVi bruger først Bayes’ sætning til at udtrykke \\(P(A\\mid C)\\) og \\(P(B\\mid C)\\). \\[\nP(A\\mid C) = \\frac{P(C\\mid A)P(A)}{P(C)} \\quad\\text{og}\\quad\nP(B\\mid C) = \\frac{P(C\\mid B)P(B)}{P(C)},\n\\] Når vi bestemmer forholdet mellem \\(P(A\\mid C)\\) og \\(P(B\\mid C)\\) vil vi kunne slippe af med nævneren, som de har til fælles:\n\\[\n\\begin{aligned}\n\\frac{P(A\\mid C)}{P(B\\mid C)} &= \\frac{\\frac{P(C\\mid A)P(A)}{P(C)}}{\\frac{P(C\\mid B)P(B)}{P(C)}}  \n= \\frac{P(C\\mid A)P(A)}{P(C)} \\cdot \\frac{P(C)}{P(C\\mid B)P(B)} \\\\\n&= \\frac{P(C\\mid A)P(A)}{P(C\\mid B)P(B)},\n\\end{aligned}\n\\] hvor vi har udnyttet, at man dividerer med en brøk ved at gange med den omvendte brøk.\nI den binære Bayes klassifikation i (3) indgår brøken \\(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\), som vi ved at benytte ovenstående kan omskrive til: \\[\n\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} =\n\\frac{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)P(Y = 0)}{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 1)P(Y = 1)}.\n\\tag{4}\\]"
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#naiv-bayes-klassifier",
    "href": "materialer/naivbayes/NaivBayes.html#naiv-bayes-klassifier",
    "title": "Naiv Bayes klassifier",
    "section": "Naiv Bayes klassifier",
    "text": "Naiv Bayes klassifier\nUd fra udtrykket for forholdet mellem de to posterior sandsynligheder \\(P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})\\) og \\(P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})\\) som indgår i (4) kan vi se, at der indgår to typer af sandsynligheder:\n\\[P(\\mathbf{X} = \\mathbf{x}\\mid Y = y) \\quad \\textrm{og} \\quad P(Y = y)\\]\nDisse benævnes henholdsvis likelihood og prior sandsynlighed, idet \\(P(\\mathbf{X} = \\mathbf{x}\\mid Y = y)\\) udtrykker likelihooden (troligheden) for at observere \\(\\mathbf{X} = \\mathbf{x}\\) givet \\(Y = y\\). Omvendt er prior sandsynligheden \\(P(Y = y)\\) et udtryk for forhåndsandsynligheden for at \\(Y = y\\). Altså bruger vi disse betegnelser:\n\\[\n\\begin{aligned}\n&\\textrm{Likelihood: }  &P(\\mathbf{X} = \\mathbf{x}\\mid Y = y) \\\\\n&\\textrm{Prior sandsynlighed: } &P(Y = y)\n\\end{aligned}\n\\]\nVi kan sammenfatte udtrykket i (4) til det såkaldte posterior forhold: \\[\n\\underbrace{\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}}_\\text{Posterior forhold} =\n\\underbrace{\\frac{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)}{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 1)}}_\\text{Likelihood forhold} \\cdot\n\\underbrace{\\frac{P(Y = 0)}{P(Y = 1)}}_\\text{Prior forhold}\n\\]\nHvis vi vender tilbage til vores spørgsmål om at stemme på blå eller rød blok og så kan vi sige, at \\[ Y=\n\\begin{cases}\n0 & \\textrm {hvis der stemmes på rød blok} \\\\\n1 & \\textrm {hvis der stemmes på blå blok} \\\\\n\\end{cases}\n\\] Hvis \\(x=(kvinde, ung, Sjælland)\\), så udtrykker \\(P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)\\) således sandsynligheden for, at en person er en ung kvinde fra Sjælland givet, at personen stemmer på rød blok. Når man skal bestemme den sandsynlighed skal vi huske, at det er sandsynligheden for det samlede udsagn med køn, alder og bopæl.\nFor at kunne beregne ovenstående sandsynligheder bliver vi nødt til at antage et eller andet, der gør det muligt. Man siger, at vi opstiller en model.\nÉn af de simpleste modeller er at antage at køn, alder og bopæl er uafhængige af hinanden givet \\(Y = y\\). Denne forsimplende antagelse har medvirket til metodens navn: Naiv Bayes eller Uafhængig Bayes klassifikation.\nDet betyder ifølge vores tidligere definition af uafhængighed at \\[\n\\begin{aligned}\nP(\\mathbf{X} = \\mathbf{x} \\mid Y = y) &=\nP(X_1 = x_1, X_2 = x_2, \\dots, X_q = x_q \\mid Y = y)\\\\\n&= P(X_1 = x_1\\mid Y = y)P(X_2 = x_2\\mid Y = y)\\cdots P(X_q = x_q \\mid Y = y)\\\\\n&= \\prod_{i=1}^q P(X_i = x_i\\mid Y = y),\n\\end{aligned}\n\\] hvor \\(\\prod\\)-symbolet i sidste linje betyder, at vi tager produktet af alle faktorerne på formen \\(P(X_i = x_i\\mid Y = y)\\) fra \\(i=1\\) op til \\(q\\) – altså præcist det, som står i linjen over. Det minder således om sum-tegnet \\[\\sum_{i=1}^n x_i = x_1 + x_2 + \\cdots + x_n,\\] men blot for multiplikation i stedet for addition."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#posterior-forholdet-score-og-vægte",
    "href": "materialer/naivbayes/NaivBayes.html#posterior-forholdet-score-og-vægte",
    "title": "Naiv Bayes klassifier",
    "section": "Posterior forholdet, score og vægte",
    "text": "Posterior forholdet, score og vægte\nSamler vi nu udtrykkene, som indgår i vores posterior forhold i (4), samtidig med at vi antager, at \\(X_1, X_2, \\cdots, X_q\\) er uafhængige af hinanden givet \\(Y\\), får vi nedenstående:\n\\[\n\\begin{aligned}\n\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})} &=\n\\frac{P(Y=0)}{P(Y=1)}\\cdot \\frac{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 0)}{P(\\mathbf{X} = \\mathbf{x} \\mid Y = 1)} \\\\\n&= \\frac{P(Y = 0)}{P(Y = 1)}\n\\prod_{i=1}^q\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)},\n\\end{aligned}\n\\]\nhvor hver faktor på højre siden bidrager ligeligt til, om observationen \\(\\mathbf{x}\\) skal klassificeres som \\(Y=0\\) eller \\(Y=1\\).\nNår vi skal lave beregninger på computer baseret på data, er det ofte væsentligt at tage højde for numerisk præcision. Alle tal på en computer skal repræsenteres af et endelig antal bits. Det betyder, at visse tal (f.eks. \\(1/3\\)) bliver afrundet efter et vist antal decimaler. Derfor kan der opstå problemer, når man enten ganger eller adderer meget små (eller store) tal sammen. For at undgå dette i udtrykket ovenfor, er det derfor tit en god idé at benytte sig af (den naturlige) logaritme på begge sider af lighedstegnet:\n\\[\n\\begin{aligned}\n\\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right) &=\n\\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right) +\n\\sum_{i=1}^q \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right),\n\\end{aligned}\n\\tag{5}\\]\nhvor vi har brugt logaritmeregnereglen \\[\\ln(a\\cdot b) = \\ln(a) + \\ln(b)\\] gentagende gange.\nVi minder om, at forholdet mellem \\(P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})\\) og \\(P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})\\) er af særlig interesse omkring værdien 1 – se (3). Når forholdet er \\(1\\) betyder det, at de to klasser er lige sandsynlige givet \\(\\mathbf{x}\\). Endvidere, når forholdet er over \\(1\\), er \\(Y=0\\) mere sandsynlig end \\(Y=1\\), og når det er under \\(1\\), er det omvendte tilfældet.\nLad os nu se på hvilken effekt det får, at vi ikke længere ser direkte på det posterior forhold \\[ \\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\] men i stedet på logaritmen af det posterior forhold \\[ \\ln \\left ( \\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right )\\]\nI figur 1 nedenfor er grafen for logaritmefunktionen tegnet for \\(x\\in ]0, 10].\\)\n\n\n\n\n\n\nFigur 1: Grafen for \\(f(x)=\\ln (x)\\).\n\n\n\nVi ved, at \\(\\ln(1) = 0\\) (hvilket også kan ses på grafen i figur 1), samt at for \\(x&lt;1\\) er \\(\\ln(x)&lt;0\\), mens for \\(x&gt;1\\) er \\(\\ln(x)&gt;0\\).\nSå når vi ser på \\[\\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right)\\] bliver det vigtige nu, om denne størrelse er positiv eller negativ.\nLad os derfor indføre \\(S\\) som en score, der er lig med logaritmen til posterior forholdet:\n\\[\nS = \\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right)\n\\tag{6}\\]\nDen binære Bayes klassifikation i (3) kan derfor i stedet skrives ved hjælp af scoren \\(S\\) på denne måde:\n\\[ C(\\mathbf{x}) = \\begin{cases}\n0 & \\textrm{hvis } \\ S&gt;0  \\\\\n1 & \\textrm{ellers} \\\\\n\\end{cases} \\tag{7}\\]\nVi ved således, at hvis \\(S&gt;0\\), så klassificerer vi \\(\\mathbf{x}\\), som \\(C(x)=0\\) og ellers \\(C(x)=1\\).\nSammenholder vi definitionen af \\(S\\) i (6) med udtrykket i (5), ser vi, at \\(S\\) også kan skrives som\n\\[\n\\begin{aligned}\nS &= \\ln \\left(\\frac{P(Y = 0 \\mid \\mathbf{X} = \\mathbf{x})}{P(Y = 1 \\mid \\mathbf{X} = \\mathbf{x})}\\right) \\\\&=\n\\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right) +\n\\sum_{i=1}^q \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right),\n\\end{aligned}\n\\tag{8}\\]\nIndfører vi nu bidragene til \\(S\\) som \\(w_0\\) og \\(w_i(x_i)\\) således \\[\nw_0 = \\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right)\n\\quad\\text{og}\\quad\nw_i(x_i) = \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right)\n\\] kan vi skrive udtrykket for \\(S\\) i (8) som \\[\nS = w_0 + \\sum_{i = 1}^q w_i(x_i),\n\\tag{9}\\] hvor det tydeliggøres, at hvis \\(w_i(x_i)&gt;0\\), så understøtter bidraget fra den \\(i\\)’te oplysning \\(x_i\\), at \\(Y=0\\) og ellers hvis \\(w_i(x_i)&lt;0\\) at \\(Y=1\\). Denne egenskab gør, at man også kan omtale \\(w_i(x_i)\\) som en slags “bevismæssig” vægt.\n\nVægten \\(w_0\\)\nVi har altså set i (5) og (9), hvordan vi kan omskrive forholdet mellem posterior sandsynlighederne for de to klasser \\(Y=0\\) og \\(Y=1\\) til en sum af bidrag.\nDet første led \\(w_0\\) afhænger ikke af nogen information \\(x_i\\), og vi har tidligere omtalt disse sandsynligheder som prior sandsynligheder. Man kan sige, at det er vores umiddelbare bud på hvad \\(Y\\) er, uden at vi kender noget som helst til informationerne i \\(\\mathbf{x}\\).\nNår vi går fra vores model, som vi har udledt i det foregående, til at skulle implementere den i en specifik anvendelse, bliver vi derfor nødt til at estimere de parametre, som indgår i modellen. For \\(w_0\\) betyder det, at vi skal estimere både \\(P(Y=0)\\) og \\(P(Y=1)\\). Her vil det være oplagt blot at estimere \\(P(Y=0)\\) og \\(P(Y=1)\\) ud fra træningsdata ved helt simpelt at bestemme andelen, som stemmer på henholdsvis rød og blå, hvorefter vi kan beregne \\[\nw_0 = \\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right)\\]\nHvis du vil se en mere teoretisk begrundelse for dette valg, kan du folde boksen nedenfor ud.\n\n\n\n\n\n\nTeoretisk begrundelse for hvordan \\(P(Y=0)\\) kan estimeres\n\n\n\n\n\nVi ønsker at bestemme det bedst mulige estimat for \\(p=P(Y=0)\\) ud fra vores træningsdata. Vi vil her tænke på resultaterne fra datasættet som kommende fra et binomialforsøg med sandsynligheds-parameter \\(p\\) og antalsparameter \\(n\\). I eksemplet kan vi derfor lade \\(Z\\) være en stokastisk variabel, der betegner antallet, som stemmer på rød blok. Det vil sige, at\n\\[ Z \\sim bin(n,p) \\]\nog fra binomialfordelingen ved vi, at\n\\[\nP(Z = r) = {n \\choose r}p^r(1-p)^{n-r} = \\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}\n\\] Når vi skal estimere \\(p\\) (altså sandsynligheden for at stemme på rød blok – det vil sige \\(P(Y=0)\\)) ud fra data benyttes en metode, som kaldes for maksimum likelihood estimation. Den går i al sin enkelhed ud på at bestemme den værdi af \\(p\\), som gør de data, vi har set, mest sandsynlige. Altså vil vi maksimere udtrykket ovenfor med hensyn til \\(p\\). Dette kan vi gøre ved at differentiere udtrykket og sætte det lig med \\(0\\). I stedet for at arbejde direkte med \\(P(Z = r)\\) er det nemmere at arbejde med udtrykket for \\(\\ln\\bigl(P(Z = r)\\bigr)\\), idet der gælder, at \\(f(p)=P(Z = r)\\) og \\(\\ln\\bigl(f(p) \\bigr)\\) har maksimum ved samme \\(p\\) (fordi logaritmefunktionen er voksende).\nVi finder derfor først et udtryk for \\(\\ln\\bigl(P(Z = r)\\bigr)\\):\n\\[ \\ln\\bigl(P(Z = r)\\bigr) = \\ln {n \\choose r} + r \\cdot \\ln(p) + (n-r) \\cdot \\ln(1-p)\\] hvor vi har brugt logaritmeregnereglerne \\[\\ln(a\\cdot b) = \\ln(a) + \\ln(b) \\quad \\textrm{og} \\quad \\ln(a^x)=x \\cdot \\ln(a)\\] Derfor bliver den afledede med hensyn til \\(p\\) \\[\n\\frac{d}{dp} \\left ( \\ln\\bigl(P(Z = r)\\bigr) \\right ) = \\frac{r}{p} - \\frac{n-r}{1-p}\n\\] Sætter vi ovenstående lig \\(0\\) og isolerer for \\(p\\), får vi, at\n\\[\\hat{p} = \\frac{r}{n}\\]\nhvilket svarer til den andel af de \\(n\\) observationer, som har \\(Y=0\\). Vi sætter en “hat” på \\(p\\) for at tydeliggøre, at det er et estimat af \\(p\\) – og altså ikke den ukendte, sande værdi af \\(p\\).\nBemærk, at man også kan vise, at denne værdi af \\(p\\) rent faktisk svarer til et maksimumssted.\n\n\n\n\n\nVægtene \\(w_i\\)\nDe øvrige bidrag til \\(S\\) afhænger af den specifikke værdi af \\(x_i\\). Det er altså her data for observationen, vi ønsker at klassificere, kommer ind i billedet. Her vil vægten \\(w_i(x_i)\\), som bidrager til den samlede score \\(S\\), afhænge af informationen2 \\(x_i\\).\n2 Hvis \\(w_i(x_i)\\) er mere eller mindre konstant for forskellige værdier af \\(X_i\\), betyder det, at den \\(i\\)’te information ikke er særlig informativ (og måske bør udelades fra modellen).Ved hver information \\(X_i\\) estimeres \\(P(X_i = x_i \\mid Y = y)\\) ved at se på andelen af \\(x_i\\) blandt alle træningsdata med \\(Y = y\\). Vægtene \\(w_i\\) estimeres således på tilsvarende måde som for \\(w_0\\).\nNår disse estimater er fundet, kan man bestemme \\[w_i(x_i) = \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right)\n\\]"
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#eksempel-med-rødblå-blok",
    "href": "materialer/naivbayes/NaivBayes.html#eksempel-med-rødblå-blok",
    "title": "Naiv Bayes klassifier",
    "section": "Eksempel med rød/blå blok",
    "text": "Eksempel med rød/blå blok\nLad os se på eksemplet fra tidligere med at stemme på rød eller blå blok, hvor vi tænker på \\(Y=0\\) som en stemme på rød blok og \\(Y=1\\) som en stemme på blå blok.\nVi havde allerede følgende information fra træningsdata:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlle\nMænd\nKvinder\nUnge\nÆldre\nSjælland\nJylland\nAnden bopæl\n\n\n\n\nRød\n\\(51.85 \\%\\)\n\\(48.00 \\%\\)\n\\(55.00 \\%\\)\n\\(65.00 \\%\\)\n\\(47.47 \\%\\)\n\\(54.00 \\%\\)\n\\(49.90 \\%\\)\n\\(52.78 \\%\\)\n\n\nBlå\n\\(48.15 \\%\\)\n\\(52.00 \\%\\)\n\\(45.00 \\%\\)\n\\(35.00 \\%\\)\n\\(52.53 \\%\\)\n\\(46.00 \\%\\)\n\\(50.10 \\%\\)\n\\(47.22 \\%\\)\n\n\nAntal\n\\(10000\\)\n\\(4500\\)\n\\(5500\\)\n\\(2500\\)\n\\(7500\\)\n\\(3000\\)\n\\(4500\\)\n\\(2500\\)\n\n\n\nFra dette bestemmes først vægten \\(w_0\\) ved \\[w_0 = \\ln \\left(\\frac{P(Y = 0)}{P(Y = 1)}\\right)=\\ln\\left(\\frac{51.85\\%}{48.15\\%}\\right)=0.074\\]\nFor at kunne beregne vores vægte \\[w_i(x_i) = \\ln \\left(\\frac{P(X_i = x_i \\mid Y = 0)}{P(X_i = x_i \\mid Y = 1)}\\right)\n\\]\nskal vi for at beregne tælleren i ovenstående brøk have fat på hvor stor en andel af de stemmer, der går til rød blok, som kommer fra henholdsvis mænd, kvinder, unge, ældre og så videre.\nVi tager beregningen for kvinder og starter med at finde \\(P(X_1 = kvinde \\mid Y = 0)\\). Her ved vi at \\(55 \\%\\) af de i alt \\(5500\\) kvinder stemte på rød blok. Samtidig ved vi, at der var \\(51.85 \\%\\) ud af i alt \\(10000\\) adspurgte (det vil sige \\(5185\\) pesroner), som stemte på rød blok. Derfor får vi \\[P(X_1 = kvinde \\mid Y = 0)=\\frac{55\\% \\cdot 5500}{5185}=58.34 \\%\\] Da \\(X_1\\) kun kan antage værdierne kvinde og mand, ved vi også, at\n\\[P(X_1 = mand \\mid Y = 0)=100\\%-58.34 \\%=41.66\\%.\\] På tilsvarende måde kan vi finde \\(P(X_1 = kvinde \\mid Y = 1)\\). Her ved vi, at \\(45 \\%\\) af de \\(5500\\) kvinder stemte på blå blok, og samtidig ved vi, at der var \\(4815\\) stemmer på blå blok i alt (igen \\(48.15 \\%\\) af \\(10000\\)). Derfor får vi \\[P(X_1 = kvinde \\mid Y = 1)=\\frac{45\\%\\cdot 5500}{4815}= 51.40 \\%\\] og \\[P(X_1 = mand \\mid Y = 1)=100\\%-51.40 \\%=48.60\\%\\] Alle tilsvarende sandsynligheder kan beregnes, så man kan se hvor stor en andel af stemmerne på de to blokke, der kommer fra hver gruppe. Resultatet ses i nedenstående tabel:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMænd\nKvinder\nUnge\nÆldre\nSjælland\nJylland\nAnden bopæl\n\n\n\n\nRød\n\\(41.66 \\%\\)\n\\(58.34 \\%\\)\n\\(31.34 \\%\\)\n\\(68.66 \\%\\)\n\\(31.24 \\%\\)\n\\(43.31 \\%\\)\n\\(25.45 \\%\\)\n\n\nBlå\n\\(48.60 \\%\\)\n\\(51.40 \\%\\)\n\\(18.17 \\%\\)\n\\(81.83 \\%\\)\n\\(28.66 \\%\\)\n\\(46.82 \\%\\)\n\\(24.52 \\%\\)\n\n\n\nNu kan vi bestemme vægtene \\(w_i(x_i)\\). Her findes \\(w_1(kvinde)\\) ved\n\\[w_1(kvinde) = \\ln \\left(\\frac{P(X_1 = kvinde \\mid Y = 0)}{P(X_1 = kvinde \\mid Y = 1)}\\right)=\\ln \\left(\\frac{58.34\\%}{51.40\\%}\\right)=0.1267\\] Herunder ses vægtene for alle grupper:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(w_0\\)\n\\(w_1\\)\n\n\\(w_2\\)\n\n\\(w_3\\)\n\n\n\n\n\n\n\nMænd\nKvinder\nUnge\nÆldre\nSjælland\nJylland\nAnden bopæl\n\n\n\\(0.074\\)\n\\(-0.154\\)\n\\(0.127\\)\n\\(0.545\\)\n\\(-0.175\\)\n\\(0.086\\)\n\\(-0.078\\)\n\\(0.037\\)\n\n\n\nTidligere havde vi indset, at når \\(w_i(x_i)&gt;0\\), så understøtter bidraget fra den \\(i\\)’te oplysning \\(x_i\\), at \\(Y=0\\) (altså at stemme på rød blok). I tabellen ovenfor ses det derfor, at oplysningerne kvinde, ung, Sjælland og anden bopæl gør det mere sandsynligt med en stemme på rød blok, mens oplysningerne mand, ældre og Jylland gør det mere sandsynligt med en stemme på blå blok.\nVi kan nu beregne scoren \\(S\\) for en kvinde, som er ældre og fra Sjælland, altså hvor \\(x=(kvinde,ældre,Sjælland)\\). \\[\\begin{equation}\n\\begin{split}\nS &  = w_0 + \\sum_{i = 1}^q w_i(x_i)=w_0+w_1(kvinde)+w_2(ældre)+w_3(Sjælland) \\\\\n& =0.074+0.127+(-0.175)+0.086=0.112\n\\end{split}\n\\end{equation}\\]\nDerved bliver forudsigelsen ud fra Bayes Naive metode, at en ældre kvinde, der bor på Sjælland, med størst sandsynlighed stemmer på rød blok."
  },
  {
    "objectID": "materialer/naivbayes/NaivBayes.html#opgave-ham-or-spam",
    "href": "materialer/naivbayes/NaivBayes.html#opgave-ham-or-spam",
    "title": "Naiv Bayes klassifier",
    "section": "Opgave: Ham or Spam?",
    "text": "Opgave: Ham or Spam?\nI denne opgave skal vi se på, hvordan man kan lave et simpelt spamfilter. Vi har et datasæt med \\(35000\\) mails med oplysninger om emailens oprindelse (Danmark, Europa uden Danmark, USA og anden oprindelse), afsenders mailadresse (firma, Google, Hotmail og anden) samt indhold (dating, spil og andet). Målet er, at man gerne ud fra disse oplysninger gerne automatisk vil kunne afgøre, om det er spam og derved at mailen ikke skal vises i mail-boxen, eller om det er Ham (non-spam). For hver af de \\(35000\\) mails er det også noteret om, der er tale om spam eller ham.\nTræningsdata består af\n\n\n\n\nOprindelse\n\n\n\n\n\n\n\n\nDK\nEuropa\nUSA\nAndet\n\n\nSpam\n\\(20 \\%\\)\n\\(30 \\%\\)\n\\(35 \\%\\)\n\\(55 \\%\\)\n\n\nHam\n\\(80 \\%\\)\n\\(70 \\%\\)\n\\(65 \\%\\)\n\\(45\\%\\)\n\n\nAntal\n\\(10000\\)\n\\(12000\\)\n\\(8000\\)\n\\(5000\\)\n\n\n\nog\n\n\n\n\nMail\n\n\n\nIndhold\n\n\n\n\n\n\n\nFirma\nGoogle\nHotmail\nAndet\nDating\nSpil\nAndet\n\n\nSpam\n\\(10 \\%\\)\n\\(20 \\%\\)\n\\(60 \\%\\)\n\\(80 \\%\\)\n\\(80 \\%\\)\n\\(90 \\%\\)\n\\(12.5 \\%\\)\n\n\nHam\n\\(90 \\%\\)\n\\(80 \\%\\)\n\\(40 \\%\\)\n\\(20 \\%\\)\n\\(20\\%\\)\n\\(10\\%\\)\n\\(87.5\\%\\)\n\n\nAntal\n\\(17000\\)\n\\(6450\\)\n\\(5400\\)\n\\(6150\\)\n\\(4325\\)\n\\(4975\\)\n\\(25700\\)\n\n\n\n\n\n\n\n\n\nOpgave 1\n\n\n\n\n\n\nHvis man blot modtager en mail fra en Hotmail-konto, vil man da tænke, at det er spam eller ham?\nHvis man blot modtager en tilfældig mail, vil man da tænke, at det er spam eller ham?\nForklar hvorfor det kan være svært at afgøre, om det er spam, hvis man modtager en mail fra Danmark, som er sendt fra en firma-mail og hvor indhold er relateret til dating.\n\n\n\n\n\n\n\n\n\n\nOpgave 2\n\n\n\n\n\nIndfør selv stokastiske variable \\(X1\\), \\(X2\\), \\(X3\\) og \\(Y\\) og angiv udfaldsrum for hver af dem.\n\n\n\n\n\n\n\n\n\nOpgave 3\n\n\n\n\n\n\nOpstil posterior forholdet\n\n\\[ \\frac{P(Y = 0 \\mid X_1=x_1, X_2=x_2, X_3=x_3)}{P(Y = 1 \\mid X_1=x_1, X_2=x_2, X_3=x_3)}\\]\nog forklar det smarte, der er sket ved at bruge Bayes formel.\n\nRedegør for betydningen af forholdet og forklar hvorfor man ser på, om det er over eller under \\(1\\).\n\n\n\n\n\n\n\n\n\n\nOpgave 4\n\n\n\n\n\nAngiv den antagelse (modelforudsætning), som anvendes ved Bayes Naive klassifikation, og forklar hvad det gør for beregningen af sandsynlighederne fra opgave 3.\n\n\n\n\n\n\n\n\n\nOpgave 5\n\n\n\n\n\nBestem prior forholdet\n\\[\\frac{P(Y=0)}{P(Y=1)}\\]\nog beregn på den baggrund vægten\n\\[w_0 = \\ln \\left ( \\frac{P(Y=0)}{P(Y=1)} \\right )\\]\n\n\n\n\n\n\n\n\n\nOpgave 6\n\n\n\n\n\nBestem alle betingede sandsynligheder\n\\[P(X_i=x_i \\mid Y=0) \\quad \\textrm{og} \\quad P(X_i=x_i \\mid Y=1)\\]\nfor hver enkelt information, givet at det er henholdsvis spam og ham (i alt 22 sandsynligheder) og forklar idéen bag én af disse udregninger.\n\n\n\n\n\n\n\n\n\nOpgave 7\n\n\n\n\n\nFor hver af de 11 informationer bestemmes forholdet mellem sandsynlighederne\n\\[ \\frac{P(X_i=x_i \\mid Y=0)}{P(X_i=x_i \\mid Y=1)}\\]\n\n\n\n\n\n\n\n\n\nOpgave 8\n\n\n\n\n\nForklar hvordan man kommer fra resultaterne i opgave 7 til vægte og udregn vægtene hørende til hver af informationerne (i alt 11).\n\n\n\n\n\n\n\n\n\nOpgave 9\n\n\n\n\n\nForklar hvad der sker ved at benytte logaritmen på udtrykket fra opgave 3 og 4, hvor man ellers ganger faktorer sammen.\n\n\n\n\n\n\n\n\n\nOpgave 10\n\n\n\n\n\nAfgør ud fra de vægte, som du har beregnet i opgave 8, hvilke informationer der taler for spam, og hvilke der taler for ham.\n\n\n\n\n\n\n\n\n\nOpgave 11\n\n\n\n\n\nAfgør ved at beregne scoren \\(S\\), om man vil tænke at en mail er ham eller spam i følgende to situationer:\n\nMailens oprindelse er andet, den er sendt fra en hotmail-konto og omhandler ikke dating eller spil.\nMailen er en firma-mail fra Danmark med indhold relateret til dating."
  },
  {
    "objectID": "materialer/afstande/afstand.html",
    "href": "materialer/afstande/afstand.html",
    "title": "Afstande, nærmest, størst, mindst",
    "section": "",
    "text": "Når vi adskiller eller samler data bygger vi på en form for afstand. De \\(k\\) nærmeste naboer er dem, der ligger tættest på i én eller anden forstand. Hvis det drejer sig om dem, hvis højder er tætte på hinanden eller måske dem, der vejer nogenlunde det samme, er det klart, hvad man mener. Der er tal, man umiddelbart kan sammenligne. Men hvad med at sammenligne både vægt og højde? Hvad betyder så mest? Er der lige langt mellem en person A, der vejer 80 kg og er 1,80 m høj og en anden, B, der vejer 90 kg og er 2,00 m eller mellem A og C, der vejer 70 kg og er 1,60 m? Det er ikke klart, selvom vi da kan plotte de tre punkter i et (vægt, højde) koordinatsystem og endda bruge Pythagoras og få den samme afstand.1 Udregner man BMI, er \\(A\\) tættere på \\(B\\) end på \\(C\\). Det kommer nok også an på, hvad vi gerne vil udtale os om: Er de nogenlunde lige gode til at løbe langt? Eller hurtigt? Mere kompliceret bliver det, hvis vi også vil inddrage øjenfarve, skostørrelse eller måske, om de køber rigtig meget mælk. Der er mange eksempler på afstande, som ikke umiddelbart er fysisk afstand. For eksempel mellem ord (LINK) eller mellem DNA (Link)"
  },
  {
    "objectID": "materialer/afstande/afstand.html#hierarkisk-clustering",
    "href": "materialer/afstande/afstand.html#hierarkisk-clustering",
    "title": "Afstande, nærmest, størst, mindst",
    "section": "Hierarkisk clustering",
    "text": "Hierarkisk clustering\nHer kender vi alle parvise afstande. Og ikke andet.\nUdfra den information laver vi et dendogram, hvor i første omgang par af datapunkter \"mødes\" i den højde, der svarer til deres afstand. Men der er mere: Hvornår skal datapunktet \\(p\\) mødes med \\(qr\\), som mødtes tidligere? Hvornår skal \\(pqr\\) mødes med \\(ab\\)? Det er linkage-reglerne.\n\nSingle linkage: \\(pqr\\) mødes med \\(ab\\) i den højde, hvor minimumsafstanden mellem de to grupper af punkter nås:\nMinimum af \\(d(a,p),d(a,q), d(a,r), d(b,p), d(b,q), d(p,r)\\)\nComplete linkage: \\(pqr\\) mødes med \\(ab\\), når den maksimale afstand mellem punkter i de to grupper er nået.\nMaksimum af \\(d(a,p),d(a,q), d(a,r), d(b,p), d(b,q), d(p,r)\\)\nMiddelafstand- average linkage: Når den gennemsnitlige afstand er nået. \\(\\frac{1}{2\\cdot 3}(d(a,p)+d(a,q)+ d(a,r)+ d(b,p)+ d(b,q)+ d(p,r))\\)\n\n(OBS: Her skal være tegninger og diagrammer -dendrogrammer. Og eksempler på, hvad forskellen er på de forskellige linkagekrav)\nKlyngeanalyse af DNA eller for eksempel mRNA giver anledning til dendrogrammer, som kaldes de phylogenetiske træer for de arter/sygdomme,... der svarer til den analyserede DNA.\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2859286/\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6130602/"
  },
  {
    "objectID": "materialer/afstande/afstand.html#k-means-clustering",
    "href": "materialer/afstande/afstand.html#k-means-clustering",
    "title": "Afstande, nærmest, størst, mindst",
    "section": "k-means clustering",
    "text": "k-means clustering\nVores data er punkter med \\(d\\) koordinater. Afstanden er Euklidisk. Vi vælger \\(k\\), det antal clusters, det skal ende med. Målet er at opdele data i \\(k\\) dele, \\(S_1, S_2,\\ldots , S_k\\) så den samlede gennemsnitlige kvadratiske afstand \\[\\Sigma_{i=1}^{k}\\Sigma_{p,q\\in S_i}\\frac{1}{2|S_i|}\\|p-q\\|^2\\] indenfor de \\(k\\) clusters er mindst mulig."
  },
  {
    "objectID": "materialer/afstande/afstand.html#footnotes",
    "href": "materialer/afstande/afstand.html#footnotes",
    "title": "Afstande, nærmest, størst, mindst",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAfstanden bliver \\(10^2+0,2^2\\). Bemærk, at det er udregnet udfra vægt i kg og højde i meter. Med højde i cm ville det være \\(10^2+20^2\\), men stadig samme afstand fra A til B som fra A til C. Se Afstand udfra Data for mere info om effekten af at skifte enheder. Det kan godt lave om på, hvilke punkter, der ligger nærmest.↩︎"
  },
  {
    "objectID": "materialer/afstande/MetrikDetAbstrakteAfstandsBegreb.html",
    "href": "materialer/afstande/MetrikDetAbstrakteAfstandsBegreb.html",
    "title": "Definition af en metrik – det abstrakte afstandsbegreb",
    "section": "",
    "text": "Man har ikke frit valg til at bestemme, hvad man vil bruge som afstandsmål. Hvis det skal give mening, skal man have en metrik – det betyder, at afstanden skal opfylde nogle betingelser:\nEn metrik på en mængde \\(M\\) er en funktion \\(d\\) fra \\(M\\times M\\) til \\(\\mathbb{R}\\) – altså en funktion, som tager to elementer i \\(M\\) og giver et reelt tal.\nHvis en funktion \\(d\\) skal være en metrik, så vil vi kræve, at den opfylder følgende fire betingelser:\nFor alle \\(p,q,r\\) i \\(M\\) skal der gælde, at\n\n\\(d(p,q)\\geq 0\\). Med ord: Alle afstande er positive eller \\(0\\).\n\\(d(p,p)=0\\) og \\(d(p,q)=0\\) hvis og kun hvis \\(p=q\\). Med ord: Afstanden fra et punkt til sig selv er \\(0\\), og ingen andre afstande er \\(0\\).\n\\(d(p,q)=d(q,p)\\). Det vil sige, at afstanden er symmetrisk. Med ord: Der er lige så langt fra \\(p\\) til \\(q\\) som fra \\(q\\) til \\(p\\).\n\\(d(p,q)+d(q,r)\\geq d(p,r)\\). Det kaldes for trekantsuligheden. Med ord: Der er mindst lige så langt fra \\(p\\) til \\(r\\) via \\(q\\), som direkte fra \\(p\\) til \\(r\\).\n\nLad os tage et velkendt eksempel.\n\nEksempel 1 (Euklidisk afstand som metrik) Lad \\(M\\) være alle punkter i planen og lad metrikken være den euklidiske afstand, som vi kender. Funktionen \\(d\\) vil så tage to punkter \\(P(x_1,y_1)\\) og \\(Q(x_2,y_2)\\) i planen og give et reelt tal som output svarende til den euklidiske afstand mellem \\(P\\) og \\(Q\\). Det vil sige, at \\[ d(P,Q) = \\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\\] Vi vil senere vise, at denne funktion opfylder betingelserne for en metrik, som defineret ovenfor.\n\nDet er en meget kort definition. Og meget, meget generel. \\(M\\) er en mængde - der er en strengt logisk måde at gå til mængder på, men lad os her sige en samling af objekter, som vi også kalder elementer af mængden. Læg mærke til, at vi her bare graver problemet lidt længere ned i sandet – fejer det ind under gulvtæppet – for hvad er \"objekter\"? Det kommer vi ikke nærmere her.\nDet er ret nemt at acceptere, at de tre krav er rimelige. Men er det nok? Og er det nu alligevel rimeligt? Hvad med symmetrien? Der er vel længere \\(10\\) km op ad bakke end \\(10\\) km ned ad bakke, hvis man tænker på arbejdsindsats. Så måske giver det ikke altid mening?1\n1 Hvis funktionen \\(d\\) opfylder 1,2,4, er det en quasimetrik. Opfylder den 1,2,3, er det en semimetrik. Opfylder den 1, 3 og 4, og første del af 2 (\\(d(p,p)=0\\), men der kan være andre afstande, der er \\(0\\)) er det en pseudometrik. Der findes såmænd også præmetrikker, metametrikker, pseudoquasimetrikker og sikkert andre – \"falske metrikker\".2 Ordet \"rum\" skal man ikke lægge for meget i. Der er ikke anden information i det end definitionen. Intuition skal man være varsom med.Definitionen af metrik som her, er den, vi bruger i matematik. Den har vist sig nyttig. Der er en skov af artikler og bøger, hvor man kan se, hvad man ved, når man har en metrik. En mængde med en metrik kaldes et metrisk rum.2\n\nEksempel 2 (Den diskrete metrik) På en mængde \\(M\\) er funktionen \\(d\\) givet ved.\n\n\\(d(p,p)=0\\)\nHvis \\(p\\neq q\\) er \\(d(p,q)=1\\).\n\nDet er en metrik – den opfylder definitionen ovenfor. Men det er ikke nogen specielt nyttig metrik. Alle elementer ligger lige tæt på alle andre, så der er ikke ny information – udover, om to elementer er ens eller ej.\n\n\nEksempel 3 (Euklidisk afstand som metrik, fortsat) Vi vil vise, at den euklidiske afstand mellem to punkter rent faktisk opfylder betingelserne for en metrik, som vi definerede dem ovenfor:\n\nDen første betingelse er opfyldt, da \\[d(P,Q)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2} \\geq 0\\]\nI den anden betingelse er der to ting at vise. For det første ses det nemt, at \\[d(P,P)=\\sqrt{(x_1-x_1)^2+(y_1-y_1)^2} = \\sqrt{0}=0\\] For det andet – hvis \\[d(P,Q)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}=0\\] så kan det kun lade sig gøre, hvis både \\[(x_2-x_1)^2=0 \\quad \\textrm{og} \\quad (y_2-y_1)^2=0\\] Det kan igen kun lade sig gøre3, hvis \\[x_1=x_2 \\quad \\textrm{og} \\quad y_1=y_2\\] Det vil sige, at \\(P=Q\\), og den anden betingelse er således også opfyldt.\nDa \\((a-b)^2=(b-a)^2\\) får vi, at \\[d(P,Q)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}=d(Q,P)\\] og den tredje betingelse er opfyldt.\nDet kræver lidt mere at bevise trekantsuligheden, men intuitivt virker det fornuftigt nok. Hvis du i trekant \\(PQR\\), skal fra \\(P\\) til \\(R\\), så bliver turen dertil ikke kortere, hvis du først går om \\(Q\\).\n\n3 Brug nulreglen.\n\n\n\n\n\n\nOpgave: Levenshteinafstanden\n\n\n\n\n\nVis, at Levenshteinafstanden giver en metrik.\n\nHvilken mængde er det mon en metrik på? Her kan man vælge – hvilke bogstaver må bruges? Vil I begrænse længden på de ord, der kan optræde?\nOvervej, at afstanden mellem to ord er længden af den (eller rettere en - der kan være flere veje, som er lige lange) korteste mulige vej fra det ene til det andet i et netværk (en graf). \n\nNu skulle det være til at indse, at de fire betingelser er opfyldt.\n\n\n\n\nEksempel 4 (Ikke-metrik) En elev er træt af kvadratrødder og tænker, at man vel kan droppe den euklidiske afstand og i stedet definere en afstand mellem to punkter \\(p(x_1,y_1)\\) og \\(q(x_2,y_2)\\) i planen som følger:\n\\[D(p,q)=(x_2-x_1)^2+(y_2-y_1)^2 \\tag{1}\\]\nDer er bare et lille problem: \\(D\\) er ikke en metrik! Den opfylder nemlig ikke trekantsuligheden. Men hvordan kan man se det? Husk på, at vi bare skal finde ét eksempel – det vil sige tre punkter \\(p,q,r\\), hvor trekantsuligheden ikke holder. Så har vi vist, at \\(D\\) ikke er en metrik.\nEt konkret eksempel: \\(p=(0,0)\\), \\(q=(2,0)\\), \\(r=(4,0)\\). Se figur 1.\n\n\n\n\n\n\nFigur 1: Koordinatsystem med punkterne \\(p=(0,0)\\), \\(q=(2,0)\\) og \\(r=(4,0)\\).\n\n\n\nAfstanden fra \\(p\\) til \\(r\\) er \\(D(p,r)=4^2+0^2=16\\), mens afstanden fra \\(p\\) til \\(q\\) er \\(D(p,q)=2^2+0^2=4\\) og det samme gælder afstanden fra \\(q\\) til \\(r\\): \\(D(q,r)=2^2+0^2=4\\) så \\[D(p,q)+D(q,r)=8\\] mens \\[D(p,r)=16\\] Altså er \\[ D(p,q)+D(q,r) \\ngeq D(p,r) \\]\nEt andet eksempel, som ligner en rigtig trekant: \\(p=(0,0)\\) \\(q=(2,1)\\), \\(r=(4,0)\\). Se figur 2.\n\n\n\n\n\n\nFigur 2: Koordinatsystem med punkterne \\(p=(0,0)\\), \\(q=(2,1)\\) og \\(r=(4,0)\\).\n\n\n\nHer er \\(D(p,q)=2^2+1^2=5\\) og \\(D(q,r)=(4-2)^2+1^2=5\\) så \\[D(p,q)+D(q,r)=10\\] mens \\[D(p,r)=4^2+0^2=16\\] Igen er det med dette afstandsmål kortere at gå fra \\(p\\) til \\(r\\) via \\(q\\) end at gå direkte. Og det er altså derfor ikke en metrik.\n\n\n\n\n\n\n\nOpgave: Ikke-metrik\n\n\n\n\n\nBrug funktionen\n\\[D(p,q)=(x_2-x_1)^2+(y_2-y_1)^2\\]\nfra eksempel 4. Vi vil undersøge, hvornår \\(D(p,q)+D(q,r) \\geq D(p,r)\\), således at trekantsuligheden er opfyldt.\nHer regner vi på trekanter \\(pqr\\) med: \\(p=(0,0)\\), \\(q=(2,y)\\) og \\(r=(4,0)\\), hvor midterpunktet \\(q\\) flyttes længere væk fra førsteaksen. Brug app’en nedenfor og find det \\(y\\), hvor \\(D(p,q)+D(q,r)=D(p,r)\\).\n\nHvad er \\(\\angle pqr\\), når denne ligning er opfyldt?\nKunne man have indset det uden at regne?\nHvad skal \\(\\angle pqr\\) være for at trekantsuligheden er opfyldt: \\(D(p,q)+D(q,r) \\geq D(p,r)\\)?"
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemStrenge.html",
    "href": "materialer/afstande/AfstandeMellemStrenge.html",
    "title": "Afstande mellem ord",
    "section": "",
    "text": "Et ord er en følge eller en streng af bogstaver eller tal. Det kunne for eksempel være 12DvbdN34fdg eller hnaikgoh (nej, det behøver ikke give mening). Det kunne også være en DNA-sekvens, et ord i en tekst eller noget helt andet1. Man siger, at længden af en streng er antallet af bogstaver i strengen.\n1 Ofte gør man det desuden binært, så det er en streng af \\(0\\) og \\(1\\) såsom \\(00110110.\\) Det er fornuftigt nok, eftersom computere opererer med den slags strenge.Vi vil i det følgende se på såkaldte edit-afstande, som basalt set tæller, hvor mange ændringer, man skal lave, for at komme fra den ene streng til den anden. Det kommer naturligvis til at afhænge af, hvilke typer ændringer, man tillader. Lad os her se på nogle af dem.\n\nHammingafstanden\nHammingafstanden mellem to lige lange strenge er antallet af pladser, hvor de to strenge er forskellige. Afstanden fra sne til sno er derfor \\(1\\). Afstanden fra sne til neg er \\(3\\), fordi de to strenge er forskellige på alle pladser. Det svarer til, at man må ændre et bogstav ad gangen:\n\\[ sne \\rightarrow nne \\rightarrow nee \\rightarrow neg\\] Dette er illustreret i figur 1 ved de tre grønne kanter fra sne til neg.\n\n\n\n\n\n\nFigur 1: Hver knude i figuren svarer til et ord. En kant imellem to knuder svarer til, at der findes et \"move\" mellem de to ord enten ved hjælp af Hamming-, Levenshtein- eller Damerau-Levenshteinafstanden (angivet med henholdsvis grøn, lilla og pink).\n\n\n\n\n\nLevenshteinafstanden\nLevenshteinafstanden har flere tilladte ændringer: Man må ændre bogstaver, som i Hamming, men man må også indsætte og fjerne bogstaver. Levenshteinafstanden er det mindste antal sådanne ændringer, man skal lave for at nå fra det ene ord til det andet. Ordene/strengene behøver ikke have samme længde - man kan jo indsætte og fjerne bogstaver.\nSe på figur 1:\n\nAfstanden fra sne til see er \\(1\\), ligesom Hammingafstanden.\nAfstanden fra sne til sneg er også \\(1\\), fordi vi blot har tilføjet et g – og her er Hammingafstanden slet ikke meningsfuld. Den er simpelthen ikke defineret.\nAfstanden fra sne til neg er \\(2\\) – via disse ændringer:\n\\[sne \\rightarrow sneg \\rightarrow neg\\]\nHammingafstanden, som vi fandt ovenfor, er \\(3\\).\n\nBemærk, at vi i ovenstående eksempel også kunne have valgt\n\\[sne \\rightarrow ne \\rightarrow neg\\] som også har \\(2\\) \"moves\".\nJo flere tilladte ændringer, jo kortere afstand. Der er algoritmer, der finder den mindste vej mellem to ord – det er dog ikke helt så klart, hvordan man regner den ud, som det er for Hammingafstanden.\n\n\nDamerau-Levenshteinafstanden\nDamerau-Levenshteinafstanden er som Levenshtein, men man tillader nu også ombytning af to bogstaver, som står ved siden af hinanden. Hvis man skriver teskt på en telefon eller pc, er det let at bytte om på den måde. Hvis man så har en liste over ord, der giver mening, kan man opdage, at teskt ikke giver mening, men at ordet tekst ligger meget tæt på - afstand \\(1\\) i Damerau-Levenshteinafstand – og \\(2\\) i Hamming- eller Levenshteinafstand. Ordet teske har også Hammingafstand \\(1\\) til teskt, så man kan ikke være sikker på, hvad det oprindelige var.\nI figur 2 ses et eksempel på hvilke \"moves\", der er tilladt mellem forskellige ord ved hjælp af Hamming-, Levenshtein- eller Damerau-Levenshteinafstanden.\n\n\n\n\n\n\nFigur 2: Hver knude i figuren svarer til et ord. En kant imellem to knuder svarer til, at der findes et \"move\" mellem de to ord enten ved hjælp af Hamming-, Levenshtein- eller Damerau-Levenshteinafstanden (angivet med henholdsvis grøn, lilla og pink).\n\n\n\n\n\nAfstande mellem navne\nNavne som Peter, Pieter, Pietro, Petrus, Peder, Per, Pelle, Pekka, Peer, Petur, Pedro, Pierre, Pjotr, Pyotr, Petar eller måske Katarina, Katharina, Katrina, Katrine, Katrin, Cathryn, Kathryn, Catherine har samme oprindelse. Der er stor forskel på, hvor hyppigt, de optræder i forskellige lande. Overvej, om edit-afstandene ovenfor kan bruges til for eksempel at afsløre, hvor tæt på hinanden lande med Peter som hyppigst, er på lande med Pyotr."
  },
  {
    "objectID": "materialer/retningsafledede/retningsafledede.html",
    "href": "materialer/retningsafledede/retningsafledede.html",
    "title": "Retningsafledede og gradientnedstigning",
    "section": "",
    "text": "Introduktion\nI det almindelige gymnasiepensum indgår nogle vigtige begreber indenfor infinitesimalregningen for funktioner af én og to variable. For funktioner af én variabel siges en funktion \\(f\\) at være kontinuert i et punkt \\(x_{0}\\), hvis funktionsværdien \\(f(x)\\) nærmer sig funktionsværdien \\(f(x_{0})\\) når \\(x\\) nærmer sig \\(x_{0}\\). Det vil sige, at\n\\[\n\\lim_{x \\rightarrow x_{0}}{f\\left( x \\right) = f(x_{0})}\n\\]\nFunktionen \\(f\\) siges at være differentiabel i \\(x_{0}\\), hvis hældningen af sekanterne gennem de to punkter \\((x_{0},f\\left( x_{0} \\right))\\) og \\((x,f\\left( x \\right))\\) på grafen for \\(f\\) nærmer sig et fast tal \\(f'(x_{0})\\), når \\(x\\) nærmer sig \\(x_{0}\\):\n\\[\n\\lim_{x \\rightarrow x_{0}}{\\frac{f\\left( x \\right) - f(x_{0})}{x - x_{0}} = f'(x_{0})}\n\\]\nIntuitivt kan man tænke på egenskaben kontinuitet i et punkt, som at grafen for funktionen ikke har et spring i punktet, og på egenskaben differentiabilitet i et punkt, som at grafen for funktionen hverken har spring eller knæk i punktet.\n\n\n\n\nHældningen \\(f'(x_{0})\\) i punktet \\((x_{0},f\\left( x_{0} \\right))\\) på grafen er så en grænseværdi af nogle sekanthældninger, som hver for sig er gennemsnitshældninger for et mindre og mindre stykke af grafen. Har man studeret grænseværdibegrebet lidt nærmere, ved man, at den intuitive fortolkning ikke er særligt præcis og heller ikke helt rigtig, men alligevel er denne fortolkning god til at give en forståelse af gymnasiematematikken.\nFor funktioner af to variable siges en funktion \\(f\\) at være kontinuert i et punkt \\({(x}_{0},y_{0})\\), hvis funktionsværdien \\(f(x,y)\\) nærmer sig funktionsværdien \\(f(x_{0},y_{0})\\), når \\((x,y)\\) nærmer sig \\({(x}_{0},y_{0})\\):\n\\[\n\\lim_{(x,y) \\rightarrow (x_{0},y_{0})}{f\\left( x,y \\right) = f(x_{0},y_{0})}\n\\]\nDefinitionen er direkte overført fra den tilsvarende definition for funktioner af én variabel. Det virker jo umiddelbart meget fornuftigt, men der er faktisk lidt at tænke over. Det kan du læse mere om her.\nDesværre kan man ikke tilsvarende genbruge differentiabilitetsbegrebet fra funktioner af én variabel på denne måde\n\\[\n\\lim_{(x,y) \\rightarrow (x_{0},y_{0})}{\\frac{f\\left( x,y \\right) - f\\left( x_{0},y_{0} \\right)}{\\left( x,y \\right) - {(x}_{0},y_{0})} = f'(x_{0},y_{0})}\n\\]\nHer giver brøken på venstre side repræsenterende sekanthældningen ikke mening, da man ikke kan dividere med et punkt eller en vektor. I stedet tager man udgangspunkt i en alternativ definition af differentiabilitet for funktioner af én variabel, hvor man har kaldt skridtet fra \\(x_{0}\\) til \\(x\\) for \\(h\\):\n\\[\n\\lim_{h \\rightarrow 0}{\\frac{f\\left( x_{0} + h \\right) - f(x_{0})}{h} = f'(x_{0})}\n\\]\nMan bruger det til at definere de to første ordens partielle afledede for en funktion \\(f\\) af to variable \\[\n\\begin{aligned}\n&\\lim_{h \\rightarrow 0} \\frac{f\\left( x_{0} + h,y_{0} \\right) - f(x_{0},y_{0})}{h} = f_{x}^{'}(x_{0},y_{0})\\\\\n&\\lim_{h \\rightarrow 0} \\frac{f\\left( x_{0},y_{0} + h \\right) - f(x_{0},y_{0})}{h} = f_{y}^{'}(x_{0},y_{0})\n\\end{aligned}\n\\] hvis grænserne eksisterer. Her tager man et skridt \\(h\\) i enten \\(x\\)-aksens eller \\(y\\)-aksens retning ud fra punktet \\((x_{0},y_{0})\\) og beregner en hældning af grafen i den retning ved hjælp af en grænseværdi af sekanthældninger for snitfunktionerne \\(f(x,y_{0})\\) og \\(f(x_{0},y)\\), som hver for sig er almindelige funktioner af én variabel, da man kun ændrer enten \\(x\\)-koordinaten eller \\(y\\)-koordinaten.\nGradientvektoren defineres som vektoren med de partielle afledede som koordinater:\n\\[\n\\nabla f\\left( x_{0},y_{0} \\right) = \\begin{pmatrix}\nf_{x}^{'}\\left( x_{0},y_{0} \\right) \\\\\nf_{y}^{'}\\left( x_{0},y_{0} \\right) \\\\\n\\end{pmatrix}\n\\]\nDet oplyses i gymnasiepensum, at denne vektor angiver den retning, man skal bevæge sig væk fra punktet \\((x_{0},y_{0})\\), for at funktionsværdierne \\(f(x,y)\\) vokser mest muligt. Vi vil i det følgende se nærmere på denne egenskab og bruge den til at løse optimeringsproblemer numerisk.\n\n\nRetningsafledede\nVi vil nu se på væksten i andre retninger end blot i aksernes retning. Vi angiver retningen med en enhedsvektor - det vil sige en vektor med længde 1:\n\\[\n\\vec{u} = \\begin{pmatrix}\nu_{1} \\\\\nu_{2} \\\\\n\\end{pmatrix}\n\\] hvor altså \\(\\lvert \\vec u \\rvert = 1\\).\nVi definerer nu den retningsafledede af \\(f\\) i punktet \\((x_{0},y_{0})\\) i retningen \\(\\vec{u}\\) ved\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\lim_{h \\rightarrow 0}\\frac{f\\left( x_{0} + hu_{1},y_{0} + hu_{2} \\right) - f(x_{0},y_{0})}{h}\n\\tag{1}\\]\nhvis grænsen eksisterer.\nBemærk, at hvis \\(\\vec{u}\\) peger i \\(x\\)-aksens retning, så bliver den retningsafledede til \\(f_{x}^{'}(x_{0},y_{0})\\), og hvis den peger i \\(y\\)-aksens retning, bliver den til \\(f_{y}^{'}(x_{0},y_{0})\\). Man udregner en sekanthældning ved at tage et skridt \\(h\\) i \\(\\vec{u}\\)’s retning og dividere den fundne funktionstilvækst med \\(h\\). Derefter lader man \\(h\\) gå mod 0. Det giver hældningen af grafen for \\(f\\) i punktet \\((x_{0},y_{0})\\) i retningen \\(\\vec{u}\\).\nIdéen med den retningsafledede er illustreret i app’en nedenfor. Til venstre ses en repræsentant for \\(\\vec u\\) i \\(xy\\)-planen. Man kan ændre på den retning, som \\(\\vec u\\) peger i, ved at trække i skyderen. Til højre ses grafen for en funktion \\(f\\) af to variable, hvor et punkt \\(P(x_0,y_0,f(x_0,y_0))\\) på grafen er indtegnet. Samtidig vises den snitkurve som fås, hvis man på grafen i punktet \\(P\\) bevæger sig langs en linje i retningen \\(\\vec u\\). Denne snitkurve har i punktet \\(P\\) en tangent, som også er indtegnet, og denne tangents hældning vil netop svarer til størrelsen af den retningsafledede \\(D_{\\vec{u}}f\\left( x_{0},y_{0} \\right)\\). Hvis man ændrer på den retning, som \\(\\vec u\\) peger i, kan man se, hvordan størrelsen af den retningsafledede ændrer sig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDet viser sig, at man kan udregne de retningsafledede med et prikprodukt:\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\nabla f(x_{0},y_{0}) \\cdot \\vec{u}\n\\]\nVi vil argumentere for formlen, men lad os først se på konsekvenserne af den. Vi ved fra almindelig vektorregning, at\n\\[\n\\vec{a} \\cdot \\vec{b} = \\lvert \\vec{a} \\rvert \\cdot \\lvert \\vec{b} \\rvert \\cdot \\cos(v)\n\\]\nhvor \\(v\\) er vinklen mellem de to vektorer. Da \\(\\lvert \\vec{u} \\rvert = 1\\) betyder det, at\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\lvert \\nabla f(x_{0},y_{0}) \\rvert \\cdot \\cos(v)\n\\]\nhvor \\(v\\) er vinklen mellem gradientvektoren \\(\\nabla f\\left( x_{0},y_{0} \\right)\\) og den valgte retning \\(\\vec{u}\\).\nVi ved, at \\(-1 \\leq \\cos(v) \\leq 1\\) samt at \\(\\cos(0^{{^\\circ}})=1\\) og \\(\\cos(180^{{^\\circ}})=-1\\). Det følger derfor, at den retningsafledede er størst (og dermed at \\(f\\) vokser mest), når \\(\\vec{u}\\) peger i \\(\\nabla f(x_{0},y_{0})\\)’s retning. Og tilsvarende at den retningsafledede er mindst (og dermed at \\(f\\) aftager mest), når \\(\\vec{u}\\) peger i \\(-\\nabla f(x_{0},y_{0})\\)’s retning. Det vil sige, at den retningsaflededes størsteværdi er\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\ \\ \\ \\lvert \\nabla f(x_{0},y_{0}) \\rvert\n\\]\nnår \\(v = 0^{{^\\circ}}\\) og retningsaflededes mindsteværdi er\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = - \\lvert \\nabla f(x_{0},y_{0}) \\rvert\n\\]\nnår \\(v = 180^{{^\\circ}}\\). Det var netop, hvad vi gerne ville vise.\nPrincippet er illustreret i app’en herunder. Gradientvektoren \\(\\nabla f(x_{0},y_{0})\\) er indtegnet (med blå) og man kan se, at den retningsafledede antager den største værdi, netop når \\(\\vec u\\) peger i gradientens retning (prøv at trække i skyderen). Og omvendt antager den retningsafledede den mindste værdi, når \\(\\vec u\\) peger i minus gradientens retning.\n\n\n\n\n\nMiddelværdisætningen\nFor at argumentere for formlen for de retningsafledede udregnet som et prikprodukt, skal vi bruge middelværdisætningen for funktioner af én variabel:\n\n\nSætning 1 (Middelværdisætningen) Hvis \\(f\\) er kontinuert på \\(\\left\\lbrack a;b \\right\\rbrack\\) og differentiabel i \\(\\left\\rbrack a;b \\right\\lbrack\\), så findes der et tal \\(c\\) mellem \\(a\\) og \\(b\\), så tangenthældningen i \\(c\\) er lig med middelværdien af hældningen på hele intervallet \\(\\left\\lbrack a;b \\right\\rbrack\\). Det vil sige, at \\[f^{'}\\left( c \\right) = \\frac{f\\left( b \\right) - f(a)}{b - a}\\]\n\n\nResultatet i middelværdisætningen kan omskrives til\n\\[\nf\\left( b \\right) - f\\left( a \\right) = f^{'}\\left( c \\right) \\cdot (b - a)\n\\tag{2}\\]\nsom er det, vi får brug for. Middelværdisætningen virker indlysende korrekt, hvis man prøver at tegne situationen, og beviset for middelværdisætningen kan findes i flere gymnasiebøger.\nInden vi går til argumentet for formlen for de retningsafledede, vil vi se på et enkelt eksempel med middelværdisætningen.\n\nEksempel 1 Funktionen \\(f\\left( x \\right) = \\sqrt{x}\\) er kontinuert på \\(\\left\\lbrack 0;4 \\right\\rbrack\\) og differentiabel i \\(\\left\\rbrack 0;4 \\right\\lbrack\\), så betingelserne for at bruge middelværdisætningen er opfyldt.\nDer findes så et tal \\(c\\) mellem 0 og 4, så \\(f^{'}\\left( c \\right) = \\frac{f\\left( 4 \\right) - f(0)}{4 - 0}\\).\nVi ved, at \\(f^{'}\\left( x \\right) = \\frac{1}{2\\sqrt{x}}\\) så ligningen ovenfor bliver \\[\n\\frac{1}{2\\sqrt{c}} = \\frac{\\sqrt{4} - \\sqrt{0}}{4 - 0}\n\\] Det vil sige, at \\[\n\\frac{1}{2\\sqrt{c}} = \\frac{1}{2}\n\\] hvilket giver \\(c = 1\\).\nTangenthældningen af grafen for \\(f\\left( x \\right) = \\sqrt{x}\\) i \\(c = 1\\) er altså det samme som middelværdien af hældningen af grafen på hele intervallet \\(\\left\\lbrack a;b \\right\\rbrack = \\left\\lbrack 0;4 \\right\\rbrack\\), det vil sige hældningen af den sekant, der forbinder startpunktet \\((0,f\\left( 0 \\right))\\) og slutpunktet \\((4,f\\left( 4 \\right))\\).\nPå figur 1 illustreres dette princip.\n\n\n\n\n\n\n\nFigur 1: Illustration af middelværdisætningen. Her har tangenten i \\((1,f(1))\\) (den grønne linje) samme hældning som sekanten gennem \\((0,f(0))\\) og \\((4,f(4))\\) (den blå linje).\n\n\n\nMiddelværdisætningen siger altså bare, at hvis man forbinder start og slutpunktet – den blå linje – og udregner dens hældning, så kan man altid finde mindst et punkt i det indre af intervallet, hvor tangenten i punktet – den grønne linje – har samme hældning. I eksemplet fandt vi et bestemt \\(c\\), som vi ifølge middelværdisætningen vidste, at vi kunne. Når vi i det følgende skal tænke endnu mere generelt, så bliver middelværdisætningen nyttig.\nVi vender nu tilbage til definitionen af de retningsafledede. Vi får i det følgende brug for at antage, at både \\(f_{x}^{'}(x,y)\\) og \\(f_{y}^{'}\\left( x,y \\right)\\) eksisterer, så vi kan bruge middelværdisætningen. Desuden får vi også brug for at antage, at \\(f_{x}^{'}(x,y)\\) og \\(f_{y}^{'}\\left( x,y \\right)\\) er kontinuerte på en omegn af \\((x_{0},y_{0})\\).\nVi omskriver nu tælleren i (1) for at kunne bringe middelværdisætningen i spil \\[\n\\begin{aligned}\nf( x_{0} + h \\cdot u_{1}, y_{0} + h &\\cdot u_{2}) - f(x_{0}, y_{0})  = \\\\\n& f\\left( x_{0} + h \\cdot u_{1},y_{0} +  h \\cdot u_{2} \\right) \\\\\n& \\color{red}- f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right)  + f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) \\color{black}\\\\\n&- f(x_{0},y_{0})\n\\end{aligned}\n\\] Bemærk, at vi har lagt et led til og trukket det samme led fra (markeret med rødt). Det svarer til, at vi har indskudt et punkt i \\(xy\\)-planen, som illustreret i figur 2.\n\n\n\n\n\n\nFigur 2: Et rødt punkt er indskud i \\(xy\\)-planen.\n\n\n\nVi ser nu, at de to første led kun afviger på \\(x\\)-koordinaten (markeret med blåt nedenfor), og de to sidste led afviger kun på \\(y\\)-koordinaten (markeret med grønt): \\[\n\\begin{aligned}\nf\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f(x_{0},y_{0})   &= \\\\\n\\color{blue} f\\left( x_{0} + h \\cdot u_{1},y_{0} +  h \\cdot u_{2} \\right)  - & \\color{blue} f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right)  \\color{black} + \\\\  \\color{green} f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) - & \\color{green} f(x_{0},y_{0})\n\\end{aligned}\n\\tag{3}\\]\nAfvigelsen på henholdsvis \\(x\\)- og \\(y\\)-koordinaten er vist i figur 3.\n\n\n\n\n\n\nFigur 3: Afvigelsen på \\(x\\)-koordinaten er markeret med blåt, mens afvigelsen på \\(y\\)-koordinaten er markeret med grønt.\n\n\n\nVed at bruge den omskrevne middelværdisætning i (2) på de to snitfunktioner \\(f\\left( x,y_{0} + h \\cdot u_{2} \\right)\\) som en funktion af \\(x\\) og \\(f(x_{0},y)\\) som en funktion af \\(y\\), får vi nu følgende:\n\\[\n\\begin{aligned}\n\\color{blue} f\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) =  \\color{blue} f_{x}^{'}(c_{1},y_{0} + h \\cdot u_{2}) \\cdot h \\cdot u_{1}\n\\end{aligned}\n\\] og \\[\n\\color{green} f\\left( x_{0},y_{0} + h \\cdot u_{2} \\right) - f\\left( x_{0},y_{0} \\right) = f_{y}^{'}(x_{0},c_{2}) \\cdot h \\cdot u_{2}\n\\]\nHer har vi brugt, at den afledede af en snitfunktion, hvor vi kun varierer \\(x\\) er \\(f_{x}^{'}\\), og den afledede af en snitfunktion, hvor vi kun varierer \\(y\\) er \\(f_{y}^{'}\\). Tallet \\(c_{1}\\) ligger mellem \\(x_{0}\\) og \\(x_{0} + h \\cdot u_{1}\\), og tallet \\(c_{2}\\) ligger mellem \\(y_{0}\\) og \\(y_{0} + h \\cdot u_{2}\\). Dette er vist i figur 4.\n\n\n\n\n\n\nFigur 4: Tallet \\(c_{1}\\) ligger mellem \\(x_{0}\\) og \\(x_{0} + h \\cdot u_{1}\\), og tallet \\(c_{2}\\) ligger mellem \\(y_{0}\\) og \\(y_{0} + h \\cdot u_{2}\\).\n\n\n\nIndsætter vi de to udtryk ovenfor på højreside i (3) får vi \\[\n\\begin{multline}\nf\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f(x_{0},y_{0})  = \\color{blue} f_{x}^{'}(c_{1},y_{0} + h \\cdot u_{2}) \\cdot h \\cdot u_{1} \\color{black} + \\\\ \\color{green} f_{y}^{'}(x_{0},c_{2}) \\cdot h \\cdot u_{2} \\\\\n\\end{multline}\n\\]\nOg bruges dette i definitionen for den retningsafledede i (1) ender vi med \\[\n\\begin{aligned}\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) &= \\lim_{h \\rightarrow 0}\\frac{f\\left( x_{0} + h \\cdot u_{1},y_{0} + h \\cdot u_{2} \\right) - f(x_{0},y_{0})}{h}\n\\\\\n&=\n\\lim_{h \\rightarrow 0}\\frac{f_{x}^{'}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) \\cdot h \\cdot u_{1} + f_{y}^{'}(x_{0},c_{2}) \\cdot h \\cdot u_{2}\\ }{h}\n\\end{aligned}\n\\] Vi kan nu dividere \\(h\\) op i hvert led og får \\[\n\\begin{aligned}\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right)\n&= \\underset{h \\rightarrow 0}{\\text{lim}} f_{x}^{'}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) \\cdot u_{1} + f_{y}^{'}(x_{0},c_{2}) \\cdot u_{2}\\\n\\\\\n&= \\lim_{h \\rightarrow 0}\\begin{pmatrix}\nf_{x}^{'}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) \\\\\nf_{y}^{'}(x_{0},c_{2})\n\\end{pmatrix} \\cdot\n\\begin{pmatrix}\nu_{1} \\\\\nu_{2}\n\\end{pmatrix}\n\\end{aligned}\n\\tag{4}\\] hvis grænsen eksisterer.\nHusk på, at \\(c_1\\) ligger i intervallet \\((x_0,x_0+h \\cdot u_1)\\) og \\(c_2\\) ligger i intervallet \\((y_0,y_0+h \\cdot u_2)\\). Derfor vil\n\\[\n\\lim_{h \\rightarrow 0}\\left( c_{1},y_{0} + h \\cdot u_{2} \\right) =\n(x_{0},y_{0})\n\\] og \\[\n\\lim_{h \\rightarrow 0}\\left( x_{0},c_{2} \\right) = \\ (x_{0},y_{0})\n\\]\nVi startede med at antage, at de partielle afledede er kontinuerte. Det får vi brug for nu. Det betyder nemlig, at grænseværdien i (4) eksisterer, og vi får det ønskede resultat\n\\[\nD_{\\vec{u}}f\\left( x_{0},y_{0} \\right) = \\begin{pmatrix}\nf_{x}^{'}\\left( x_{0},y_{0} \\right) \\\\\nf_{y}^{'}(x_{0},y_{0}) \\\\\n\\end{pmatrix} \\cdot \\begin{pmatrix}\nu_{1} \\\\\nu_{2} \\\\\n\\end{pmatrix} = \\nabla f(x_{0},y_{0}) \\cdot \\vec{u}\n\\] Det var netop, hvad vi ønskede at vise1.\n1 Vi startede med at antage, at de partielle afledede eksisterer og er kontinuerte på en omegn. Bemærk, at vi ud fra den antagelse nu har vist, at alle de retningsafledede også vil eksistere.\n\nOptimering\nBetragt en funktion \\(f\\) givet ved forskriften \\[\nf\\left( x,y \\right) = \\left( \\left( x - 5 \\right)^{2} + 3 \\right) \\cdot \\left( 5 + \\left( y - 10 \\right)^{2} \\right) + 30\n\\]\nHvis man ser lidt på forskriften, kan man måske overbevise sig selv om, at funktionen har et minimum på 45, som fås, når \\(\\left( x,y \\right) = (5,10)\\).\nGrafen ses herunder.\n\n\n\nMan kan lave en iterativ metode til at finde minimumspunktet ved at udnytte egenskaben ved gradientvektoren:\n\nVælg et startpunkt \\((x_0,y_0)\\) som et første gæt på et minimumspunkt.\n\nVi udnytter nu, at \\(- \\nabla f(x_0,y_0)\\) angiver den retning, hvor funktionsværdien falder mest i punktet \\((x_0,y_0,f(x_0,y_0))\\).\n\nGå derfor et lille skridt i retningen \\(- \\nabla f(x_0,y_0)\\). Det giver så det næste punkt \\((x_1,y_1)\\), som forhåbentlig er et bedre bud på et minimumspunkt.\nProcessen foregår i definitionsmængden, men på grafen svarer det til at gå et lille stykke den stejleste vej ned ad bakken.\nProcessen itereres så gentagne gange indtil man forhåbentlig når minimumspunktet.\n\nVælger vi med den konkrete funktion et startpunkt på\n\\[\n(x_0,y_0) = ( - 3,4)\n\\]\nog vælger vi i hvert skridt at lægge -0,001 gange den negative gradientvektor i punktet til, så kan nogle af de følgende \\((x,y)\\)-punkter ses til venstre i figur 5. Læg her mærke til hvordan vi nærmer os det globale minimumssted i \\((5,10)\\). Til højre i figur 5 ses det også hvordan vi ved hjælp af gradientnedstigning, nærmer os den globale minimumsværdi på \\(f(5,10)=45\\).\n\n\n\n\n\n\nFigur 5: Til venstre ses et udvalg af nogle af de \\((x,y)\\)-punkter, som genereres i forbindelse med gradientnedstigning. Til højre ses et udvalg af nogle af de funktionsværdier, som genereres i forbindelse med gradientnedstigning.\n\n\n\nVi ser, at den iterative gradientnedstigning faktisk nærmer sig det globale minimumspunkt. Så om ikke andet så virker metoden i hvert fald i dette konkrete tilfælde.\n\n\nTræning af neurale netværk\nAt lede efter et globalt minimumspunkt eller i det mindste et brugbart lokalt minimumspunkt for en funktion af rigtig mange variable er et problem, man står overfor, når man skal træne et neuralt netværk og have fastlagt en masse vægte i netværket.\nDet kan ikke gøres analytisk, så derfor bruger man netop en iterativ proces baseret på gradientnedstigning som metode til at finde frem til minimumspunktet. Eksemplet ovenfor illustrerer derfor idéen bag en central del af træningen af et neuralt netværk.\nLæs mere om hvordan gradientnedstigning konkret bruges her: Perceptroner og Kunstige neurale netværk."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html",
    "href": "materialer/perceptron/perceptron.html",
    "title": "Perceptroner",
    "section": "",
    "text": "Perceptroner... Hvorfor nu det? Jo, for det er faktisk sådan nogle, du ikke vidste, at du ikke kunne leve uden! Nu skal du høre hvorfor.\n\n\nDe senere år er det blevet populært, at diverse medier laver forskellige kandidattests. Sådan nogle tests kan laves på mange forskellige måder - man kunne blandt andet bruge perceptroner! Testene fungerer som regel på den måde, at man bliver stillet en række forskellige spørgsmål og så skal man svare på en skala fra meget uenig til meget enig. Disse kategorier af svar kunne f.eks. oversættes til matematik på denne måde:\n\n\n\n\n\n\n\n\n\n\n\n\nHelt enig\nOvervejende enig\nHverken/eller\nOvervejende uenig\nHelt uenig\n\n\n\n\n2\n1\n0\n-1\n-2\n\n\n\n\n\nLad os prøve at gøre det helt simpelt. I stedet for at komme med et bud på hvem man skal stemme på, så vil vi blot forsøge at komme med et bud på, om man skal stemme på rød eller blå blok (det er sikkert en håbløs simplificering, men det må du tale med din samfundsfagslærer om ).\nLad os sige at vi vil basere vores bud på to spørgsmål:\n\nJeg synes, at indkomstskatten skal sættes ned.\nJeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift.\n\nVi kan sikkert hurtigt blive enige om, at hvis man er meget enig i begge spørgsmål, så hører man formentlig til i blå blok og modsat, hvis man er meget uenig i begge spørgsmål, så hører man nok mere hjemme i rød blok. Så at lave en perceptron, som kan hjælpe os med at forudsige det, er nok ikke raketvidenskab, men det kan ikke desto mindre hjælpe os med at forstå de bagvedliggende principper og hvordan disse sidenhen kan generaliseres.\nLad os prøve at blive lidt mere specifikke og indføre to variable \\(x_1\\) og \\(x_2\\), hvor\n\n\\(x_1\\): svaret på Jeg synes, at indkomstskatten skal sættes ned angivet på en skala fra -2 til 2\n\n\\(x_2\\): svaret på Jeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift angivet på en skala fra -2 til 2.\n\nVores beslutning vil vi nu også kvantificere vha. en variabel \\(t\\), som kan antage to værdier, nemlig \\(-1\\) og \\(1\\). Hvis vi hører hjemme i blå blok, vil vi sætte \\(t=1\\), mens vi vil sætte \\(t=-1\\), hvis vi vil sætte vores krydset ved et rødt parti. Altså:\n\\[\n\\begin{aligned}\nt&=-1: &\\text{Rød blok} \\\\\nt&=1: &\\text{Blå blok} \\\\\n\\end{aligned}\n\\]\nNu forestiller vi os, at vi har bedt seks personer (som godt ved, hvem de vil stemme på - måske er det ligefrem politikere vi har spurgt) om at svare på de to spørgsmål og samtidig tilkendegive, om de vil stemme på blå eller rød blok. Lad os f.eks. sige, at den første person er meget enig i at indkomstskatten skal sættes ned (dvs. \\(x_1=2\\)), og at denne person er overvejende enig i at danske virksomheder ikke skal pålægges en CO2-afgift (dvs. \\(x_2=1\\)). Desuden oplyser denne person, at han/hun vil stemme på blå blok (dvs. \\(t=1\\)). Det kan udtrykkes sådan her: \\[\n(x_1,x_2)=(2,1) \\quad \\Rightarrow \\quad  t=1\n\\tag{1}\\]\nOg sådan kunne man opstille andre eksempler: \\[\n\\begin{aligned}\n&(x_1,x_2)=(-1,1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(-1,-1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(1,1) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(2,2) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(-2,-1) \\quad \\Rightarrow \\quad t=-1 \\\\\n\\end{aligned}\n\\] Det første eksempel siger for eksempel, at en person har været overvejende uenig i at sætte indkomstskatten ned (\\(x_1=-1\\)), overvejende enig i at danske virksomheder ikke skal pålægges en CO2-afgift (\\(x_2=1\\)) og samtidig vil denne person stemme på rød blok (\\(t=-1\\)).\nVi kan prøve at indtegne \\((x_1,x_2)\\)-punkterne i et koordinatsystem og samtidig angive den tilhørende værdi af \\(t\\) med en farve. Det vil se sådan her ud:\n\n\n\n\n\n\n\n\nFigur 1: Illustration af svaret på spørgsmål 1 (\\(1.\\) aksen) og spørgsmål 2 (\\(2.\\) aksen) med en markering af om man vil stemme på rød eller blå blok.\n\n\n\n\n\nDet kunne godt se ud som om, at det vil være muligt at indtegne en ret linje på en sådan måde, at alle punkter som ligger over linjen skulle farves blå (svarende til \"her stemmer vi på blå blok\"), mens alle punkter under linjen skulle farves røde (svarende til \"her stemmer vi på rød blok\"). En tilfældig indtegnet linje ses på figur 2.\n\n\n\n\n\n\n\n\nFigur 2: Illustration af svaret på spørgsmål 1 (\\(1.\\) aksen) og spørgsmål 2 (\\(2.\\) aksen) med en markering af om man vil stemme på rød eller blå blok. En tilfældig linje er indtegnet.\n\n\n\n\n\nHerunder ser du et bud på en linje, som ser ud til at være god til at adskille de blå punkter fra de røde – faktisk er der jo uendeligt mange linjer, som vil kunne adskille de blå punkter fra de røde:\n\n\n\n\n\n\n\n\nFigur 3: Illustration af svaret på spørgsmål 1 (\\(1.\\) aksen) og spørgsmål 2 (\\(2.\\) aksen) med en markering af om man vil stemme på rød eller blå blok. Her er indtegnet en linje, som kan separere de blå punkter fra de røde.\n\n\n\n\n\nLinjen på figur 3 har ligning \\[\\begin{aligned}\ny=-1.2 \\cdot x+1.5.\\end{aligned}\\] Men nu kaldte vi jo faktisk ikke de to variable for \\(x\\) og \\(y\\), men derimod for \\(x_1\\) og \\(x_2\\). Med denne notation får vi altså, at \\[\n\\begin{aligned}\nx_2=-1.2 \\cdot x_1+1.5\n\\end{aligned}\n\\] Hvis vi bruger denne ligning til at skelne imellem blå og røde punkter, så vil vi sige, at alle punkter, som ligger over linjen skal være blå. Det vil være det samme som at sige, at alle de blå punkter opfylder uligheden \\[\n\\begin{aligned}\nx_2&gt;-1.2 \\cdot x_1+1.5.\n\\end{aligned}\n\\] Eller skrevet på en anden måde: \\[\n\\begin{aligned}\n1.2 \\cdot x_1+ 1 \\cdot x_2&gt;1.5.\n\\end{aligned}\n\\] Her kalder man værdi \\(1.5\\) på højreside for threshold værdien (på dansk: tærskelværdi), fordi det er denne værdi, som afgør, om vi skal farve et punkt rødt eller blåt. Værdierne \\(1.2\\) og \\(1\\) kaldes for vægte, fordi de bestemmer, hvor meget inputværdierne \\(x_1\\) og \\(x_2\\) skal vægtes i forhold til hinanden.\nEn helt tredje måde at skrive det samme på vil være \\[\n\\begin{aligned}\n-1.5+1.2 \\cdot x_1+ 1 \\cdot x_2&gt;0.\n\\end{aligned}\n\\] Nu kalder man så bare værdien \\(-1.5\\) for en bias, men i virkeligheden er det jo bare threshold værdien med modsat fortegn1.\n1 Der er forskellige overvejelser i forhold til valget af denne skrivemåde. For det første er vi gået væk fra \\(x\\) og \\(y\\) og over til \\(x_1\\) og \\(x_2\\). Det giver mening, fordi vi ofte tænker på \\(y\\) som den afhængige variabel og \\(x\\) som den uafhængige variabel. Denne fortolkning af de to variable giver ikke mening i denne sammenhæng. Derudover kan vi beskrive en vilkårlig linje i planen ved hjælp af ligningen \\(ax_1+bx_2+c=0\\) – også de lodrette linjer. Holder vi derimod fast i \\(y=ax+b\\), så kan vi ikke “fange” de lodrette linjer.Vi har nu faktisk udledt en regel, som for tid og evighed kan hjælpe os med at afgøre, om vi skal stemme på rød eller blå blok. Den kan opsummeres sådan her:\n\n\n\n\n\n\nHvem skal jeg stemme på?\n\n\n\nSvar på en skala fra -2 til 2 på følgende spørgsmål:\n\\(x_1\\): \"Jeg synes, at indkomstskatten skal sættes ned\"\n\\(x_2\\): \"Jeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift\"\nhvor 2 svarer til \"Meget enig\" og -2 svarer til \"Meget uenig\".\nBeregn nu \\(o\\) (for outputværdi) på denne måde \\[\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 \\geq 0 \\\\\n-1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 &lt; 0. \\\\\n\\end{cases}\\end{aligned}\\] Reglen er nu: \\[\\begin{aligned}\n&\\text{Hvis } o=1: \\quad &\\text{Stem blå blok.}\\\\\n&\\text{Hvis } o=-1: \\quad &\\text{Stem rød blok.}\\\\\\end{aligned}\\]\n\n\nMan siger også, at man på baggrund af inputværdierne kan lave en klassificering (eller kategorisering). Det betyder, at vi på baggrund af inputværdierne kan beregne, om vi er i kategorien \"Blå blok\" (\\(o=1\\)) eller i kategorien \"Rød blok\" (\\(o=-1\\)). Grafisk svarer det til, at man indtegner sit \\((x_1, x_2)\\)-punkt i koordinatsystemet i figur 3 og ser så på om punkt ligger over eller under linjen (ligger det over skal vi stemme blå blok).\n\nEksempel 1 Lad os sige at en vælger hverken er enig eller uenig i, at indkomstskatten skal sættes ned. Det vil sige, at \\(x_1=0\\). Samtidig er denne vælger meget enig i, at danske virksomheder ikke skal pålægges en CO2-afgift. Altså er \\(x_2=2\\). Vi udregner nu: \\[\n-1.5+1.2 \\cdot x_1+x_2=-1.5+1.2 \\cdot 0+2=0.5\n\\] Og da denne værdi er større end \\(0\\), sætter vi \\(o=1\\). Det vil sige, at vi vil anbefale denne vælger at stemme blå blok.\n\nDet er da smart! Og det her er faktisk lige præcis idéen bag perceptroner, som den amerikanske psykolog Frank Rosenblatt foreslog helt tilbage i \\(1958\\). Den klassiske perceptron er defineret ved, at perceptronen kan modtage input \\[\n\\begin{aligned}\nx_1, x_2, \\dots, x_n,\n\\end{aligned}\n\\] hvor hver enkel inputværdi i princippet kan være et vilkårligt reelt tal. I vores eksempel har vi dog begrænset inputværdierne til \\(x_1, x_2 \\in \\{-2,-1,0,1,2 \\}\\). Vi beregner så en outputværdi \\(o\\) vha. vægtene \\(w_1, w_2, \\dots, w_n\\) og en biasværdi, som vi her vil kalde for \\(w_0\\) på denne måde: \\[\n\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n \\geq 0 \\\\\n-1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n &lt; 0. \\\\\n\\end{cases}\n\\end{aligned}\n\\] Grafisk kan det illustreres sådan her:\n\n\n\n\n\n\nFigur 4: Grafisk illustration af en perceptron.\n\n\n\nHer illustrerer sumtegnet i cirklen, at vi tager en vægtet sum af alle inputværdierne (inklusiv et input (\\(x_0\\)), som altid er \\(1\\), og som vægtes med \\(w_0\\) svarende til, at vi får vores bias med), mens grafen af trappefunktionen i firkanten viser, at vi diskretiserer denne vægtede sum, sådan at outputværdien enten er \\(-1\\) eller \\(1\\).\n\n\n\nI denne video forklarer vi ovenstående, men med udgangspunkt i et andet eksempel."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#eksempel-hvem-skal-jeg-stemme-på-ved-næste-valg",
    "href": "materialer/perceptron/perceptron.html#eksempel-hvem-skal-jeg-stemme-på-ved-næste-valg",
    "title": "Perceptroner",
    "section": "",
    "text": "De senere år er det blevet populært, at diverse medier laver forskellige kandidattests. Sådan nogle tests kan laves på mange forskellige måder - man kunne blandt andet bruge perceptroner! Testene fungerer som regel på den måde, at man bliver stillet en række forskellige spørgsmål og så skal man svare på en skala fra meget uenig til meget enig. Disse kategorier af svar kunne f.eks. oversættes til matematik på denne måde:\n\n\n\n\n\n\n\n\n\n\n\n\nHelt enig\nOvervejende enig\nHverken/eller\nOvervejende uenig\nHelt uenig\n\n\n\n\n2\n1\n0\n-1\n-2\n\n\n\n\n\nLad os prøve at gøre det helt simpelt. I stedet for at komme med et bud på hvem man skal stemme på, så vil vi blot forsøge at komme med et bud på, om man skal stemme på rød eller blå blok (det er sikkert en håbløs simplificering, men det må du tale med din samfundsfagslærer om ).\nLad os sige at vi vil basere vores bud på to spørgsmål:\n\nJeg synes, at indkomstskatten skal sættes ned.\nJeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift.\n\nVi kan sikkert hurtigt blive enige om, at hvis man er meget enig i begge spørgsmål, så hører man formentlig til i blå blok og modsat, hvis man er meget uenig i begge spørgsmål, så hører man nok mere hjemme i rød blok. Så at lave en perceptron, som kan hjælpe os med at forudsige det, er nok ikke raketvidenskab, men det kan ikke desto mindre hjælpe os med at forstå de bagvedliggende principper og hvordan disse sidenhen kan generaliseres.\nLad os prøve at blive lidt mere specifikke og indføre to variable \\(x_1\\) og \\(x_2\\), hvor\n\n\\(x_1\\): svaret på Jeg synes, at indkomstskatten skal sættes ned angivet på en skala fra -2 til 2\n\n\\(x_2\\): svaret på Jeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift angivet på en skala fra -2 til 2.\n\nVores beslutning vil vi nu også kvantificere vha. en variabel \\(t\\), som kan antage to værdier, nemlig \\(-1\\) og \\(1\\). Hvis vi hører hjemme i blå blok, vil vi sætte \\(t=1\\), mens vi vil sætte \\(t=-1\\), hvis vi vil sætte vores krydset ved et rødt parti. Altså:\n\\[\n\\begin{aligned}\nt&=-1: &\\text{Rød blok} \\\\\nt&=1: &\\text{Blå blok} \\\\\n\\end{aligned}\n\\]\nNu forestiller vi os, at vi har bedt seks personer (som godt ved, hvem de vil stemme på - måske er det ligefrem politikere vi har spurgt) om at svare på de to spørgsmål og samtidig tilkendegive, om de vil stemme på blå eller rød blok. Lad os f.eks. sige, at den første person er meget enig i at indkomstskatten skal sættes ned (dvs. \\(x_1=2\\)), og at denne person er overvejende enig i at danske virksomheder ikke skal pålægges en CO2-afgift (dvs. \\(x_2=1\\)). Desuden oplyser denne person, at han/hun vil stemme på blå blok (dvs. \\(t=1\\)). Det kan udtrykkes sådan her: \\[\n(x_1,x_2)=(2,1) \\quad \\Rightarrow \\quad  t=1\n\\tag{1}\\]\nOg sådan kunne man opstille andre eksempler: \\[\n\\begin{aligned}\n&(x_1,x_2)=(-1,1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(-1,-1) \\quad \\Rightarrow \\quad  t=-1 \\\\\n&(x_1,x_2)=(1,1) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(2,2) \\quad \\Rightarrow \\quad  t=1 \\\\\n&(x_1,x_2)=(-2,-1) \\quad \\Rightarrow \\quad t=-1 \\\\\n\\end{aligned}\n\\] Det første eksempel siger for eksempel, at en person har været overvejende uenig i at sætte indkomstskatten ned (\\(x_1=-1\\)), overvejende enig i at danske virksomheder ikke skal pålægges en CO2-afgift (\\(x_2=1\\)) og samtidig vil denne person stemme på rød blok (\\(t=-1\\)).\nVi kan prøve at indtegne \\((x_1,x_2)\\)-punkterne i et koordinatsystem og samtidig angive den tilhørende værdi af \\(t\\) med en farve. Det vil se sådan her ud:\n\n\n\n\n\n\n\n\nFigur 1: Illustration af svaret på spørgsmål 1 (\\(1.\\) aksen) og spørgsmål 2 (\\(2.\\) aksen) med en markering af om man vil stemme på rød eller blå blok.\n\n\n\n\n\nDet kunne godt se ud som om, at det vil være muligt at indtegne en ret linje på en sådan måde, at alle punkter som ligger over linjen skulle farves blå (svarende til \"her stemmer vi på blå blok\"), mens alle punkter under linjen skulle farves røde (svarende til \"her stemmer vi på rød blok\"). En tilfældig indtegnet linje ses på figur 2.\n\n\n\n\n\n\n\n\nFigur 2: Illustration af svaret på spørgsmål 1 (\\(1.\\) aksen) og spørgsmål 2 (\\(2.\\) aksen) med en markering af om man vil stemme på rød eller blå blok. En tilfældig linje er indtegnet.\n\n\n\n\n\nHerunder ser du et bud på en linje, som ser ud til at være god til at adskille de blå punkter fra de røde – faktisk er der jo uendeligt mange linjer, som vil kunne adskille de blå punkter fra de røde:\n\n\n\n\n\n\n\n\nFigur 3: Illustration af svaret på spørgsmål 1 (\\(1.\\) aksen) og spørgsmål 2 (\\(2.\\) aksen) med en markering af om man vil stemme på rød eller blå blok. Her er indtegnet en linje, som kan separere de blå punkter fra de røde.\n\n\n\n\n\nLinjen på figur 3 har ligning \\[\\begin{aligned}\ny=-1.2 \\cdot x+1.5.\\end{aligned}\\] Men nu kaldte vi jo faktisk ikke de to variable for \\(x\\) og \\(y\\), men derimod for \\(x_1\\) og \\(x_2\\). Med denne notation får vi altså, at \\[\n\\begin{aligned}\nx_2=-1.2 \\cdot x_1+1.5\n\\end{aligned}\n\\] Hvis vi bruger denne ligning til at skelne imellem blå og røde punkter, så vil vi sige, at alle punkter, som ligger over linjen skal være blå. Det vil være det samme som at sige, at alle de blå punkter opfylder uligheden \\[\n\\begin{aligned}\nx_2&gt;-1.2 \\cdot x_1+1.5.\n\\end{aligned}\n\\] Eller skrevet på en anden måde: \\[\n\\begin{aligned}\n1.2 \\cdot x_1+ 1 \\cdot x_2&gt;1.5.\n\\end{aligned}\n\\] Her kalder man værdi \\(1.5\\) på højreside for threshold værdien (på dansk: tærskelværdi), fordi det er denne værdi, som afgør, om vi skal farve et punkt rødt eller blåt. Værdierne \\(1.2\\) og \\(1\\) kaldes for vægte, fordi de bestemmer, hvor meget inputværdierne \\(x_1\\) og \\(x_2\\) skal vægtes i forhold til hinanden.\nEn helt tredje måde at skrive det samme på vil være \\[\n\\begin{aligned}\n-1.5+1.2 \\cdot x_1+ 1 \\cdot x_2&gt;0.\n\\end{aligned}\n\\] Nu kalder man så bare værdien \\(-1.5\\) for en bias, men i virkeligheden er det jo bare threshold værdien med modsat fortegn1.\n1 Der er forskellige overvejelser i forhold til valget af denne skrivemåde. For det første er vi gået væk fra \\(x\\) og \\(y\\) og over til \\(x_1\\) og \\(x_2\\). Det giver mening, fordi vi ofte tænker på \\(y\\) som den afhængige variabel og \\(x\\) som den uafhængige variabel. Denne fortolkning af de to variable giver ikke mening i denne sammenhæng. Derudover kan vi beskrive en vilkårlig linje i planen ved hjælp af ligningen \\(ax_1+bx_2+c=0\\) – også de lodrette linjer. Holder vi derimod fast i \\(y=ax+b\\), så kan vi ikke “fange” de lodrette linjer.Vi har nu faktisk udledt en regel, som for tid og evighed kan hjælpe os med at afgøre, om vi skal stemme på rød eller blå blok. Den kan opsummeres sådan her:\n\n\n\n\n\n\nHvem skal jeg stemme på?\n\n\n\nSvar på en skala fra -2 til 2 på følgende spørgsmål:\n\\(x_1\\): \"Jeg synes, at indkomstskatten skal sættes ned\"\n\\(x_2\\): \"Jeg synes ikke, at danske virksomheder skal pålægges en CO2-afgift\"\nhvor 2 svarer til \"Meget enig\" og -2 svarer til \"Meget uenig\".\nBeregn nu \\(o\\) (for outputværdi) på denne måde \\[\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 \\geq 0 \\\\\n-1 & \\text{hvis } -1.5+1.2 \\cdot x_1+ 1 \\cdot x_2 &lt; 0. \\\\\n\\end{cases}\\end{aligned}\\] Reglen er nu: \\[\\begin{aligned}\n&\\text{Hvis } o=1: \\quad &\\text{Stem blå blok.}\\\\\n&\\text{Hvis } o=-1: \\quad &\\text{Stem rød blok.}\\\\\\end{aligned}\\]\n\n\nMan siger også, at man på baggrund af inputværdierne kan lave en klassificering (eller kategorisering). Det betyder, at vi på baggrund af inputværdierne kan beregne, om vi er i kategorien \"Blå blok\" (\\(o=1\\)) eller i kategorien \"Rød blok\" (\\(o=-1\\)). Grafisk svarer det til, at man indtegner sit \\((x_1, x_2)\\)-punkt i koordinatsystemet i figur 3 og ser så på om punkt ligger over eller under linjen (ligger det over skal vi stemme blå blok).\n\nEksempel 1 Lad os sige at en vælger hverken er enig eller uenig i, at indkomstskatten skal sættes ned. Det vil sige, at \\(x_1=0\\). Samtidig er denne vælger meget enig i, at danske virksomheder ikke skal pålægges en CO2-afgift. Altså er \\(x_2=2\\). Vi udregner nu: \\[\n-1.5+1.2 \\cdot x_1+x_2=-1.5+1.2 \\cdot 0+2=0.5\n\\] Og da denne værdi er større end \\(0\\), sætter vi \\(o=1\\). Det vil sige, at vi vil anbefale denne vælger at stemme blå blok.\n\nDet er da smart! Og det her er faktisk lige præcis idéen bag perceptroner, som den amerikanske psykolog Frank Rosenblatt foreslog helt tilbage i \\(1958\\). Den klassiske perceptron er defineret ved, at perceptronen kan modtage input \\[\n\\begin{aligned}\nx_1, x_2, \\dots, x_n,\n\\end{aligned}\n\\] hvor hver enkel inputværdi i princippet kan være et vilkårligt reelt tal. I vores eksempel har vi dog begrænset inputværdierne til \\(x_1, x_2 \\in \\{-2,-1,0,1,2 \\}\\). Vi beregner så en outputværdi \\(o\\) vha. vægtene \\(w_1, w_2, \\dots, w_n\\) og en biasværdi, som vi her vil kalde for \\(w_0\\) på denne måde: \\[\n\\begin{aligned}\no = \\begin{cases}\n1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n \\geq 0 \\\\\n-1 & \\text{hvis } w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n &lt; 0. \\\\\n\\end{cases}\n\\end{aligned}\n\\] Grafisk kan det illustreres sådan her:\n\n\n\n\n\n\nFigur 4: Grafisk illustration af en perceptron.\n\n\n\nHer illustrerer sumtegnet i cirklen, at vi tager en vægtet sum af alle inputværdierne (inklusiv et input (\\(x_0\\)), som altid er \\(1\\), og som vægtes med \\(w_0\\) svarende til, at vi får vores bias med), mens grafen af trappefunktionen i firkanten viser, at vi diskretiserer denne vægtede sum, sådan at outputværdien enten er \\(-1\\) eller \\(1\\)."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-hvad-er-en-perceptron",
    "href": "materialer/perceptron/perceptron.html#video-hvad-er-en-perceptron",
    "title": "Perceptroner",
    "section": "",
    "text": "I denne video forklarer vi ovenstående, men med udgangspunkt i et andet eksempel."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-perceptron-learning-algoritmen",
    "href": "materialer/perceptron/perceptron.html#video-perceptron-learning-algoritmen",
    "title": "Perceptroner",
    "section": "VIDEO: Perceptron Learning Algoritmen",
    "text": "VIDEO: Perceptron Learning Algoritmen\nI denne video forklarer vi perceptron learning algoritmen."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-perceptron-learning-versus-adaline",
    "href": "materialer/perceptron/perceptron.html#video-perceptron-learning-versus-adaline",
    "title": "Perceptroner",
    "section": "VIDEO: Perceptron Learning versus Adaline",
    "text": "VIDEO: Perceptron Learning versus Adaline\nI denne video forklarer vi idéen bag Adaline og indfører tabsfunktionen."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#gradientnedstigning",
    "href": "materialer/perceptron/perceptron.html#gradientnedstigning",
    "title": "Perceptroner",
    "section": "Gradientnedstigning",
    "text": "Gradientnedstigning\nFor at gøre det bruges en metode, som kaldes for gradientnedstigning. For at forklare hvad det går ud på, er det nemmest at se på en tabsfunktion, som kun afhænger af to vægte \\(w_0\\) og \\(w_1\\). I det tilfælde får vi \\[\n\\begin{aligned}\nE(w_0, w_1) = \\frac{1}{2} \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1}) \\right)^2.\n\\end{aligned}\n\\] Da tabsfunktionenkun afhænger af to variable, kan vi tegne grafen for den. Et eksempel herpå ses i figur 9.\n\n\n\n\n\n\nFigur 9: Grafen for en tabsfunktion som afhænger af vægtene \\(w_0\\) og \\(w_1\\).\n\n\n\nIdéen er nu, at vi gerne vil bestemme vægtene \\(w_0\\) og \\(w_1\\), sådan at tabsfunktionen minimeres. Tænk lidt over det. Det giver god mening, at bestemme vægtene sådan at den samlede fejl, perceptronen begår på træningsdata, bliver så lille som mulig. Vi ved faktisk godt, hvordan man bestemmer minimum for en funktion af to variable. Løs ligningerne \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_0} = 0 \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial w_1} = 0.\\end{aligned}\n\\] Det er en overkommelig opgave at finde de partielle afledede og sætte dem lig med \\(0\\) i det tilfælde, hvor tabsfunktionen kun afhænger af to vægte. Men vi skal senere se, at perceptroner bliver fundamentale byggesten i kunstige neurale netværk, og her viser det sig, at denne fremgangsmåde med at sætte de partielle afledede lig \\(0\\), er helt håbløs! Derfor bruger man gradientnedstigning.\nForestil dig at grafen for tabsfunktionen i figur 9 er et landskab med en dal. Dit mål er at finde ned i dalen. Du er blevet placeret et tilfældigt sted i landskabet svarende til tilfældige værdier af \\(w_0\\) og \\(w_1\\). Hvad gør du? Jo, du kommer i tanke om, at du har lært, at hvis du går i gradientens \\[\n\\begin{aligned}\n\\nabla E(w_0,w_1) = \\begin{pmatrix} \\frac{\\partial E }{\\partial w_0}(w_0,w_1) \\\\ \\\\ \\frac{\\partial E }{\\partial w_1}(w_0,w_1) \\end{pmatrix}\n\\end{aligned}\n\\] retning, så kommer du til at gå i den retning, hvor det går allermest opad bakke! Men hov det er jo ikke det, vi vil! Vi vil gå allermest nedad bakke, så vi ender i dalen. Hvad gør vi så? Vi vender os da bare \\(180^{\\circ}\\) og går i den modsatte retning - så ender vi nede i dalen! Det vil sige, at vi går i retning af minus gradienten: \\[\n\\begin{aligned}\n- \\nabla E(w_0,w_1) = \\begin{pmatrix} - \\frac{\\partial E }{\\partial w_0}(w_0,w_1) \\\\ \\\\ - \\frac{\\partial E }{\\partial w_1}(w_0,w_1) \\end{pmatrix}\n\\end{aligned}\n\\] Fremgangsmåden bliver derfor den, at vi starter i nogle tilfældige \\((w_0, w_1)\\)-værdier og så bevæger vi os et lille skridt i den negative gradients retning. Så ender vi i et nyt punkt, hvor vi igen beregner gradienten og går igen et lille skridt i den negative gradients retning. Sådan fortsætter vi indtil værdien af tabsfunktionen ikke rigtig ændrer sig mere – det svarer til at vi har ramt dalen. Derfor bliver vores opdateringsregler med denne metode \\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 - \\eta \\cdot \\frac{\\partial E }{\\partial w_0} \\\\\nw_1 \\leftarrow & w_1 - \\eta \\cdot \\frac{\\partial E }{\\partial w_1} \\\\\n&\\vdots  \\\\\nw_n \\leftarrow & w_n - \\eta \\cdot \\frac{\\partial E }{\\partial w_n} \\\\\n\\end{aligned}\n\\] Her er \\(\\eta\\) igen en learning rate f.eks. \\(0.05\\), som sørger for, at vi hele tiden bare tager et lille skridt i den negative gradients retning. Man vælger værdien af \\(\\eta\\) lille for ikke at lave alt for store justeringer af vægtene ad gangen. Det svarer grafisk til, at vi lige så stille går ned af den bakke, som tabsfunktionen giver (se figur 9). Hvis vi tager for store skridt, risikerer vi helt, at komme til at “træde forbi” det minimum, som vi gerne vil lande i. Omvendt vil alt for små skridt føre til, at vi alt for langsomt nærmer os minimum. Så værdien af \\(\\eta\\) angiver altså, hvor meget vi er villige til at justere vægtene og dermed hvor hurtige eller hvor langsomme, vi bevæger os ned mod minimum. Af den grund giver det god mening, at \\(\\eta\\) kaldes for en learning rate - fordi den afgører, hvor hurtigt vi lærer af vores træningsdata.\nNu mangler vi bare at få bestemt de partielle afledede. Ved at bruge sumreglen og kædereglen for differentiation får vi fra (3), at \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_i} &= \\frac{1}{2} \\sum_{m=1}^{M} \\frac{\\partial}{\\partial w_i}\\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)^2 \\\\\n&= \\frac{1}{2} \\sum_{m=1}^{M} 2 \\cdot \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\\\ & \\quad  \\quad \\quad  \\quad \\quad  \\quad \\cdot \\frac{\\partial}{\\partial w_i} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n} ) \\right) \\\\\n&= \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (-x_{m,i} \\right)\n\\end{aligned}\n\\] for \\(i \\in \\{1, 2, \\dots, n\\}\\).\nLæg mærke til at når vi differentierer den indre funktion \\[\n\\begin{aligned}\nt_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n})\n\\end{aligned}\n\\] med hensyn til \\(w_i\\), så vil alle led være at betragte som konstanter bortset fra leddet \\[\n\\begin{aligned}\n-w_i \\cdot x_{m,i}\n\\end{aligned}\n\\] og når vi differentierer dette led med hensyn til \\(w_i\\) får vi netop \\(- x_{m,i}\\). Læg også mærke til at hvis vi differentierer med hensyn til \\(w_0\\), så får vi, \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial w_0} = \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (-1 \\right).\n\\end{aligned}\n\\] Altså bliver vores opdateringsregler \\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 + \\eta \\cdot \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right)  \\\\\nw_1 \\leftarrow & w_1 + \\eta \\cdot  \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (x_{m,1} \\right)\\\\\n&\\vdots  \\\\\nw_n \\leftarrow & w_n + \\eta \\cdot  \\sum_{m=1}^{M} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\cdot \\left (x_{m,n} \\right)\\\\\n\\end{aligned}\n\\] Vi kan altså sammenfatte gradientnedstigningsalgoritmen på denne måde:\n\nSæt alle vægte \\(w_0, w_1, \\dots, w_n\\) til et tilfældigt tal (f.eks. \\(0.5\\)).\nVælg en værdi af \\(\\eta\\) (f.eks. \\(0.05\\)).\nUdregn på baggrund af alle træningsdata fejlene: \\[\n\\begin{aligned}\nerror_0 &= \\sum_{m=1}^{M} \\left (t_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\right) \\\\\nerror_1 &= \\sum_{m=1}^{M} \\left (t_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\cdot x_{m,1} \\right) \\\\\n&\\vdots \\\\\nerror_n &= \\sum_{m=1}^{M} \\left (t_m-(w_0 + w_1 \\cdot x_{m,1} + \\cdots + w_n \\cdot x_{m,n}) \\cdot x_{m,n} \\right)\n\\end{aligned}\n\\]\nOpdatér alle vægtene: \\[\n\\begin{aligned}\nw_0  \\leftarrow & w_0 + \\eta \\cdot error_0 \\\\\nw_1  \\leftarrow & w_1 + \\eta \\cdot error_1 \\\\\n& \\vdots \\\\\nw_n  \\leftarrow & w_n + \\eta \\cdot error_n\n\\end{aligned}\n\\]\nStart forfra indtil vægtene ikke ændrer sig (særlig meget)."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#eksempel-på-gradientnedstigning",
    "href": "materialer/perceptron/perceptron.html#eksempel-på-gradientnedstigning",
    "title": "Perceptroner",
    "section": "Eksempel på gradientnedstigning",
    "text": "Eksempel på gradientnedstigning\nLad os prøve at bruge gradientnedstigning på vores eksempel omkring kandidattest. I dette simple eksempel bliver vores opdateringsregler nu \\[\n\\begin{aligned}\nw_0 \\leftarrow & w_0 + \\eta \\cdot \\sum_{m=1}^{6} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + w_2 \\cdot x_{m,2}) \\right)  \\\\\nw_1 \\leftarrow & w_1 + \\eta \\cdot  \\sum_{m=1}^{6} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1} + w_2 \\cdot x_{m,2}) \\right) \\cdot \\left (x_{m,1} \\right)\\\\\nw_2 \\leftarrow & w_2 + \\eta \\cdot  \\sum_{m=1}^{6} \\left (t_m-\n(w_0 + w_1 \\cdot x_{m,1}  + w_2 \\cdot x_{m,2}) \\right) \\cdot \\left (x_{m,2} \\right)\n\\end{aligned}\n\\] Hvis man bruger disse opdateringsregler på dataene fra tabel 1, så ender man med følgende værdier af vægtene \\[\n\\begin{aligned}\nw_0 =-0.0769 , w_1=0.6410, w_2=-0.0598\n\\end{aligned}\n\\] hvor vi startede med værdierne \\(w_0=0.5, w_1=0.5\\) og \\(w_2=0.5\\) og hvor vi satte \\(\\eta=0.05\\). Dette svarer til linjen med ligning \\[\n\\begin{aligned}\n-0.0769 + 0.6410 \\cdot x_1 - 0.0598 \\cdot x_2 = 0.\n\\end{aligned}\n\\] Linjen er indtegnet sammen med træningsdata i figur 10.\n\n\n\n\n\n\n\n\nFigur 10: Ilustration af svaret på spørgsmål 1 (\\(1.\\) aksen) og spørgsmål 2 (\\(2.\\) aksen) med en markering af om man vil stemme på rød eller blok blå. Her er den linje indtegnet, som stammer fra gradientnedstigningsalgoritmen (med startværdier \\(w_0=0.5, w_1=0.5, w_2=0.5\\) og \\(\\eta=0.05\\)).\n\n\n\n\n\nBemærk, at til forskel fra perceptron learning algoritmen så ender man med de samme værdier af vægtene, selvom man vælger andre startværdier. Det er fordi, vi finder et globalt minimum for tabsfunktionen, som er uafhængig af det punkt, hvor vi starter med at lede. Det svarer til, at ligegyldigt hvor du bliver placeret i landskabet i figur 9, så vil du til sidst ende i dalen, hvis du hele tiden går små skridt i den negative gradients retning.\n\nEksempel 3 Vi ser igen på eksempel 1 og eksempel 2, hvor \\(x_1=0\\) og \\(x_2=1\\). Baseret på vægtene fra gradientnedstigning, får vi: \\[\n-0.0769+0.6410 \\cdot x_1 -0.0598 \\cdot x_2=-0.0769+0.6410 \\cdot 0 -0.0598 \\cdot 1=-0.1367\n\\] Da denne værdi er mindre end \\(0\\), sætter vi \\(o=-1\\). Altså er vi tilbage til at befale vælgeren at stemme på rød blok."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#forskel-på-perceptron-learning-og-gradientnedstigning",
    "href": "materialer/perceptron/perceptron.html#forskel-på-perceptron-learning-og-gradientnedstigning",
    "title": "Perceptroner",
    "section": "Forskel på perceptron learning og gradientnedstigning",
    "text": "Forskel på perceptron learning og gradientnedstigning\nDer er flere forskelle på perceptron learning algoritmen og gradientnedstigning. Vi har allerede været inde på, at perceptron learning algoritmen går i kuk, hvis data ikke er lineært separable, som i figur 8. Mere formelt vil perceptron learning algoritmen ikke konvergere. I det tilfælde vil gradientnedstigning alligevel komme med et bud på værdier af vægtene, som kan bruges til at kategorisere træningsdata, selvom alle træningsdata ikke vil blive klassificeret korrekt (fordi de netop ikke er lineært separable). En anden forskel ligger i hvornår vægtene opdateres. I perceptron learning algoritmen opdateres vægtene efter hvert træningseksempel. I gradientnedstigning bruger man alle træningsdata for at lave en enkelt opdatering af vægtene. Hvis man har mange træningsdata, kan det godt blive lidt tungt. Så kan man i stedet for vælge at bruge et mindre, tilfældigt udvalg af data (for eksempel \\(10\\%\\)) til hver opdatering og så til næste opdatering bruge et nyt tilfældigt udvalg af data. Denne fremgangsmåde kaldes for stokastisk gradientnedstigning. En anden mulighed er at lave online gradientnedstigning, hvor man opdaterer vægtene for hvert træningseksempel, som i perceptron learning algoritmen. Selvom det kun er en tilnærmelse til rigtig gradientnedstigning, så har det alligevel en række fordele: 1) Det er meget hurtigere at opdatere vægtene. 2) I nogle anvendelser vil man have brug for løbende at opdatere vægtene på baggrund af nye træningsdata. I stedet for at gemme alle de mange træningsdata kan man bare opdatere vægtene, hver gang man får et nyt træningseksempel til rådighed og så eventuelt slette træningseksemplet igen, når vægtene er blevet opdateret. Det er både hurtigere og mere pladsbesparende."
  },
  {
    "objectID": "materialer/perceptron/perceptron.html#video-adaline",
    "href": "materialer/perceptron/perceptron.html#video-adaline",
    "title": "Perceptroner",
    "section": "VIDEO: Adaline",
    "text": "VIDEO: Adaline\nI denne video forklarer vi, hvad gradientnedstigning går ud på, og hvordan gradientnedstigning bruges til at opdatere vægtene i Adaline."
  },
  {
    "objectID": "referencer.html",
    "href": "referencer.html",
    "title": "Referencer",
    "section": "",
    "text": "Websider\n\nDTU har lavet et undervisningsmateriale, som handler om, hvordan kunstig intelligens kan bruges til at genkende lyde: Regn lyden ud\nOnline kursus hvor man lærer helt grundlæggende om kunstig intelligens (men ikke så matematisk som materialet her på siden): Elements of AI\nDanske Gymnasier har afholdt en konference om betydningen af AI og ChatGPT. Her på siden kan man genhøre oplæg fra blandt andet Vincent Hendricks, Stefan Herman m.fl.\nDen Store Danske har skrevet lidt om, hvad kunstig intelligens er - herunder også et historisk oprids af udviklingen inden for kunstig intelligens. Kunstig intelligens\nPå siden dataekspeditioner findes en række forløb hvoraf flere omhandler AI.\n\n\n\nVideoer\n\n3Blue1Brown har lavet en række fine videoer, som forklarer, hvad et kunstigt neuralt netværk er, og hvordan det trænes (på engelsk): Neural networks\n\n\n\nBøger\n\nOnline bog om kunstige neurale netværk af Michael Nielsen (på engelsk): Neural networks and deep learning"
  },
  {
    "objectID": "forloeb.html",
    "href": "forloeb.html",
    "title": "AI forløb",
    "section": "",
    "text": "AI forløb"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI - Aalborg Intelligence",
    "section": "",
    "text": "Her på sitet finder du materiale og undervisningsforløb om kunstig intelligens rettet mod danske gymnasieelever.\n\n\n\n\n\n\n\n\n\n\nUndervisningsforløb\n\n\nForskellige undervisningsforløb til matematik i gymnasiet, som inddrager AI. Der findes forløb til både A-, B- og C-niveau.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaterialer\n\n\nNoter om diverse AI relaterede emner.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRO\n\n\nIdéer til hvordan AI kan inddrages i SRO.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRP\n\n\nIdéer til hvordan AI kan inddrages i SRP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApps\n\n\nApps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferencer\n\n\nDiverse referencer til andre AI materialer.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nProjektet “AI - Aalborg Intelligence” er finansieret af Novo Nordisk Fonden og løber frem til 2026.\nSiden her er i et tidligt stadie med ganske få tilgængelige materialer på nuværende tidspunkt (2023), og mere materiale tilføjes løbende."
  },
  {
    "objectID": "undervisningsforlob.html",
    "href": "undervisningsforlob.html",
    "title": "Undervisningsforløb",
    "section": "",
    "text": "Siden er under opbygning, men vil komme til at indholde undervisningsforløb til alle niveauer.\n\n\n\n\n\n\n\n\n\n\nHvem ligner du mest?\n\n\nEvnen til at skelne mellem forskellige kategorier er helt central for os som mennesker. Vi kan langt oftest kende forskel på et æble og en pære, på en cykel og en knallert…\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptroner og rødder\n\n\nEt andengradspolynomium kan have enten ingen, én eller to rødder – og måske kan du ligefrem huske en metode til at bestemme antallet af rødder. Men kan man mon træne en…\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Om os",
    "section": "",
    "text": "Projektet Aalborg Intelligence, som er finansieret af Novo Nordisk Fonden, er forankret på Institut for Matematiske Fag på Aalborg Universitet (AAU), og inkluderer en repræsentant fra de fem gymnasier i Aalborg."
  },
  {
    "objectID": "about.html#gymnasielærere",
    "href": "about.html#gymnasielærere",
    "title": "Om os",
    "section": "Gymnasielærere",
    "text": "Gymnasielærere\n\nMalene Cramer Engebjerg (Aalborghus Gymnasium)\nAllan Frendrup (Nørresundby Gymnasium)\nNikolaj Hess-Nielsen (Aalborg Katedralskole)\nMette Kristensen (Hasseris Gymnasium)\nJan B. Sørensen (Aalborg City Gymnasium)"
  },
  {
    "objectID": "teacher.html",
    "href": "teacher.html",
    "title": "For lærerne",
    "section": "",
    "text": "Her kommer information om hvordan materialerne kan bruges i forskellige typer forløb i gymnasiet.\nVi har på nuværende tidspunkt erfaring med SRO i matematik og samfundsfag samt forskellige typer af SRP. Kontakt Ege Rubak for nærmere information."
  },
  {
    "objectID": "srp.html",
    "href": "srp.html",
    "title": "SRP",
    "section": "",
    "text": "I arbejdet med studieretningsprojektet kan matematik og AI indgå i et samarbejde med en lang række andre fag. Idéer til sådanne samarbejder findes herunder. Under nogle af emnerne er der også indsat konkrete forslag til problemformuleringer.\nHvis man ønsker, at inddrage kunstige neurale netværk kan noten om kunstige neurale netværk benyttes. En mulig fremgangsmåde er at bede eleven udlede opdateringsreglerne for et konkret, lille netværk med f.eks. ét skjult lag.\nEn anden mulighed er at bruge noten om perceptroner - eventuelt kombineret med noten om retningsafledede og gradientnedstigning."
  },
  {
    "objectID": "srp.html#samfundsfag-og-matematik",
    "href": "srp.html#samfundsfag-og-matematik",
    "title": "SRP",
    "section": "Samfundsfag og matematik",
    "text": "Samfundsfag og matematik\n\n\n\n\n\n\nKandidattest\n\n\n\n\n\nUdarbejdelse af kandidattest i forbindelse med valg. [samfundsfag A]\n\nMaterialer\nNoten om perceptroner.\n\n\n\n\n\n\n\n\n\n\nOvervågning\n\n\n\n\n\nBrugen af kunstig intelligens i forbindelse med ansigtsgenkendelse. Herunder kan emner som persondataloven, retssikkerhed og/eller partiernes holdning til overvågning behandles. [samfundsfag A]\n\nMaterialer\nNoten om kunstige neurale netværk."
  },
  {
    "objectID": "srp.html#dansk-og-matematik",
    "href": "srp.html#dansk-og-matematik",
    "title": "SRP",
    "section": "Dansk og matematik",
    "text": "Dansk og matematik\n\n\n\n\n\n\nAI og anvendelser\n\n\n\n\n\nFormidlingsopgave hvor AI metoder behandles og derefter formidles f.eks. som en populærvidenskabelig artikel. Eleverne skal skrive en danskfaglig meta-del, hvor de redegør for deres overvejelser og valg med hensyn til målgruppe, virkemidler med videre.\n\nMaterialer\nNoten om kunstige neurale netværk.\nNoten om perceptroner.\nNoten om naiv Bayes klassifier."
  },
  {
    "objectID": "srp.html#engelsk-og-matematik",
    "href": "srp.html#engelsk-og-matematik",
    "title": "SRP",
    "section": "Engelsk og matematik",
    "text": "Engelsk og matematik\n\n\n\n\n\n\nMachines like me\n\n\n\n\n\nRedegørelse for hvad et kunstigt neuralt netværk er. I engelsk perspektiveres der til Ian McEwans bog “Machines like me”. [engelsk A]"
  },
  {
    "objectID": "srp.html#idræt-og-matematik",
    "href": "srp.html#idræt-og-matematik",
    "title": "SRP",
    "section": "Idræt og matematik",
    "text": "Idræt og matematik\n\n\n\n\n\n\nBaseball og machine learning\n\n\n\n\n\nImplementering af et kunstig neuralt netværk, som kan forudsige baseball tegn (app til implementering af netværk er under udarbejdelse). [idræt C, evt. innovativ]\n\nMaterialer\nStealing Baseball Signs with a Phone (Machine Learning)."
  },
  {
    "objectID": "srp.html#biologi-og-matematik",
    "href": "srp.html#biologi-og-matematik",
    "title": "SRP",
    "section": "Biologi og matematik",
    "text": "Biologi og matematik\n\n\n\n\n\n\nDiagnosticering af sygdomme\n\n\n\n\n\nRedegørelse for hvordan et kunstigt neuralt netværk kan trænes, så det kan anvendes i forbindelse med diagnosticering af sygdomme - herunder kan opdateringsreglerne for et lille, simpelt netværk udledes. [biologi A]\n\nMaterialer\nMeet the computer diagnosing cancer."
  },
  {
    "objectID": "srp.html#informatik-og-matematik",
    "href": "srp.html#informatik-og-matematik",
    "title": "SRP",
    "section": "Informatik og matematik",
    "text": "Informatik og matematik\n\n\n\n\n\n\nGenkendelse af håndskrevne tal\n\n\n\n\n\nImplementering af et kunstig neuralt netværk med ét skjult lag, som kan kende forskel på f.eks. håndskrevne 2- og 9-taller. [informatik B, innovativ opgave]\n\nProblemformulering\nUdarbejd et løsningsforslag til hvordan man oversætter håndskrevne tal, så de kan genkendes af en computer. I den forbindelse skal du:\n\nRedegør for hvad der forstås ved et kunstigt neuralt netværk, hvor du tager udgangspunkt i et netværk med ét skjult lag. Kom herunder ind på feedforward og backpropagation.\nImplementer et kunstig neuralt netværk med ét skjult lag, som kan bruges til at kende forskel på 2- og 9-taller (brug en passende delmængde af MNIST train-datasættet).\nVurder dit løsningsforslag i forhold til styrker og svagheder samt graden af innovation. Inddrag i den forbindelse en passende delmængde af MNIST test-datasættet.\n\n\n\nMaterialer\nNetværket kan trænes på en passende delmængde af MNIST datasættet.\n\n\n\n\n\n\n\n\n\n\nKunstig intelligens - muligheder og begrænsninger\n\n\n\n\n\nRedegørelse for hvordan et kunstigt neuralt netværk trænes. Diskussion af de etiske problemstillinger, som kan opstå i forbindelse med anvendelsen af kunstig intelligens og/eller diskussion af de muligheder og begrænsninger, der er ved brugen kunstig intelligens. [informatik C]\n\nProblemformulering 1\n\nRedegør kort for begrebet ”kunstig intelligens” - herunder ”deep learning”.\nForklar hvordan et kunstig neuralt netværk virker. Herunder ønskes en redegørelse for hvordan et kunstigt neuralt netværk lærer vha. backpropagation og hvordan kædereglen benyttes i den forbindelse.\nDiskuter de etiske problemstillinger som kan opstå i anvendelsen af kunstig intelligens.\n\n\n\nProblemformulering 2\n\nRedegør for udviklingen inden for kunstig intelligens. Inddrag begreberne machine learning, deep learning samt supervised og unsupervised learning.\nRedegør for teorien bag kunstige neurale netværk herunder hvordan kunstige neurale netværk lærer vha. backpropagation og costfunktionen. Forklar også hvordan kædereglen benyttes i den forbindelse.\nDiskuter hvilke muligheder og begrænsninger der er ved brugen af machine learning. Inddrag bilag 1.\n\nBilag 1"
  },
  {
    "objectID": "srp.html#psykologi-og-matematik",
    "href": "srp.html#psykologi-og-matematik",
    "title": "SRP",
    "section": "Psykologi og matematik",
    "text": "Psykologi og matematik\n\n\n\n\n\n\nPrediktion af psykisk sygdom ved hjælp af deep learning\n\n\n\n\n\nForklare hvordan kunstige neurale netværk kan bruges til at prediktere psykisk sygdom baseret på register og genetiske data.\n\nMaterialer\nNoten om kunstige neurale netværk.\nDeep Learning for Cross-Diagnostic Prediction of Mental Disorder Diagnosis and Prognosis Using Danish Nationwide Register and Genetic Data."
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Apps",
    "section": "",
    "text": "AI apps\nSiden er under opbygning."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html",
    "href": "materialer/krydsvalidering/krydsvalidering.html",
    "title": "Overfitting, modeludvælgelse og krydsvalidering",
    "section": "",
    "text": "Denne note handler om, hvad man kan gøre, når man har flere forskellige modeller for data at vælge imellem og gerne vil vælge den bedste. Noten introducerer først polynomiel regression, der bruges som gennemgående eksempel. Mod slutningen diskuteres, hvordan de samme principper kan bruges i forbindelse med nogle af de andre algoritmer, der er gennemgået her på siden."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#polynomiel-regression",
    "href": "materialer/krydsvalidering/krydsvalidering.html#polynomiel-regression",
    "title": "Overfitting, modeludvælgelse og krydsvalidering",
    "section": "Polynomiel regression",
    "text": "Polynomiel regression\n\nLineær regression\nFra gymnasieundervisningen kender I lineær regression. Lad os sige, at vi har datapunkter \\((x_i,y_i)\\), hvor \\(i=1,2,\\ldots,n\\). Vi vil gerne finde den rette linje, der bedst beskriver punkterne. I denne note kalder vi linjens skæring for \\(a_0\\) og hældningen for \\(a_1\\). Linjen har altså funktionsforskriften1\n1 Du er vant til, at forskriften for en lineær funktion er på formen \\(f(x)=ax+b\\). Men lige om lidt viser skrivemåden \\(f(x)=a_0+a_1x\\) sig nyttig. I forhold til det, du kender, svarer det til, at \\(a_0=b\\) og \\(a_1=a\\).\\[f(x) = a_0 + a_1x.\\]\nFor at finde den bedste linje til at beskrive vores data, søger vi de værdier \\(a_0\\) og \\(a_1\\), som gør, at \\(a_0 + a_1x_i\\) er så tæt på \\(y_i\\) som muligt. Vi vil altså gerne gøre afvigelserne fra linjen \\(y_i - (a_0 + a_1 x_i)\\) så små som muligt. Bemærk, at disse afvigelser svarer til residualerne\n\\[\nr_i=y_i - (a_0 + a_1 x_i).\n\\]\nSom et samlet mål for hvor store disse afvigelser er for alle vores punkter, kigger vi på kvadratsummen af afvigelserne/residualerne\n\\[\n\\begin{aligned}\nE &= \\left(y_1 - (a_0 + a_1 x_1) \\right)^2 + \\left(y_2 - (a_0 + a_1 x_2) \\right)^2 + \\cdots + \\left(y_n - (a_0 + a_1 x_n) \\right)^2 \\\\\n& = r_1^2 + r_2^2 + \\cdots + r_n^2\n\\end{aligned}\n\\]\nNu er det lidt omstændeligt at skrive summen ud, som vi har gjort det ovenfor. I matematik vil man ofte skrive en sådan sum lidt mere kompakt ved hjælp af et summationstegn. Gør vi det, ser det sådan her ud:\n\\[\n\\begin{aligned}\nE &=\\sum_{i=1}^n \\left(y_i - (a_0 + a_1x_i) \\right)^2 \\\\\n&= \\sum_{i=1}^n r_i^2 .\n\\end{aligned}\n\\]\nVi vælger så de værdier \\(a_0\\) og \\(a_1\\), der gør \\(E\\) mindst mulig. Dette kaldes mindste kvadraters metode.\n\n\nKvadratisk regression\nHvad nu hvis det slet ikke ligner, at der er en lineær sammenhæng, når vi tegner vores datapunkter ind i et koordinatsystem? Er det så overhovedet en god idé at forsøge med en lineær regression? På figur 1 ser det for eksempel ikke ud til at punkterne følger en ret linje.\n\n\n\n\n\n\n\n\nFigur 1: Til venstre ses et punktplot af et datasæt. Til højre er den bedste rette linje indtegnet.\n\n\n\n\n\n\n\n\n\n\n\nDatasættet\n\n\n\n\n\nDatasættet fra figur 1 ses i tabellen herunder, hvis du selv vil prøve at lave lineær regression på data. Data kan også hentes som en Excel-fil her.\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(y\\)\n\n\n\n\n0.125\n1.71\n\n\n0.25\n1.95\n\n\n0.375\n1.877\n\n\n0.5\n1.914\n\n\n0.625\n2.341\n\n\n0.75\n1.692\n\n\n0.875\n2.473\n\n\n1\n2.217\n\n\n1.125\n2.199\n\n\n1.25\n1.962\n\n\n1.375\n2.125\n\n\n1.5\n2.595\n\n\n1.625\n2.021\n\n\n1.75\n1.894\n\n\n1.875\n1.309\n\n\n2\n1.545\n\n\n2.125\n0.7685\n\n\n2.25\n0.638\n\n\n2.375\n0.7456\n\n\n2.5\n0.1396\n\n\n\n\n\n\n\n\nDet fremgår også af residualplottet i figur 2. Her kan vi tydeligt se, at der er et mønster i den måde, residualerne fordeler sig omkring \\(x\\)-aksen. Det er altså tegn på, at en ret linje ikke er velegnet til at beskrive datapunkterne.\n\n\n\n\n\n\n\n\nFigur 2: Residualplottet for den bedste rette linje indtegnet i figur 1.\n\n\n\n\n\nNår \\(x\\) ligger mellem 0 og 1, kunne der godt se ud til at være en svagt stigende tendens i figur 1, mens der ser ud til at være en aftagende tendens for \\(x&gt;1.5\\). Det svarer til, at residualerne i figur 2 først er negative, så positive og dernæst negative igen. Den rette linje i figur 1 ser heller ikke ud til at følge punkterne særlig godt. Måske en parabel passer bedre på data?\n\n\n\n\n\n\n\n\nFigur 3: Datasættet fra figur 1, men nu med en parabel indtegnet.\n\n\n\n\n\nDet ser ud til, at parablen i figur 3 følger datapunkterne langt bedre. Vi kunne således prøve at modellere \\(y\\) ved hjælp af et andengradspolynomium i \\(x\\). Lad \\(f\\) betegne andengradspolynomiet2\n2 I gymnasiet skriver vi som regel forskriften for et andengradspolynomium på formen \\(f(x)=ax^2+bx+c\\). Med notationen, som vi bruger her, svarer det til, at \\(a_0=c, a_1=b\\) og \\(a_2=a\\).\\[\nf(x) = a_0 + a_1x + a_2x^2\n\\]\nmed koefficienter \\(a_0,a_1,a_2\\in \\mathbb{R}\\).\nHvordan finder man så det andengradspolynomium, der bedst beskriver datapunkterne? Tilgangen er faktisk den samme som den mindste kvadraters metode, I kender fra lineær regression. Vi søger de værdier \\(a_0, a_1\\) og \\(a_2\\), som gør, at \\(f(x_i)\\) kommer så tæt på \\(y_i\\) som muligt. Vi vil altså gerne gøre forskellene \\(y_i - f(x_i)\\) så små som muligt. Vi kigger derfor på kvadratsummen af disse forskelle \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2 = \\sum_{i=1}^n \\left(y_i - (a_0 + a_1x_i + a_2x_i^2)\\right)^2.\\] Vi søger så de værdier \\(a_0,a_1\\) og \\(a_2\\), der minimerer \\(E\\).\nGør man det i vores lille dataeksempel, fås netop den parabel, der er tegnet ind i koordinatsystemet i figur 3. Vi ser, at den beskriver data langt bedre end den rette linje.\nEksemplet viser vigtigheden af at tegne et residualplot for at vurdere anvendeligheden af den lineære model. Ellers kan man nemt komme til at overse en eventuel ikke-lineær sammenhæng.\n\n\nPolynomiel regression generelt\nMen hvordan kan vi nu vide, at et andengradspolynomium er det bedste til at beskrive data? Måske et polynomium af endnu højere grad ville være bedre? Man kan tilpasse tredje- og højeregradspolynomier til data på en helt tilsvarende måde. Vi kan for eksempel prøve at tilpasse et tredjegradspolynomium\n\\[\nf(x) = a_0 + a_1x + a_2x^2 +a_3x^3.\n\\]\nDet bedste tredjegradspolynomium er igen det, der minimerer kvadratsummen \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2 = \\sum_{i=1}^n \\left(y_i - (a_0 + a_1x_i + a_2x_i^2 + a_3x_i^3 )\\right)^2.\\] Grafen for det bedste tredjegradspolynomium er indtegnet med grøn for vores dataeksempel i figur 4. Andengradspolynomiet er indtegnet med rød til sammenligning.\n\n\n\n\n\n\n\n\nFigur 4: Et andengradspolynomium (rød) og et tredjegradspolynomium (grøn) fittet til data.\n\n\n\n\n\nDet er ikke så let at se forskel. De to polynomier ser ud til at passe nogenlunde lige godt på vores data. Men i figur 5 har vi zoomet ud på figuren ovenfor, og her er der en klar forskel:\n\n\n\n\n\n\n\n\nFigur 5: Plottet fra figur 4, men hvor der nu er zoomet lidt ud. Her ses det tydeligt, at der i “enderne” bliver stor forskel på anden- og tredjegradspolynomiet.\n\n\n\n\n\nSelv om der ikke var stor forskel på anden- og tredjegradspolynomiet på intervallet \\([0;2,5]\\) hvor alle \\(x\\)-værdierne i vores datasæt lå, så er der stor forskel, når vi kommer uden for dette interval. Man skal derfor passe på med at drage konklusioner om \\(x\\)-værdier uden for intervallet, hvor \\(x\\)-værdierne i vores datasæt ligger (det kaldes at ekstrapolere), da disse kan være meget følsomme over for, hvilken grad vi har valgt for vores polynomium. I det hele taget er det en ulempe ved polynomiel regression, at polynomierne har en tendens til at opføre sig vildt i enderne."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#overfitting",
    "href": "materialer/krydsvalidering/krydsvalidering.html#overfitting",
    "title": "Overfitting, modeludvælgelse og krydsvalidering",
    "section": "Overfitting",
    "text": "Overfitting\nDet er altså svært at afgøre med det blotte øje, om anden- eller tredjegradspolynomiet passer bedst til punkterne. Hvordan vælger vi så, hvad der er bedst? Som mål for, hvor tæt polynomiet er på data, kan vi kigge på kvadratsummen af afvigelserne \\(y_i - f(x_i)\\), altså \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2.\\] For andengradspolynomiet får vi en kvadratsum på \\(E=1.14\\), mens vi får \\(E=1.10\\) for tredjegradspolynomiet. Tredjegradspolynomiet kommer altså tættere på data end andengradspolynomiet. Det er på den anden side ikke så overraskende, for ved at sætte \\(a_3=0\\) i et tredjegradspolynomium fås et andengradspolynomium. Andengradspolynomier er altså specialtilfælde af tredjegradspolynomier. Vi vil derfor altid kunne tilpasse data mindst lige så godt med et tredjegradspolynomium som med et andengradspolynomium.\nKan det så altid betale sig at bruge et polynomium af højere grad? Lad os prøve med et syvendegradspolynomium. Vi søger \\[f(x) = a_0 + a_1x + a_2x^2 +a_3x^3 + a_4x^4 + a_5x^5 +a_6x^6 +a_7x^7,\\] der minimerer kvadratsummen \\[E=\\sum_{i=1}^n (y_i - f(x_i))^2 .\\] Det bedste syvendegradspolynomium i vores lille dataeksempel er indtegnet med blå på figur 6 nedenfor:\n\n\n\n\n\n\n\n\nFigur 6: Et andengradspolynomium (rød) og et syvendegradspolynomium (blå) fittet til data.\n\n\n\n\n\nKvadratsummen er på kun \\(E=0.90\\), så umiddelbart virker det til at være en meget bedre model. Der er dog visse problemer. Det ses, at grafen bugter sig meget for at komme så tæt som muligt på datapunkterne. Dels virker det urealistisk, at den faktiske sammenhæng mellem \\(x\\) og \\(y\\) skulle være så kompliceret. Dels opstår der et problem, hvis vi kommer med nye datapunkter. I figur 7 er polynomierne fra før tegnet sammen med 20 nye datapunkter i grøn (som stammer fra den samme underliggende fordeling). Nu beskriver syvendegradspolynomiet pludselig ikke datapunkterne så godt længere.\n\n\n\n\n\n\n\n\nFigur 7: Andengradspolynomiet (rød) og syvendegradspolynomiet (blå) fra figur 6 sammen med 20 nye datapunkter (grøn), som kommer fra den samme underliggende fordeling, som de sorte datapunkter fra figur 6.\n\n\n\n\n\nDet, der sker her, er et eksempel på det fænomen, der kaldes overfitting: syvendegradspolynomiet havde tilpasset sig for godt til lige netop de sorte datapunkter. Når graden bliver for høj, begynder polynomiet at tilpasse sig nogle strukturer i data, som i virkeligheden bare skyldes tilfældigheder. Det fungerer rigtig godt til at beskrive det oprindelige data, men til gengæld er det dårligt til at forudsige nye dataværdier.\nJo højere grad man vælger, at polynomiet skal have, desto bedre kan man tilnærme data. Med \\(n\\) datapunkter (som alle have forskellige \\(x\\)-værdier), kan man faktisk altid finde et polynomium af grad \\(n-1\\), der går igennem alle datapunkterne, men det er klart, at nye datapunkter ikke nødvendigvis følger dette polynomium særlig godt.\n\nModelfleksibilitet\nDet, vi så ovenfor, var, at vi havde forskellige modeller for data (polynomier af forskellig grad). Modellerne havde forskellig fleksibilitet (høj grad gjorde polynomiet meget fleksibelt). Når vi brugte en model med for lav fleksibilitet (lineær regression), kunne vi ikke tilpasse modellen godt nok til data. Når vi valgte en model med for høj fleksibilitet (polynomium af grad syv), opstod der problemer med overfitting, og modellen var ikke god til at beskrive nye data.\nDet tilsvarende problem opstår også i andre sammenhænge, når man har flere forskellige modeller at vælge imellem. Nogle vil være for ufleksible til at beskrive data ordentligt. Andre vil være for fleksible og føre til overfitting. Så hvordan finder vi et godt kompromis? Det handler det følgende om."
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#trænings--og-testdata",
    "href": "materialer/krydsvalidering/krydsvalidering.html#trænings--og-testdata",
    "title": "Overfitting, modeludvælgelse og krydsvalidering",
    "section": "Trænings- og testdata",
    "text": "Trænings- og testdata\nNår vi har et datasæt og prøver at tilpasse en polynomiel regressionsmodel, siger vi, at vi træner modellen. Datasættet, vi bruger til at træne modellen, kaldes træningsdata. Som vi så ovenfor, indebærer det en risiko for overfitting, når vi træner modellen. Hvis vi kommer med et nyt datasæt af samme type, passer modellen ikke nødvendigvis særlig godt.\nFor at vurdere hvilken grad af polynomiet der passer bedst, kan vi se på, hvilken model der er bedst til at forudsige (også kaldet prædiktere) \\(y\\)-værdierne i et nyt datasæt. Det nye datasæt kaldes testdata. Lad os kalde testdatapunkterne for \\((x_i^{test},y^{test}_i)\\), hvor \\(i=1,\\ldots,m\\). Man kan måle, hvor godt modellen forudsiger testdata ved at se på forskellene \\(y_i^{test}-f(x_i^{test})\\) mellem de observerede værdier \\(y_i^{test}\\) og dem, der forudsiges af polynomiet \\(f(x_i^{test})\\). Som samlet mål for, hvor godt modellen forudsiger testdata, beregner vi kvadratsummen af disse forskelle \\[E^{test} = \\sum_{i=1}^{m} \\left(y_i^{test} - {f}\\left(x_i^{test}\\right)\\right)^2.\\] Man kalder \\(E^{test}\\) for tabsfunktionen.\nI praksis har man typisk kun et datasæt til rådighed, og man er derfor nødt til først at dele data i to. Hele processen med at inddele data og først træne modellen og derefter teste den er, som følger:\n\nVælg en polynomiumsgrad \\(p\\).\nDatasættet inddeles i to dele, én del der bruges som træningsdata, og én del der bruges som testdata. Vi vil betegne punkterne i træningsdata med \\((x_i^{træn},y_i^{træn})\\), \\(i=1,\\ldots,n\\), og punkterne i testdata med \\((x_i^{test},y_i^{test})\\), \\(i=1,\\ldots,m\\).\nVi træner modellen på træningsdatasættet og finder det \\(p\\)’te-gradspolynomium \\[f(x)=a_0 + a_1 x + \\dotsm + a_px^p\\] der passer bedst på data. Mindste kvadraters metode benyttes til at bestemme \\(a_0,\\ldots,a_p\\) som de tal, der minimerer \\[E^{træn}(p)=\\sum_{i=1}^{n} (y_i^{træn} - {f}(x_i^{træn}))^2.\\] Det bedste polynomium kalder vi \\(\\hat{f}\\).\nNår vi har valgt funktionen \\(\\hat{f}\\) på baggrund af træningsdataet, tester vi den på testdataet ved at beregne \\[E^{test}(p)=\\sum_{i=1}^{m} (y_i^{test} - \\hat{f}(x_i^{test}))^2.\\] Jo mindre \\(E^{test}(p)\\) er, des bedre passer \\(\\hat{f}\\) på testdata.\n\nDenne procedure kan gentages for forskellige værdier af polynomiumsgraden \\(p\\). Det \\(p\\), der giver den mindste værdi af \\(E^{test}(p)\\) svarer altså til den model, der er bedst til at forudsige værdierne i vores testdata, og vi vælger derfor at bruge dette \\(p\\).\nI vores eksempel ovenfor får vi \\(E^{test}(2)=1.82\\) og \\(E^{test}(7)=2.29\\), når vi bruger de sorte datapunkter som træningsdata og de grønne datapunkter som testdata. Andengradspolynomiet er altså bedre end syvendegradspolynomiet til at forudsige testdata. Bemærk, at i begge tilfælde er \\(E^{test}\\) langt større end \\(E^{træn}\\), fordi modellen kun er tilpasset til træningsdata.\nNår det bedst mulige \\(p\\) er valgt, kan man så træne modellen igen på alt dataet, både test- og træningsdata, for at få et mere præcist bud på det bedste polynomium. Dette er så vores endelige model for data. I vores eksempel finder man, at \\(p=2\\) giver lavest \\(E^{test}\\). Vi slår så de sorte og grønne datapunkter sammen til et datasæt og bruger dem til at finde det bedste andengradspolynomium. Det giver vores endelige model for sammenhængen mellem \\(x\\) og \\(y\\), som bliver \\[\nf(x) = 1.36 + 1.72x - 0.87x^2.\n\\]"
  },
  {
    "objectID": "materialer/krydsvalidering/krydsvalidering.html#krydsvalidering",
    "href": "materialer/krydsvalidering/krydsvalidering.html#krydsvalidering",
    "title": "Overfitting, modeludvælgelse og krydsvalidering",
    "section": "Krydsvalidering",
    "text": "Krydsvalidering\nDer er et problem med tilgangen ovenfor. Når man træner en model, er det altid en fordel at have så meget data som muligt, da man så har mulighed for at træne modellen meget præcist. Problemet er, at hvis man bruger det meste af data som træningsdata, er der ikke meget tilbage til at teste på, og vi risikerer overfitting.\nKrydsvalidering løser dette problem på snedig vis ved at gentage trænings- og testproceduren flere gange. For at lave \\(k\\)-fold krydsvalidering deler man data op i \\(k\\) lige store og tilfældige dele. I første fold træner man modellen på alt data undtagen den første del og bruger første del som testdata. Det given en tabsfunktion \\(E_1(p)\\). Dette gentages så \\(k\\) gange, hvor man i den \\(i\\)’te fold bruger den \\(i\\)’te del af data som testdata og resten som træningsdata og får en tabsfunktion \\(E_i(p)\\). Idéen er illustreret i figur 8.\n\n\n\n\n\n\n\n\nFigur 8: Illustration af idéen ved \\(5\\)-fold krydsvalidering.\n\n\n\n\n\nSom et samlet mål for, hvor god modellen er, bruges summen af tabsfunktionerne fra de \\(k\\) fold \\[E(p)=E_1(p) + E_2(p) + \\dotsm + E_k(p).\\] Man vælger så den model, der giver den mindste værdi af \\(E(p)\\).\nFordelen ved krydsvalidering er, at man i hver fold bruger det meste af data til at træne modellen på. Samtidig bliver hvert datapunkt alt i alt brugt præcis én gang til at teste på. På den måde får man udnyttet data bedre end, hvis man bare laver en enkelt opdeling af data. Typisk vælger man \\(k=5\\) eller \\(k=10\\).\n\nKrydsvalidering i andre sammenhænge\nKrydsvalidering kan bruges i et væld af andre sammenhænge, hvor der skal vælges mellem flere forskellige prædiktionsmodeller.\nHvis det, der skal prædikteres, er en talværdi, kan man gøre som ovenfor. Algoritmen trænes på træningsdataet og bruges derefter til lave prædiktioner \\(\\hat{y}_i^{test}\\), \\(i=1,\\ldots ,m\\), af værdierne i testdatasættet. Disse sammenlignes med de faktiske værdier \\(y_i^{test}\\) ved at se på forskellene \\(y_i^{test} - \\hat{y}_i^{test}\\) og beregne tabsfunktionen \\[E^{test} = \\sum_{i=1}^m(y_i^{test}-\\hat{y}_i^{test})^2.\\] Modellen med lavest \\(E^{test}\\) er bedst til at lave nye prædiktioner.\nHvis der derimod er tale om et klassifikationsproblem, hvor der skal prædikteres en klasse (fx mand/kvinde, rød blok/blå blok, almindelig mail/spam), skal man definere tabsfunktionen lidt anderledes. Som før trænes algoritmen på træningsdataet og derefter bruges den til lave prædiktioner af klasserne \\(\\hat{y}_i^{test}\\), \\(i=1,\\ldots ,m\\), i testdatasættet. Disse sammenholdes med de faktiske klasser, og vi bruger så fejlraten som tabsfuntion. Fejlraten angiver andelen af observationerne i testdataet, der bliver klassificeret forkert, det vil sige\n\\[\nE^{test} =  \\frac{1}{m}\\cdot (\\text{antal fejlklassifikationer i testdata}).\n\\tag{1}\\]\nModellen med lavest \\(E^{test}\\) har færrest fejlklassifikationer og vælges derfor som den bedste.\nEksempler her fra siden, hvor krydsvalidering kan benyttes:\n\nI forløbet Hvem ligner du mest sammenligner man et gråt punkt med alle punkter inden for en radius \\(r\\) for at forudsige farven. Det er ikke oplagt, hvordan man skal vælge denne radius. En mulighed er at komme med nogle gode bud \\(r_1,\\ldots, r_N\\) på radier og så bruge krydsvalidering til at vælge den bedste. Én mulighed er at vælge \\(k=n\\). Så får man det, der kaldes leave-one-out krydsvalidering. I hver fold bliver der så kun et datapunkt i testdata, mens resten bruges som træningsdata. Det ene testdatapunkt farves gråt, og farven prædikteres ud fra de øvrige datapunkter, der ligger inden for den valgte radius. Prædiktionen sammenlignes med punktets rigtige farve. Dette gentages for alle datapunkter og fejlraten (1) beregnes til sidst. Vi vælger så den radius, der har lavest fejlrate.\nI noten om Kunstige neurale nerværk beskrives det, hvordan man træner et kunstigt neuralt netværk. Men når man gør det, skal man på forhånd have besluttet sig for, hvor mange skjulte lag der skal være i netværket, og hvor mange neuroner, der skal være i hvert skjult lag. Jo flere skjulte lag og jo flere neuroner – desto større fleksibilitet. Her vil krydsvalidering være oplagt til at afgøre, hvor fleksibelt netværket skal være samtidig med, at man undgår overfitting."
  },
  {
    "objectID": "materialer/retningsafledede/kontinuitet.html",
    "href": "materialer/retningsafledede/kontinuitet.html",
    "title": "Kontinuitet for funktioner af to variable",
    "section": "",
    "text": "Vi har lige påstået, at en funktion \\(f\\) af to variable siges at være kontinuert i \\((x_0,y_0)\\), hvis følgende gælder\n\\[\n\\lim_{(x,y) \\rightarrow (x_{0},y_{0})}{f\\left( x,y \\right) = f(x_{0},y_{0})}\n\\]\nMen der er faktisk grund til at dvæle lidt ved denne definition, for hvad vil det overhovedet sige, at \\((x,y) \\rightarrow (x_{0},y_{0})\\)? Forestil dig at du har været i byen, og at \\((x_0,y_0)\\) er dit hjem. Så kan man jo gå hjem på rigtig mange måder. Det kan være, at man går langs en ret linje, det kan være, at man går i zig-zag hjem eller noget helt tredje. Ovenstående definition på kontinuitet giver kun mening, hvis \\(f(x,y)\\) nærmer sig \\(f(x_0,y_0)\\) uanset på hvilken måde \\((x,y)\\) nærmer sig \\((x_0,y_0)\\).\nVi skal nu se på et eksempel, hvor en funktion \\(f\\) ikke er kontinuert i \\((0,0)\\), fordi man kan “gå hjem” på nogle måder, så \\(f(x,y)\\) ikke altid nærmer sig \\(f(0,0)\\)! Det kan godt være lidt svært at forestille sig, men her kommer eksemplet."
  },
  {
    "objectID": "materialer/retningsafledede/kontinuitet.html#skiferien",
    "href": "materialer/retningsafledede/kontinuitet.html#skiferien",
    "title": "Kontinuitet for funktioner af to variable",
    "section": "Skiferien",
    "text": "Skiferien\nVi forestiller os, at grafen for funktionen \\(f(x,y)\\) beskriver et landskab. Står vi på en flad mark er \\(f(x,y)\\) bare \\(0\\) for alle værdier af \\((x,y)\\), og det er jo ærlig talt lidt kedeligt. Men nu er din klasse taget på skiferie i et spændende land, hvor skibakkerne kan beskrives som grafen for funktionen \\(f\\) med følgende forskrift:\n\\[\nf(x,y)=\n\\begin{cases}\n\\frac{y \\cdot x^2}{y^2+x^4} \\quad \\textrm{hvis } (x,y) \\neq (0,0) \\\\\n0 \\quad \\textrm{hvis } (x,y) = (0,0)\n\\end{cases}\n\\] Jeres hotel ligger i origo – det vil sige i punktet \\((0,0,0)\\). I kan se skibakken i app’en herunder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI skal nu i gang med at undersøge, om denne funktion er kontinuert i \\((0,0)\\). I gør det ved at stille jer forskellige steder på skibakken og gå af forskellige ruter hjem. De fleste af jer går langs rette linjer (i \\(xy\\)-planen) og rapporterer, at funktionen er kontinuert (og faktisk også differentiabel) i \\((0,0)\\). I kan se nogle af de forskellige ruter herunder:\n\n\n\nNu er der et par elever, der rapporterer, at de kom vandrende ind mod origo på grafen og hele tiden var i højde \\(1/2\\) lige indtil, de faldt i et hul, da de nåede origo. Hvis det er rigtigt betyder det, at funktionen ikke er kontinuert i origo. De elever, der faldt i et hul, er kendt for ikke at gå den lige vej hjem. Denne gang afslører de, at de gik på grafen, mens de i \\(xy\\)-planen fulgte parablen med ligning \\(y=x^2\\). Du kan se disse elevers rute i app’en herunder.\n\n\n\nMen kan alle eleverne mon have ret? Vi prøver at regne på det.\n\n\n\n\n\n\nOpgave 1: Gå langs \\(y=x\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs linjen med ligning \\(y=x\\). Det vil sige bestem \\(f(x,x)\\).\nHvilken værdi går denne snitfunktion imod, når \\(x\\) går mod \\(0\\)?\n\n\n\n\n\n\n\n\n\n\nOpgave 2: Gå langs \\(y=ax\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs linjen med ligning \\(y=ax\\). Det vil sige bestem \\(f(x,ax)\\).\nHvilken værdi går denne snitfunktion imod, når \\(x\\) går mod \\(0\\)?\n\n\n\n\n\n\n\n\n\n\nOpgave 3: Gå langs \\(y\\)-aksen\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs \\(y\\)-aksen.\nHvilken værdi går denne snitfunktion imod, når \\(x\\) går mod \\(0\\)?\n\n\n\n\nI de tre foregående opgaver skulle du gerne komme frem til at \\(f(x,y) \\rightarrow f(0,0)=0\\), når \\((x,y) \\rightarrow (0,0)\\), så længe vi går langs rette linjer. Vi skal nu undersøge, hvad der sker, hvis vi går langs parabler.\n\n\n\n\n\n\nOpgave 4: Gå langs parablen med ligning \\(y=x^2\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs parablen med ligning \\(y=x^2\\). Det vil sige bestem \\(f(x,x^2)\\).\nHvilken værdi går denne snitfunktion imod, når \\(x\\) går mod \\(0\\)?\n\n\n\n\nHvis du har regnet rigtigt i ovenstående opgave, så har du fået, at\n\\[\n\\lim_{(x,y) \\rightarrow (0,0)}{f\\left( x,y \\right)} = 1/2\n\\]\nnår \\((x,y) \\rightarrow (0,0)\\) langs parablen med ligning \\(y=x^2\\). Da \\(f(0,0)=0\\) har vi altså fundet en måde at nærme os \\((0,0)\\) så\n\\[\n\\lim_{(x,y) \\rightarrow (0,0)}{f\\left( x,y \\right) \\neq f(0,0)}\n\\]\nog derfor er \\(f\\) ikke kontinuert i \\((0,0)\\), selvom det i første omgang så sådan ud (da vi gik langs rette linjer)!\nFor sjov skyld kan vi jo prøve at undersøge, om det gælder langs alle parabler.\n\n\n\n\n\n\nOpgave 5: Gå langs parablen med ligning \\(y=ax^2\\)\n\n\n\n\n\n\nFind forskriften for snitfunktionen langs parablen med ligning \\(y=ax^2\\), hvor \\(a \\neq 0\\). Det vil sige bestem \\(f(x,ax^2)\\).\nHvilken værdi går denne snitfunktion imod, når \\(x\\) går mod \\(0\\)?\n\n\n\n\nHvis du har regnes rigtig i ovenstående, har du fået, at \\[\n\\lim_{(x,y) \\rightarrow (0,0)}{f\\left( x,y \\right)} = \\frac{a}{a^2+1} \\neq 0\n\\] når \\(a \\neq 0\\). Igen har vi altså set, at\\(f\\) ikke kontinuert i \\((0,0)\\)."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html",
    "title": "Afstande mellem punkter i planen",
    "section": "",
    "text": "Du har ikke nødvendigvis tænkt over det før, men hvordan måler man egentlig afstanden mellem to punkter i planen? “Ja, man finder afstanden mellem dem”, vil du måske sige, men det er jo ikke rigtigt et brugbart svar på spørgsmålet. Den afstand, du tænker på, er formentlig længden af det linjestykke, som forbinder de to punkter, men der findes faktisk mange andre måder at definere afstanden på. Det vil vi give et par eksempler på her.\nFor at blive lidt mere præcis forestiller vi os, at vi har to punkter i planen, som vi kalder for \\(P(x_1,y_1)\\) og \\(Q(x_2,y_2)\\)."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-euklidisk_afstand",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-euklidisk_afstand",
    "title": "Afstande mellem punkter i planen",
    "section": "Euklidisk afstand",
    "text": "Euklidisk afstand\nDen euklidisk afstand mellem \\(P\\) og \\(Q\\) er \\[\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\\] Det er det, vi kender mest – og som formentlig er den afstand, du lige har tænkt på. Formlen ovenfor fremkommer ved at bruge Pythagoras.\nI app’en herunder er den euklidiske afstand illustreret. Du kan flytte rundt på punkterne og se, hvordan afstanden ændrer sig."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-manhattan_afstand",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#sec-manhattan_afstand",
    "title": "Afstande mellem punkter i planen",
    "section": "Manhattanafstanden",
    "text": "Manhattanafstanden\nManhattanafstanden er den afstand, man får, når man er tvunget til at bevæge sig langs akserne, som vi kender det fra vejene i mange amerikanske byer, herunder på Manhattan. Den kaldes også taxi-afstanden. Formlen for at bestemme Manhattanafstanden er: \\[|x_2-x_1|+|y_2-y_1|\\] I app’en herunder er Manhattanafstanden illustreret. Du kan flytte rundt på punkterne og se, hvordan afstanden ændrer sig."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#max-afstanden",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#max-afstanden",
    "title": "Afstande mellem punkter i planen",
    "section": "Max-afstanden",
    "text": "Max-afstanden\nMax-afstanden er maksimum mellem den vandret og lodrette afstand mellem \\(P\\) og \\(Q\\). Det vil sige, maksimum af \\(|x_2-x_1|\\) og \\(|y_2-y_1|\\).\nDen kaldes også skak-konge afstanden. Kongen i skak kan gå diagonalt eller langs de to akser. Et diagonalt move fra \\((a,b)\\) til \\((a+k,b+k)\\) tænkes at have længde \\(k\\) – som i skak. Skal man fra eksempelvis \\(A(1,4)\\) til \\(B(3,7)\\) kan skakkongen gå fra \\(A(1,4)\\) til \\(C(3,6)\\) – det stykke har længde \\(2\\) og derefter fra \\(C(3,6)\\) til \\(B(3,7)\\) langs \\(y\\)-aksen - et stykke på længde \\(1\\). Samlet afstand er \\(3\\), maksimum af \\(|3-1|\\) og \\(|7-4|\\). Idéen er illustreret i figur 1.\n\n\n\n\n\n\nFigur 1: Kongen i skak skal fra \\(A\\) til \\(B\\) via \\(C.\\) Bemærk, at der også er andre veje fra \\(A\\) til \\(B\\) med afstand \\(3\\). For eksempel kan kongen gå fra \\(A(1,4)\\) til \\((1,5)\\) (det giver en afstand på \\(1\\)) og dernæst lave et diagonalt move fra \\((1,5)\\) til \\(B(3,7)\\) med en afstand på \\(2\\). Den samlede afstand bliver igen \\(1+2=3\\). Der er altså flere korteste veje fra \\(A\\) til \\(B\\).\n\n\n\nI app’en herunder er max-afstanden illustreret. Du kan flytte rundt på punkterne og se, hvordan afstanden ændrer sig."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#posthusafstanden",
    "href": "materialer/afstande/AfstandeMellemPunkteriPlanen.html#posthusafstanden",
    "title": "Afstande mellem punkter i planen",
    "section": "Posthusafstanden",
    "text": "Posthusafstanden\nPosthusafstanden1 mellem \\(P\\) og \\(Q\\) finder man, ved at tænke på, at der ligger et posthus i origo \\(O(0,0)\\), og vi skal sende et brev fra \\(P\\) til \\(Q\\). Det bliver transporteret fra \\(P\\) til posthuset først og derefter fra posthuset til \\(Q\\). Hvis man anvender Pythagoras to gange, kan man se, at formlen for denne afstand er: \\[\\sqrt{x_1^2+y_1^2}+\\sqrt{x_2^2+y_2^2}\\] I app’en herunder er posthusafstanden illustreret. Du kan flytte rundt på punkterne og se, hvordan afstanden ændrer sig.\n1 Den hedder også British Rail afstanden eller, hvis man er fransk, SNCF (Société Nationale des Chemins de fer Français) -afstanden. Man tænker sig, at man altid skal rejse via London (eller Paris) for at komme med tog fra et sted til et andet."
  },
  {
    "objectID": "materialer/afstande/AfstandeMellemDNA.html",
    "href": "materialer/afstande/AfstandeMellemDNA.html",
    "title": "Afstand mellem DNA- og RNA-strenge",
    "section": "",
    "text": "RNA er strenge med bogstaverne U (uracil), G (guanin), C (cytosin), A, (adenin). DNA har ikke U, men i stedet T (thymin) og DNA er dobbelt. Bogstaverne U, G, C, A og T kaldes for nukleotider.\nVi ser her på afstande mellem DNA (eller RNA), som bygger på antallet af mutationer for at nå fra den ene til den anden og desuden, hvor hyppige disse mutationer er - hvis man ved, en mutation sker ofte, er afstanden mellem en streng uden mutationen og en med mutationen ikke så lang, som hvis mutationen er meget sjælden. Udover regler for, hvilke ændringer, man tillader, giver man derfor en omkostning ved ændringen – afstanden er ikke bare antal ændringer, men summen af, hvor \"dyre\" disse ændringer er.\nAfstand mellem DNA bruges til at analysere slægtskab og hvilke dyr, herunder mennesket, der nedstammer fra hvilke andre dyr – det kaldes for fylogenetiske træer – se mere her.\nI den sammenhæng kalder man skift mellem A og G eller mellem C og T for transitioner1. De fire andre mulige skift mellem A og C, mellem A og T, mellem G og T, mellem G og C, kaldes for transversioner. Transitioner er hyppigere mutationer end transversioner, så afstanden mellem \\(GATTACA\\) og \\(GATTACG\\) er mindre end afstanden mellem \\(GATTACA\\) og \\(GATTACC\\). Den slags udskiftning af et bogstav (et basepar) kaldes en punktmutation.\n1 A og G er puriner, mens C og T er pyrimediner. Transition bytter en purin med en purin eller en pyrimedin med en pyrimedin.Indel mutationer er indsætning (\"In\") eller fjernelse (\"Del\" for delete\") af et eller flere basepar. Det er mindre hyppigt og svarer til længere afstand. I kilden ovenfor bruges følgende omkostninger og altså afstande mellem DNA-strenge. Bemærk, at det er et valg - der er mange andre muligheder:\n\nTransition: 1\nTransversion: 2\nGap åbning: 9 (indsæt eller fjern præcis en base - altså et bogstav)\nGap forlængelse: 4 (indsæt eller fjern en base på samme sted, som er åbnet)\n\nMan kan samle de to sidste og sige, at det koster \\(5+4L\\) at indsætte eller fjerne en delstreng med \\(L\\) bogstaver midt i et ord (overvej, at I forstår, at det er samme regel).\nVi tilføjer forlængelse/forkortelse: Det koster \\(4L\\) at indsætte eller fjerne \\(L\\) bogstaver i start eller slut af et ord. Alt i alt:\n\nTransition: 1\nTransversion: 2\nIndsæt eller fjern delord med \\(L\\) bogstaver midt i et ord: \\(5+4L\\)\nForlæng/forkort: Indsæt eller fjern \\(L\\) bogstaver i start eller slut af et ord: \\(4L\\)\n\nAfstand mellem to strenge er så den kortest mulige måde, man kan komme fra den ene til den anden med de tilladte moves vægtet som her.\n\n\n\n\n\n\n\nI det følgende bruger vi meget korte strenge. Det er naturligvis ikke realistisk. Vi vil finde afstanden fra \\(AGT\\) til \\(ATG\\). Der er mange muligheder for, hvordan man kan komme fra \\(AGT\\) til \\(ATG\\), altså hvordan mutationerne kunne have set ud. For eksempel kunne det være:\n\\[AGT \\to ATGT \\to ATG\\] Det vil sige, indsæt \\(T\\) mellem \\(A\\) og \\(G\\) og fjern så det sidste \\(T\\). Det koster \\(9+4 =13\\). Altså er længden af denne vej \\(13\\). En anden mulighed er\n\\[AGT\\to ATT \\to ATG\\]\nHer er der to punktmutationer og begge er transversioner (fra \\(T\\) til \\(G\\) eller omvendt), så det koster \\(2+2=4\\). Det er den korteste vej, så afstanden er \\(4\\). At denne vej faktisk er den kortest, kræver mere eftertanke.\nHavde vi brugt samme omkostning/vægt for alle tilladte ændringer, ville begge de to veje have samme længde.\nHvad med fra \\(AGT\\) til \\(TGA\\)? Jo, det er faktisk nemmere. Det er i virkeligheden samme DNA-sekvens – man har bare læst den fra den anden ende...\nMed lange strenge, som er ens på lange stykker, finder man afstande ved først at \"aligne\". Det vil sige, at man anbringer strengene, så de passer sammen på flest mulige pladser. Og derefter udregner man afstande, men det er stadig ikke nemt – der skal algoritmer til. Her er et eksempel.\nStreng 1: \\(TCGTAGG\\)\nStreng 2: \\(TCTGTATCGA\\)\nFørste alignment: \\[\\begin{matrix}T&C&G&-&-&-&T&A&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] Det koster:\n\nIndsættelse af \\(GTA\\): \\(5+4\\cdot 3=17\\)\nTo transversioner \\(G\\leftrightarrow T\\) og \\(A\\leftrightarrow C\\) samt en transition \\(G\\leftrightarrow A\\).\nI alt \\(17+4+1=22\\).\n\nHvis man i stedet vælger denne alignment \\[\\begin{matrix}T&C&-&-&-&G&T&A&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] er transversionen mellem \\(G\\) og \\(T\\) erstattet med en transition \\(G\\leftrightarrow A\\) og omkostningen falder med \\(1\\) til \\(21.\\)\nMan indser ret let, at prisen for at klippe gør, at man ikke vil klippe to gange og bruge \\[\\begin{matrix}T&C&-&G&-&-&T&A&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] hvor man kun sparer en enkelt transition.\nMen hvad med: \\[\\begin{matrix}T&C&-&G&T&A&-&-&G&G\\\\T&C&T&G&T&A&T&C&G&A\\end{matrix}\\] Her er omkostningen \\(9\\) for det første gap og \\(13\\) for det andet. Og der er en transition i sidste plads \\(G\\leftrightarrow A,\\) så omkostningen er \\(23\\), men det er ikke helt så klart, at det er for dyrt at klippe to gange. I kan nok finde på eksempler, hvor det kan svare sig at klippe flere steder."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html",
    "href": "materialer/afstande/feature_scaling.html",
    "title": "Feature-skalering",
    "section": "",
    "text": "Det er ikke altid helt klart, hvordan man skal bestemme afstanden mellem to datapunkter, hvis koordinaterne i hvert datapunkter beskriver vidt forskellige ting. Vi vil her se nærmere på, hvilke problemer, der kan opstå, hvis man ikke tænker sig om – og hvad man kan gøre for at løse dem.\nLad os forestille os, at data består af vægt og højde for nogle personer, så hvert datapunkt er på formen \\[(v,h)\\] hvor \\(v\\) er den pågældende persons vægt og \\(h\\) er højden. Her er det faktisk ikke klart, hvad afstanden mellem to punkter \\((v_1,h_1)\\) og \\((v_2,h_2)\\) skal være. Altså, hvornår to punkter ligger tæt på hinanden.\nEt første bud kunne være at bestemme den euklidiske afstand mellem de to punkter – det der bare svarer til at bruge Pythagoras. Gør vi det får vi følgende afstandsmål mellem punkterne \\((v_1,h_1)\\) og \\((v_2,h_2)\\):\n\\[\\sqrt{(v_2-v_1)^2+(h_2-h_1)^2}\\]\nVi prøver at regne lidt på det, og forestiller os, at tre personer er givet som datapunkter i nedenstående tabel.\n\n\n\nPerson\n(vægt, højde)\n\n\n\n\nA\n\\((70 \\ kg, 165 \\ cm)\\)\n\n\nB\n\\((90 \\ kg, 180 \\ cm)\\)\n\n\nC\n\\((80 \\ kg, 190 \\ cm)\\)\n\n\n\nBruger vi Pythagoras på tallene, der står her, er:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+15^2}=25 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+25^2}\\simeq 27 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+10^2}\\simeq 14\n\\end{align*}\\]\nMed dette afstandsmål er der altså længst fra \\(A\\) til \\(C\\).\nSkifter vi nu enhed og udtrykker højden i meter får vi følgende datapunkter:\n\n\n\nPerson\n(vægt, højde)\n\n\n\n\nA\n\\((70 \\ kg, 1.65 \\ m)\\)\n\n\nB\n\\((90 \\ kg, 1.80 \\ m)\\)\n\n\nC\n\\((80 \\ kg, 1.90 \\ m)\\)\n\n\n\nNu er\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+0.15^2} \\simeq 20 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+0.25^2} \\simeq 10 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+0.10^2} \\simeq 10\n\\end{align*}\\]\nDer er nu længst fra \\(A\\) til \\(B\\).\nDet er ikke ret smart. Skal man finde de datapunkter, som ligger tættest på hinanden, er svaret tilsyneladende som vinden blæser og afhængig af hvilken enhed, vi har valgt at måle i.\nMen selv hvis begge variable er i samme enhed, kan Pythagoras brugt med hovedet under armen være uheldigt, som nedenstående eksempel illustrerer.\nVi forestiller os, at vi har data for, hvor meget familierne \\(A\\), \\(B\\) og \\(C\\) bruger på bolig og på mælk om måneden. Begge variable kan være i kroner. I tabellen ses et eksempel:\n\n\n\nFamilie\n(bolig, mælk)\n\n\n\n\nA\n\\((7500 \\ kr, 200 \\ kr)\\)\n\n\nB\n\\((7500 \\ kr, 1700 \\ kr)\\)\n\n\nC\n\\((6000 \\ kr, 200 \\ kr)\\)\n\n\n\nDet vil altså for eksempel sige, at familie \\(A\\) bruger \\(7500\\) kr på bolig og \\(200\\) kr på mælk. Her er afstanden udregnet med Pythagoras:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{0^2+1500^2} = 1500 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{1500^2+0^2} = 1500 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{1500^2+1500^2} \\simeq 2121\n\\end{align*}\\]\nAltså er der samme afstand fra \\(A\\) til \\(B\\) som fra \\(A\\) til \\(C\\), men vi vil nok mene, at \\(B\\) afviger mere fra \\(A\\) end \\(C\\) gør, fordi mælkeforbruget i familie \\(B\\) er usædvanligt.\nHar vi data for mange familier, kan vi kvantificere idéen om, hvad der er usædvanligt og bruge det til at lave en mere passende afstand."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#afstand-med-hovedet-under-armen",
    "href": "materialer/afstande/feature_scaling.html#afstand-med-hovedet-under-armen",
    "title": "Feature-skalering",
    "section": "",
    "text": "Det er ikke altid helt klart, hvordan man skal bestemme afstanden mellem to datapunkter, hvis koordinaterne i hvert datapunkter beskriver vidt forskellige ting. Vi vil her se nærmere på, hvilke problemer, der kan opstå, hvis man ikke tænker sig om – og hvad man kan gøre for at løse dem.\nLad os forestille os, at data består af vægt og højde for nogle personer, så hvert datapunkt er på formen \\[(v,h)\\] hvor \\(v\\) er den pågældende persons vægt og \\(h\\) er højden. Her er det faktisk ikke klart, hvad afstanden mellem to punkter \\((v_1,h_1)\\) og \\((v_2,h_2)\\) skal være. Altså, hvornår to punkter ligger tæt på hinanden.\nEt første bud kunne være at bestemme den euklidiske afstand mellem de to punkter – det der bare svarer til at bruge Pythagoras. Gør vi det får vi følgende afstandsmål mellem punkterne \\((v_1,h_1)\\) og \\((v_2,h_2)\\):\n\\[\\sqrt{(v_2-v_1)^2+(h_2-h_1)^2}\\]\nVi prøver at regne lidt på det, og forestiller os, at tre personer er givet som datapunkter i nedenstående tabel.\n\n\n\nPerson\n(vægt, højde)\n\n\n\n\nA\n\\((70 \\ kg, 165 \\ cm)\\)\n\n\nB\n\\((90 \\ kg, 180 \\ cm)\\)\n\n\nC\n\\((80 \\ kg, 190 \\ cm)\\)\n\n\n\nBruger vi Pythagoras på tallene, der står her, er:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+15^2}=25 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+25^2}\\simeq 27 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+10^2}\\simeq 14\n\\end{align*}\\]\nMed dette afstandsmål er der altså længst fra \\(A\\) til \\(C\\).\nSkifter vi nu enhed og udtrykker højden i meter får vi følgende datapunkter:\n\n\n\nPerson\n(vægt, højde)\n\n\n\n\nA\n\\((70 \\ kg, 1.65 \\ m)\\)\n\n\nB\n\\((90 \\ kg, 1.80 \\ m)\\)\n\n\nC\n\\((80 \\ kg, 1.90 \\ m)\\)\n\n\n\nNu er\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{20^2+0.15^2} \\simeq 20 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{10^2+0.25^2} \\simeq 10 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{10^2+0.10^2} \\simeq 10\n\\end{align*}\\]\nDer er nu længst fra \\(A\\) til \\(B\\).\nDet er ikke ret smart. Skal man finde de datapunkter, som ligger tættest på hinanden, er svaret tilsyneladende som vinden blæser og afhængig af hvilken enhed, vi har valgt at måle i.\nMen selv hvis begge variable er i samme enhed, kan Pythagoras brugt med hovedet under armen være uheldigt, som nedenstående eksempel illustrerer.\nVi forestiller os, at vi har data for, hvor meget familierne \\(A\\), \\(B\\) og \\(C\\) bruger på bolig og på mælk om måneden. Begge variable kan være i kroner. I tabellen ses et eksempel:\n\n\n\nFamilie\n(bolig, mælk)\n\n\n\n\nA\n\\((7500 \\ kr, 200 \\ kr)\\)\n\n\nB\n\\((7500 \\ kr, 1700 \\ kr)\\)\n\n\nC\n\\((6000 \\ kr, 200 \\ kr)\\)\n\n\n\nDet vil altså for eksempel sige, at familie \\(A\\) bruger \\(7500\\) kr på bolig og \\(200\\) kr på mælk. Her er afstanden udregnet med Pythagoras:\n\\[\\begin{align*}\n&\\text{Afstanden mellem A og B: } \\sqrt{0^2+1500^2} = 1500 \\\\\n&\\text{Afstanden mellem A og C: } \\sqrt{1500^2+0^2} = 1500 \\\\\n&\\text{Afstanden mellem B og C: } \\sqrt{1500^2+1500^2} \\simeq 2121\n\\end{align*}\\]\nAltså er der samme afstand fra \\(A\\) til \\(B\\) som fra \\(A\\) til \\(C\\), men vi vil nok mene, at \\(B\\) afviger mere fra \\(A\\) end \\(C\\) gør, fordi mælkeforbruget i familie \\(B\\) er usædvanligt.\nHar vi data for mange familier, kan vi kvantificere idéen om, hvad der er usædvanligt og bruge det til at lave en mere passende afstand."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#første-naive-tilgang-min-max-skalering",
    "href": "materialer/afstande/feature_scaling.html#første-naive-tilgang-min-max-skalering",
    "title": "Feature-skalering",
    "section": "Første naive tilgang: Min-Max skalering",
    "text": "Første naive tilgang: Min-Max skalering\nEksemplet ovenfor illustrerer, at det nok vil være smart at prøve at inddrage i hvilket interval \\(x\\)- og \\(y\\)-værdierne varierer. For eksempel er et udsving på \\(500\\) kr i boligudgifter ikke lige så voldsomt, som et udsving på \\(500\\) kr i mælkeudgifter. Problemet er, at den absolutte forskel på henholdsvis \\(x\\)- og \\(y\\)-værdierne ikke er sammenlignelige her.\nLad os sige, at vi betragter datapunkter \\((x_i,y_i)\\) i planen, hvor \\(x\\)-værdierne ligger mellem \\(a\\) og \\(b\\), mens \\(y\\)-værdierne ligger mellem \\(c\\) og \\(d\\). Situationen er illustreret i figur 1.\n\n\n\n\n\n\nFigur 1: Datapunkter i planen, hvor \\(x\\)-værdierne ligger mellem \\(a\\) og \\(b\\), mens \\(y\\)-værdierne ligger mellem \\(c\\) og \\(d\\).\n\n\n\nIdéen er nu, at vi skalerer, så afstandene langs \\(x\\)-aksen får samme vægt som afstandene langs \\(y\\)-aksen.\nDet vil sige, at vi definerer afstanden fra \\((x_1,y_1)\\) til \\((x_2,y_2)\\) som\n\\[\\sqrt{\\left ( \\frac{x_2-x_1}{b-a} \\right )^2+ \\left ( \\frac{y_2-y_1}{d-c} \\right )^2} \\tag{1}\\]\nDet får den betydning, at hvis \\(x\\)-værdierne f.eks. kan varierer i et langt bredere interval end \\(y\\)-værdierne (dvs. at \\(b-a&gt;d-c\\)), så bliver forskellen på \\(x\\)-værdierne i ovenstående afstandsmål skaleret mere ned (fordi vi kommer til at dividere med et større tal).\nEn anden måde at forstå dette afstandsmål på, er ved at erstatte hvert punkt \\((x_i,y_i)\\) med et nyt skaleret punkt:\n\\[(x_i,y_i)_{Norm}=\\left(\\frac{x_i-a}{b-a}, \\frac{y_i-c}{d-c}\\right) \\tag{2}\\]\nResultatet af at lave denne skalering af punkterne fra figur 1 ses i figur 2.\n\n\n\n\n\n\nFigur 2: Datapunkterne fra figur 1, men hvor alle punkter er blevet min-max skaleret ved at bruge formlen i (2).\n\n\n\nBemærk, at datapunkterne nu er skaleret på en sådan måde, at alle \\(x\\)- og \\(y\\)-værdier ligger mellem \\(0\\) og \\(1\\). Derfor giver det mening, at bruge Pythagoras på disse skalerede punkter – og gør vi det, får vi\n\\[\\sqrt{\\left(\\frac{x_2-a}{b-a}-\\frac{x_1-a}{b-a}\\right)^2 +\\left(\\frac{y_2-c}{d-c}-\\frac{y_1-c}{d-c}\\right)^2}=\\sqrt{\\left(\\frac{x_2-x_1}{b-a}\\right)^2+\\left(\\frac{y_2-y_1}{d-c}\\right)^2}\\]\nBemærk, at det præcis er afstandsmålet i (1), som vi startede ud med."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#mindre-naivt-mere-bøvlet-feature-skaling",
    "href": "materialer/afstande/feature_scaling.html#mindre-naivt-mere-bøvlet-feature-skaling",
    "title": "Feature-skalering",
    "section": "Mindre naivt, mere bøvlet: Feature-skaling",
    "text": "Mindre naivt, mere bøvlet: Feature-skaling\nDen skalering vi har præsenteret i (2) benytter ikke som sådan information om data, men kun om den mindste og største værdi, som henholdsvis \\(x\\)- og \\(y\\)-værdierne ligger imellem (nemlig \\(a\\) og \\(b\\) for \\(x\\)-værdierne og \\(c\\) og \\(d\\) for \\(y\\)-værdierne).\nEt alternativ til dette er at bruge alle data til at bestemme skaleringen (og ikke kun den største og den mindste værdi). Dette kaldes for feature-skaling, når vi arbejder med perceptroner eller neurale netværk.\nHvis de data, der skal læres fra – det vil sige træningsdata – er \\[(x_1,y_1), (x_2,y_2),\\ldots, (x_n,y_n)\\] så skalerer vi langs førsteaksen ved, at\n\nudregne et estimat for middelværdien af \\(x\\): \\[\\bar{x}=\\frac{x_1 + x_2 + \\cdots + x_n}{n}=\\frac{\\Sigma_{i=1}^nx_i}{n} \\tag{3}\\]\nog et estimat for denne variabels spredning: \\[s_x=\\sqrt{\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}{n-1}} \\tag{4}\\]\n\nFeature-skaling af \\(x_i\\) er da \\[\\hat{x}_i=\\frac{x_i-\\bar{x}}{s_x} \\tag{5}\\]\nTilsvarende estimeres middelværdi og spredning for \\(y\\) og feature-skaling udregnes: \\[\\hat{y}_i= \\frac{y_i-\\bar{y}}{s_y} \\tag{6}\\]\nResultatet af at lave denne feature skalering af punkterne fra figur 1 ses i figur 3.\n\n\n\n\n\n\nFigur 3: Datapunkterne fra figur 1, men hvor alle punkter er blevet feature skaleret ved at bruge formlerne i (5) og (6).\n\n\n\nBemærk, hvordan de feature skalerede datapunkter har \\(x\\)- og \\(y\\)-værdier, som alle1 ligger mellem \\(-2\\) og \\(2\\).\n1 I virkelighedens verden kan der godt være værdier, som er mindre end \\(-2\\) eller større end \\(2\\), men som oftest vil det være sådan, at omkring \\(95 \\%\\) af værdierne vil ligge mellem \\(-2\\) og \\(2\\) efter feature skalering.Da alle \\(x\\)- og \\(y\\)-værdier nu er på samme skala, giver det igen mening at beregne den euklidiske afstand mellem disse nye punkter. Betragter vi de skalerede punkter \\((\\hat{x}_1,\\hat{y}_1)\\) og \\((\\hat{x}_2,\\hat{y}_2)\\) så bliver den euklidiske afstand mellem dem\n\\[\\begin{align*}\n\\sqrt{(\\hat{x}_2-\\hat{x}_1)^2+(\\hat{y}_2-\\hat{y}_1)^2} &= \\sqrt{\\left(\\frac{x_2-\\bar{x}}{s_x}-\\frac{x_1-\\bar{x}}{s_x}\\right)^2 +\\left(\\frac{y_2-\\bar{y}}{s_y}-\\frac{y_1-\\bar{y}}{s_y}\\right)^2}\\\\\n& =\\sqrt{\\left(\\frac{x_2-x_1}{s_x}\\right)^2+\\left(\\frac{y_2-y_1}{s_y}\\right)^2}\n\\end{align*}\\]\nHvis vi sammenligner med den naive tilgang i (1), er den ikke helt skæv. Der skal bare skaleres med \\(s_x\\) i stedet for \\(b-a\\) og med \\(s_y\\) i stedet for \\(c-d\\).\nBemærk, at den feature skalering, som foretages i (5) og (6), svarer til at standardisere en normalfordelt stokastisk variabel \\(X \\sim N(\\mu, \\sigma)\\):\n\\[ Z = \\frac{X-\\mu}{\\sigma}\\] Derfor vil det også være sådan, at hvis de oprindelige data er normalfordelte, så vil de nye feature skalerede data være standard normalfordelte (dvs. normalfordelte med middelværdi \\(0\\) og spredning \\(1\\)). Heraf følger også, at cirka \\(95 \\%\\) af de feature skalerede data vil ligge mellem \\(-2\\) og \\(2\\), som bemærket ovenfor."
  },
  {
    "objectID": "materialer/afstande/feature_scaling.html#eksempel-min-max-og-feature-skalering",
    "href": "materialer/afstande/feature_scaling.html#eksempel-min-max-og-feature-skalering",
    "title": "Feature-skalering",
    "section": "Eksempel: Min-max og feature skalering",
    "text": "Eksempel: Min-max og feature skalering\nVi vil prøve at se på et udvidet eksempel om udgifter til bolig og mælk. Se nedenstående tabel:\n\n\n\n\n\n\nFamilie\n(bolig, mælk)\n\n\n\n\nA\n\\((7500 \\ kr, 200 \\ kr)\\)\n\n\nB\n\\((7500 \\ kr, 1700 \\ kr)\\)\n\n\nC\n\\((6000 \\ kr, 200 \\ kr)\\)\n\n\nD\n\\((5200 \\ kr, 300 \\ kr)\\)\n\n\nE\n\\((8100 \\ kr, 250 \\ kr)\\)\n\n\nF\n\\((6200 \\ kr, 350  \\ kr)\\)\n\n\nG\n\\((7700 \\ kr, 400 \\ kr)\\)\n\n\nH\n\\((5800 \\ kr, 350 \\ kr)\\)\n\n\nI\n\\((7200 \\ kr, 250 \\ kr)\\)\n\n\nJ\n\\((6800 \\ kr, 400  \\ kr)\\)\n\n\n\n\n\nTabel 1: Udvidet eksempel om udgifter til bolig og mælk.\n\n\n\nDatapunkterne fra tabellen ses indtegnet i figur 4. De tre første familier \\(A\\), \\(B\\) og \\(C\\) fra det tidligere eksempel er markeret. Bemærk, at vi tidligere har beregnet, at afstanden mellem \\(A\\) og \\(B\\) er den samme som afstanden mellem \\(A\\) og \\(C\\), hvilket også fremgår af figur 4.\n\n\n\n\n\n\nFigur 4: Datapunkterne fra eksemplet i tabel 1 omkring udgifter til bolig og mælk.\n\n\n\nVi vil nu lave både min-max skalering samt feature skalering af punkterne i tabel 1. For at lave min-max skalering får vi brug for mindste- og størsteværdi for både \\(x\\)- (bolig) og \\(y\\)-værdierne (mælk). De er: \\[\\begin{align}\na&=5200 \\quad  &\\text{og} \\quad \\quad &b=8100 \\\\\nc&=200 \\quad  &\\text{og} \\quad \\quad &d=1700 \\\\\n\\end{align}\\] Bruges disse værdier samt formlerne i (2) fås punkterne som ses i figur 5.\n\n\n\n\n\n\nFigur 5: Datapunkterne fra eksemplet omkring udgifter til bolig og mælk efter min-max skalering.\n\n\n\nLæg mærke til hvordan afstanden mellem \\(A\\) og \\(C\\) med denne skalering er blevet mindre end afstanden mellem \\(A\\) og \\(B\\), som ønsket.\nVi vil nu prøve at lave en feature skalering af punkterne i tabel 1. Vi får derfor brug for et estimat for middelværdien af henholdvis \\(x\\) og \\(y\\). De kan beregnes ved hjælp af (3) til:\n\\[\\begin{align}\n\\bar{x} &= 6800 \\\\\n\\bar{y} &= 440\n\\end{align}\\]\nBruges (4) fås et estimat for spredningen for henholdsvis \\(x\\) og \\(y\\):\n\\[\\begin{align}\n\\bar{s_x} &= 954.52 \\\\\n\\bar{s_y} &= 448.95\n\\end{align}\\]\nAnvendes formlerne i (5) og (6) til feature skalering af punkterne i tabel 1 fås punkterne, som ses i figur 6.\n\n\n\n\n\n\nFigur 6: Datapunkterne fra eksemplet omkring udgifter til bolig og mælk efter feature skalering.\n\n\n\nLæg her mærke til at afstanden mellem \\(A\\) og \\(B\\) nu er blevet endnu større end afstanden mellem \\(A\\) og \\(C\\). Det ses også, at \\(y\\)-værdien for det skalerede punkt for familie \\(B\\) er cirka \\(2.8\\). Sammenlignes dette med standard normalfordelingen, kan vi se, at der er tale om en forholdsvis ekstrem værdi2. Det vil sige, at vi ud fra de skalerede værdier kan se, at familien \\(B\\)’s mælkeforbrug på \\(1700\\) kroner rent faktisk er usædvanligt sammenlignet med de andre families mælkeforbrug.\n\n\n2 Husk på at for en standard normalfordelt stokastisk variabel vil omkring \\(95 \\%\\) af værdierne ligge mellem \\(-2\\) og \\(2\\)."
  },
  {
    "objectID": "materialer/afstande/index_afstande.html",
    "href": "materialer/afstande/index_afstande.html",
    "title": "Afstande og feature-skalering",
    "section": "",
    "text": "Det er ikke altid helt klart, hvordan man skal bestemme afstanden mellem to datapunkter, hvis koordinaterne i hvert datapunkter beskriver vidt forskellige ting. Det er faktisk ikke en gang entydigt, hvad man overhovedet skal forstå ved en afstand – eller det som man i matematik vil kalde for en metrik. Her på siden behandler vi nogle af disse problemstillinger. Vi vil se nærmere på, hvilke problemer, der kan opstå, hvis man ikke tænker sig om – og hvad man kan gøre for at løse dem.\nLæs mere i noterne herunder.\n\n\n\n\n\n\n\n\n\n\nFeature-skalering\n\n\nHvis de data, man arbejder med, måler vidt forskellige ting – måske endda på vidt forskellige skalaer – så vil man som oftest have brug for at “justere” data, så de er på…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstande mellem punkter i planen\n\n\nAfstande mellem punkter i planen er ikke entydigt – faktisk kan man måle afstanden mellem to punkter i planen på mange forskellige måder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstande mellem ord\n\n\nHvordan måler man mon afstanden mellem ord – altså sådan helt generelt?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfstand mellem DNA- og RNA-strenge\n\n\nHvordan måler man afstanden mellem DNA- og RNA-strenge?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition af en metrik – det abstrakte afstandsbegreb\n\n\nHvad skal der til for, at man kan tale om, at et afstandsmål rent faktisk måler noget, som giver mening?\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html",
    "href": "materialer/neurale_net/neurale_net.html",
    "title": "Kunstige neurale netværk",
    "section": "",
    "text": "Forestil dig at du gerne vil have en funktion \\(f\\), som tager et billede som input og som output fortæller dig, om der er en hund på billedet eller ej. Det kan illustreres sådan her:\n\n\n\n\n\n\nFigur 1: Funktion, der tager et billede som input, og som returnerer \"ja\" eller \"nej\" som output.\n\n\n\nOkay, det er måske ikke så tit, at man har brug for en funktion, som kan detektere, om der er en hund på et billede, men hvad så hvis funktionen i stedet kan afgøre, om der er en kræftknude på et røntgenbillede? Eller hvis den kan genkende håndskrevet tekst? Sidstnævnte bliver f.eks. flittigt brugt til sortering af breve. Vi ønsker os i virkeligheden at være i stand til at programmere en funktion, som kan \"tænke\" som et menneske. Når jeg ser et billede, kan jeg på ingen tid afgøre, om der er en hund på billedet eller ej. Står jeg med et brev i hånden, kan jeg som regel også læse navn og adresse. En læge vil også kunne kigge på et røntgenbillede og afgøre, om der er en kræftknude eller ej. Det er i bund og grund, det vi forstår ved kunstig intelligens. At få computeren til at \"tænke\" som et menneske. Nu kunne man måske godt indvende \"hvad skal det til for?\". Der er vel ingen grund til at få en computer til at finde kræftknuder på et røntgenbillede, hvis vi allerede kan få en læge til det? Men hvad nu, hvis computeren faktisk kan opdage kræftknuder tidligere end lægen? Eller hvad hvis man har så mange røntgenbilleder, at det vil være smart, at få en computer til at kigge dem igennem først?"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-1",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-1",
    "title": "Kunstige neurale netværk",
    "section": "VIDEO: Kunstige neurale netværk 1",
    "text": "VIDEO: Kunstige neurale netværk 1\nI denne video forklarer lidt om hvad et kunstigt neuralt netværk er og lidt om hvilke input- og outputværdier, man kan bruge."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-2",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-2",
    "title": "Kunstige neurale netværk",
    "section": "VIDEO: Kunstige neurale netværk 2",
    "text": "VIDEO: Kunstige neurale netværk 2\nI denne video giver vi et eksempel på et simpelt kunstig neuralt netværk og forklarer feedforward."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-3",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-3",
    "title": "Kunstige neurale netværk",
    "section": "VIDEO: Kunstige neurale netværk 3",
    "text": "VIDEO: Kunstige neurale netværk 3\nI videoen her forklarer vi, hvad targetværdier er, og hvordan tabsfunktionen defineres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-4",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-4",
    "title": "Kunstige neurale netværk",
    "section": "VIDEO: Kunstige neurale netværk 4",
    "text": "VIDEO: Kunstige neurale netværk 4\nI denne video bliver gradientnedstigning forklaret."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-opdatering_w",
    "href": "materialer/neurale_net/neurale_net.html#sec-opdatering_w",
    "title": "Kunstige neurale netværk",
    "section": "Opdatering af \\(w\\)-vægtene",
    "text": "Opdatering af \\(w\\)-vægtene\nNår man bruger backpropagation, starter man med at finde de partielle afledede for de vægte, som direkte påvirker outputværdien \\(o\\). På figur 4 fremgår det, at det er vægtene \\(w_0, w_1\\) og \\(w_2\\) (husk at vi kalder vores bias for \\(w_0\\)). Lad os starte med at finde den partielle afledede for \\(w_1\\). Ved at bruge kædereglen får vi: \\[\n\\frac{\\partial E}{\\partial w_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial w_1}\n\\] Vi ved fra (12), at \\(E=\\frac{1}{2}(t-o)^2\\) og derfor er: \\[\n\\frac{d E}{d o} = \\frac{1}{2} \\cdot 2 \\cdot (t-o) \\cdot (-1) = -(t-o)\n\\tag{14}\\] Fra (10) har vi, at \\(o=\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)\\) og derfor får vi \\[\n\\frac{\\partial o}{\\partial w_1} =\\sigma'(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0) \\cdot z_1\n\\] Vi har tidligere vist, at \\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\) og derfor har vi \\[\n\\frac{\\partial o}{\\partial w_1} =\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)(1-\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)) \\cdot z_1\n\\] Bruger vi nu, at \\(o=\\sigma(w_1 \\cdot z_1 + w_2 \\cdot z_2 + w_0)\\) kan vi skrive ovenstående lidt mere kompakt: \\[\n\\frac{\\partial o}{\\partial w_1} =o(1-o) \\cdot z_1\n\\] Alt i alt får vi altså, at \\[\n\\frac{\\partial E}{\\partial w_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial w_1}\n= -(t-o) \\cdot o \\cdot (1-o) \\cdot z_1\n\\tag{15}\\] Vi kan nu udlede den første opdateringsregel for vægten \\(w_1\\) ved at bruge idéen fra (13): \\[\nw_1 \\leftarrow w_1 - \\eta  \\cdot \\frac{\\partial E}{\\partial w_1}\n\\] Indsættes udtrykket fra (15), får vi \\[\nw_1 \\leftarrow w_1 - \\eta  \\cdot (-(t-o) \\cdot o \\cdot (1-o) \\cdot z_1)\n\\] Det vil sige, at \\[\nw_1 \\leftarrow w_1 + \\eta  \\cdot (t-o) \\cdot o \\cdot (1-o) \\cdot z_1\n\\] Det er værd at dvæle lidt ved opdateringsleddet \\(\\eta  \\cdot (t-o) \\cdot o \\cdot (1-o) \\cdot z_1\\) på højresiden fordi det faktisk giver intuitiv god mening. For det første er \\(\\eta\\), det vi som sagt kalder for vores learning rate - et lille positivt tal, som sørger for, at vi ikke tager for store skridt på vores vej ned i dalen (til det lokale minimum). Faktoren \\(t-o\\) er jo netop fejlen. Nemlig forskellen mellem det vi ønsker \\(t\\) (target), og det som netværket giver \\(o\\) (output). Jo større fejl/forskel, desto mere må vi justere vægten. Ser vi på faktoren \\(o\\cdot(1-o)\\), så vil det være sådan, at hvis outputværdien \\(o\\) er tæt på enten \\(0\\) eller \\(1\\) (man siger at neuronen er \"mættet\"), så vil \\(o\\cdot(1-o)\\) være tæt på \\(0\\). Det vil sige, at hvis outputværdien er tæt på \\(0\\) eller \\(1\\), så ændrer vi heller ikke så meget på vægten. Endelig er der faktoren \\(z_1\\), som er inputtet fra det foregående lag (se figur 4). Hvis værdien af denne er (numerisk) stor, så får det også stor betydning for opdateringsleddet (eller tænk på det omvendt: hvis \\(z_1\\) er tæt på \\(0\\), så har \\(z_1\\) alligevel ikke så stor indflydelse på outputværdien, og så giver det heller ikke mening, at justere så meget på den tilhørende vægt \\(w_1\\)).\nDet viser sig faktisk, at faktoren \\((t-o) \\cdot o \\cdot (1-o)\\) kommer til at gå igen rigtige mange gange i det følgende. Det bliver i længden lidt tungt at slæbe rundt på. Derfor vælger vi at definere \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\tag{16}\\] og derfor kan opdateringsreglen for \\(w_1\\) nu også skrives: \\[\nw_1 \\leftarrow w_1 + \\eta  \\cdot \\delta \\cdot z_1\n\\tag{17}\\]\nHelt analogt med ovenstående kan man udlede opdateringsregler for \\(w_2\\) og \\(w_0\\). Resultatet er samlet her.\n\n\n\n\n\n\nOpdateringsregler for \\(w\\)-vægtene\n\n\n\n\\[\\begin{align*}\nw_0 &\\leftarrow w_0 + \\eta  \\cdot \\delta  \\\\\nw_1 &\\leftarrow w_1 + \\eta  \\cdot \\delta \\cdot z_1 \\\\\nw_2 &\\leftarrow w_2 + \\eta  \\cdot \\delta \\cdot z_2 \\\\\n\\end{align*}\\] hvor \\[\\delta = (t-o) \\cdot o \\cdot (1-o)\\]\n\n\nMen hvordan foregår det der med de opdateringsregler så egentligt? Jo altså vi starter med at sætte vægtene mere eller mindre tilfældigt. Så laver vi ved hjælp af vores træningseksempel \\((\\vec{x},t)\\) et feedforward i netværket, som det er beskrevet i afsnit 3. Derfor får vi beregnet outputværdien \\(o\\) samt \\(z_1\\) og \\(z_2\\) (husk at \\(z_1\\) og \\(z_2\\) bruges til at beregne \\(o\\)). Desuden kender vi jo fra vores træningsdata target-værdien \\(t\\). Og voila! Alt hvad der indgår på højresiderne i ovenstående opdateringsregler har vi nu adgang til, og vi kan derfor beregne de nye \\(w\\) vægte.\nSå mangler vi bare at finde opdateringsreglerne for de restende vægte!"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-5",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-5",
    "title": "Kunstige neurale netværk",
    "section": "VIDEO: Kunstige neurale netværk 5",
    "text": "VIDEO: Kunstige neurale netværk 5\nI videoen her forklarer vi hvordan \\(w\\)-vægtene opdateres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-opdatering_uv",
    "href": "materialer/neurale_net/neurale_net.html#sec-opdatering_uv",
    "title": "Kunstige neurale netværk",
    "section": "Opdatering af \\(u\\)- og \\(v\\)-vægtene",
    "text": "Opdatering af \\(u\\)- og \\(v\\)-vægtene\nVi går nu et trin længere tilbage i netværket - væk fra outputlaget. Her kan vi se neuronerne, som fyrer værdierne \\(z_1\\) og \\(z_2\\), som bliver påvirket af \\(u\\)- og \\(v\\)-vægtene. Lad os her starte med at bestemme opdateringsreglerne for \\(v\\)-vægtene. For at gøre det skal vi finde ud af hvordan \\(v\\)-vægtene påvirker neuronerne længere fremme i netværket. Se igen på figur 4. Her er det tydeligt, at \\(v\\)-vægtene påvirker den mørkegrønne neuron, som fyrer værdien \\(z_1\\), som igen påvirker outputværdien. Derfor kan vi bruge kædereglen på følgende måde: \\[\n\\frac{\\partial E}{\\partial v_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial v_1}\n\\] Vi ved allerede fra (14), at \\[\n\\frac{d E}{d o} = -(t-o)\n\\] Den partielle afledede af \\(o\\) med hensyn til \\(z_1\\) finder vi ved at bruge definitionen af outputværiden \\(o\\) i (10) \\[\n\\begin{aligned}\n\\frac{\\partial o}{\\partial z_1} &= \\sigma'(w_1 \\cdot z_1+w_2 \\cdot z_2 + w_0) \\cdot w_1  \\\\\n&= \\sigma(w_1 \\cdot z_1+w_2 \\cdot z_2 + w_0) \\cdot (1-\\sigma(w_1 \\cdot z_1+w_2 \\cdot z_2 + w_0)) \\cdot w_1  \\\\\n&= o \\cdot (1-o) \\cdot w_1\n\\end{aligned}\n\\tag{18}\\] hvor vi igen har brugt sætning 1. Og endelig ved at udnytte definitionen af \\(z_1\\) i (8) får vi, at \\[\\begin{align}\n\\frac{\\partial z_1}{\\partial v_1} &= \\sigma'(v_1 \\cdot y_1+v_2 \\cdot y_2 + v_0) \\cdot y_1 \\\\\n&= \\sigma(v_1 \\cdot y_1+v_2 \\cdot y_2 + v_0) \\cdot (1-\\sigma(v_1 \\cdot y_1+v_2 \\cdot y_2 + v_0)) \\cdot y_1 \\\\\n&= z_1 \\cdot (1-z_1) \\cdot y_1\n\\end{align}\\] Sætter vi det hele sammen får vi, at \\[\n\\frac{\\partial E}{\\partial v_1} = \\underbrace{-(t-o)}_{\\frac{\\partial E}{\\partial o}}  \\cdot \\underbrace{o \\cdot (1-o) \\cdot w_1}_{\\frac{\\partial o}{\\partial z_1}} \\cdot \\underbrace{z_1 \\cdot (1-z_1) \\cdot y_1}_{\\frac{\\partial z_1}{\\partial v_1}}\n\\] og bruger vi definitionen af \\(\\delta\\) i (16) får vi et lidt mere kompakt udtryk \\[\n\\frac{\\partial E}{\\partial v_1} = -\\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_1\n\\] Opdateringsreglen for \\(v_1\\) bliver derfor \\[\nv_1 \\leftarrow v_1 - \\eta \\cdot \\frac{\\partial E}{\\partial v_1}\n\\] og med det netop udledte udtryk for \\(\\frac{\\partial E}{\\partial v_1}\\) får vi \\[\nv_1 \\leftarrow v_1 - \\eta \\cdot (-\\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1)\\cdot y_1)\n\\] Det vil sige, at \\[\nv_1 \\leftarrow v_1 + \\eta \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_1\n\\] Læg igen mærke til, at når vi har været igennem et feedforward i netværket, så kender vi alle de størrelser, som indgår i ovenstående udtryk.\nPå helt tilsvarende vis kan man bestemme opdateringsreglerne for \\(v_0\\) og \\(v_2\\). De tre opdateringsregler for \\(v\\)-vægtene ses her:\n\n\n\n\n\n\nOpdateringsregler for \\(v\\)-vægtene\n\n\n\n\\[\\begin{align}\nv_0 &\\leftarrow v_0 + \\eta  \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\\\\nv_1 &\\leftarrow v_1 + \\eta  \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_1 \\\\\nv_2 &\\leftarrow v_2 + \\eta  \\cdot \\delta \\cdot w_1 \\cdot z_1 \\cdot (1-z_1) \\cdot y_2 \\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]\n\n\nOpdateringsreglerne for \\(u\\)-vægtene findes på præcis samme måde. Her skal man blot se, at \\(u\\)-vægtene har indflydelse på outputtet via \\(z_2\\) (se figur 4). Derfor skal man f.eks. finde den partielle afledede af \\(E\\) med hensyn til \\(u_1\\) ved at bruge kædereglen på denne måde \\[\n\\frac{\\partial E}{\\partial u_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial u_1}\n\\] Udregninger svarende til det netop gennemgåede giver os\n\n\n\n\n\n\nOpdateringsregler for \\(u\\)-vægtene\n\n\n\n\\[\\begin{align}\nu_0 &\\leftarrow u_0 + \\eta  \\cdot \\delta \\cdot w_2 \\cdot z_2 \\cdot (1-z_2) \\\\\nu_1 &\\leftarrow u_1 + \\eta  \\cdot \\delta \\cdot w_2 \\cdot z_2 \\cdot (1-z_2) \\cdot y_1 \\\\\nu_2 &\\leftarrow u_2 + \\eta  \\cdot \\delta \\cdot w_2 \\cdot z_2 \\cdot (1-z_2) \\cdot y_2 \\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-6",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-6",
    "title": "Kunstige neurale netværk",
    "section": "VIDEO: Kunstige neurale netværk 6",
    "text": "VIDEO: Kunstige neurale netværk 6\nI videoen her forklarer vi, hvordan \\(v\\)-vægtene opdateres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-opdatering_rs",
    "href": "materialer/neurale_net/neurale_net.html#sec-opdatering_rs",
    "title": "Kunstige neurale netværk",
    "section": "Opdatering af \\(r\\)- og \\(s\\)-vægtene",
    "text": "Opdatering af \\(r\\)- og \\(s\\)-vægtene\nSå er vi endelig fremme ved \\(r\\)- og \\(s\\) vægtene. Start lige med at tage en dyb indånding! Nu bliver det lidt mere kompliceret. Se på figur 4. Lad os starte med at finde den partielle afledede af \\(E\\) med hensyn til \\(r_1\\). Når man ser på netværket, kan man se, at \\(r_1\\) i første omgang påvirker \\(y_1\\), \\(y_1\\) påvirker både \\(z_1\\) og \\(z_2\\), som så til sidst påvirker outputværdien \\(o\\). Det kan illustreres sådan her \\[\n\\begin{matrix}\n& & & & z_1 & & & \\\\\n& & & \\nearrow & & \\searrow & & \\\\\nr_1 & \\rightarrow & y_1 & & & & \\rightarrow & o \\\\\n& & & \\searrow & & \\nearrow & & \\\\\n& & & & z_2 & & & \\\\\n\\end{matrix}\n\\]\nBalladen er, at \\(y_1\\) både påvirker \\(z_1\\) og \\(z_2\\), og det gør det hele lidt mere kompliceret. Lad os lige starte med at se bort fra det. Ifølge kædereglen får vi så: \\[\n\\frac{\\partial E}{\\partial r_1} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial y_1}  \\cdot \\frac{\\partial y_1}{\\partial r_1}\n\\] Men så var det jo, at \\(o\\) i virkeligheden afhænger af \\(y_1\\) både via \\(z_1\\) og \\(z_2\\). Man kunne skrive det sådan her: \\[\no(z_1(y_1), z_2(y_1))\n\\] Bemærk, at \\(z_1\\) og \\(z_2\\) jo også afhænger af \\(y_2\\), men når vi skal differentiere med hensyn til \\(y_1\\), så er \\(y_2\\) at betragte som en konstant. Og når konstanter bliver differentieret, så giver det som bekendt \\(0\\).\nDerfor: For at finde den partielle afledede af \\(o\\) med hensyn til \\(y_1\\) må vi benytte kædereglen for funktioner af flere variable. Den siger, at \\[\n\\frac{\\partial o}{\\partial y_1} = \\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial y_1} + \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial y_1}\n\\] Det samlede udtryk for den partielle afledede af \\(E\\) med hensyn til \\(r_1\\) bliver derfor \\[\n\\frac{\\partial E}{\\partial r_1} = \\frac{d E}{d o} \\cdot\n\\left(\n\\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial y_1} + \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial y_1}\n\\right)\n\\cdot \\frac{\\partial y_1}{\\partial r_1}\n  \\tag{19}\\] Vi finder hver af de afledede, som indgår i ovenstående udtryk én ad gangen. Vi ved allerede fra (14), at \\[\n\\frac{d E}{d o} = \\frac{1}{2} \\cdot 2 \\cdot (t-o) \\cdot (-1) = -(t-o)\n\\] Vi ved også fra (18), at \\[\n\\frac{\\partial o}{\\partial z_1} = o \\cdot (1-o) \\cdot w_1\n\\] Differentieres \\(z_1\\) (se (8)) med hensyn til \\(y_1\\) får vi \\[\\begin{align}\n\\frac{\\partial z_1}{\\partial y_1} &= \\sigma'(v_1 \\cdot y_1 + v_2 \\cdot y_2 + v_0)\\cdot v_1 \\\\\n&= z_1 \\cdot (1-z_1) \\cdot v_1\n\\end{align}\\] hvor vi igen har brugt sætning 1 og definitionen af \\(z_1\\) i (8). Helt tilsvarende kan vi finde \\(\\frac{\\partial o}{\\partial z_2}\\) og \\(\\frac{\\partial z_2}{\\partial y_1}\\) (se (9)) \\[\n\\frac{\\partial o}{\\partial z_2} = o \\cdot (1-o)\\cdot w_2\n\\] og \\[\n\\frac{\\partial z_2}{\\partial y_1} = z_2 \\cdot (1-z_2)\\cdot u_1\n\\] Den sidste partielle afledede \\(\\frac{\\partial y_1}{\\partial r_1}\\) finder vi ved at differentiere udtrykket for \\(y_1\\) i (4), hvor vi endnu engang udnytter sætning 1.\nIndsætter vi nu alle de udtryk, som vi netop har udledt, i (19) får vi et temmelig langt udtryk for \\(\\frac{\\partial E}{\\partial r_1}\\): \\[\\begin{align}\n\\frac{\\partial E}{\\partial r_1} &= \\underbrace{-(t-o)}_{\\frac{dE}{do}}\\cdot \\\\ &\\Big( \\underbrace{o\\cdot(1-o)\\cdot w_1}_{\\frac{\\partial o}{\\partial z_1}} \\cdot \\underbrace{z_1\\cdot(1-z_1)\\cdot v_1}_{\\frac{\\partial z_1}{\\partial y_1}}+\\underbrace{o\\cdot(1-o)\\cdot w_2}_{\\frac{\\partial o}{\\partial z_2}}\\cdot \\underbrace{z_2\\cdot(1-z_2)\\cdot u_1}_{\\frac{\\partial z_2}{\\partial y_1}}\\Big)\\cdot \\\\ & \\qquad \\underbrace{y_1\\cdot(1-y_1)\\cdot x_1}_{\\frac{\\partial y_1}{\\partial r_1}}\n\\end{align}\\] Og sætter vi \\(o\\cdot(1-o)\\) uden for parentesen og erstatter \\((t-o)\\cdot o\\cdot (1-o)\\) med \\(\\delta\\) får vi \\[\n\\frac{\\partial E}{\\partial r_1}\n=-\\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_1\n\\] Helt i tråd med tidligere får vi altså følgende opdateringsregel for \\(r_1\\) \\[r_1 \\leftarrow r_1 - \\eta \\cdot \\frac{\\partial E}{\\partial r_1} \\] Det vil sige \\[\nr_1 \\leftarrow r_1 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_1\n\\]\nUdleder man tilsvarende opdateringsregler for \\(r_2, r_3, r_4\\) og \\(r_0\\) vil man se, at det eneste som kommer til at ændre sig i ovenstående er den sidste faktor \\(x_1\\), som bliver erstattet med henholdsvis \\(x_2, x_3, x_4\\) og \\(1\\). Derfor får vi samlet set\n\n\n\n\n\n\nOpdateringsregler for \\(r\\)-vægtene\n\n\n\n\\[\\begin{align}\nr_0 &\\leftarrow r_0 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\\\\nr_1 &\\leftarrow r_1 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_1\\\\\nr_2 &\\leftarrow r_2 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_2\\\\\nr_3 &\\leftarrow r_3 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_3\\\\\nr_4 &\\leftarrow r_4 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_1+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_1 \\Big) \\cdot y_1\\cdot (1-y_1) \\cdot x_4\\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]\n\n\nOpdateringen af \\(s\\)-vægtene foregår på samme måde. Hvis du ser på figur 4, kan du se, at alle \\(s\\)-vægtene påvirker \\(y_2\\), som så påvirker både \\(z_1\\) og \\(z_2\\), som i sidste ende påvirker outputtet \\(o\\). Ser vi generelt på vægten \\(s_i\\), hvor \\(i=0, 1, 2, 3\\) eller \\(4\\), har vi altså \\[\n\\begin{matrix}\n& & & & z_1 & & & \\\\\n& & & \\nearrow & & \\searrow & & \\\\\ns_i & \\rightarrow & y_2 & & & & \\rightarrow & o \\\\\n& & & \\searrow & & \\nearrow & & \\\\\n& & & & z_2 & & & \\\\\n\\end{matrix}\n\\] Som tidligere kan vi starte med at skrive \\[\n\\frac{\\partial E}{\\partial s_i} = \\frac{d E}{d o} \\cdot \\frac{\\partial o}{\\partial y_2}  \\cdot \\frac{\\partial y_2}{\\partial s_i}\n\\] og bruger vi igen kædreglen for funktioner af flere variable, får vi \\[\n\\frac{\\partial E}{\\partial s_i} = \\frac{d E}{d o} \\cdot\n\\left(\n\\frac{\\partial o}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial y_2} + \\frac{\\partial o}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial y_2}\n\\right)\n\\cdot \\frac{\\partial y_2}{\\partial s_i}\n\\] I ovenstående udtryk bliver det klart, at opdateringsreglerne vil blive ens bortset fra den sidste faktor.\nNu udledes alle de partielle afledede, fuldstændig som for \\(r\\)-vægtene og vi ender med følgende opdateringsregler for \\(s\\)-vægtene:\n\n\n\n\n\n\nOpdateringsregler for \\(s\\)-vægtene\n\n\n\n\\[\\begin{align}\ns_0 &\\leftarrow s_0 +  \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2) \\\\\ns_1 &\\leftarrow s_1 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2) \\cdot x_1\\\\\ns_2 &\\leftarrow s_2 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2)  \\cdot x_2\\\\\ns_3 &\\leftarrow s_3 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2)  \\cdot x_3\\\\\ns_4 &\\leftarrow s_4 + \\eta \\cdot \\delta \\cdot \\Big(w_1 \\cdot z_1\\cdot (1-z_1)\\cdot v_2+w_2 \\cdot z_2 \\cdot (1-z_2)\\cdot  u_2 \\Big) \\cdot y_2\\cdot (1-y_2)  \\cdot x_4\\\\\n\\end{align}\\] hvor \\[\n\\delta = (t-o) \\cdot o \\cdot (1-o)\n\\]\n\n\nDet var faktisk det! Altså det blev jo en værre omgang bogstavgymnastik, men faktum er, at vi er i mål med at udlede backpropagation algoritmen for vores simple netværk i figur 4. Hurra for det!"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-7",
    "href": "materialer/neurale_net/neurale_net.html#video-kunstige-neurale-netværk-7",
    "title": "Kunstige neurale netværk",
    "section": "VIDEO: Kunstige neurale netværk 7",
    "text": "VIDEO: Kunstige neurale netværk 7\nI denne video forklares hvordan \\(r\\)-vægtene opdateres."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-feedforward_indekser",
    "href": "materialer/neurale_net/neurale_net.html#sec-feedforward_indekser",
    "title": "Kunstige neurale netværk",
    "section": "Feedforward med indekser",
    "text": "Feedforward med indekser\nVi vil nu se på, hvordan de forskellige \\(a_j^{(k)}\\)-værdier beregnes. Det vil sige, at vi altså skal se på, hvordan de forskellige feedforward-ligninger ser ud med vores nye notation.\n\n\n\n\n\n\nFigur 9: Udregning af \\(a_1^{(2)}\\).\n\n\n\nLad os se på et konkret eksempel, så bliver det lidt nemmere at forholde sig til. Vi starter med at udregne outputværdien \\(a_1^{(2)}\\) for den første neuron i det andet lag. Denne neuron får input fra alle neuroner i det foregående lag (som her er inputlaget). Bruger vi den notation for vægtene, som vi lige har indført, så starter vi med at beregne: \\[\nz_1^{(2)} = w_{11}^{(2)} \\cdot x_1 + w_{12}^{(2)} \\cdot x_2 + w_{13}^{(2)} \\cdot x_3 + w_{14}^{(2)} \\cdot x_4 + b_1^{(2)}\n\\] Der er to ting at bemærke her: 1) Vi vælger, at kalde udtrykket på højreside for \\(z_1^{(2)}\\) og, 2) vi har kaldt biasen for \\(b_1^{(2)}\\).\nBruger vi nu de mere generelle udtryk for inputværdierne \\(a_1^{(1)}, a_2^{(1)}, \\dots, a_4^{(1)}\\) kan vi skrive: \\[\\begin{align}\nz_1^{(2)} &= w_{11}^{(2)} \\cdot a_1^{(1)} + w_{12}^{(2)} \\cdot a_2^{(1)} + w_{13}^{(2)} \\cdot a_3^{(1)} + w_{14}^{(2)} \\cdot a_4^{(1)} + b_1^{(2)} \\\\\n&= \\sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)}\n\\end{align}\\] Og endelig finder vi outputværdien \\(a_1^{(2)}\\) for den første neuron i det andet lag ved som tidligere at anvende sigmoid-funktionen på ovenstående udtryk: \\[\\begin{align}\na_1^{(2)} &= \\sigma(z_1^{(2)}) \\\\\n&= \\sigma \\left( \\sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)} \\right)\n\\end{align}\\] Det her er faktisk notationsmæssigt selve idéen. Folder vi det ud til hele det andet lag får vi derfor:\n\n\n\n\n\n\nFeedforwardligninger til lag 2\n\n\n\nBeregn først: \\[\\begin{align}\nz_1^{(2)} &= \\sum_{i=1}^{4} w_{1i}^{(2)} a_i^{(1)} +  b_1^{(2)} \\\\\n& \\\\\nz_2^{(2)} &=\\sum_{i=1}^{4} w_{2i}^{(2)} a_i^{(1)} +  b_2^{(2)} \\\\\n& \\\\\nz_3^{(2)} &= \\sum_{i=1}^{4} w_{3i}^{(2)} a_i^{(1)} +  b_3^{(2)} \\\\\n\\end{align}\\] Outputværdierne for neuroner i det andet lag udregnes dernæst på denne måde: \\[\\begin{align}\na_1^{(2)} &= \\sigma(z_1^{(2)}) \\\\\n& \\\\\na_2^{(2)} &= \\sigma(z_2^{(2)}) \\\\\n&\\\\\na_3^{(2)} &= \\sigma(z_3^{(2)}) \\\\\n\\end{align}\\]\n\n\nOg vover vi pelsen, kan vi helt generelt skrive:\n\n\n\n\n\n\nFeedforward-ligninger til lag 2\n\n\n\nBeregn først: \\[\\begin{align}\nz_j^{(2)} = \\sum_{i=1}^{4} w_{ji}^{(2)} a_i^{(1)} +  b_j^{(2)}\n\\end{align}\\] Outputværdierne for neuroner i det andet lag udregnes dernæst på denne måde: \\[\\begin{align}\na_j^{(2)} &= \\sigma(z_j^{(2)})\n\\end{align}\\] for \\(j \\in \\{1, 2, 3 \\}\\).\n\n\nFordelen ved denne notation er, at det nu er utrolig nemt at opskrive feedforward-ligningerne for lag 3 og 4 - det er blot nogle indekser, som skal ændres lidt. I det tredje lag er der to neuroner, hvis outputværdier beregnes på denne måde:\n\n\n\n\n\n\nFeedforward-ligninger til lag 3\n\n\n\nBeregn først: \\[\nz_j^{(3)} = \\sum_{i=1}^{3} w_{ji}^{(3)} a_i^{(2)} +  b_j^{(3)}\n\\tag{20}\\] Outputværdierne for neuroner i det tredje lag udregnes dernæst på denne måde: \\[\\begin{align}\na_j^{(3)} &= \\sigma(z_j^{(3)})\n\\end{align}\\] for \\(j \\in \\{1, 2 \\}\\).\n\n\nOg endelig beregnes outputtet fra hele netværket i det fjerde lag:\n\n\n\n\n\n\nFeedforward-ligninger til lag 4\n\n\n\nUdregn først: \\[\nz_j^{(4)} = \\sum_{i=1}^{2} w_{ji}^{(4)} a_i^{(3)} +  b_j^{(4)}\n\\tag{21}\\] Outputværdierne for neuroner i det tredje lag udregnes dernæst på denne måde: \\[\\begin{align}\ny_j = a_j^{(4)} &= \\sigma(z_j^{(4)})\n\\end{align}\\] for \\(j \\in \\{1, 2, 3 \\}\\).\n\n\nDet fremgår nu tydeligt, at feedforward-ligningerne er på fuldstændig samme form, og vi vil derfor helt generelt kunne skrive:\n\n\n\n\n\n\nFeedforward-ligninger generelt\n\n\n\nBeregn først: \\[\\begin{align}\nz_j^{(k)} = \\sum_{i} w_{ji}^{(k)} a_i^{(k-1)} +  b_j^{(k)}\n\\end{align}\\] Outputværdierne for neuroner i det \\(k\\)’te lag udregnes dernæst på denne måde: \\[\\begin{align}\na_j^{(k)} &= \\sigma(z_j^{(k)})\n\\end{align}\\] for \\(k \\in \\{2, 3, 4 \\}\\).\n\n\nNår man bruger feedforward, starter man altså med at udregne outputværdierne for det første skjulte lag, dernæst for det andet skjulte lag og så videre, indtil man når til outputværdierne for selve netværket (deraf navnet: feedforward ). Bemærk her, at det ikke giver mening at udregne \\(a_j^{(1)}\\), fordi det svarer til inputværdierne til netværket."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#sec-backpropagation_indekser",
    "href": "materialer/neurale_net/neurale_net.html#sec-backpropagation_indekser",
    "title": "Kunstige neurale netværk",
    "section": "Backpropagation med indekser",
    "text": "Backpropagation med indekser\nLad os nu se på hvordan backpropagation fungerer. Vi skal altså have opskrevet vores opdateringsregler med den nye notation, og vi vil gribe det an, ligesom vi gjorde det i afsnit 4. Nemlig ved at starte i det sidste lag (her lag \\(4\\)) og finde opdateringsreglerne for de vægte og bias, som har direkte indflydelse på outputværdierne fra lag \\(4\\).\n\nOpdateringsregler for lag \\(4\\)\nVi er altså i første omgang på jagt efter \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(4)}},\n\\] for \\(j \\in \\{1, 2, 3\\}\\), \\(i \\in \\{1, 2\\}\\). Tabsfunktionen \\(E\\), som hører til netværket i figur 7, bliver her: \\[\nE=\\frac{1}{2} \\sum_{j=1}^3 \\left( t_j-y_j \\right)^2 = \\frac{1}{2} \\sum_{j=1}^3 \\left( t_j-a_j^{(4)} \\right)^2\n\\tag{22}\\] hvor igen \\(t_j\\) er den ønskede target-værdi for den \\(j\\)’te outputneuron.\nLad os starte med at bestemme \\(\\frac{\\partial E}{\\partial w_{ji}^{(4)}}\\). Vi må derfor først se på, hvordan \\(w_{ji}^{(4)}\\) påvirker tabsfunktionen \\(E\\). Da \\(w_{ji}^{(4)}\\) kun indgår i udtrykket for beregningen af \\(z_j^{(4)}\\), som igen bruges til beregningen af \\(a_j^{(4)}\\), som dernæst direkte påvirker tabsfunktionen, kan vi skrive: \\[\nw_{ji}^{(4)} \\rightarrow z_j^{(4)} \\rightarrow a_j^{(4)} \\rightarrow E\n\\] Bruger vi først kædereglen én gang, får vi derfor \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} = \\frac{\\partial E}{\\partial z_j^{(4)}} \\cdot \\frac{\\partial z_j^{(4)}}{\\partial w_{ji}^{(4)}}\n\\tag{23}\\] og bruges kædereglen igen, kan første faktor udfoldes yderligere \\[\n\\frac{\\partial E}{\\partial z_j^{(4)}} = \\frac{\\partial E}{\\partial a_j^{(4)}} \\cdot  \\frac{\\partial a_j^{(4)}}{\\partial z_j^{(4)}}\n\\] Lad os starte med at udregne \\(\\frac{\\partial E}{\\partial z_j^{(4)}}\\) ved at udregne hver faktor på højresiden i ovenstående udtryk for sig. Fra (22) får vi, at \\[\nE=\\frac{1}{2} \\sum_{j=1}^3 \\left( t_j-a_j^{(4)} \\right)^2 = \\frac{1}{2} \\left( \\left( t_1-a_1^{(4)} \\right)^2 + \\left( t_2-a_2^{(4)} \\right)^2+ \\left( t_3-a_3^{(4)}\\right)^2 \\right)\n\\] Hvis vi f.eks. skal differentiere ovenstående med hensyn til \\(a_2^{(4)}\\), kan vi se at alle de led, som ikke indeholder \\(a_2^{(4)}\\), vil være at betragte som konstanter, når vi differentierer - og når vi differentierer konstanter, får vi som bekendt \\(0\\). Derfor får vi, at \\[\n\\frac{\\partial E}{\\partial a_2^{(4)}} = \\frac{1}{2}\\cdot 2 \\cdot (t_2-a_2^{(4)})\\cdot (-1) = -(t_2-a_2^{(4)})\n\\] På tilsvarende vis har vi derfor generelt, at \\[\n\\frac{\\partial E}{\\partial a_j^{(4)}} = -(t_j-a_j^{(4)})\n\\tag{24}\\] Vi ved også, at \\[\na_j^{(4)} = \\sigma(z_j^{(4)})\n\\] Og bruger vi endnu en gang resultatet fra sætning 1 får vi, at \\[\n\\frac{\\partial a_j^{(4)}}{\\partial z_j^{(4)}}  = \\sigma'(z_j^{(4)})= \\sigma(z_j^{(4)}) \\cdot (1-\\sigma(z_j^{(4)}))= a_j^{(4)}\\cdot (1-a_j^{(4)})\n\\tag{25}\\] Indtil videre har vi altså, at \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(4)}} &= \\frac{\\partial E}{\\partial a_j^{(4)}} \\cdot  \\frac{\\partial a_j^{(4)}}{\\partial z_j^{(4)}} \\\\\n&=-(t_j-a_j^{(4)}) \\cdot  a_j^{(4)}\\cdot (1-a_j^{(4)})\n\\end{align}\\] I forhold til det videre arbejde viser det sig hensigtsmæssigt, at lave en samlet betegnelse for \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(4)}}  \n&=-(t_j-a_j^{(4)}) \\cdot  a_j^{(4)}\\cdot (1-a_j^{(4)})\n\\end{align}\\] Vi sætter derfor \\[\n\\delta_j^{(4)} = \\frac{\\partial E}{\\partial z_j^{(4)}}  = -(t_j-a_j^{(4)}) \\cdot a_j^{(4)} \\cdot (1-a_j^{(4)})\n\\] Udtrykket \\(\\delta_j^{(4)}\\) kalder man også for fejlleddet for det fjerde lag, men det kommer vi tilbage til senere.\nVi har nu fundet den første faktor i (23), og mangler derfor kun at bestemme \\(\\frac{\\partial z_j^{(4)}}{\\partial w_{ji}^{(4)}}\\). Bruger vi (21) ser vi, at \\[\nz_j^{(4)} =  \\sum_{i=1}^{2} w_{ji}^{(4)} a_i^{(3)} +  b_j^{(4)}  =\nw_{j1}^{(4)} a_1^{(3)}+ w_{j2}^{(4)} a_2^{(3)} +  b_j^{(4)}\n\\] Skal vi f.eks. differentiere dette udtryk med hensyn til \\(w_{j1}^{(4)}\\), får vi (fordi de fleste led i ovenstående, vil være at betragte som konstanter) \\[\n\\frac{\\partial z_j^{(4)}}{\\partial w_{j1}^{(4)}} =  a_1^{(3)}\n\\] Og helt tilsvarende hvis vi differentierer med hensyn til \\(w_{j2}^{(4)}\\), får vi \\[\n\\frac{\\partial z_j^{(4)}}{\\partial w_{j2}^{(4)}} =  a_2^{(3)}\n\\] Generelt har vi derfor, at \\[\n\\frac{\\partial z_j^{(4)}}{\\partial w_{ji}^{(4)}} = a_i^{(3)}\n\\tag{26}\\]\nSamler vi nu de tre udtryk, som vi netop har udledt og indsætter i (23) får vi \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} =  -(t_j-a_j^{(4)}) \\cdot a_j^{(4)} \\cdot (1-a_j^{(4)}) \\cdot a_i^{(3)}\n\\] og med den lidt kortere notation, som vi indførte ovenfor, kan vi nu skrive \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(4)}} =  \\delta_j^{(4)} \\cdot a_i^{(3)}\n\\] For at finde opdateringsreglerne for biasene, må vi først bestemme de partielle afledede af \\(E\\) med hensyn til \\(b_j^{(4)}\\). På helt tilsvarende vis får vi, at \\[\n\\frac{\\partial E}{\\partial b_j^{(4)}} = \\frac{\\partial E}{\\partial z_j^{(4)}} \\cdot \\frac{\\partial z_j^{(4)}}{\\partial b_j^{(4)}}\n\\] Vi ved allerede, at \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(4)}}  = \\delta_j^{(4)}\n\\end{align}\\] og ser man på ligningen i (21), ses det nemt, at \\[\n\\frac{\\partial z_j^{(4)}}{\\partial b_j^{(4)}} = 1\n\\] og derfor har vi, at \\[\n\\frac{\\partial E}{\\partial b_j^{(4)}}  =\\delta_j^{(4)}\n\\] Opdateringsreglerne for de vægte og bias, som hører til outputlaget (lag \\(4\\)) er derfor \\[\nw_{ji}^{(4)} \\leftarrow w_{ji}^{(4)} - \\eta \\cdot \\frac{\\partial E}{\\partial w_{ji}^{(4)}} = w_{ji}^{(4)} - \\eta \\cdot \\delta_j^{(4)} \\cdot a_i^{(3)}\n\\] og \\[\nb_j^{(4)} \\leftarrow b_j^{(4)} - \\eta \\cdot \\frac{\\partial E}{\\partial b_j^{(4)}}  = b_j^{(4)} - \\eta \\cdot \\delta_j^{(4)}\n\\] Vi kan altså opsummere:\n\n\n\n\n\n\nOpdateringsregler til vægte og bias i outputlaget (lag 4)\n\n\n\nVægtene i outputlaget opdateres på denne måde: \\[\nw_{ji}^{(4)} \\leftarrow w_{ji}^{(4)} - \\eta \\cdot \\delta_j^{(4)} \\cdot a_i^{(3)}\n\\] Biasene i outputlaget opdateres på denne måde: \\[\nb_j^{(4)} \\leftarrow b_j^{(4)} - \\eta \\cdot \\delta_j^{(4)}\n\\] hvor \\[\n\\delta_j^{(4)} = \\frac{\\partial E}{\\partial z_j^{(4)}}= -(t_j-a_j^{(4)}) \\cdot a_j^{(4)} \\cdot (1-a_j^{(4)})\n\\tag{27}\\]\n\n\nUdtrykket \\(\\delta_j^{(4)}\\) kalder man, som nævnt tidligere, også for fejlleddet i den \\(j\\)’te række i det fjerde lag, og man kan se på ovenstående opdateringsregler, at dette fejlled netop indgå i opdateringen af både vægtene og biasene. Faktisk kan vi præcis, som vi gjorde det på side , tillægge dette fejlled en intuitiv god mening. Det kommer vi tilbage til igen senere!\nBemærk, at hvis vi i vores netværk starter med at vælge mere eller mindre tilfældige vægte, så kan vi på baggrund af dem bruge feedforwardligningerne til at udregne, \\(a_j^{(4)}\\)- og \\(a_i^{(3)}\\)- værdierne. Samtidig kender vi target-værdierne \\(t_j\\), og vi kan derfor også udregne fejlleddene \\(\\delta_j^{(4)}\\). Vi har altså alt, hvad vi skal bruge for at benytte ovenstående opdateringsregler.\n\n\nOpdateringsregler for lag \\(3\\)\nVi bevæger os nu et trin længere bagud i netværket og udleder opdateringsreglerne for det næstsidste lag - lag \\(3\\). Altså skal vi have bestemt \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(3)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(3)}},\n\\] for \\(j \\in \\{1, 2\\}\\), \\(i \\in \\{1, 2, 3\\}\\). Vi må igen se på, hvordan \\(w_{ji}^{(3)}\\) påvirker tabsfunktionen \\(E\\). Ser vi på figur figur 7, kan vi se, at \\(w_{ji}^{(3)}\\) direkte påvirker \\(z_j^{(3)}\\), som igen direkte påvirker \\(a_j^{(3)}\\). Nu vil den \\(j\\)’te neuron i det tredje lag fyre værdien \\(a_j^{(3)}\\) til alle neuroner i det fjerde lag. Altså vil \\(a_j^{(3)}\\) påvirke \\(z_1^{(4)}, z_2^{(4)}\\) og \\(z_3^{(4)}\\), som bruges til beregning af \\(a_1^{(4)}, a_2^{(4)}\\) og \\(a_3^{(4)}\\), som så igen vil påvirke tabsfunktionen \\(E\\). Det kan illustreres på denne måde \\[\n\\begin{matrix}\n& & & & z_1^{(4)} \\rightarrow a_1^{(4)} & & \\\\\n& & & \\nearrow  & &  \\searrow & \\\\\nw_{ji}^{(3)} & \\rightarrow & z_j^{(3)} \\rightarrow a_j^{(3)} & \\rightarrow &\nz_2^{(4)} \\rightarrow a_2^{(4)} & \\rightarrow & E \\\\\n& & & \\searrow & &  \\nearrow &  \\\\\n& & & & z_3^{(4)} \\rightarrow a_3^{(4)} & & \\\\\n\\end{matrix}\n\\] I første omgang kan vi skrive \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(3)}} = \\frac{\\partial E}{\\partial z_j^{(3)}} \\cdot \\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}}\n\\tag{28}\\] og så gentagne gange bruge kædereglen til at udfolde dette udtryk.\nLad os starte med det nemmeste, nemlig \\(\\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}}\\). Ser vi på definitionen af \\(z_j^{(3)}\\) i (20), kan vi argumentere helt tilsvarende, som da vi ovenfor udledte udtrykket i (26) og får \\[\n\\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}} = a_i^{(2)}\n\\]\nLad os nu kaste os over den første faktor i (28). Vi kan starte med at udnytte denne lidt overordnede måde, som \\(z_j^{(3)}\\) påvirker \\(E\\) på \\[\nz_j^{(3)} \\rightarrow a_j^{(3)} \\rightarrow E\n\\] Kædereglen giver os derfor i første omgang \\[\n\\frac{\\partial E}{\\partial z_j^{(3)}} = \\frac{\\partial E}{\\partial a_j^{(3)}} \\cdot\n\\frac{\\partial a_j^{(3)}}{\\partial z_j^{(3)}}\n\\tag{29}\\] Igen er sidste faktor nem nok, idet \\[\na_j^{(3)} = \\sigma (z_j^{(3)})\n\\] og derfor er \\[\n\\frac{\\partial a_j^{(3)}}{\\partial z_j^{(3)}} = \\sigma' (z_j^{(3)})=\\sigma(z_j^{(3)})\\cdot (1-\\sigma(z_j^{(3)}))= a_j^{(3)} \\cdot (1-a_j^{(3)}),\n\\] hvor vi endnu en gang har benytte sætning 1.\nNår vi skal bestemme \\(\\frac{\\partial E}{\\partial a_j^{(3)}}\\) kommer vi ikke udenom kædereglen for funktioner af flere variable. Det bliver tydeligt, når vi zoomer ind på hvordan \\(a_j^{(3)}\\) påvirker \\(E\\): \\[\n\\begin{matrix}\n  & & z_1^{(4)} \\rightarrow a_1^{(4)} & & \\\\\n  & \\nearrow  & &  \\searrow & \\\\\n  a_j^{(3)} & \\rightarrow &\nz_2^{(4)} \\rightarrow a_2^{(4)} & \\rightarrow & E \\\\\n& \\searrow & &  \\nearrow &  \\\\\n& & z_3^{(4)} \\rightarrow a_3^{(4)} & & \\\\\n\\end{matrix}\n\\] For at vi senere kan udnytte nogle af de ligninger, som vi udledte i lag \\(4\\), vil vi faktisk bare nøjes med at se på det, på denne måde: \\[\n\\begin{matrix}\n  & & z_1^{(4)} & & \\\\\n  & \\nearrow  & &  \\searrow & \\\\\n  a_j^{(3)} & \\rightarrow &\nz_2^{(4)} & \\rightarrow & E \\\\\n& \\searrow & &  \\nearrow &  \\\\\n& & z_3^{(4)}  & & \\\\\n\\end{matrix}\n\\] Nu er vi endelig klar til at bruge kædereglen for funktioner af flere variable: \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(3)}} &=  \\frac{\\partial E}{\\partial z_1^{(4)}} \\cdot \\frac{\\partial z_1^{(4)}}{\\partial a_j^{(3)}}  + \\frac{\\partial E}{\\partial z_2^{(4)}} \\cdot \\frac{\\partial z_2^{(4)}}{\\partial a_j^{(3)}} + \\frac{\\partial E}{\\partial z_3^{(4)}} \\cdot \\frac{\\partial z_3^{(4)}}{\\partial a_j^{(3)}} \\\\\n&= \\sum_{k=1}^{3}\\frac{\\partial E}{\\partial z_k^{(4)}} \\cdot \\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}  \n\\end{align}\\] Se nu dukker der noget op, som vi har set før! Nemlig det fejlled, som vi definerede i (27), og som vi allerede har regnet ud, da vi opdaterede vægtene og biasene i lag \\(4\\). Tænk lige over det - det er faktisk ret fedt! Dvs. at vi kan skrive: \\[\n\\begin{aligned}\n\\frac{\\partial E}{\\partial a_j^{(3)}} &= \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot \\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}  \n\\end{aligned}\n\\tag{30}\\] Så mangler vi kun lige at finde \\(\\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}\\)! Fra (21) har vi, at \\[\nz_k^{(4)} = \\sum_i w_{ki}^{(4)} a_i^{(3)}+b_k^{(4)}\n\\] så \\[\n\\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}} = \\frac{\\partial}{\\partial a_j^{(3)}} \\left(\\sum_i w_{ki}^{(4)} a_i^{(3)}+b_k^{(4)} \\right)\n\\] Når vi skal differentierer summen i ovenstående udtryk, får vi kun et led med, når \\(i=j\\), fordi i alle andre tilfælde, vil vi med hensyn til \\(a_j^{(3)}\\) skulle differentiere en konstant. Og da \\[\n\\frac{\\partial}{\\partial a_j^{(3)}}\\left ( w_{kj}^{(4)} a_j^{(3)} \\right) = w_{kj}^{(4)}\n\\] har vi altså, at \\[\n\\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}} = w_{kj}^{(4)}\n\\] Indsætter vi dette i (30), har vi nu \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(3)}} = \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot \\frac{\\partial z_k^{(4)}}{\\partial a_j^{(3)}}  = \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot w_{kj}^{(4)}\n\\end{align}\\]\nNu skal vi i første omgang tilbage til (29) og indsætte det vi netop er kommet frem til: \\[\\begin{align}\n\\frac{\\partial E}{\\partial z_j^{(3)}}=\\underbrace{\\left ( \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot w_{kj}^{(4)}\\right )}_{\\frac{\\partial E}{\\partial a_j^{(3)}}}\n\\cdot \\underbrace{a_j^{(3)} \\cdot (1-a_j^{(3)})}_{\\frac{\\partial a_j^{(3)}}{\\partial z_j^{(3)}}}\n\\end{align}\\] Som vi gjorde i afsnit 5.2.1, vil vi også give dette lidt lange udtryk en særlig betegnelse, nemlig \\[\\begin{align}\n\\delta_j^{(3)} = \\frac{\\partial E}{\\partial z_j^{(3)}}=\\left ( \\sum_{k=1}^{3} \\delta_k^{(4)} \\cdot w_{kj}^{(4)}\\right )\n\\cdot a_j^{(3)} \\cdot (1-a_j^{(3)})\n\\end{align}\\] Det kan vist godt være lidt svært at bevare overblikket her, men nu er vi faktisk i mål! Vi indsætter i (28) \\[\\begin{align}\n\\frac{\\partial E}{\\partial w_{ji}^{(3)}} &= \\frac{\\partial E}{\\partial z_j^{(3)}} \\cdot \\frac{\\partial z_j^{(3)}}{\\partial w_{ji}^{(3)}} \\\\\n&= \\delta_j^{(3)} \\cdot a_i^{(2)}\n\\end{align}\\]\nDet er nu en smal sag at bestemme \\(\\frac{\\partial E}{\\partial b_j^{(3)}}\\), da \\[\\begin{align}\n\\frac{\\partial E}{\\partial b_j^{(3)}} &= \\frac{\\partial E}{\\partial z_j^{(3)}} \\cdot \\frac{\\partial z_j^{(3)}}{\\partial b_j^{(3)}} \\\\ &= \\delta_j^{(3)}\\cdot \\frac{\\partial z_j^{(3)}}{\\partial b_j^{(3)}}\n\\end{align}\\] Fra (20) har vi, at \\[\nz_j^{(3)} = \\sum_{i=1}^3 w_{ji}^{(3)} a_i^{(2)} + b_j^{(3)}\n\\] og derfor er \\[\n\\frac{\\partial z_j^{(3)}}{\\partial b_j^{(3)}} = 1\n\\] Altså får vi \\[\\begin{align}\n\\frac{\\partial E}{\\partial b_j^{(3)}} = \\delta_j^{(3)}\n\\end{align}\\] Glæden er stor, da vi nu har alle ingredienser til at opskrive opdateringsreglerne for det tredje lag!\n\n\n\n\n\n\nOpdateringsregler til vægte og bias i lag 3\n\n\n\nVægtene i outputlaget opdateres på denne måde: \\[\nw_{ji}^{(3)} \\leftarrow w_{ji}^{(3)} - \\eta \\cdot \\delta_j^{(3)} \\cdot a_i^{(2)}\n\\] Biasene i outputlaget opdateres på denne måde: \\[\nb_j^{(3)} \\leftarrow b_j^{(3)} - \\eta \\cdot \\delta_j^{(3)}\n\\] hvor \\[\n\\delta_j^{(3)} = \\frac{\\partial E}{\\partial z_j^{(3)}}=\\left( \\sum_{k=1}^{3}\\delta_k^{(4)} \\cdot w_{kj}^{(4)}   \\right) \\cdot a_j^{(3)} \\cdot (1-a_j^{(3)})\n\\tag{31}\\]\n\n\nBemærk, at udgangspunktet for ovenstående er, at vi først har lavet et feedforward i netværket, så vi har alle \\(a_i^{(2)}\\)- og \\(a_j^{(3)}\\)-værdier. Derudover har vi allerede opdateret vægtene og biasene i lag \\(4\\). Derfor kender vi også fejleddene \\(\\delta_k^{(4)}\\) fra lag \\(4\\), som indgår i beregningen af \\(\\delta_j^{(3)}\\) i (31). Altså er det muligt at foretage de beregninger, som opdateringsreglerne i lag \\(3\\) kræver.\n\n\nOpdateringsregler for lag \\(2\\)\nVi er nu fremme ved det sidste lag, hvor vi skal have opdateret vægte og bias (husk på at \\(a_i^{(1)}\\)-værdierne jo ikke skal beregnes, men er inputværdierne til netværket). Den gode nyhed her er, at der absolut intet nyt er under solen! Vi vil derfor heller ikke gå i drabelige deltaljer med alle udregninger her, men blot skitsere idéen.\nVi er nu på jagt efter \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(2)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(2)}}\n\\] og kigger vi på vores netværk i figur 7 kan vi se følgende afhængigheder: \\[\n\\begin{matrix}\n& & & & z_1^{(3)}  & & \\\\\n& & & \\nearrow  & &  \\searrow & \\\\\nw_{ji}^{(2)} & \\rightarrow & z_j^{(2)} \\rightarrow a_j^{(2)} &  &  &  & E \\\\\n& & & \\searrow & &  \\nearrow &  \\\\\n& & & & z_2^{(3)} & & \\\\\n\\end{matrix}\n\\] Vi kan nu igen skrive \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(2)}} = \\frac{\\partial E}{\\partial z_j^{(2)}} \\cdot \\frac{\\partial z_j^{(2)}}{\\partial w_{ji}^{(2)}}\n\\tag{32}\\] Helt analogt til tidligere ses det nemt, at \\[\n\\frac{\\partial z_j^{(2)}}{\\partial w_{ji}^{(2)}} = a_i^{(1)}\n\\] og kædreglen giver igen, at \\[\n\\frac{\\partial E}{\\partial z_j^{(2)}}= \\frac{\\partial E}{\\partial a_j^{(2)}} \\cdot\n\\frac{\\partial a_j^{(2)}}{\\partial z_j^{(2)}}\n\\tag{33}\\] Her fås også uden problemer, at den sidste faktor kan skrives som \\[\n\\frac{\\partial a_j^{(2)}}{\\partial z_j^{(2)}} = a_j^{(2)} \\cdot (1-a_j^{(2)})\n\\] og bruger man kædereglen for funktioner af flere variable, kommer man frem til, at \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(2)}} &= \\sum_{k=1}^2 \\frac{\\partial E}{\\partial z_k^{(3)}}\n\\cdot \\frac{\\partial  z_k^{(3)}}{\\partial a_j^{(2)}} \\\\\n&= \\sum_{k=1}^2  \\delta_k^{(3)} w_{kj}^{(3)},\n\\end{align}\\] hvor vi allerede har udregnet \\(\\delta_k^{(3)}\\), da vi opdaterede vægtene og biasene i lag \\(3\\).\nIndsætter vi i (33) og samtidig definerer fejlleddet \\(\\delta_j^{(2)}\\) for det andet lag, får vi \\[\n\\delta_j^{(2)} = \\frac{\\partial E}{\\partial z_j^{(2)}} = \\left ( \\sum_{k=1}^2  \\delta_k^{(3)} w_{kj}^{(3)} \\right ) \\cdot a_j^{(2)} \\cdot (1-a_j^{(2)})\n\\] Alt i alt ender vi med \\[\\begin{align}\n\\frac{\\partial E}{\\partial w_{ji}^{(2)}} &= \\delta_j^{(2)} \\cdot a_i^{(1)} \\\\\n&= \\delta_j^{(2)} \\cdot x_i\n\\end{align}\\] fordi alle \\(a_i^{(1)}\\)-værdierne svarer til selve inputværdierne \\(x_i\\) til netværket.\nDet er nu ikke svært at se, at \\[\n\\frac{\\partial E}{\\partial b_j^{(2)}} = \\delta_j^{(2)}\n\\] og vi får derfor følgende opdateringsregler for lag 2:\n\n\n\n\n\n\nOpdateringsregler til vægte og bias i lag 2\n\n\n\nVægtene i outputlaget opdateres på denne måde: \\[\\begin{align}\nw_{ji}^{(2)} \\leftarrow w_{ji}^{(2)} - \\eta \\cdot \\delta_j^{(2)} \\cdot a_i^{(1)}\n\\end{align}\\] Biasene i outputlaget opdateres på denne måde: \\[\\begin{align}\nb_j^{(2)} \\leftarrow b_j^{(2)} - \\eta \\cdot \\delta_j^{(2)}\n\\end{align}\\] hvor \\[\\begin{align}\n\\delta_j^{(2)} = \\frac{\\partial E}{\\partial z_j^{(2)}}= \\left ( \\sum_{k=1}^2  \\delta_k^{(3)} w_{kj}^{(3)} \\right ) \\cdot a_j^{(2)} \\cdot (1-a_j^{(2)})\n\\end{align}\\] {#eq-error_term_j_2}"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#var-det-så-egentlig-smart-med-alle-de-indekser",
    "href": "materialer/neurale_net/neurale_net.html#var-det-så-egentlig-smart-med-alle-de-indekser",
    "title": "Kunstige neurale netværk",
    "section": "Var det så egentlig smart med alle de indekser?",
    "text": "Var det så egentlig smart med alle de indekser?\nHvis man er nået hertil, kan man godt følge sig en lille smule forpustet. Der har godt nok været mange indekser at holde styr på! Både nogle der var sænkede, og nogle der var hævede og sat i parenteser! Alligevel kan man måske godt se fidusen nu.\nHvis vi ser på de opdateringsregler, som vi lige har udledt, så kan man se, at selve opdateringsreglerne af vægte og bias følger præcis samme form. Faktisk kan man, hvis man sammenligner opdateringsreglerne for de tre lag se, at opdateringsreglerne er på denne form:\n\n\n\n\n\n\nGenerelle opdateringsregler til vægte og bias\n\n\n\nVægtene opdateres generelt på denne måde: \\[\\begin{align}\nw_{ji}^{(\\text{lag})} \\leftarrow w_{ji}^{(\\text{lag})} - \\eta \\cdot \\delta_j^{(\\text{lag})} \\cdot a_i^{(\\text{lag-1})}\n\\end{align}\\] Biasene i outputlaget opdateres på denne måde: \\[\\begin{align}\nb_j^{(\\text{lag})} \\leftarrow b_j^{(\\text{lag})} - \\eta \\cdot \\delta_j^{(\\text{lag})}\n\\end{align}\\]\n\n\nBemærk her, at da vi allerede har lavet en feedforward i netværket, så kender vi outputværdierne \\(a_i^{(\\text{lag})}\\) i alle lag. Det vil sige, at vi kan opdatere vægtene og biasene, når blot vi kan beregne fejlleddene.\nDen eneste reelle forskel på opdateringsreglerne er, at fejlleddene udregnes lidt forskelligt, alt efter om der er tale om outputlaget eller et skjult lag:\n\n\n\n\n\n\nBeregning af fejlleddene\n\n\n\nFejlleddene i outputlaget beregnes på denne måde: \\[\\begin{align}\n\\delta_j^{(\\text{outputlag})} = -(t_j-y_j) \\cdot y_j \\cdot (1-y_j),\n\\end{align}\\] idet outputværdierne fra netværket netop er \\(y_1, y_2, \\dots\\).\nFejlleddene i et skjult lag beregnes på denne måde: \\[\\begin{align}\n\\delta_j^{(\\text{lag})} = \\left ( \\sum_{k}  \\delta_k^{(\\text{lag+1})} w_{kj}^{(\\text{lag+1})} \\right ) \\cdot a_j^{(\\text{lag})} \\cdot (1-a_j^{(\\text{lag})})\n\\end{align}\\]\n\n\nBemærk her, at fejlleddene fra outputlaget uden videre kan beregnes, da vi kender target-værdierne \\(t_j\\) og outputværdierne \\(y_j\\) fra netværket (fordi vi allerede har lavet en feedforward). Vi kan også beregne fejlleddene i alle skjulte lag, idet vi hele tiden arbejder bagud i netværket (backpropagation). Det vil sige, at vi hele tiden har adgang til fejlleddene i laget længere fremme (lag+1), hvor (lag+1) første gang vil svare til outputlaget. Desuden kender vi pga. feedforward alle outputværdier \\(a_j^{(\\text{(lag)})}\\) og alle vægte \\(w_{kj}^{(\\text{(lag+1)})}\\). Derfor kan vi også beregne fejlleddene i alle de skjulte lag.\nDenne indsigt og den generelle overordnede struktur på opdateringsreglerne, var meget svær at indse med fremgangsmåde i afsnit 4. Her druknede alt bare i et sandt bogstavshelvede!\nDer er et par andre interessante ting at sige om beregningen af fejlleddene. Lad os først se på outputlaget: \\[\n\\delta_j^{(\\text{outputlag})} = -(t_j-y_j) \\cdot y_j \\cdot (1-y_j)\n\\] Hvis der er stor forskel på target-værdien \\(t_j\\) og outputværdien \\(y_j\\), så bliver forskellen \\(t_j-y_j\\) numerisk stor. Altså vil en stor forskel på det, vi ønsker, og det vi får ud af netværket betyde, at fejlleddet bliver større og i sidste ende, at de vægte, som direkte påvirker outputtet, også vil blive opdateret meget. Endelig ser vi igen, at hvis outputneuronen er mættet (dvs. at \\(y_j\\) enten er tæt på \\(0\\) eller \\(1\\)), så vil fejlleddet ikke blive opdateret i samme grad, som hvis outputneuronen ikke havde været mættet (fordi hvis \\(y_j\\) enten er tæt på \\(0\\) eller \\(1\\), så vil \\(y_j \\cdot (1-y_j)\\) være tæt på \\(0\\)).\nVi ser altså, at fejlleddet fra det sidste lag direkte afhænger af hvor stor forskellen er på target-værdi og outputværdi.\nSer vi så på fejlleddene fra de skjulte lag: \\[\n\\delta_j^{(\\text{lag})} = \\left ( \\sum_{k}  \\delta_k^{(\\text{lag+1})} w_{kj}^{(\\text{lag+1})} \\right ) \\cdot a_j^{(\\text{lag})} \\cdot (1-a_j^{(\\text{lag})})\n\\] Så kan vi igen se, at hvis den tilhørende outputneuron, som fyrer værdien \\(a_j^{(\\text{lag})}\\), er mættet, så vil fejlleddet være tættere på \\(0\\), end hvis neuronen ikke havde været mættet. Samtidig kan vi også se, at der i fejlleddet indgår en vægtet sum af alle fejlleddene fra laget længere fremme: \\[\n\\sum_{k}  \\delta_k^{(\\text{lag+1})} w_{kj}^{(\\text{lag+1})}\n\\] På den måde vil store fejl i laget længere fremme også få indflydelse på fejlleddet i det nuværende lag."
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#backpropagation---generelt",
    "href": "materialer/neurale_net/neurale_net.html#backpropagation---generelt",
    "title": "Kunstige neurale netværk",
    "section": "Backpropagation - generelt",
    "text": "Backpropagation - generelt\nLad os så se på backpropagation. Vi ved fra det foregående, at der reelt set kun er to ting, vi skal gøre:\n\nFinde opdateringsreglerne for vægte og bias i outputlaget\nFinde opdateringsreglerne for vægte og bias i et vilkårligt skjult lag\n\n\nOpdateringsregler i outputlaget\nVores tabsfunktion er stadig \\[\nE= \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_i - a_i^{(K)} \\right )^2,\n\\] hvor \\(t_i\\) igen er targetværdierne. Vi skal bestemme \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(K)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(K)}}\n\\] Vi gør præcis som i afsnit 5.2.1. Vi indser først, at vi har denne direkte afhængighed fra \\(w_{ji}^{(K)}\\) til \\(E\\): \\[\nw_{ji}^{(K)} \\rightarrow z_j^{(K)} \\rightarrow a_j^{(K)} \\rightarrow E\n\\] Derfor får vi \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(K)}} = \\frac{\\partial E}{\\partial z_j^{(K)}} \\cdot \\frac{\\partial z_j^{(K)}}{\\partial w_{ji}^{(K)}}\n\\tag{36}\\] På grund feedforwardligningen i (34) får vi for det første, at \\[\n\\frac{\\partial z_j^{(K)}}{\\partial w_{ji}^{(K)}} = a_i^{(K-1)}\n\\tag{37}\\] Nu bruger vi kædereglen til at bestemme \\[\n\\frac{\\partial E}{\\partial z_j^{(K)}} = \\frac{\\partial E}{\\partial a_j^{(K)}} \\cdot  \\frac{\\partial a_j^{(K)}}{\\partial z_j^{(K)}}\n\\tag{38}\\] På grund af feedforwardligningen i (35) og sætning 1 får vi sidste faktor til \\[\n\\frac{\\partial a_j^{(K)}}{\\partial z_j^{(K)}}  =  a_j^{(K)}\\cdot (1-a_j^{(K)})\n\\] Endelig får vi, ved at differentiere tabsfunktionen med hensyn til \\(a_j^{(K)}\\) \\[\n\\frac{\\partial E}{\\partial a_j^{(K)}} = -(t_j-a_j^{(K)})\n\\] Vi definerer nu igen fejlleddet for outputlaget \\(\\delta_j^{(K)}\\), som tidligere \\[\n\\delta_j^{(K)} = \\frac{\\partial E}{\\partial z_j^{(K)}}\n\\] og indsætter vi det, vi netop har udledt, i (38) får vi \\[\n\\delta_j^{(K)} =  -(t_j-a_j^{(K)}) \\cdot a_j^{(K)} \\cdot (1-a_j^{(K)})\n\\] Indsætter vi nu det hele i (36), har vi altså: \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(K)}} = \\delta_j^{(K)}  \\cdot a_i^{(K-1)}\n\\] Det er ikke svært at overbevise sig selv om, at \\[\n\\frac{\\partial E}{\\partial b_j^{(K)}} = \\delta_j^{(K)}\n\\] og derfor har vi:\n\n\n\n\n\n\nGenerelle opdateringsregler til vægte og bias i outputlaget (lag \\(K\\))\n\n\n\nVægtene i outputlaget opdateres på denne måde: \\[\\begin{align}\nw_{ji}^{(K)} \\leftarrow w_{ji}^{(K)} - \\eta \\cdot \\delta_j^{(K)} \\cdot a_i^{(K-1)}\n\\end{align}\\] Biasene i outputlaget opdateres på denne måde: \\[\\begin{align}\nb_j^{(K)} \\leftarrow b_j^{(K)} - \\eta \\cdot \\delta_j^{(K)}\n\\end{align}\\] hvor \\[\\begin{align}\n\\delta_j^{(K)} = \\frac{\\partial E}{\\partial z_j^{(K)}}= -(t_j-a_j^{(K)}) \\cdot a_j^{(K)} \\cdot (1-a_j^{(K)})\n\\end{align}\\]\n\n\n\n\nOpdateringsregler i et vilkårligt skjult lag\nVi ser nu på et vilkårligt skjult lag \\(k\\), som hverken er inputlaget eller outputlaget. Det vil sige, at \\(k \\in \\{2, 3, \\dots, K-1 \\}\\). Vi antager, at vi har kørt backpropagation på alle lag, der ligger længere fremme i netværket, og specielt har vi altså beregnet fejlleddene i lag \\(k+1\\): \\[\n\\delta_j^{(k+1)} = \\frac{\\partial E}{\\partial z_j^{(k+1)}}\n\\] Vi indser først, at vi har denne afhængighed fra \\(w_{ji}^{(k)}\\) til tabsfunktionen \\(E\\): \\[\n\\begin{matrix}\n& & & & z_1^{(k+1)}  & & \\\\\n& & & \\nearrow  & \\vdots &  \\searrow & \\\\\nw_{ji}^{(k)} & \\rightarrow & z_j^{(k)} \\rightarrow a_j^{(k)} & \\rightarrow  &  z_j^{(k+1)} & \\rightarrow  & E \\\\\n& & & \\searrow & \\vdots &  \\nearrow &  \\\\\n& & & & z_{n_{k+1}}^{(k+1)} & & \\\\\n\\end{matrix}\n\\tag{39}\\] Vi starter som tidligere med at bruge kædereglen én gang: \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} = \\frac{\\partial E}{\\partial z_j^{(k)}} \\cdot \\frac{\\partial z_j^{(k)}}{\\partial w_{ji}^{(k)}}\n\\tag{40}\\] Fra feedforwardligningen i (34) får vi for det første, at \\[\n\\frac{\\partial z_j^{(k)}}{\\partial w_{ji}^{(k)}} = a_i^{(k-1)}\n\\] Endnu en anvendelse af kædereglen, og hvor vi også i samme hug definerer fejlleddet \\(\\delta_j^{(k)}\\) for det \\(k\\)’te skjulte lag, giver: \\[\n\\delta_j^{(k)} = \\frac{\\partial E}{\\partial z_j^{(k)}}= \\frac{\\partial E}{\\partial a_j^{(k)}} \\cdot\n\\frac{\\partial a_j^{(k)}}{\\partial z_j^{(k)}}\n\\tag{41}\\] Den sidste partielle afledede kan vi udlede fra feedforwardligningen (35) og sætning 1: \\[\n\\frac{\\partial a_j^{(k)}}{\\partial z_j^{(k)}} = a_j^{(k)} \\cdot (1-a_j^{(k)})\n\\] For at beregne \\(\\frac{\\partial E}{\\partial a_j^{(k)}}\\) må vi have fat i kædereglen for funktioner af flere variable (se illustrationen i (39): \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(k)}} = \\sum_{i=1}^{n_{k+1}} \\frac{\\partial E}{\\partial z_i^{(k+1)}}\n\\cdot \\frac{\\partial  z_i^{(k+1)}}{\\partial a_j^{(k)}}\n\\end{align}\\] Vi udnytter nu, at vi allerede kender fejlleddene fra lag \\(k+1\\) og kan derfor omskrive til \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(k)}} = \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)}\n\\cdot \\frac{\\partial  z_i^{(k+1)}}{\\partial a_j^{(k)}}\n\\end{align}\\] Endelig får vi fra feedforwardligningen (34), at \\[\\begin{align}\n\\frac{\\partial  z_i^{(k+1)}}{\\partial a_j^{(k)}} = w_{ij}^{(k)}\n\\end{align}\\] og derfor er \\[\\begin{align}\n\\frac{\\partial E}{\\partial a_j^{(k)}} = \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)}\n\\cdot w_{ij}^{(k)}\n\\end{align}\\] Ved at indsætte i (41) får vi nu fejlleddet i det \\(k\\)’te lag \\[\n\\delta_j^{(k)} = \\frac{\\partial E}{\\partial z_j^{(k)}}=  \\left ( \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)} \\cdot w_{ij}^{(k)} \\right) \\cdot a_j^{(k)} \\cdot (1-a_j^{(k)})\n\\] Altså er \\[\\begin{align}\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} = \\delta_j^{(k)} \\cdot a_i^{(k-1)}\n\\end{align}\\] og tilsvarende får vi også, at \\[\n\\frac{\\partial E}{\\partial b_j^{(k)}} = \\delta_j^{(k)}\n\\] Opdateringsreglerne for et vilkårligt skjult lag bliver så:\n\n\n\n\n\n\nOpdateringsregler til vægte og bias i et vilkårligt skjult lag \\(k\\)\n\n\n\nVægtene i outputlaget opdateres på denne måde: \\[\\begin{align}\nw_{ji}^{(k)} \\leftarrow w_{ji}^{(k)} - \\eta \\cdot \\delta_j^{(k)} \\cdot a_i^{(k-1)}\n\\end{align}\\] Biasene i outputlaget opdateres på denne måde: \\[\\begin{align}\nb_j^{(k)} \\leftarrow b_j^{(k)} - \\eta \\cdot \\delta_j^{(k)}\n\\end{align}\\] hvor \\[\\begin{align}\n\\delta_j^{(k)} = \\frac{\\partial E}{\\partial z_j^{(k)}}=  \\left ( \\sum_{i=1}^{n_{k+1}} \\delta_i^{(k+1)} \\cdot w_{ij}^{(k)} \\right) \\cdot a_j^{(k)} \\cdot (1-a_j^{(k)})\n\\end{align}\\]"
  },
  {
    "objectID": "materialer/neurale_net/neurale_net.html#stokastisk-gradientnedstigning",
    "href": "materialer/neurale_net/neurale_net.html#stokastisk-gradientnedstigning",
    "title": "Kunstige neurale netværk",
    "section": "Stokastisk gradientnedstigning",
    "text": "Stokastisk gradientnedstigning\nVi har faktisk snydt lidt… Okay – indrømmet – det er lidt træls at komme at sige nu! Men i alt hvad vi har lavet indtil nu, har vi kun kigget på ét træningseksempel. Vi har ladet inputværdierne for det ene træningseksempel \"kører igennem\" netværket (feedforward), beregnet tabsfunktionen og brugt resultatet herfra til at opdatere alle vægtene (backpropagation). Men vi har jo ikke kun ét træningseksempel. Vi har faktisk rigtig mange! Måske ligefrem tusindvis af træningsdata. Men hvad gør man så?\nLad os lige genopfriske den tabsfunktion, som vi endte med i det helt generelle tilfælde: \\[\nE= \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_i - a_i^{(K)} \\right )^2.\n\\tag{42}\\] Her er \\(t_i\\) target-værdien for den \\(i\\)’te outputneuron for lige præcis det træningseksempel vi står med. Husk på at et givet træningseksempel består af inputværdierne \\[\nx_1, x_2, \\dots, x_{n_1}\n\\] og de ønskede target-værdier \\[\nt_1, t_2, \\dots, t_{n_K}.\n\\] Når vi kører disse inputværdier igennem netværket, får de selvfølgelig i sidste ende direkte betydning for outputværdierne i det sidste lag (\\(K\\)): \\[\na_1^{(K)}, a_2^{(K)}, \\cdots, a_{n_K}^{(K)}.\n\\] Det vil sige, at i vores tabsfunktion i (42), så afhænger både \\(t_i\\)’erne og \\(a_i^{(K)}\\)’erne af træningseksemplet. Hvis vi sådan lidt generelt benævner vores træningseksempel med \\(x\\), så vil det kunne udtrykkes sådan her: \\[\nE_x= \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_{x,i} - a_{x,i}^{(K)} \\right )^2,\n\\] hvor så \\(t_{x,i}\\) er target-værdien for den \\(i\\)’te outputneuron fra træningsdata \\(x\\) og \\(a_{x,i}^{(K)}\\) er outputværdien for den \\(i\\)’te outputneuron, som er beregnet på baggrund af inputværdierne fra træningsdata \\(x\\).\nDen samlede tabsfunktion, som er den, vi i virkeligheden ønsker at minimere, bliver så gennemsnittet af tabsfunktionerne hørende til de enkelte træningsdata: \\[\nE = \\frac{1}{n} \\sum_x E_x= \\frac{1}{n} \\sum_x \\left ( \\frac{1}{2} \\sum_{i=1}^{n_K} \\left ( t_{x,i} - a_{x,i}^{(K)} \\right )^2 \\right ).\n\\tag{43}\\] Husk på at vi er ude efter \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} \\quad \\text{og} \\quad \\frac{\\partial E}{\\partial b_j^{(k)}}\n\\] for \\(k \\in \\{2, 3, \\dots, K\\}\\) og hvor \\(E\\) nu er summen i (43). Heldigvis kan vi differentiere ledvis, og der gælder derfor \\[\n\\frac{\\partial E}{\\partial w_{ji}^{(k)}} = \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial w_{ji}^{(k)}}\n\\] og tilsvarende \\[\n\\frac{\\partial E}{\\partial b_j^{(k)}} = \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial b_j^{(k)}}\n\\] Det kommer så til at betyde, at opdateringsreglerne nu generelt bliver på formen \\[\nw_{ji}^{(k)} \\leftarrow w_{ji}^{(k)}-\\eta \\cdot \\frac{\\partial E}{\\partial w_{ji}^{(k)}}  =\nw_{ji}^{(k)}-\\eta \\cdot \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial w_{ji}^{(k)}}\n\\tag{44}\\] og tilsvarende for biasene \\[\nb_j^{(k)} \\leftarrow b_j^{(k)} -\\eta \\cdot \\frac{\\partial E}{\\partial b_j^{(k)}}  =\nb_j^{(k)}-\\eta \\cdot \\frac{1}{n} \\sum_x \\frac{\\partial E_x}{\\partial b_j^{(k)}}\n\\tag{45}\\] Alle leddene \\(\\frac{\\partial E_x}{\\partial w_{ji}^{(k)}}\\) og \\(\\frac{\\partial E_x}{\\partial b_j^{(k)}}\\), som indgår i opdateringsreglerne, svarer netop til hvad vi har udledt i de foregående afsnit, fordi vi jo netop her kun så på ét træningseksempel ad gangen. Hvis vi overfører dette til opdateringsreglerne i outputlaget, så vil vi f.eks. få\n\n\n\n\n\n\nGenerelle opdateringsregler til vægte og bias i outputlaget (lag \\(K\\)) med brug af alle træningsdata\n\n\n\nVægtene i outputlaget opdateres på denne måde: \\[\\begin{align}\nw_{ji}^{(K)} \\leftarrow w_{ji}^{(K)} - \\eta \\cdot \\frac{1}{n} \\sum_x \\left ( \\delta_{x,j}^{(K)} \\cdot a_{x,i}^{(K-1)} \\right )\n\\end{align}\\] Biasene i outputlaget opdateres på denne måde: \\[\\begin{align}\nb_j^{(K)} \\leftarrow b_j^{(K)} - \\eta \\cdot \\frac{1}{n} \\sum_x \\left ( \\delta_{x,j}^{(K)} \\right )\n\\end{align}\\] hvor \\[\\begin{align}\n\\delta_{x,j}^{(K)} = \\frac{\\partial E_x}{\\partial z_j^{(K)}}= -(t_{x,j}-a_{x,j}^{(K)}) \\cdot a_{x,j}^{(K)} \\cdot (1-a_{x,j}^{(K)})\n\\end{align}\\]\n\n\nOg helt tilsvarende vil det se ud for opdateringsreglerne i de skjulte lag.\nLad os lige dvæle lidt ved, hvad det her, det egentlig betyder. Lad os sige at vi har \\(1000\\) træningsdata. Så skal vi lade de \\(1000\\) træningsdata kører igennem netværket, så vi kan beregne de \\(1000\\) led, som indgår i de ovenstående summer. Herefter kan vi opdatere alle vægte og bias én gang. Det vil blot være ét lille skridt på vej ned i dalen mod det lokale minimum, som vi er på jagt efter. Dette lille skridt skal gentages rigtig mange gange indtil værdierne af alle vægte og bias ser ud til at begynde at konvergere – svarende til at vi har ramt det lokale minimum.\nSå selvom gradientnedstigning kan bruges til at finde et lokalt minimum for tabsfunktionen \\(E\\), så er det faktisk også en beregningsmæssig stor og tung opgave! Derfor er der forsket meget videre i at gøre det endnu bedre og endnu hurtigere. I algoritmer som disse er der ofte et trade-off: Man kan gøre noget hurtigere ved at bruge mere hukommelse – eller bruge mindre hukommelse ved at gøre det en smule langsommere. En af de teknikker, der er kommet ud af den forskning, er, at man kan bruge mindre hukommelse ved i hvert opdateringsskridt kun at bruge en tilfældigt udvalgt del af træningsdata – det kunne f.eks. være \\(10\\%\\) af alle træningsdata. Så vil man i hvert skridt stadig bruge opdateringsreglerne i (44) og (45), men hvor der nu kun summeres over de \\(10 \\%\\) af træningsdatene. Hver gang man laver et nyt opdateringsskridt, vil man tage en ny tilfældigt udvalgt del af træningsdata. Denne teknik kalder man stokastisk gradientnedstigning (stochastic gradient descent). Og der er endnu flere af sådanne små ændringer, der enten gør algoritmen hurtigere eller at den bruger mindre hukommelse. Det vil komme an på den enkelte anvendelse, hvad der er vigtigst her."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html",
    "href": "undervisningsforloeb/test_for_sygdomme.html",
    "title": "Test for sygdomme",
    "section": "",
    "text": "Forudsætninger og tidsforbrug\n\n\n\n\n\nForløbet kræver kendskab til:\n\nSandsynlighedsregning\nBetingede sandsynligheder\n\nTidsforbrug: Ca. 2 x 90 minutter."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#sensitivitet-og-specificitet",
    "href": "undervisningsforloeb/test_for_sygdomme.html#sensitivitet-og-specificitet",
    "title": "Test for sygdomme",
    "section": "Sensitivitet og specificitet",
    "text": "Sensitivitet og specificitet\nNår man tester for en sygdom, så vil man måske umiddelbart tænke, at hvis testen er positiv, så er man syg, og hvis testen er negativ, så er man rask. Men det behøver faktisk ikke at være tilfældet. Man kan godt være rask, selvom testen er positiv (det kalder man for en falsk positiv), og man kan godt være syg, som testen er negativ (det kalder man for en falsk negativ). Det er fordi, at der ikke findes nogen test, som er helt perfekt!\nDet vil sige, at resultatet af en test vil falde i én af følgende fire kategorier:\n\n\n\n\nSyg\nRask\n\n\n\n\nPositiv test\nSand positiv (SP)\nFalsk positiv (FP)\n\n\nNegativ test\nFalsk negativ (FN)\nSand negativ (SN)\n\n\n\nDet er klart, at man selvfølgelig helst vil have en test, hvor flest mulige lander i diagonalen med sande positiver og sande negativer.\nEn god test skal derfor have følgende egenskaber:\n\nHvis testen anvendes på en syg person, så skal sandsynligheden for at testen bliver positiv være høj.\nHvis testen anvendes på en rask person, så skal sandsynligheden for at testen bliver negativ være høj.\n\nDisse to betingede sandsynligheder kaldes for henholdsvis sensitivitet og specificitet og kan skrives matematisk sådan her:\n\\[\n\\mathrm{sensitivitet } = P(\\textrm{positiv test } | \\textrm{ syg})\n\\] og\n\\[\n\\mathrm{specificitet } = P(\\textrm{negativ test } | \\textrm{ rask})\n\\]\n\n\n\n\n\n\nOpgave 1: Sensitivitet og specificitet\n\n\n\n\n\nVælg en sygdom som du vil arbejde med (eller som din lærer har bestemt, at du skal arbejde med!). Det kan for eksempel være corona, klamydia, RS virus eller influenza.\n\nUndersøg sensitivitet og specificitet for forskellige tests for den sygdom, som du har valgt.\nKan du lave en test hvor sensitiviteten er 100% (du behøver ikke at bekymre dig om specificiteten)?\nKan du lave en test hvor specificiteten er 100% (du behøver ikke at bekymre dig om sensitiviteten)?\n\n\n\n\nDet er klart, at hvis en test skal være god, så ønsker vi, at både sensitiviteten og specificiteten er tæt på 100%."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#prævalens",
    "href": "undervisningsforloeb/test_for_sygdomme.html#prævalens",
    "title": "Test for sygdomme",
    "section": "Prævalens",
    "text": "Prævalens\nBlandt alle dem, vi tester, vil en vis andel i virkeligheden være syge. Det kaldes for sygdommens prævalens. Det vil sige:\n\\[\n\\mathrm{prævalens } = P(\\textrm{syg})\n\\]\n\n\n\n\n\n\nOpgave 2: Prævalens\n\n\n\n\n\n\nUndersøg prævalensen for den sygdom, som du arbejder med.\n\n\n\n\n\n\n\n\n\n\nOpgave 3: Sensitivitet, specificitet og prævalens\n\n\n\n\n\nVi forestiller os nu, at du tester 10000 personer for den sygdom, som du arbejder med og lad os sige, at følgende er oplyst (du må også gerne bruge de tal, som du har fundet i de foregående opgaver):\nPrævalens: \\(P(\\textrm{syg})= 5 \\%\\)\nSensitivitet: \\(P(\\textrm{positiv test } | \\textrm{ syg}) = 86 \\%\\)\nSpecificitet: \\(P(\\textrm{negativ test } | \\textrm{ rask}) = 92 \\%\\)\n\nUdfyld nedenstående tabel (start med at bestemme det samlede antal syge og raske):\n\n\n\n\n\nSyg\nRask\nI alt\n\n\n\n\nPositiv test\n\n\n\n\n\nNegativ test\n\n\n\n\n\nI alt"
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#positiv-og-negativ-prædiktiv-værdi",
    "href": "undervisningsforloeb/test_for_sygdomme.html#positiv-og-negativ-prædiktiv-værdi",
    "title": "Test for sygdomme",
    "section": "Positiv og negativ prædiktiv værdi",
    "text": "Positiv og negativ prædiktiv værdi\nHvis du bliver testet for en sygdom, så vil du enten stå med en positiv eller en negativ test, og du er dybest set slet ikke interesseret i ovenstående sandsynligheder (sensitivitet, specificitet og prævalens)! Du vil i stedet stille dig selv ét af følgende to spørgsmål:\n\nMin test er positiv - hvad er sandsygligheden for, at jeg rent faktisk er syg?\n\neller\n\nMin test er negativ - hvad er sandsygligheden for, at jeg rent faktisk er rask?\n\nDu vil jo gerne undgå, at din test enten er falsk positiv eller falsk negativ.\nOvenstående sandsynligheder kaldes for den positive prædiktive værdi og den negative prædiktive værdi. Skrevet som en betinget sandsynlighed bliver det:\n\\[\n\\text{positiv prædiktiv værdi } = P(\\textrm{syg } | \\textrm{ positiv test})\n\\] og\n\\[\n\\text{negativ prædiktiv værdi } = P(\\textrm{rask } | \\textrm{ negativ test})\n\\]\n\n\n\n\n\n\nOpgave 4: Positiv og negativ prædiktiv værdi\n\n\n\n\n\nHvis du har brugt oplysningerne fra den forrige opgave, skulle du gerne have fået følgende tabel:\n\n\n\n\nSyg\nRask\nI alt\n\n\n\n\nPositiv test\n\\(430\\)\n\\(760\\)\n\\(1190\\)\n\n\nNegativ test\n\\(70\\)\n\\(8740\\)\n\\(8810\\)\n\n\nI alt\n\\(500\\)\n\\(9500\\)\n\\(10000\\)\n\n\n\n\nBenyt ovenstående tabel til at udregne den positive og negative prædiktive værdi.\n\n\n\n\nDu undrer dig måske over, at den positive prædiktive værdi er så forholdsvis lav (36.1%), mens den negative prædiktive værdi er så tæt på 100% (99.2%). Men det er fordi, at den positive og negative prædiktive værdi ikke kun afhænger af testens sensitivitet og specificitet, men også af prævalensen af sygdommen (i den gruppe vi tester iblandt). Hvis vi ser på, hvad vi ved, inden vi overhovedet begynder at teste (det kaldes for prior sandsynligheder), så er det følgende:\n\\[P(\\textrm{syg})= 5 \\%\\]\nog dermed også at\n\\[P(\\textrm{rask})= 95 \\%\\] Det vil sige, at inden vi har taget testen, er vi ret sikre på, at vi er raske. Får vi så (som forventet) en negativ test, så bliver vi bare endnu mere sikre på, at vi er raske (svarende til en negativ prædiktiv værdi på 99.2%). Får vi derimod en positiv test, så bliver vi lidt mere sikre på, at vi er syge. Vi opjusterer altså fra en prior sandsynlighed på 5% til en positiv prædiktiv værdi på 36.1%. Men fordi at sandsynligheden for at være syg på forhånden er så lille, så vil en positiv test stadig efterlade en vis chance for, at vi rent faktisk ikke er syge alligevel!\nDet virker måske underligt, men forestil dig, at vi laver graviditetstest blandt mænd. Da ingen test er perfekt (sensitivitet og specificitet vil altid være under 100%), så vil der før eller siden ske det, at en af mændene tester positiv. Men her er det ret tydeligt, at prævalensen (det vil sige sandsynligheden for at være gravid) blandt dem vi tester (det vil sige mænd) er 0%. Derfor bliver den positive prædiktive værdi også 0%, selvom testen er positiv! Men det er selvfølgelig også lidt åndsvagt at lave graviditetstest blandt mænd…!\nHvis vi tester en hel befolkning for eksempelvis corona, så vil prævalensen være forholdsvis lav. Tester vi derimod kun blandt personer, som har symptomer på corona, så vil prævalens straks være højere. Vi skal nu undersøge, hvilken betydning det har på den positive og negative prædiktive værdi.\n\n\n\n\n\n\nOpgave 5: Positiv og negativ prædiktiv værdi og forskellige prævalenser\n\n\n\n\n\nVi forestiller os igen, at vi tester 10000 personer for den sygdom, som vi arbejder med og lad os sige, at sensitivitet og specificitet er som før, men at prævalensen varierer, som angivet nedenfor:\nPrævalens: \\(P(\\textrm{syg})\\) på henholdsvis \\(1 \\%\\), \\(5 \\%\\), \\(20 \\%\\) og \\(40 \\%\\).\nSensitivitet: \\(P(\\textrm{positiv test } | \\textrm{ syg}) = 86 \\%\\)\nSpecificitet: \\(P(\\textrm{negativ test } | \\textrm{ rask}) = 92 \\%\\)\n\nUdfyld tabeller som nedenstående for hver af de fire forskellige prævalenser (husk at du allerede har tabellen for prævalensen på 5% fra opgave 3!):\n\n\n\n\n\nSyg\nRask\nI alt\n\n\n\n\nPositiv test\n\n\n\n\n\nNegativ test\n\n\n\n\n\nI alt\n\n\n\n\n\n\n\nBeregn positiv prædiktiv værdi (\\(P(\\textrm{syg } | \\textrm{ positiv test})\\)) og negativ prædiktiv værdi (\\(P(\\textrm{rask } | \\textrm{ negativ test})\\)) for de fire forskellige prævalenser og udfyld denne tabel:\n\n\n\n\nPrævalens\nPositiv prædiktiv værdi\nNegativ prædiktiv værdi\n\n\n\n\n\\(1 \\%\\)\n\n\n\n\n\\(5 \\%\\)\n\n\n\n\n\\(20 \\%\\)\n\n\n\n\n\\(40 \\%\\)\n\n\n\n\n\n\nHvad sker der med henholdsvis den positive og den negative prædiktive værdi, når prævalensen stiger? Hvordan giver det mening?\n\n\n\n\n\n\n\n\n\n\nOpgave 6: Hurtigtest for corona\n\n\n\n\n\nLæs artiklen Antigentest gav 47% falsk negative svar.\n\nUdfyld på baggrund af artiklen tabellen med antal raske/syge og positive/negative.\nUdregn testens sensitivitet og specificitet.\nUdregn prævalensen.\nUdregn den positive og negative prædiktive værdi."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#bayes-formel-mest-for-a-niveau",
    "href": "undervisningsforloeb/test_for_sygdomme.html#bayes-formel-mest-for-a-niveau",
    "title": "Test for sygdomme",
    "section": "Bayes formel (mest for A-niveau)",
    "text": "Bayes formel (mest for A-niveau)\nHvis du har læst med her (link til Allans note om Bayes klassificer - eller måske vi skal have splittet de generelle afsnit om betingede sandsynligheder og Bayes formel ud i en note for sig selv?) så ved du, at en betinget sandsynlighed er defineret på følgende måde:\n\\[\nP(A | B) = \\frac{P(A,B)}{P(B)}\n\\]\n(er det ok at skrive \\(A,B\\) i stedet for fællesmængden - der er alligevel ingen mængdelære tilbage i gymnasiet?). Og du har lært, at hvis man bruger det lidt smart, så kan man bevise Bayes’ sætning, som siger, at\n\\[\nP(A | B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\nDet vil vi nu udnytte til at opskrive et udtryk for den positive prædiktive værdi:\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{P(\\text{positiv test } | \\text{ syg}) \\cdot P(\\text{syg})}{P(\\text{positiv test})}\n\\]\nUdnytter vi definitionen af sensitivitet og prævalens, så kan vi omskrive tælleren til\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{\\text{sensitivitet} \\cdot \\text{prævalens}}{P(\\text{positiv test})}\n\\tag{1}\\]\nNu mangler vi at finde et udtryk for nævneren. Der må gælde, at\n\\[\nP(\\text{positiv test}) = P(\\text{positiv test, syg}) + P(\\text{positiv test, rask})\n\\] Bruger vi definitionen på betingede sandsynligheder, kan vi skrive ovenstående som\n\\[\nP(\\text{positiv test}) = P(\\text{positiv test } | \\text{ syg} ) \\cdot P(\\text{syg}) + P(\\text{positiv test } | \\text{ rask}) \\cdot P(\\text{rask})\n\\]\nVi udnytter nu, at \\[P(\\text{rask})+P(\\text{syg})=1\\] og dermed at \\[P(\\text{rask}) = 1- P(\\text{syg})\\] Tilsvarende er også \\[P(\\text{positiv test } | \\text{rask}) = 1-P(\\text{syg test } | \\text{rask})\\] Derfor er\n\\[\nP(\\text{positiv test}) = P(\\text{positiv test } | \\text{ syg} ) \\cdot P(\\text{syg}) + \\left ( 1 - P(\\text{negativ test } | \\text{ rask}) \\right )  \\cdot \\left ( 1- P(\\text{syg}) \\right )\n\\]\nMen nu er sandsynligheden for at teste positiv alene udtrykt ved hjælp af sensitiviteten, specificiteten og prævalensen:\n\\[\nP(\\text{positiv test}) = \\text{sensitivitet} \\cdot \\text{prævalens} + \\left ( 1 - \\text{specificitet} \\right )  \\cdot \\left ( 1- \\text{prævalens} \\right )\n\\] Indsætter vi dette i (1), får vi\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{\\text{sensitivitet} \\cdot \\text{prævalens}}{\\text{sensitivitet} \\cdot \\text{prævalens} + \\left ( 1 - \\text{specificitet} \\right )  \\cdot \\left ( 1- \\text{prævalens} \\right )}\n\\tag{2}\\]\nBruger vi denne formel til at udregne den positive prædiktive værdi i det tilfælde, hvor prævalensen er 5%, sensitiviteten er 86% og specificiteten er 92%, får vi\n\\[\nP(\\text{syg } | \\text{ positiv test}) = \\frac{0.86 \\cdot 0.05}{0.86 \\cdot 0.05 + (1-0.92) \\cdot (1-0.05)} = 0.361=36.1 \\%\n\\]\nDet skulle meget gerne stemme med det, du har fået i opgave 4 (men hvor den positive prædiktive værdi blev beregner på baggrund af tabelværdier).\n\n\n\n\n\n\nOpgave 7: Beregning af positiv prædiktiv værdi for forskellige prævalenser\n\n\n\n\n\nAntag, at vi bruger den samme sensitivitet, specificitet og prævalenser som tidligere:\nPrævalens: \\(P(\\textrm{syg})\\) på henholdsvis \\(1 \\%\\), \\(5 \\%\\), \\(20 \\%\\) og \\(40 \\%\\).\nSensitivitet: \\(P(\\textrm{positiv test } | \\textrm{ syg}) = 86 \\%\\)\nSpecificitet: \\(P(\\textrm{negativ test } | \\textrm{ rask}) = 92 \\%\\)\n\nBrug nu formlen i (2) til at beregn den positive prædiktiv værdi (\\(P(\\textrm{syg } | \\textrm{ positiv test})\\)) for de fire forskellige prævalenser og udfyld denne tabel:\n\n\n\n\nPrævalens\nPositiv prædiktiv værdi\n\n\n\n\n\\(1 \\%\\)\n\n\n\n\\(5 \\%\\)\n\n\n\n\\(20 \\%\\)\n\n\n\n\\(40 \\%\\)\n\n\n\n\n\nKontroller at dit resultat stemmer med det, du fik i opgave 5.\n\n\n\n\n\n\n\n\n\n\nOpgave 8: Formel for negative prædiktiv værdi (svær)\n\n\n\n\n\n\nOpstil en formel for udregning af den negative prædiktive værdi ved at følge udledningen af formlen for den positive prædiktive værdi ovenfor.\nBrug din formel til at udregne negativ prædiktiv værdi for prævalenserne fra opgave 7.\nKontroller at dit resultat stemmer med det du fik i opgave 5."
  },
  {
    "objectID": "undervisningsforloeb/test_for_sygdomme.html#videre-læsning",
    "href": "undervisningsforloeb/test_for_sygdomme.html#videre-læsning",
    "title": "Test for sygdomme",
    "section": "Videre læsning",
    "text": "Videre læsning\nEpidemimatematik: Test for smitte og sygdomme"
  },
  {
    "objectID": "undervisningsforloeb/kNN_forlob_overvaagning.html",
    "href": "undervisningsforloeb/kNN_forlob_overvaagning.html",
    "title": "Overvågning i Monitorbian",
    "section": "",
    "text": "Forudsætninger og tidsforbrug\n\n\n\n\n\nForløbet kræver kendskab til:\n\nKoordinatsystemer\nPunkter og afstande mellem punkter\nProcentregning\n\nTidsforbrug: ca. 90 minutter."
  },
  {
    "objectID": "undervisningsforloeb/kNN_forlob_overvaagning.html#velkommen-til-monitorbian",
    "href": "undervisningsforloeb/kNN_forlob_overvaagning.html#velkommen-til-monitorbian",
    "title": "Overvågning i Monitorbian",
    "section": "Velkommen til Monitorbian",
    "text": "Velkommen til Monitorbian\nLandet Monitorbian ønsker at blive en vaskeægte overvågningsstat! Men efterretningstjenesten i Monitorbian ved meget lidt om overvågning. Derfor har de ansat jer som intelligence officerer til at løse denne opgave. Tillykke med jeres nye job! Lad os smøge ærmerne op og komme i gang! 😄\nI Monitorbian findes der to forskellige slags indbyggere: Nogle nedstammer fra Anders And, mens andre nedstammer fra Fedtmule. På figur 1 kan du se, hvordan de forskellige indbyggere ser ud.\n\n\n\n\n\n\nFigur 1: Billede af de to slags indbyggere i Monitorbian. Indbyggeren til venstre nedstammer fra Anders And, mens indbyggeren til højre nedstammer fra Fedtmule.\n\n\n\n\nFeatures\nFor at overvåge indbyggerne er vi nødt til at identificere nogle egenskaber ved indbyggerne, som kan bruges til at adskille dem fra hinanden. Sådan en egenskab kaldes for en feature. En feature kunne for eksempel være en indbyggers vægt. Det vil være en god feature, hvis de to forskellige slags indbyggere har forholdsvis forskellig vægt. En anden feature kunne være øjenfarve, men hvis det ikke på en eller anden måde kan være med til at skelne de to slags indbyggere fra hinanden, så vil øjenfarve være et dårlig valg af feature i denne sammenhæng.\n\n\n\n\n\n\nOpgave 1: Features\n\n\n\n\n\nSe på billedet i figur 1 og find på nogle flere features.\n\n\n\n\n\nTræningsdata\nSom I lige har set, er der rigtig mange egenskaber ved indbyggerne, der kan bruges som features. Men som intelligence officerer er vi nødt til at træffe et valg og beslutte os for, hvad vi vil gå videre med. I har derfor netop været til møde i sikkerhedsudvalget, hvor det er blevet besluttet, at højde (målt i \\(cm\\)) og fodareal (målt i \\(cm^2\\)) er de to features, som I skal arbejde videre med. Disse to features er forholdsvis nemme at scanne, og fremadrettet bliver det derfor sådan, at hver gang en indbygger i Monitorbian går ind i en offentlig bygning, så bliver vedkommende scannet og højde og fodareal bliver målt.\nI skal nu have lavet en algoritme, som kan forudsige, hvilken slags indbygger der er tale om – alene baseret på viden om en given indbyggers højde og fodareal. Man siger, at vi gerne vil klassificere indbyggerne – her i to klasser: Anders And og Fedtmule.\nFor at gøre det har vi brug for træningsdata. Træningsdata består af en masse data fra forskellige indbyggere, hvor de to features er blevet målt samtidig med, at det for hver indbygger er angivet om vedkommende nedstammer fra Anders And eller fra Fedtmule. Denne sidste oplysning er jo lige præcis den oplysning, som vi gerne fremadrettet vil kunne forudsige1. I træningsdata angiver vi altså den værdi, som vi gerne vil prædiktere. Derfor kalder man også denne værdi for en targetværdi.\n1 Man siger også, at vi gerne vil prædiktere hvilken slags indbygger, der er tale om.\n\n\n\n\n\nOpgave 2: Træningsdata\n\n\n\n\n\nNedenstående viser en tabel med træningsdata, men targetværdien mangler. Angiv targetværdien:\n\n\n\n\n\n\n\n\n\nIndbygger\nFodareal (\\(cm^2\\))\nHøjde (\\(cm\\))\nTargetværdi\n\n\n\n\n\n197\n123\n\n\n\n\n214\n155\n\n\n\n\n255\n115\n\n\n\n\n297\n96\n\n\n\n\n213\n74\n\n\n\n\n173\n138\n\n\n\n\n272\n115\n\n\n\n\n235\n94\n\n\n\n\n311\n99\n\n\n\n\n334\n116\n\n\n\n\n276\n151\n\n\n\n\n283\n92\n\n\n\n\n234\n132\n\n\n\n\n172\n97\n\n\n\n\n278\n74\n\n\n\n\n241\n75\n\n\n\n\n220\n62\n\n\n\n\n249\n86\n\n\n\n\n138\n96\n\n\n\n\n252\n93\n\n\n\n\n\n\n\n\n\nNærmeste naboer (kNN)\nDer findes en lang række af metoder til at lave klassifikation, som er det, vi har brug for her. Nogle af dem bruger meget avanceret matematik og enorme computerkræfter og kan anvendes til diagnosticering af sygdomme, klassificere dokumenter i forskellig typer, genkende objekter i billeder og videoer. Helt så avancerede metoder får vi dog ikke brug for her.\nVi vil i stedet fokusere på én af de simpleste, men alligevel effektive metoder til at klassificere observationer. Metoden kaldes på engelsk k-nearest neighbors eller på dansk k-nærmeste naboer, og forkortes ofte som kNN. kNN beror på den simple antagelse, at observationer, som er tæt på hinanden, også ligner hinanden. I vores eksempel vil det være, at indbyggere, som har cirka samme højde og fodareal, vil vi antage, hører til i den samme klasse.\nFor at bestemme hvilke naboer, der ligger tæt på hinanden, er vi nødt til at kunne beregne afstanden mellem to punkter. Du husker nok, at afstanden \\(d\\) mellem punktet \\(P(x_1,y_1)\\) og punktet \\(Q(x_2,y_2)\\) er\n\\[\nd = \\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\n\\] På figur 2 nedenfor ser I de træningsdata, som I selv angav en targetværdi for i den foregående opgave. Nogle af indbyggerne var det måske svært at afgøre oprindelsen af, men her ses den korrekte klassificering2.\n2 Man kan forestille sig, at en sådan klassificering er baseret på yderligere test og undersøgelser, som man normalvis ikke vil have til rådighed.\n\n\n\n\n\nFigur 2: Datasættet med fodareal ud af \\(x\\)-aksen og højde op af \\(y\\)-aksen. De røde punkter svarer til Fedtmule-indbyggere, mens de blå svarer til Anders And-indbyggere.\n\n\n\n\n\n\n\n\n\nOpgave 3: Afstande\n\n\n\n\n\nBeregn afstanden fra det grå punkt til de syv punkter, som er markeret i figur 3 herunder. Sørg for at skrive de beregnede afstande ned – du skal bruge dem senere!\n\n\n\n\n\n\n\n\n\nFigur 3: Et udsnit af data med et nyt gråt punkt indsat. De røde punkter svarer til Fedtmule-indbyggere, mens de blå svarer til Anders And-indbyggere.\n\n\n\nNår \\(k\\)-nærmeste naboer bruges til at klassificere en ny indbygger benyttes en flertalsafstemning (også kaldet majoritetsbeslutning). Det vil sige, at en ny indbygger bliver prædikteret til at tilhøre den klasse, som de fleste af indbyggerens \\(k\\)-nærmeste naboer tilhører. Hvis for eksempel \\(k=5\\), og vi har en ny indbygger, som vi gerne vil afgøre klassen for, så ser vi simpelthen på de 5 nærmeste naboer til denne indbygger og tæller op, hvilke klasser de tilhører. Hvis to af dem nedstammer fra Anders And og tre fra Fedtmule, så vil en flertalsafstemning sige, at den nye indbygger nok er i Fedtmule-klassen. Man kan komme ud for, at præcis halvdelen af de \\(k\\) nærmeste naboer nedstammer fra Anders And, mens præcis den anden halvdel nedstammer fra Fedtmule. I det tilfælde vil vi sige, at klassifikationen er uafklaret.\nDenne idé vil vi nu bruge.\n\n\n\n\n\n\nOpgave 4: Afstand til ny og ukendt indbygger\n\n\n\n\n\nI figur 3 svarer det grå punkt til en ny indbygger, som skal klassificeres, og de nærmeste naboer svarer til de punkter, som du lige har beregnet afstanden til. Baseret på en flertalsafstemning blandt de fem nærmeste naboer (det vil sige, at \\(k=5\\)) vil du så sige, at den nye indbygger stammer fra Fedtmule eller fra Anders And?\n\n\n\n\n\n\n\n\n\nOpgave 5: Flertalsafstemning for forskellige værdier af \\(k\\)\n\n\n\n\n\nDer er ingen, som siger, at vi skal se på de fem nærmeste naboer. Vi kan lige så godt se på dén nærmeste nabo, på de to nærmeste naboer eller på de tre nærmeste naboer. Vi vil nu se på, hvad der sker, hvis vi ændrer på antallet af nærmeste naboer \\(k\\).\nSe igen på figur 3 og de afstande, som du beregnede i opgave 3. Du skal nu for forskellige værdier af \\(k\\) afgøre, om den nye indbygger skal klassificeres som en Fedtmule- eller en Anders And-indbygger.\nUdfyld en tabel som nedenstående (hvis for eksempel der er tre ud af fire punkter, som er blå, skal andelen sættes til 3/4).\n\n\n\n\\(k\\)\nBlå/Rød/Uafklaret (prædiktion)\nAndel\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\nValg af \\(k\\) og testdata\nSom du har set ovenfor, vil forskellige valg af \\(k\\) give forskellige resultater. Så hvordan vælger vi mon den bedst mulige værdi af \\(k\\)? For at afgøre det vil vi opdele vores data i to dele: træningsdata og testdata. Det kunne for eksempel være sådan, at af alle de data vi har, så bruger vi 80% som træningsdata. Det er træningsdata, som vi bruger til at lave prædiktionen med. De resterende 20% af data vil vi lade være testdata, hvor vi bruger testdata til at måle, hvor nøjagtig vores algoritme er.\nIdéen er nu denne:\n\nVi vælger en værdi af \\(k\\) – det kunne for eksempel være \\(k=5\\).\nVi ser så på hver eneste indbygger i testdata og lader som om, at vi ikke kender denne indbyggers oprindelse. Det vil sige, at vi lader som om, at vi ikke kender targetværdien. Vi vil nu bruge træningsdata til at lave en prædiktion for denne indbygger baseret på den valgte værdi af \\(k\\). Men da vi jo i virkeligheden godt kender denne indbyggers oprindelse, så får vi nu mulighed for at afgøre, om prædiktionen er rigtig eller forkert.\n\nLad os forestille os, at vi har 500 data i alt, og at vi lader 20% af disse være testdata. Det vil sige, at testdata består af 100 datapunkter. For hvert af disse datapunkter laver vi en prædiktion. Så enten prædikterer vi, at datapunktet er blåt eller rødt baseret på en flertalsafstemning af de \\(k\\) nærmeste naboer i træningsdatasættet. Holder vi denne prædiktion op mod den faktiske værdi, kan vi opstille en såkaldt confusion tabel. Et eksempel på en sådan ses her:\n\n\n\n\nPrædikteret blå\nPrædikteret rød\n\n\n\n\nFaktisk blå\n41\n4\n\n\nFaktisk rød\n2\n53\n\n\n\n\nVi kan her se, at 41 datapunkter blev prædikteret til at være blå og faktisk også er blå. Tilsvarende er 53 af datapunkterne prædikteret til at være røde, mens de faktiske også er røde. I alt 2+4=6 datapunkter har fået prædikteret en forkert farve sammenlignet med deres faktiske farve. Altså kan vi her se, at med den valgte værdi af \\(k\\) har vores kNN algoritme lavet en fejl i 6% af tilfældene, mens den har prædikteret korrekt i 94% af tilfældene. Vi kan nu lave tilsvarende beregninger for forskellige værdier af \\(k\\) og helt enkelt vælge den værdi af \\(k\\), som giver den mindste fejlprocent, når vi tester på vores testdata.\nVi ser nu igen på vores simple datasæt, og deler det op i et træningsdatasæt og et testdatasæt. På figur 4 er testdata markeret med et kryds. Vi vil for forskellige værdier af \\(k\\) prædiktere farven på testdata (samtidig med at vi jo godt kender den faktiske værdi).\n\n\n\n\n\n\nFigur 4: Testdata er markeret med et kryds.\n\n\n\n\n\n\n\n\n\nOpgave 6: Valg af \\(k\\)\n\n\n\n\n\n\nBrug app’en herunder til at afgøre den prædikterede værdi af hvert testdata for \\(k=1, 2, 3, 4, 5\\). Du kan for hvert testdatapunkt få tegnet en cirkel rundt om (hvor du kan justere på radius), som kan hjælpe dig med at finde de nærmeste naboer. Udfyld en tabel som nedenstående (husk at den prædikterede farve kan være blå, rød eller uafklaret) – skriv den enten ned på et stykke papir eller brug Prædiktion for forskellige værdier af k.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestdata\nFaktisk\nPrædiktion \\(k=1\\)\nPrædiktion \\(k=2\\)\nPrædiktion \\(k=3\\)\nPrædiktion \\(k=4\\)\nPrædiktion \\(k=5\\)\n\n\n\n\n1\nRød\n\n\n\n\n\n\n\n2\nRød\n\n\n\n\n\n\n\n3\nBlå\n\n\n\n\n\n\n\n4\nBlå\n\n\n\n\n\n\n\n\n\nUdfyld for hver værdi af \\(k\\) en confusion tabel. Hvis den prædikterede farve er uafklaret, skal det tælle som en fejl. Skriv igen ned på papir eller brug Confusion tabeller.\nUdregn for hver værdi af \\(k\\) fejlprocenten. Hvilken værdi af \\(k\\) giver den mindste fejlprocent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonusopgave (svær og kan springes over): Hvilke muligheder er der?\n\n\n\n\n\nSe på tabellen i opgave 5 Flertalsafstemning for forskellige værdier af \\(k\\)  ovenfor.\n\nFår man altid den samme prædiktion for alle værdier af \\(k\\)?\n\nVi forestiller os nu, at vi har et nyt datasæt, som vi ikke har lavet beregninger på.\n\nEr det muligt, at der kommer til at stå blå ved \\(k=1\\) og samtidig rød ved \\(k=2\\)?\nFor hvilke værdier af \\(k\\) kan der stå uafklaret?\nHvilke mulige andele kan optræde i tabellen for \\(k=1, 2, 3, 4, 5\\)?\nPrøv at opstille alle muligheder for tabeller (for \\(k=1, 2, 3, 4\\)).\n\n\n\n\n\n\nData mining i Orange\nHer til sidst skal vi lege lidt med et program, som hedder Orange. Du kan få hjælp til at installere programmet her.\nStart med at se denne video:\n\n\n\n\n\n\n\nOpgave 7: kNN i Orange baseret på features\n\n\n\n\n\n\nInstaller Orange.\nOpbyg modellen, som det er vist i videoen ovenfor. For at gøre det får du brug for de træningsdata og testdata, som vi har brugt i det foregående.\nPrøv at ændre på værdien af \\(k\\) (\\(k=1, 2, 3, 4, 5\\)) og se om du i Orange kan genskabe resultaterne fra opgave 6.\n\n\n\n\nDet er også muligt at bruge kNN uden selv at udvælge features. I stedet kan man bruge billederne fra tabellen i opgave 2 direkte. Se her hvordan man gør det:\n\n\n\n\n\n\n\nOpgave 8: kNN i Orange baseret på billeder\n\n\n\n\n\n\nFind selv på en værdi som du gerne vil prædiktere ud fra billeder.\nTag billeder som kan bruges som træningsdata og opbyg en model, som det er vist i videoen ovenfor."
  },
  {
    "objectID": "undervisningsforloeb/klimaudfordring_innovation.html",
    "href": "undervisningsforloeb/klimaudfordring_innovation.html",
    "title": "Miljø- og klimaudfodringer",
    "section": "",
    "text": "Forudsætninger og tidsforbrug\n\n\n\n\n\nForløbet er et tværfagligt samarbejde med dansk og kræver kendskab til:\n\nPopulærvidenskabelige artikler som genre\n\nTidsforbrug: ca. 4 x 90 minutter i hvert fag."
  },
  {
    "objectID": "undervisningsforloeb/klimaudfordring_innovation.html#opgaveformulering",
    "href": "undervisningsforloeb/klimaudfordring_innovation.html#opgaveformulering",
    "title": "Miljø- og klimaudfodringer",
    "section": "Opgaveformulering",
    "text": "Opgaveformulering\n\nRedegør for dele af matematikken bag kunstig intelligens og præsenter et forslag til en mulig innovativ anvendelse af kunstig intelligens indenfor emnet ”klimaforandringer”.\nRedegør i den forbindelse for hvilket trænings- og testdata, der skal indsamles, og for fordelene ved den efterfølgende potentielle anvendelse af kunstig intelligens.\nSkriv en populærvidenskabelig artikel om jeres emne (omfang: 3-4 sider).\nBegrund jeres valg i forbindelse med layout, sprogbrug samt nyheds- og relevanskriterier og vurdér hvordan artiklen kan bidrage til nye indsigter og handlemuligheder.\n\nMateriale i dansk: Changemaker-modellen.\nMateriale i matematik: Forskellige AI metoder."
  },
  {
    "objectID": "sro.html",
    "href": "sro.html",
    "title": "SRO",
    "section": "",
    "text": "I arbejdet med studieretningsopgaven kan matematik og AI indgå i et samarbejde med en lang række andre fag. Konkrete forløb er beskrevet herunder."
  },
  {
    "objectID": "sro.html#samfundsfag-og-matematik",
    "href": "sro.html#samfundsfag-og-matematik",
    "title": "SRO",
    "section": "Samfundsfag og matematik",
    "text": "Samfundsfag og matematik\n\n\n\n\n\n\nUlighed\n\n\n\n\n\nDer tages afsæt i følgende holdninger til ulighed uden at sætte partier på:\n\n”Blå blok”: Øget ulighed er en drivkraft for øget vækst, som giver øget velstand for alle.\n”Rød blok”: Større lighed er et kendetegn ved de bedst fungerende demokratier og lykkeligste samfund. Det giver samtidig de bedste muligheder for alle og samlet set de bedste rammevilkår for virksomheder.\n\n\nProblemformulering\nHvad er ulighed, og er det et problem i Danmark?\n\nRedegør kort for begrebet ulighed - herunder holdninger til ulighed.\nForklar hvordan kunstig intelligens kan bruges ved kandidattests i forbindelse med valg og lav en simpel kandidattest ud fra nogle få velvalgte spørgsmål om aspekter af ulighed, som skal give en anbefaling om at stemme på enten rød eller blå blok.\nKom desuden ind på forskellige matematiske mål for ulighed herunder Gini-koefficienten.\nDiskussionsspørgsmålet er op til jer (Måske kan uligheden begrænses? Skal den begrænses? Hvordan kan den begrænses? Fordele og ulemper ved ulighed og så videre).\n\n\n\nMaterialer\nNoten om perceptroner.\nPerceptron app - under udarbejdelse.\nJensby, Jakob & Brøndum, Peter (2020): Ulighedens mange ansigter."
  }
]