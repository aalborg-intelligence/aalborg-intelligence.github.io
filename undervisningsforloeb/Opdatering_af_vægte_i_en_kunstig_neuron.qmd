---
title: "Opdatering af vægte i en kunstig neuron"
description-meta: 'En øvelse i at opdatere vægtene i en kunstig neuron.'
image: "../materialer/kunstige_neuroner/images/simplet_netvaerk1.png"
categories:
  - A-niveau
  - Kort
---

::: {.callout-caution collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug

+ Simple neurale netværk.
+ Sigmoid aktiveringsfunktion.
+ Gradientnedstigning.

Forudsætningerne kan f.eks. dækkes vha. [noten om kunstige neuroner](../materialer/kunstige_neurokunstige_neuroner.qmd){target="_blank"}.

**Tidsforbrug:** Ca. 90 minutter.

:::

::: {.purpose}

### Formål

Gennem detaljerede beregninger at forstå, hvordan vægtene i et simpelt neuralt netværk opdateres med brug af sigmoid aktiveringsfunktionen og gradientnedstigning.

Dette kan ses som et skridt på vejen til at forstå, hvordan vægtene opdateres i et generelt neuralt netværk.

:::

## Et meget lille datasæt

I neurale netværk er der ofte rigtige mange features, rigtigt mange vægte og rigtige mange datapunkter. 

For bedre at forstå, hvor vægtene opdateres i et simpelt neuralt netværk, vil vi her se på et meget lille eksempel, så det mere manuelt er muligt at lave opdateringen af vægtene.

Konkret vil vi se på to features $x_1$ og $x_2$ og en targetværdi $t$ ud fra følgende 3 datapunkter.

|$x_1$ | $x_2$ | $t$ |
|:---:|:---:|:---:|
| -1 | 2 | ja = 1 |
| 0 | 4 | nej = 0 |
| 1 | 7 | ja = 1 |

Vi vælger en learning-rate på $$\eta = 0.1$$ 
*sigmoid-funktionen* som aktiveringsfunktion $$\sigma(x)=\frac{1}{1+e^{-x}}$$ 
og *squared-error* som tabsfunktion $$E(w_0, w_1, w_2) = \frac{1}{2} \sum_{m=1}^{3} \left (t^{(m)}-\sigma(w_0 + w_1 \cdot x_1^{(m)} + w_2 \cdot x_2^{(m)}) \right)^2$$ 
Endeligt vælger vi startvægtene $$w_0=0.1, w_1=0.1, w_2=0.1$$.

## Første opdatering af vægtene

Du skal nu gå gennem de enkelte beregninger, som skal til for at opdatere vægtene. Der er efterfølgende facit til hver del.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 1: Beregn værdien af tabsfunktionen
* Udregn først $s^{(1)} = w_0 + w_1 \cdot x_1^{(1)} + w_2 \cdot x_2^{(1)}$ for det første datapunkt.
* Udregn så $o^{(1)} = \sigma (s^{(1)})$.
* Udregn derefter $e^{(1)} = (t^{(1)} - o^{(1)})^2$, hvilket giver det første punkts bidrag til tabsfunktionen.
* Gentag for hver af de 2 øvrige datapunkter.
* Læg de 3 beregnede værdier sammen og divider med 2, så $E=\frac{1}{2} \cdot (e^{(1)}+e^{(2)}+e^{(3)})$.

:::


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 2: Beregn de partielle afledede

* Beregn $$\frac{\partial E}{\partial w_0} = - \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \cdot 1$$
* Beregn $$\frac{\partial E}{\partial w_1} = - \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \cdot x_1^{(m)}$$
* Beregn $$\frac{\partial E}{\partial w_2} = - \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \cdot x_2^{(m)}$$

:::


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 3: Opdater vægtene

Beregn de opdaterede vægte
$$
\begin{aligned}
w_0 \leftarrow & w_0 - \eta \cdot \frac{\partial E}{\partial w_0}   \\
w_1 \leftarrow & w_1 - \eta \cdot \frac{\partial E}{\partial w_1}  \\
w_2 \leftarrow & w_2 - \eta \cdot \frac{\partial E}{\partial w_2} 
\end{aligned}
$$

:::


## Anden opdatering af vægtene
Overvej, om du kan strømline dine beregninger, evt. i Excel eller i dit CAS værktøj, så det bliver hurtigere at opdatere vægtene en gang mere på samme måde.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 4: Opdater vægtene anden gang

* Beregn tabsfunktionen.
* Beregn de opdaterede vægte.

:::

Bemærk, at værdien af tabsfunktionen er blevet lidt mindre. Formålet er jo netop at minimere den gennem gradientnedstigning, så som regel bør værdien bliver mindre, hver gang vægtene opdateres.


## App til simple neurale netværk
Med [perceptron app'en](../apps/perceptron_app.qmd){target="_blank"} kan du lave de samme beregninger bare automatisk.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 5: Afprøv perceptron app'en

* Lav et Excelark med feature- og targetværdierne.
* Indlæs data fra Excelarket i perceptron app'en.
* Vælg de korrekte værdier i felterne på app'en. Herunder skal feature-skalering være slået fra.
* Se om app'en giver samme resultater for tabsfunktionen og vægtene, som du selv fik efter 1 iteration og efter 2 iterationer.

:::

## Cross-entropy som tabsfunktion
Du kan læse om *cross-entropy* i [noten om tabsfunktioner](../materialer/tabsfunktioner/tabsfunktioner.qmd#cross-entropy){target="_blank"}.

::: {.callout-note collapse="false" appearance="minimal"}
### (Ekstra) Opgave 6: Cross-entropy tabsfunktion 
* Gentag opgave 1-5 men med *cross-entropy* som tabsfunktion i stedet for *squared-error*.

:::


## Løsninger til opgaver
[Facitliste](Opdatering_af_vægte_i_et_simpelt_neuralt_netværk_løsninger.qmd){target="_blank"}.

