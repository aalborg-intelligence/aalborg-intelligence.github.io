---
title: "Tabsfunktioner"
image: ""
description-meta: "I langt de fleste tilfælde sker træning af AI modeller ved at minimere en tabsfunktion. Hvad tabsfunktioner er for en størrelse, skal du lære om i dette forløb."
from: markdown+emoji
format:
    html:
      self-contained: true 
      toc: true
      toc-title: Indhold
      toc-location: left
      related-formats-title: "Andre formater"
      link-external-newwindow: true
#    pdf: default
reference-location: margin
editor_options: 
  chunk_output_type: console
categories:
  - A-niveau
crossref:
  fig-prefix: figur   # (default is "Figure")
  tbl-prefix: tabel    # (default is "Table")
  exm-prefix: eksempel
  thm-prefix: sætning
  sec-prefix: afsnit
  eq-prefix: ''
  fig-title: Figur
  exm-title: Eksempel
  thm-title: Sætning
  tbl-title: Tabel
label:
    fig: Figur
fig-cap-location: margin
tab-cap-location: margin
execute:
  echo: false
  warning: false
---

::: {.callout collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug
Forløbet kræver kendskab til:

+ Differentialregning herunder differentiation af sammensatte funktioner.

**Tidsforbrug:** Ca. 1-2 x 90 minutter.

:::


::: {.purpose}

### Formål

I langt de fleste tilfælde sker træning af AI modeller ved at minimere en tabsfunktion. Lidt løst sagt kan man sige, at en tabsfunktion måler, hvor god en AI model er til at forudsige det, vi gerne vil have den til at sige noget om. 

I dette forløb kommer du til at lære noget om, hvordan tabsfunktioner kan se ud og hvad træningsdata er for en størrelse.

:::

## Under udarbejdelse!
Husk henvisning til forløb om aktiveringsfunktioner




## Tabsfunktion, targetværdier og aktiveringsfunktioner

På engelsk bliver tabsfunktioner også kaldet for *error functions*. Overordnet set vil vi gerne have en AI model med så lille et tab eller så lille en fejl som muligt. Derfor skal tabsfunktionen minimeres. 

Når tabsfunktionen skal minimeres bruger man såkaldte **træningsdata**. Det betyder, at man i træningsdata har givet en række **inputværdier** $x_1, x_2, \dots, x_n$ på baggrund af hvilke, man ønsker at prædiktere en såkaldt **targetværdi** $t$, som også er en del af træningsdata. 

Lad os tage et eksempel fra den virkelige verden. Vi vil gerne på baggrund af en blodprøve kunne prædiktere om en patient har kræft eller ej. Her kan inputværdierne $x_1, x_2, \dots, x_n$ være forskellige ting, man måler i blodet (spørg en biologilærer om hvad det kunne være). Targetværdien $t$ kan antage to værdier:

$$
t=
\begin{cases}
1 & \textrm{hvis patienten har kræft} \\
0 & \textrm{hvis patienten ikke har kræft} \\
\end{cases}
$$
Man kunne også have valgt:
$$
t=
\begin{cases}
1 & \textrm{hvis patienten har kræft} \\
-1 & \textrm{hvis patienten ikke har kræft} \\
\end{cases}
$$
Eller noget helt tredje! Det kommer vi tilbage til senere. Lad os for nu sige at vi vælger den første mulighed, hvor $t \in \{0,1\}$. 

At bestemme, om targetværdien er $0$ eller $1$,  beror på faglig ekspertise indenfor det genstandsfelt, hvor AI modellen skal anvendes. I eksemplet med kræft vil det for eksempel være baseret på forskellige diagnostiske tests, som en læge kan bruge til at vurdere, om patienten har kræft[^3].

[^3]: Måske er disse tests først taget et stykke tid efter blodprøven, fordi det ikke er muligt at stille diagnose på tidspunktet for blodprøven. I så fald kan man måske være heldig at få udviklet en AI model, som kan prædiktere kræft *tidligere* end med gængse metoder. 

** Lille data eksempel her med én inputværdi **


Man \"fodrer\" så sin AI algoritme med en hel masse inputværdier med tilhørende targetværdier og finder den AI model, som giver den mindste værdi af tabsfunktionen.
Det kaldes for *supervised learning*.

I AI modellen vil det typisk være sådan, at de forskellige inputværdier vægtes med en række vægte $w_0, w_1, \dots, w_p$. Når AI modellen trænes, er det dybest set bare disse vægte, man \"skruer\" på, sådan at modellen bliver god til at prædiktere det, den er trænet på. For store kunstige neurale netværk -- for eksempel de store sprogmodeller -- taler vi om milliarder af vægte! 

Vi kan altså tænke på en AI model, som en funktion $f$, der afhænger af vægtene $w_0, w_1, \dots, w_p$:

$$
f(w_0, w_1, \dots, w_p).
$$
Funktionen afhænger selvfølgelig også af hele træningsdatasættet, men eftersom det er vægtene, man skal justere, alt imens træningsdata er fastlagt, vil vi blot tænke på $f$ som en funktion af vægtene.

Vi forestiller os nu, at vi har bestemt minimum for tabsfunktionen og dermed fundet de værdier af vægtene, som minimerer tabsfunktionen. Det er nu disse værdier, som giver os vores endelige AI model. Spørgsmålet er nu, hvordan den bruges til prædiktion. Det kommer her:

Ofte vil værdimængden for $f$ være $(0,1)$. Det betyder, at vi kan tolke værdien af $f$ som en sandsynlighed. Vi forestiller os, at vi får et nyt sæt af inputværdier -- for eksempel målingerne fra en ny blodprøve, og vi vil gerne finde ud af, om patienten har kræft eller ej. Disse værdier \"sendes\" nu ind i funktionen $f$ og ud kommer en ouputværdi, som vi vil kalde for $o$. Værdien af $o$ betragtes nu som sandsynligheden for, at den rigtige targetværdi er $1$. Det kunne for eksempel være sådan her:

$$
\textrm{prædiktion}=
\begin{cases}
\textrm{patienten har kræft} & \textrm{hvis } o \geq 0.5 \\
\textrm{patienten har ikke kræft} & \textrm{hvis } o < 0.5 \\
\end{cases}
$$

Men hvis det skal give mening, så kræver det altså, at vi har fundet de værdier af vægtene, som gør, at denne prædiktion rent faktisk bliver god. 

Vi har hele tiden sagt, at det gør vi ved at minimere tabsfunktionen, men det kræver jo, at vi har en tabsfunktion. Ofte kan man bruge denne **tabsfunktion**:

$$
E = \frac{1}{2} \sum \left (t-o \right)^2,
$$ {#eq-tabsfunktion}

<!-- 
$$
\begin{aligned}
E(w_0, w_1, &\dots, w_n) \\ &= \frac{1}{2} \sum_{m=1}^{M} \left (t_m-
f(w_0 + w_1 \cdot x_{m,1} + \cdots + w_n \cdot x_{m,n}) \right)^2,
\end{aligned}
$$
-->

hvor der summeres over alle træningsdata. For det første kan vi se, at $E \geq 0$, fordi der er tale om en kvadreret sum. For det andet kan vi se, at hvis AI modellen er god, så vil den beregnede sandsynlighed $o$, for at patienten har kræft være tæt på $1$, når $t=1$, og $o$ vil være tæt på $0$, når $t=0$. Det betyder, at de kvadrerede forskelle $(t-o)^2$ i det tilfælde vil være små, og dermed vil tabsfunktionen også være lille. Altså lever $E$ op til de krav, vi stiller til en tabsfunktion. 

Outputværdien $o$ beregnes ofte som

$$
o = f(w_0+w_1x_1+w_2x_2+\cdots+w_nx_n),
$$
 
hvor $f$ er den funktion, som vi kalder for en **aktiveringsfunktion**. For perceptroner og simple neurale netværk svarer $x_1, x_2, \dots, x_n$ direkte til inputværdierne, som AI modellen i sidste ende skal virke på. I et kunstig neuralt netværk er det mere kompliceret, men ovenstående er korrekt, hvis man tænker på $x_1, x_2, \dots, x_n$, som de værdier neuronerne i det sidste skjulte lag sender videre til outputlaget.

Hvis tabsfunktionen i (@eq-tabsfunktion) skal give mening, så kan vi nu se, at hvis targetværdien $t \in \{0,1\}$, så bør værdimængden for $f$ tilsvarende være $(0,1)$. Mens hvis $t \in \{-1,1\}$, så bør[^2] værdimængden for $f$ tilsvarende være $(-1,1)$. Forestiller man sig, at targetværdien er huspriser -- det vil sige, at $t \in (0,\infty)$ -- ja så vil det give mening, at også outputværdien $o \in (0,\infty)$. Det vigtige er altså, at værdimængden for aktiveringsfunktionen er på samme skala som targetværdierne. 

 
[^2]: Når en perceptron trænes ved hjælp af Adaline, er dette faktisk ikke tilfældet. Her er targetværdien $t \in \{-1,1\}$, mens outputværdien $o \in \mathbb{R}$. Det kan give nogle problemer, som er beskrevet i noten om [simple neurale netværk](../simple_neurale_net/simple_neurale_net.qmd).

Når AI modellen trænes ved at finde minimum for tabsfunktionen, så gøres det ofte ved hjælp af [gradientnedstigning](../materialer/gradientnedstigning/gradientnedstigning.qmd) -- men den konkrete metode er ikke så vigtig lige nu. Det vigtige er at forstå, at når man skal finde minimum for en funktion, så har man brug for at kunne differentiere. Derfor er det vigtigt at kunne differentiere den anvendte aktiveringsfunktion $f$. 

Desuden viser det sig vigtigt, at det ikke er alt for beregningsmæssigt tungt at beregne funktionsværdierne $f(x)$ og $f'(x)$. Det skal simpelthen gøres så mange gange -- derfor dur det ikke, at det tager for lang tid. Det er således ønskværdigt, hvis en aktiveringsfunktions afledede funktion $f'(x)$ kan beregnes forholdvis simpelt ved hjælp af $f(x)$. Det betyder nemlig, at hvis vi allerede har udregnet $f(x)$, så kræver det ikke ret meget også at udregne $f'(x)$.    

I det nedenstående vil vi nu behandle en række af de mest anvendte aktiveringsfunktioner. Vi finder deres afledede funktioner, og vi vil se, hvordan de afledede funktioner alle kan udtrykkes ved hjælp af den oprindelig aktiveringsfunktion.


## Sigmoid

**Sigmoid**-funktionen har forskrift

$$
f(x)=\frac{1}{1+e^{-x}}.
$$

Grafen for Sigmoid-funktionen ses i @fig-sigmoid.

![Grafen for sigmoid-funktionen.](aktiveringsfunktioner/sigmoid.png){width=75% #fig-sigmoid}

Det ser på @fig-sigmoid ud som om, at værdimængden for $f$ er $(0,1)$. Hvis du vil have et lidt bedre argument for det, kan du læse i boksen herunder.

::: {.callout-tip collapse="true" appearance="minimal"}

## Argument for værdimængden for $f$

Vi vil her argumentere for, at værdimængden for $f$ er $(0,1)$.

På figuren herunder ses grafen for $e^{-x}$.

![](aktiveringsfunktioner/exp_minusx.png){width=50% fig-align="center"}

Da $e^{-x}$ er en aftagende eksponentialfunktion vil

$$
e^{-x} \rightarrow 0 \quad \textrm{når} \quad x \rightarrow \infty
$$
og

$$
e^{-x} \rightarrow \infty \quad \textrm{når} \quad x \rightarrow -\infty.
$$

Det betyder, at
$$
\frac{1}{1+e^{-x}} \rightarrow 1 \quad \textrm{når} \quad x \rightarrow \infty
$$

og

$$
\frac{1}{1+e^{-x}} \rightarrow 0 \quad \textrm{når} \quad x \rightarrow -\infty.
$$

Det vil sige, at værdimængden for $f$ er $(0,1)$.

:::

De følgende opgaver gå ud på at vise, at

$$
f'(x)= \frac{e^{-x}}{(1+e^{-x})^2}
$$
og at $f'(x)$ kan udtrykkes ved hjælp af $f(x)$ på denne måde

$$
f'(x)= f(x)\cdot (1-f(x)).
$$

::: {.callout-note collapse="true" appearance="minimal"}
### Opgave 1: Differentiation af sigmoid-funktionen

Vi skal vise, at
$$
f'(x)= \frac{e^{-x}}{(1+e^{-x})^2}.
$$
Vi skal starte med at se, at vi kan tænke på sigmoid-funktionen
$$
f(x)=\frac{1}{1+e^{-x}}.
$$
som en \"dobbelt sammensat\" funktion. Sigmoid-funktionen består nemlig af en brøk på formen
$\frac{1}{x}$ og af eksponentialfunktionen $e^{-x}$. 

Derfor skal du:

* Start med at opskrive differentialkvotienten for 
$$\frac{1}{x} \quad \textrm{og} \quad e^{-x}.$$

* Brug ovenstående til at vise, at
$$
f'(x)= \frac{e^{-x}}{(1+e^{-x})^2}.
$$

:::

::: {.callout-note collapse="true" appearance="minimal"}
### Opgave 2: Differentiation af sigmoid-funktionen -- omskrivning af $f'(x)$

Vi skal nu vise, at
$$
f'(x)= f(x)\cdot (1-f(x)).
$$
når 
$$
f(x)=\frac{1}{1+e^{-x}}.
$$

* Start med at udregne 
$$1-f(x).$$
*Hint! Sæt på fælles brøkstreg ved at skrive $1$ som $\frac{1+e^{-x}}{1+e^{-x}}$*.

* Vis nu at
$$
f(x)\cdot (1-f(x)) = \frac{e^{-x}}{(1+e^{-x})^2}=f'(x).
$$
*Husk, at man ganger to brøker med hinanden ved at gange tæller med tæller og nævner med nævner.*

:::

## Softsign

**Softsign**-funktionen har forskrift

$$
f(x)=\frac{x}{1+|x|}.
$$
Husk på at $|x|$ betyder den numeriske værdi af $x$. Det vil sige

$$
|x| = 
\begin{cases}
x & \textrm{hvis } x \geq 0 \\ 
-x & \textrm{hvis } x < 0 \\ 
\end{cases}
$$ {#eq-numerisk_x}
Det betyder for eksempel at $|7|=7$ og $|-7|=7$. Grafen for $|x|$ ses i @fig-numerisk_x.

![Grafen for $|x|$.](aktiveringsfunktioner/numerisk_x.png){width=75% #fig-numerisk_x}

Grafen for softsign-funktionen $f$ ses i @fig-softsign.

![Grafen for softsign-funktionen.](aktiveringsfunktioner/softsign.png){width=75% #fig-softsign}

Da den numeriske værdi af $x$ indgår i forskriften, kunne man få den tanke, at $f$ måske hverken er kontinuert eller differentiabel i $0$. For eksempel kan man i @fig-numerisk_x se, at $|x|$ ikke er differentiabel i $0$. 

Men bruger vi definitionen på $|x|$, får vi

$$
f(x) = 
\begin{cases}
\frac{x}{1+x} & \textrm{hvis } x \geq 0 \\
\\
\frac{x}{1-x} & \textrm{hvis } x < 0 \\
\end{cases}
$$ {#eq-def_softsign}

Ud fra denne omskrivning kan man vise, at $f$ rent faktisk er kontinuert i $0$. Det kan du læse mere om i boksen herunder, hvis du har lyst.

På @fig-softsign ser det ud som om, at værdimængden for $f$ er $(-1,1)$ (også det argumenterer vi for i boksen). Det vil sige, at hvis vi skal bruge softsign-funktionen som aktiveringsfunktion, så skal targetværdierne være $\pm 1$.



::: {.callout-tip collapse="true" appearance="minimal"}

## Argument for kontinuitet i $0$ og værdimængde for $f$

Lad os først argumentere for, at $f$ er kontinuert i $0$. For det første ser vi, at $f(0)=0/(1+0)=0$ og $f(x) \rightarrow 0$, når $x$ nærmer sig $0$ både fra højre og venstre. Det betyder, at $f$ *er* kontinuert i $0$.

Vi ser også, at for store positive værdier af $x$ vil
$$
f(x)= \frac{x}{1+x} \approx \frac{x}{x}=1
$$
og for store negative værdier af $x$ vil 
$$
f(x)= \frac{x}{1-x} \approx \frac{x}{-x}=-1
$$
Det betyder, at 
$$
f(x) \rightarrow 1 \quad \textrm{når} \quad x \rightarrow \infty
$$
og 

$$
f(x) \rightarrow -1 \quad \textrm{når} \quad x \rightarrow - \infty
$$
hvilket stemmer fint overens med @fig-softsign. 

:::

I nedenstående opgaver skal vi vise, at

$$
f'(x)=\frac{1}{\left ( 1+ |x| \right )^2}
$$ {#eq-softsign_diff1}

og at den afledte kan findes ved hjælp af funktionsværdien selv på denne måde

$$
f'(x)=(1-|f(x)|)^2.
$$ {#eq-softsign_diff2}


::: {.callout-note collapse="true" appearance="minimal"}
### Opgave 1: Differentiation af softsign-funktionen

For at vise at
$$
f'(x)=\frac{1}{\left ( 1+ |x| \right )^2}
$$
vil vi starte med at bruge en brøkregneregel til at omskrive funktionsudtrykket i (@eq-def_softsign):

$$
f(x) = 
\begin{cases}
x \cdot \frac{1}{1+x} & \textrm{hvis } x \geq 0 \\
\\
x \cdot \frac{1}{1-x} & \textrm{hvis } x < 0 \\
\end{cases}
$$ {#eq-def_softsign2}



* Antag først, at $x > 0$ og vis ved hjælp af produktreglen for differentiation, at
$$
f'(x)=\frac{1}{(1+x)^2} = \frac{1}{(1+|x|)^2}.
$$
*OBS! Du får på et tidspunkt brug for at sætte på fælles brøkstreg -- fællesnævneren er her $(1+x)^2$.*

* Antag nu at $x<0$ og vis igen ved hjælp af produktreglen for differentiation at
$$
f'(x)=\frac{1}{(1-x)^2} = \frac{1}{(1+|x|)^2}.
$$

* Tegn grafen for $f'$. Synes du, at det ser ud som om, at $f'$ er differentiabel?
:::


::: {.callout-note collapse="true" appearance="minimal"}
### Opgave 2: Differentiation af softsign-funktionen -- omskrivning af $f'(x)$

Vi vil nu vise, at den afledede softsign-funktion kan udtrykkes ved hjælp af softsign-funktionen selv:
$$
f'(x)=(1-|f(x)|)^2.
$$

* Start med at overvise dig selv om, at
$$
|f(x)|=f(|x|)
$$
ved at bruge definitionen i (@eq-numerisk_x).

* Vis at 
$$
(1-f(|x|))^2 = \frac{1}{(1+|x|)^2}=f'(x)
$$
*Hint! Skriv $1$ som $\frac{1+|x|}{1+|x|}$.*

:::

## Hyperbolsk tangens

Funktionen hyperbolsk tangens, $\tanh$, har forskrift
$$
\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$


Grafen for hyperbolsk tangens er vist i @fig-tanh.

![Grafen for hyperbolsk tangens.](aktiveringsfunktioner/tanh.png){width=75% #fig-tanh}

Ifølge figuren ser det ud til, at $Vm(f)=(-1,1)$. Det argumenterer vi nærmere for i boksen herunder.

::: {.callout-tip collapse="true" appearance="minimal"}

## Argument for værdimængden for $\tanh$

På figuren herunder ses grafen for den voksende eksponentialfunktion $e^x$ (blå) og for den aftagende eksponentialfunktion $e^{-x}$ (grøn).

![](aktiveringsfunktioner/exp_plusminusx.png){width=50% fig-align="center"}

Her ses det, at for store positive værdier af $x$ er $e^{-x} \approx 0$. Det vil sige, at for store positive værdier af $x$ er

$$
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} \approx \frac{e^x-0}{e^x+0}=\frac{e^x}{e^x}=1.
$$

Omvendt gælder for store negative værdier af $x$ er $e^x \approx 0$. Det vil sige, at for store negative værdier af $x$ er

$$
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} \approx \frac{0-e^{-x}}{0+e^{-x}}=\frac{-e^{-x}}{e^{-x}}=-1.
$$
Det betyder, at 
$$
\tanh(x) \rightarrow 1 \quad \textrm{når} \quad x \rightarrow \infty
$$
og 

$$
\tanh(x) \rightarrow -1 \quad \textrm{når} \quad x \rightarrow - \infty
$$
hvilket stemmer fint overens med @fig-tanh. 

:::

I nedenstående opgave skal vi vise, at $\tanh$ differentieret er

$$
\tanh'(x)=1-\left ( \tanh(x) \right )^2. 
$$

For at bevise det er det nemmeste at bruge kvotientreglen for differentiation. Måske har du hørt om den -- måske har du ikke. Men her kommer den:

::: {.highlight }

**Kvotientreglen for differentiation **

$$
\left ( \frac{f(x)}{g(x)}\right)' = \frac{f'(x) \cdot g(x)-f(x) \cdot g'(x)}{(g(x))^2}, \quad g(x) \neq 0
$$
:::

::: {.callout-note collapse="true" appearance="minimal"}
### Opgave 1: Differentiation af softsign-funktionen og omskrivning

Vi skal vise, at tangens hyperbolsk
$$
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$
differentieret er
$$
\tanh'(x)=1-\left ( \tanh(x) \right )^2. 
$$

* Brug kvotientreglen for differentiation til at vise, at
$$
\tanh'(x)= 1 - \left (\frac{e^x-e^{-x}}{e^x+e^{-x}} \right)^2
$$
*Hint! På et tidspunkt får du brug for brøkregnereglen $\frac{a+b}{c}=\frac{a}{c}+\frac{b}{c}$.*

* Brug definitionen af tangens hyperbolsk til at indse at
$$
\tanh'(x)=1-\left ( \tanh(x) \right )^2. 
$$
:::


## ReLU

Aktiveringsfunktionen **ReLU** som står for **Reflected Linear Unit** har forskrift

$$
f(x) = 
\begin{cases}
x & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}
$$
og grafen for ReLU-funktionen ses i @fig-ReLU.

![Grafen for ReLU-funktionen.](aktiveringsfunktioner/ReLU.png){width=75% #fig-ReLU}

Værdimængden for ReLU-funktionen er $[0, \infty)$.

Det er ret tydeligt, at ReLU-funktionen ikke er differentiabel i $0$. Men hvis vi definerer, at 
$f'(0)$ skal være $0$ så ses det nemt, at

$$
f'(x) = 
\begin{cases}
1 & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}.
$$

ReLU-funktionen adskiller dig fra de andre aktiveringsfunktioner ved, at værdimængden er ubegrænset. Hvis man ønsker at bruge aktiveringsfunktionen til at modellere en sandsynlighed, som beskrevet tidligere, så dur det selvfølgelig ikke. Men i praktisk viser ReLU-funktionen sig at være utrolig anvendelig som aktiveringsfunktion i de skjult lag i kunstige neurale netværk. For det første kan nogle af de andre aktiveringsfunktioner resulterer i det, vi i afsnittet om [valg af tabsfunktion](../neurale_net/neurale_net.qmd#valg-af-tabsfunktion) i noten om kunstige neurale netværk, kalder for *slow learning*. Det betyder kort sagt, at det går for langsomt med at finde minimum for tabsfunktionen. Dét problem har ReLU-funktionen ikke. For det andet er det meget hurtigt og nemt at udregne både ReLU-funktionen selv og også dens afledede. Det er for eksempel til sammenligning beregningsmæssigt tungere, at udregne sigmoid-funktionen og dennes afledede. Hvis man har et netværk med millioner af neuroner, så er denne beregningsmæssige forskel ikke uvæsentlig.

For yderligere læsning henviser vi til referencerne i afsnittet [videre læsning](aktiveringsfunktioner.qmd#sec-videre).

## Overblik

I tabellen herunder finder du et overblik over de forskellige aktiveringsfunktioner, som vi har behandlet ovenfor.

| Navn | $f(x)$ | Graf | $Vm(f)$ | $f'(x)$ |
|:------:|:------:|:------:|:------:|:------:|:------:|
| Sigmoid | $\frac{1}{1+e^{-x}}$ | ![](aktiveringsfunktioner/sigmoid.png) | $(0,1)$ | $f(x)\cdot(1-f(x))$ |
| Softsign | $\frac{x}{1+|x|}$ | ![](aktiveringsfunktioner/softsign.png) | $(-1,1)$ | $(1-|f(x)|)^2$ | 
| Hyperbolsk tangens | $\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | ![](aktiveringsfunktioner/tanh.png) | $(-1,1)$ | $1-\left ( \tanh(x) \right )^2$| 
| ReLU | $\begin{cases}
x & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}$ | ![](aktiveringsfunktioner/ReLU.png) | $[0,\infty)$ | $\begin{cases}
1 & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}$| 

## Videre læsning {#sec-videre}

* [Activation Functions in Neural Networks: With 15 examples](https://encord.com/blog/activation-functions-neural-networks/)
* [RELU and SIGMOID Activation Functions in a Neural Network](https://www.shiksha.com/online-courses/articles/relu-and-sigmoid-activation-function/)