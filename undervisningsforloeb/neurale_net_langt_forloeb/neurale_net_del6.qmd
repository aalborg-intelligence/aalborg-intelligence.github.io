---
title: "Del 6: Andre tabsfunktioner"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

::: {.estimeret_tid}

Forventet tid ca. 1 x 90 min.

Hele denne del er en fortsættlse af eksemplet om \"slow learning\" og kan derfor springes over.

::: 

## Aktivitet 1 - Cross entropy tabsfunktionen {.aimat}

I [del 3](neurale_net_del3.qmd) så vi, at der kan opstå problemer med slow learning. Det opstår på grund af opdateringsreglerne:

$$
\begin{aligned}
w_0^{(\textrm{ny})} \leftarrow & w_0 + \eta \cdot \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \\
w_1^{(\textrm{ny})} \leftarrow & w_1 + \eta \cdot \sum_{m=1}^{3} \left (t^{(m)}-o^{(m)} \right) \cdot o^{(m)}\cdot (1-o^{(m)}) \cdot x^{(m)}
\end{aligned}
$$ {#eq-opdatering}

hvor man kan være i den uheldige situation, at man har fået startet gradientnedstigning i en $(w_0, w_1)$ værdi, hvor neuron er fejlagtigt \"mættet\". Det betyder, at den prædikterer en outputværdi $o$ tæt på $0$, selvom target er $1$ og omvendt, når target er $0$. I begge tilfælde vil opdateringsleddet i (@eq-opdatering) være tæt på $0$ -- enten fordi $o^{(m)}$ er tæt på $0$ eller fordi $1-o^{(m)}$ er tæt på $0$.

Vi skal i denne del se på, at det skyldes valget af *squared error* tabsfunktionen.

[Noten om tabsfunktioner](../../materialer/tabsfunktioner/tabsfunktioner.qmd)




::: {.callout-note collapse="false" appearance="minimal"}

### Opgave 1: 


:::
