---
title: "Opdatering af vægte i neuralt netværk med ét skjult lag"
description-meta: 'En øvelse i at opdatere vægtene i et neuralt netværk med ét skjult lag.'
image: "../undervisnings_forloeb/Opdaterng_af_vægte/simplet_netvaerk.png"
categories:
  - A-niveau
  - Kort
---

::: {.callout-caution collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug

+ Neurale netværk.
+ Sigmoid aktiveringsfunktion.
+ Gradientnedstigning.
+ Cross entropy tabsfunktion

Forudsætningerne kan med fordel være dækket vha. [noten om simple neurale netværk](../materialer/simple_neurale_netværk/simple_neurale_netværk.qmd){target="_blank"}, da notationen derfra vil blive anvendt i dette forløb, samt [noten om tabsfunktioner](..//materialer/tabsfunktioner/tabsfunktioner.html){target="_blank"}

Dette forløb ligner [forløbet om opdatering af vægte i neuralt netværk med 2 skjulte lag](../undervisningsforloeb/Opdatering_af_vægte_i_NN_med_2_skjulte_lag.qmd){target="_blank"}, men er lidt sværere, da man desuden skal modificere nogle elementer i forhold til noten om simple neurale netværk.

**Tidsforbrug:** Ca. 90 minutter.

:::

::: {.purpose}

### Formål

Gennem detaljerede beregninger at forstå, hvordan vægtene i et neuralt netværk til klassifikation med ét skjult lag opdateres med brug af sigmoid aktiveringsfunktionen og gradientnedstigning med cross entropy som tabsfunktion.

Dette kan ses som et skridt på vejen til at forstå, hvordan vægtene opdateres i et generelt neuralt netværk.

:::

## Et meget lille datasæt

I neurale netværk er der ofte rigtige mange features, rigtigt mange vægte og rigtige mange datapunkter. 

For bedre at forstå, hvor vægtene opdateres i et neuralt netværk, vil vi her se på et meget lille eksempel, så det mere manuelt er muligt at lave opdateringen af vægtene.

Vi vil lave et netværk med 2 neuroner i inputlaget $x$, 1 neuron i et skjult lag $y$ og 1 neuron i outputlaget $o$. Konkret vil vi se på to features $x_1$ og $x_2$ og en targetværdi $t$ ud fra følgende 3 datapunkter.

|$x_1$ | $x_2$ | $t$ |
|:---:|:---:|:---:|
| 1 | 2 | 0 |
| 2 | 3 | 1 |
| 3 | 7 | 0 |

Vi vælger en learning-rate på $$\eta = 0.1$$ 
*sigmoid-funktionen* som aktiveringsfunktion $$\sigma(x)=\frac{1}{1+e^{-x}}$$
og *cross-entropy* som tabsfunktion 
$$
E =  - \sum_{m=1}^{M} \left( (t^{(m)} \cdot \ln(o^{(m)}) + (1-t^{(m)}) \cdot \ln(1-o^{(m)}) \right) 
$$
Endeligt vælger vi startvægtene fra input til skjult lag som $$r_0=0.5 \ (bias), r_1=0.5, r_2=0.5$$

og fra skjult lag til outout som $$w_0=0.5 \ (bias), \ w_1 =0.5$$

## Plan
- Feed-forward fra $x$ laget til $y$ laget.
- Feed-forward fra $y$ laget til $o$ laget.
- Opskriv opdateringsregler for $w$-vægtene.
- Lav backprobagation på $w$-vægtene.
- Opskriv opdateringsregler for $r$-vægtene.
- Lav backprobagation på $r$-vægtene.

## Opgaver

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 1: Feed-forward fra $x$ til $y$ lag.
* Udregn $r_1 \cdot x_1^{(m)} + r_2 \cdot x_2^{(m)} + r_0$ for hver af de 3 punkter.
* Udregn $y^{(m)}=\sigma(r_1 \cdot x_1^{(m)} + r_2 \cdot x_2^{(m)} + r_0)$ for hver af de 3 punkter.

:::

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 2: Feed-forward fra $y$ til $o$ lag.
* Udregn på tilsvarende vis $o^{(m)}$ for hver af de 3 punkter. 

:::

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 3: Opskriv opdateringsregler for $w$-vægtene
* Opskriv opdateringsreglen. Husk, at tabsfunktionen er cross-entropy.

:::


::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 4: Backpropagation på $w$ vægte.
* Udregn $\delta_w^{(m)}$ vha. opdateringsreglen for hver af de 3 punkter.
* Udregn $\sum_{m=1}^{3} \delta_w^{(m)}$ for de 3 punkter.
* Udregn $w_0^{ny} = w_0 + \eta \cdot \sum_{m=1}^{3} \delta_w^{(m)}$
* Udregn $\sum_{m=1}^{3} \delta_w^{(m)} \cdot z^{(m)}$ for de 3 punkter.
* Udregn $w_1^{ny} = w_1 + \eta \cdot \sum_{m=1}^3 \delta_w^{(m)} \cdot z^{(m)}$
:::

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 5: Opskriv opdateringsregler for $r$-vægtene
* Opskriv opdateringsreglen.

:::

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 6: Backpropagation på $r$ vægte.
* Lav tilsvarende udregninger og opdatering af $r$ vægtene.

:::

### Opgave 7: Udregn værdien af tabsfunktionen
* Udregn værdien af tabsfunktionen inden opdateringen.
* Udregn værdien af tabsfunktionen efter opdateringen.
Værdien skulle meget gerne være blevet mindre med opdateringen.

## Anden opdatering af vægtene
Overvej, om du kan strømline dine beregninger, evt. i Excel eller i dit CAS værktøj, så det bliver hurtigere at opdatere vægtene en gang mere på samme måde.

::: {.callout-note collapse="false" appearance="minimal"}
### Opgave 8: Opdater vægtene anden gang

* Beregn de opdaterede vægte.
* Beregn tabsfunktionen på de opdaterede vægte.

:::

Bemærk, at værdien af tabsfunktionen er blevet lidt mindre. Formålet er jo netop at minimere den gennem gradientnedstigning, så som regel bør værdien bliver mindre, hver gang vægtene opdateres.


## App til simple neurale netværk
** mangler **
Her mangler noget med at bruge appen (der ikke er klar endnu) til at optimere vægtene, og efterfølgende lave prediktion.

:::


## Løsninger til opgaver
[Facitliste](Opdatering_af_vægte_i_NN1_facit.qmd){target="_blank"}.

