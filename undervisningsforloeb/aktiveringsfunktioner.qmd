---
title: "Aktiveringsfunktioner"
image: "aktiveringsfunktioner/aktiveringsfkt.jpg"
description-meta: "I opbygningen af kunstige neurale netværk er aktiveringsfunktioner helt centrale. Og aktiveringsfunktioner skal differentieres -- det handler dette forløb om."
categories:
  - A-niveau
  - Kort
---

::: {.callout-caution collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug
Forløbet kræver kendskab til:

+ Differentialregning herunder differentiation af sammensatte funktioner og produktreglen.

**Tidsforbrug:** Ca. 1-2 x 90 minutter alt efter hvor mange aktiveringsfunktioner, I ønsker at arbejde med. I kan også arbejde i grupper og lade hver gruppe arbejde med hver sin aktiveringsfunktion.

:::


::: {.purpose}

### Formål

I opbygningen af kunstige neurale netværk er aktiveringsfunktioner helt centrale. Hvis ikke man bruger aktiveringsfunktioner i et kunstigt neuralt netværk, vil man faktisk bare bygge en stor lineær funktion af inputværdierne. Og lineære funktioner kan man ikke prædiktere ret meget med!

Når man skal træne en [perceptron](../materialer/perceptron/perceptron.qmd), et [simpelt neuralt netværk](../materialer/simple_neurale_net/simple_neurale_net.qmd) eller et helt [generelt neuralt netværk](../materialer/neurale_net/neurale_net.qmd), skal man kunne differentiere de aktiveringsfunktioner, som indgår. I dette forløb vil vi arbejde med nogle af de mange forskellige aktiveringsfunktioner, som anvendes i den virkelige verden. Vi skal se på grafer og værdimængde -- og så skal vi mest af alt differentiere dem!

:::

## Introduktion

Når man træner en AI model, sker det som regel ved, at man forsøger at minimere de fejl, som modellen laver, når den anvendes på data, hvor man allerede kender svaret. 

For at blive lidt mere konkret så minimerer man en såkaldt **tabsfunktion** $E$ ($E$ for *error function*), som har til formål at \"måle\", hvor god en AI model[^1] er. En tabsfunktion $E$ har altid den egenskab, at $E \geq 0$, og at en lille værdi af $E$ svarer til en god model (der er et lille tab), mens en stor værdi af $E$ svarer til en mindre god model. Derfor vælger man den model, som giver den mindste værdi af tabsfunktionen. 

[^1]: Med AI model tænker vi her på en perceptron, et simplet neuralt netværk, et generelt neuralt netværk eller en anden form for funktion, som kan bruges til at prædiktere et eller andet.

AI modellen trænes altså ved at finde minimum for tabsfunktionen. Det gøres ofte ved hjælp af [gradientnedstigning](../materialer/gradientnedstigning/gradientnedstigning.qmd) -- men den konkrete metode er ikke så vigtig lige nu. Det vigtige er her at forstå, at hvis man skal finde minimum for en funktion, så har man brug for at kunne differentiere. 

Tabsfunktionen er en sammensat funktion. Den er sammensat af lineære funktioner, som vi kender rigtig godt fra tidligere, andengradspolynomier,  og en særlig klasse af funktioner, som kaldes for **aktiveringsfunktioner**. Og det giver nok mening, at hvis man skal differentiere selve tabsfunktionen, så må man også  kunne differentiere den anvendte aktiveringsfunktion $f$. 

Desuden viser det sig vigtigt, at det ikke må være alt for beregningsmæssigt tungt at beregne funktionsværdierne $f(x)$ og $f'(x)$. Det skal simpelthen gøres så mange gange -- derfor dur det ikke, at det tager for lang tid. Det er derfor ønskværdigt, hvis en aktiveringsfunktions afledede funktion $f'(x)$ kan beregnes forholdvis simpelt ved hjælp af $f(x)$. Det betyder nemlig, at hvis vi allerede har udregnet $f(x)$, så kræver det ikke ret meget også at udregne $f'(x)$. Det kan illustreres med et eksempel:

:::{#exm-aktiveringsfunktioner1}

Funktionen

$$
f(x)=e^{kx}
$$

har afledede funktion

$$
f'(x)=k \cdot e^{kx}
$$

Det vil sige, at vi kan udtrykke $f'(x)$ ved hjælp af $f(x)$ på denne måde:

$$
f'(x)= k \cdot f(x).
$$
Det betyder, at hvis man allerede har udregnet funktionsværdien $f(x_0)$, så kan man meget nemt udregne tangenthældningen $f'(x_0)$ i punktet $(x_0,f(x_0))$ ved at gange $f(x_0)$ med $k$.

:::

Men ikke alle funktioner har denne egenskab, hvilket det næste eksempel illustrerer:

:::{#exm-aktiveringsfunktioner2}

Grafen for funktionen

$$
f(x)=2x^3-3x^2-4x+5
$$
ses i @fig-eks2.

![Grafen for funktionen $f(x)=2x^3-3x^2-4x+5$.](aktiveringsfunktioner/eksempel2.png){width=75% #fig-eks2}

På grafen er der markeret tre punkter, hvor funktionsværdien er $2$. I disse tre punkter er tangenten til grafen også indtegnet (som stiplede linjer). Her ses det tydeligt, at disse tangenters hældninger *ikke* er ens. Det betyder derfor, at $f'(x)$ ikke kan beregnes simpelt alene ud fra funktionsværdien $f(x)$.

:::

Hvis du vil vide lidt mere om, hvad det der AI egentlig går ud på, så kan du se denne video:

{{< video https://youtu.be/ivrBEopralQ >}}


I det nedenstående vil vi nu behandle en række af de mest anvendte aktiveringsfunktioner. Vi finder deres afledede funktioner, og vi vil se, hvordan de afledede funktioner alle kan udtrykkes ved hjælp af den oprindelig aktiveringsfunktion.


## Sigmoid

{{< include aktiveringsfunktioner/sigmoid.qmd >}}


## Softsign

{{< include aktiveringsfunktioner/softsign.qmd >}}


## Hyperbolsk tangens

{{< include aktiveringsfunktioner/hyperbolsk_tangens.qmd >}}



## ReLU

{{< include aktiveringsfunktioner/ReLU.qmd >}}



For yderligere læsning henviser vi til referencerne i afsnittet [videre læsning](aktiveringsfunktioner.qmd#sec-videre).

## Overblik

I tabellen herunder finder du et overblik over de forskellige aktiveringsfunktioner, som vi har behandlet ovenfor.

| Navn | $f(x)$ | Graf | $Vm(f)$ | $f'(x)$ |
|:------:|:------:|:------:|:------:|:------:|:------:|
| Sigmoid | $\frac{1}{1+e^{-x}}$ | ![](aktiveringsfunktioner/sigmoid.png) | $(0,1)$ | $f(x)\cdot(1-f(x))$ |
| Softsign | $\frac{x}{1+|x|}$ | ![](aktiveringsfunktioner/softsign.png) | $(-1,1)$ | $(1-|f(x)|)^2$ | 
| Hyperbolsk tangens | $\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | ![](aktiveringsfunktioner/tanh.png) | $(-1,1)$ | $1-\left ( \tanh(x) \right )^2$| 
| ReLU | $\begin{cases}
x & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}$ | ![](aktiveringsfunktioner/ReLU.png) | $[0,\infty)$ | $\begin{cases}
1 & \textrm{hvis } x > 0 \\ 
0 & \textrm{hvis } x \leq 0 \\ 
\end{cases}$| 

## Videre læsning {#sec-videre}

* [Activation Functions in Neural Networks: With 15 examples](https://encord.com/blog/activation-functions-neural-networks/)
* [RELU and SIGMOID Activation Functions in a Neural Network](https://www.shiksha.com/online-courses/articles/relu-and-sigmoid-activation-function/)