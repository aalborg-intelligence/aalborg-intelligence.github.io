---
##title: "Materialer om kunstige neurale netværk"
description: "Forskellige noter -- med forskelligt fokus og sværhedsgrad -- om kunstige neurale netværk."
image: "images/materialer.png"
      
listing:
  id: main-listing-content_NN
  sort: false
  fields: 
    - image
    - title
    - description
  contents: 
    - path: materialer/perceptron/perceptron.qmd
    - path: materialer/kunstige_neuroner/kunstige_neuroner.qmd
    - path: materialer/simple_neurale_net/simple_neurale_net.qmd
    - path: materialer/softmax/softmax.qmd
    - path: materialer/neurale_net/neurale_net.qmd
  type: grid 
  grid-columns: 2
---

# Materialer om kunstige neurale netværk

På denne side finder du forskellige noter om kunstige neurale netværk. Noterne varierer i sværhedsgrad og i matematisk fokus. I afsnittet [overblik](#overblik) finder du et samlet overblik, så det er nemmere at orientere sig i de forskellige noter. 

Noterne er skrevet til elever i gymnasiet og kan læses uafhængigt af hinanden. [Overblikket](#overblik) er til gengæld nok nemmest at læse for lærerne. 

Sværhedsgraden af noterne er klassificeret fra \"forholdvis nem\" (\*) til \"svær\" (\*\*\*\*).

## Link til materialer

::: {#main-listing-content_NN}
:::


## Overblik

### Perceptroner

![](materialer/perceptron/images/perceptron.png){style='float:right;'  width=40%}

Noten beskriver først \"Perceptron Learning Algoritmen\", som kan bruges i forbindelse med binær klassifikation, men som kun konvergerer, hvis data er lineært separabel. Der gives ikke noget matematisk argument for dette, men det sandsynliggøres, hvorfor algoritmen virker. I behandlingen af  \"Perceptron Learning Algoritmen\" anvendes der ikke eksplicit en tabsfunktion.  

Dernæst beskrives \"ADALINE\", som også bruges til binær klassifikation, men som konvergerer -- også i det tilfælde, hvor data ikke er lineært separabel. Opdateringsreglerne til ADALINE algoritmen udledes ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. Den anvendte aktiveringsfunktion er her identiteten, hvilket gør denne udledning lettere end i de efterfølgende noter.

| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion | Sværhedsgrad |
|:---:|:---:|:---:|:---:|
| Binær | Ingen/identiteten | Ingen/*squared error* | \* | 

### Kunstige neuroner

![](materialer/kunstige_neuroner/images/simplet_netvaerk2.png){style='float:right;'  width=40%}

Noten behandler byggestenene til de mere generelle kunstige neurale netværk nemlig de *kunstige neuroner*. De kunstige neuroner bruges -- som de beskrives her -- til binær klassifikation. Opdateringsreglerne til justering af vægtene udledes ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. Til forskel fra ADALINE algoritmen anvendes her **sigmoid**-funktionen som aktiveringsfunktion, og der er også henvisning til en side, som beskriver hvilken fordel det har sammenlignet med ADALINE. 

| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion | Sværhedsgrad |
|:---:|:---:|:---:|:---:|
| Binær | Sigmoid | *Squared error* | \*\* | 


### Simple neurale netværk

![](materialer/simple_neurale_net/images/long_simple_NN.png){style='float:right;'  width=40%}

Denne noten behandler et egentlig kunstigt neuralt netværk dog med den væsentlige forsimpling, at der kun er to skjulte lag, hvor hver af disse lag kun består af én kunstige neuron. Det har den fordel, at det nu giver mening at tale om **feedforward** og **backpropagation**, men uden at sidstnævnte drukner i kædereglen af flere variable. Til gengæld bruges der tid på at forklare og anvende den kæderegel, som eleverne allerede kender. Også her udledes opdateringsreglerne ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. 

| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion | Sværhedsgrad |
|:---:|:---:|:---:|:---:|
| Binær | Sigmoid | *Squared error* | \*\*\* | 


### Softmax

![](materialer/softmax/images/netvaerk_generelt.png){style='float:right;'  width=40%}

I denne noten behandles det tilfælde, hvor man ikke er interesseret i binær klassifikation, men derimod er ønsket at kunne prædiktere blandt mere end to klasser. Noten er holdt simpel på den måde, at det beskrevne netværk ikke indeholder nogle skjulte lag, men outputlaget består i stedet af flere neuroner. Det får den konsekvens, at den anvendte aktiveringsfunktion nu er den såkaldte **softmax** funktion, som kan se som en udvidelse af sigmoid-funktionen. Derudover behandles også *cross-entropy* tabsfunktionen, som minimeres ved hjælp af gradientnedstigning. 

| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion | Sværhedsgrad |
|:---:|:---:|:---:|:---:|
| Multipel | Sigmoid | *Cross-entropy* | \*\*\*\* | 


### Kunstige neurale netværk

![](materialer/neurale_net/images/simple_NN_weights.png){style='float:right;'  width=40%}

Noten gennemgår et mere generelt kunstigt neuralt netværk med to skjulte lag, som hver består af to neuroner. Netværket bruges her til binær klassifikation, som aktiveringsfunktion anvendes igen sigmoid-funktionen og vægtene opdateres ved at minimere *squared error* tabsfunktionen ved hjælp af gradientnedstigning. 
Da de skjulte lag består af flere neuroner end én, må **kædereglen for funktioner af flere variable** bringes i spil for at finde de partielle afledede, som anvendes i forbindelse med gradientnedstigning. 

Desuden gives der i denne note også en forklaring på fordelene ved *cross-entropy* tabsfunktionen sammenlignet med *squared error* tabsfunktionen, der linkes til en side hvor opdateringsreglerne i et helt generelt kunstigt neuralt netværk forklares (ved hjælp af en masse indekser!) og endelig gives der en overordnet beskrivelse af *convolutional neural networks*, som bruges i forbindelse med prædiktion af billeder.


| Type af klassifikation | Aktiveringsfunktion | Tabsfunktion |  Sværhedsgrad |
|:---:|:---:|:---:|:---:|
| Primært binær | Sigmoid | *Squared error* | \*\*\*\* | 





